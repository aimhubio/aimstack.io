(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[195],{3301:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{ei:function(){return pick}});let pick=(obj,keys)=>keys.reduce((acc,key)=>(acc[key]=obj[key],acc),{})},3986:function(__unused_webpack_module,__unused_webpack_exports,__webpack_require__){(window.__NEXT_P=window.__NEXT_P||[]).push(["/blog",function(){return __webpack_require__(693)}])},4213:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{Z:function(){return BlogList_BlogList}});var jsx_runtime=__webpack_require__(5893);__webpack_require__(7294);var styles=__webpack_require__(5270);let BlogListStyle=(0,styles.zo)("ul",{display:"flex",flexWrap:"wrap",marginLeft:"-40px","@bp1":{marginLeft:"-54px"},"@bp3":{marginLeft:"0"}}),BlogItem=(0,styles.zo)("li",{width:"calc((100% / 3) - 40px)",marginLeft:"40px",marginBottom:"40px","@bp1":{width:"calc((100% / 2) - 54px)",marginLeft:"54px",marginBottom:"54px"},"@bp3":{width:"100%",marginLeft:"0",marginBottom:"48px"}});var Card_style=__webpack_require__(5349),foundations=__webpack_require__(9923),dist_image=__webpack_require__(8782),image_default=__webpack_require__.n(dist_image),UIkit=__webpack_require__(8561),next_link=__webpack_require__(1664),link_default=__webpack_require__.n(next_link);let Card=props=>(0,jsx_runtime.jsxs)(Card_style.UE,{children:[(0,jsx_runtime.jsx)(Card_style.fb,{children:(0,jsx_runtime.jsx)(link_default(),{href:"/blog/".concat(props.slug),children:(0,jsx_runtime.jsx)(image_default(),{src:props.image,alt:props.title,layout:"fill",objectFit:"cover",objectPosition:"top"})})}),(0,jsx_runtime.jsxs)(Card_style.aY,{children:[(0,jsx_runtime.jsx)(Card_style.WD,{children:(0,jsx_runtime.jsx)(link_default(),{href:"/category/".concat(props.categories[0].toLowerCase()),children:(0,jsx_runtime.jsxs)(foundations.xv,{size:1,css:{display:"flex",alignItems:"center"},children:[(0,jsx_runtime.jsx)(UIkit.JO,{name:"folder",size:14}),props.categories[0]]})})}),(0,jsx_runtime.jsx)(foundations.xv,{as:"h3",size:6,className:"title",truncate:!0,children:(0,jsx_runtime.jsx)(link_default(),{href:"/blog/".concat(props.slug),children:props.title})}),(0,jsx_runtime.jsx)(foundations.xv,{size:2,className:"title",lineClamp:!0,css:{my:"$6",$$lineClamp:3,fontFamily:"$Lora"},children:(0,jsx_runtime.jsx)(link_default(),{href:"/blog/".concat(props.slug),children:props.description})}),(0,jsx_runtime.jsx)(Card_style.eW,{children:props.views&&(0,jsx_runtime.jsx)(foundations.xv,{size:1,link:"secondary",children:(0,jsx_runtime.jsxs)(link_default(),{href:"/blog/".concat(props.slug),children:[(0,jsx_runtime.jsx)(UIkit.JO,{name:"eye",size:14}),props.views,"K"]})})})]})]}),BlogList=param=>{let{blogList}=param;return(0,jsx_runtime.jsx)(BlogListStyle,{children:blogList.map(blog=>(0,jsx_runtime.jsx)(BlogItem,{children:(0,jsx_runtime.jsx)(Card,{...blog},blog.slug)},blog.slug))})};var BlogList_BlogList=BlogList},5349:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{UE:function(){return CardStyle},WD:function(){return Category},aY:function(){return CardContent},eW:function(){return CardFooter},fb:function(){return ImageWrapper}});var styles__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(5270);let CardStyle=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{}),ImageWrapper=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{position:"relative",height:"0",paddingBottom:"65%"}),CardContent=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{paddingTop:"$6"}),Category=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{marginBottom:"$2",display:"flex",alignItems:"center",color:"$grey",transition:"$main",".icon":{marginRight:"$1",fill:"$grey",transition:"$main"},"&:hover":{color:"$darkGray",".icon":{fill:"$darkGray"}}}),CardFooter=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{display:"flex",alignItems:"center",justifyContent:"flex-end",".icon":{marginRight:"$1",fill:"$grey"}})},5065:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{Z:function(){return Pagnation}});var jsx_runtime=__webpack_require__(5893);__webpack_require__(7294);var next_link=__webpack_require__(1664),link_default=__webpack_require__.n(next_link),styles=__webpack_require__(5270);let PaginationStyle=(0,styles.zo)("nav",{my:"$10"}),PaginationList=(0,styles.zo)("ul",{display:"flex",alignItems:"center",justifyContent:"center",li:{height:"44px",width:"44px",fontSize:"$1",fontWeight:"$4",textAlign:"center",marginRight:"$1",a:{display:"block",lineHeight:"44px",transition:"$main","&.active":{backgroundColor:"$primary",color:"$white"},"&:hover":{"&:not(.active)":{color:"$primary"},".icon":{fill:"$primary"}}}}});var UIkit=__webpack_require__(8561);let Pagination=param=>{let{currentPage,totalPostCount,pathname}=param,pageIntoArray=Array.from(Array(totalPostCount).keys());return pageIntoArray.length<=1?null:(0,jsx_runtime.jsx)(PaginationStyle,{children:(0,jsx_runtime.jsxs)(PaginationList,{children:[currentPage>1&&(0,jsx_runtime.jsx)("li",{children:(0,jsx_runtime.jsx)(link_default(),{href:{pathname:"/".concat(pathname),query:{page:currentPage-1}},children:(0,jsx_runtime.jsx)(UIkit.JO,{name:"chevron-left"})})}),pageIntoArray.map(page=>(0,jsx_runtime.jsx)("li",{children:(0,jsx_runtime.jsx)(link_default(),{href:{pathname:"/".concat(pathname),query:{page:page+1}},className:currentPage===page+1?"active":"",children:page+1})},page)),currentPage<pageIntoArray.length&&(0,jsx_runtime.jsx)("li",{children:(0,jsx_runtime.jsx)(link_default(),{href:{pathname:"/".concat(pathname),query:{page:currentPage+1}},children:(0,jsx_runtime.jsx)(UIkit.JO,{name:"chevron-right"})})})]})})};var Pagnation=Pagination},233:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";var react_jsx_runtime__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(5893);__webpack_require__(7294);var next_seo__WEBPACK_IMPORTED_MODULE_2__=__webpack_require__(2962),config__WEBPACK_IMPORTED_MODULE_3__=__webpack_require__(3346);let Seo=props=>(0,react_jsx_runtime__WEBPACK_IMPORTED_MODULE_0__.jsx)(next_seo__WEBPACK_IMPORTED_MODULE_2__.PB,{title:props.title,description:props.description,openGraph:{url:"".concat(config__WEBPACK_IMPORTED_MODULE_3__.ZP).concat(props.path),title:props.title,description:props.description,type:props.type||"website",locale:"en_US",article:{publishedTime:props.date,authors:["".concat(config__WEBPACK_IMPORTED_MODULE_3__.ZP,"about-us")]},images:[{url:props.image,width:1224,height:724,alt:props.title,type:"image/jpeg"}],site_name:"Aimstack"},twitter:{cardType:"summary_large_image"}});__webpack_exports__.Z=Seo},3346:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{c7:function(){return GITHUB_API},ou:function(){return show_per_page}});let GITHUB_API="https://api.github.com/repos/aimhubio/aim/",show_per_page=9;__webpack_exports__.ZP="https://v4.aimstack.io/"},693:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.r(__webpack_exports__),__webpack_require__.d(__webpack_exports__,{default:function(){return Blog}});var jsx_runtime=__webpack_require__(5893),react=__webpack_require__(7294),foundations=__webpack_require__(9923),BlogList=__webpack_require__(4213),SEO=__webpack_require__(233),utils=__webpack_require__(558),dist=__webpack_require__(3301),_index_namespaceObject=JSON.parse('[{"title":"Aim 1.3.5 — Activity View and X-axis alignment","date":"2021-02-05T11:16:41.748Z","author":"Gev Soghomonian","description":"Aim v1.3.5 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open source MLOps tools. Check out the new features […]","slug":"aim-1-3-5-—-activity-view-and-x-axis-alignment","image":"/images/dynamic/1st.gif","draft":false,"categories":["New Releases"],"body":{"raw":"\\n\\n## Aim Release — Activity View and X-axis alignment\\n\\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is now available. Thanks to the incredible [Aim community](https://discord.com/invite/zXq2NfVdtF) for the feedback and support on building democratized open source AI dev tools.\\n\\nCheck out the new features at play.aimstack.io\\n\\nHere are the highlights of the features:\\n\\n## Activity View on the Dashboard\\n\\nThe **Activity View** shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away\\n\\n**Statistics** displays the overall count of [Experiments and Runs. ](https://github.com/aimhubio/aim#concepts)\\n\\n![](/images/dynamic/1st.gif \\"The new dashboard in action!\\")\\n\\n## **X-axis alignment by epoch**\\n\\nX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.\\n\\n![](/images/dynamic/2nd.gif \\"No more misaligned metrics!\\")\\n\\n## **Ordering runs both on Explore and on Dashboard**\\n\\n\\n\\nNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\\n\\nOn Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\\n\\n![](/images/dynamic/3.gif \\"Sort the runs both on Dashboard and Explore!\\")\\n\\n## Learn More\\n\\n\\n\\nWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\\n\\nWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month\xa0\uD83D\uDE0A). Try out Aim, join the\xa0[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support\xa0\uD83D\uDE4C.","html":"<h2>Aim Release — Activity View and X-axis alignment</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a> v1.3.5 is now available. Thanks to the incredible <a href=\\"https://discord.com/invite/zXq2NfVdtF\\">Aim community</a> for the feedback and support on building democratized open source AI dev tools.</p>\\n<p>Check out the new features at play.aimstack.io</p>\\n<p>Here are the highlights of the features:</p>\\n<h2>Activity View on the Dashboard</h2>\\n<p>The <strong>Activity View</strong> shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away</p>\\n<p><strong>Statistics</strong> displays the overall count of <a href=\\"https://github.com/aimhubio/aim#concepts\\">Experiments and Runs. </a></p>\\n<p><img src=\\"/images/dynamic/1st.gif\\" alt=\\"\\" title=\\"The new dashboard in action!\\"></p>\\n<h2><strong>X-axis alignment by epoch</strong></h2>\\n<p>X-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.</p>\\n<p><img src=\\"/images/dynamic/2nd.gif\\" alt=\\"\\" title=\\"No more misaligned metrics!\\"></p>\\n<h2><strong>Ordering runs both on Explore and on Dashboard</strong></h2>\\n<p>Now you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.</p>\\n<p>On Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.</p>\\n<p><img src=\\"/images/dynamic/3.gif\\" alt=\\"\\" title=\\"Sort the runs both on Dashboard and Explore!\\"></p>\\n<h2>Learn More</h2>\\n<p>We have been working on Aim for the past 6 months and excited to see this much interest from the community.</p>\\n<p>We are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month\xa0\uD83D\uDE0A). Try out Aim, join the\xa0<a href=\\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\\">Aim community</a>, share your feedback.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support\xa0\uD83D\uDE4C.</p>"},"_id":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","_raw":{"sourceFilePath":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileName":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-1-3-5-—-activity-view-and-x-axis-alignment"},"type":"Post"},{"title":"Aim 1.3.8 — Enhanced Context Table and Advanced Group Coloring","date":"2021-03-01T11:09:06.858Z","author":"Gev Soghomonian","description":"Aim 1.3.8 featuring Advanced Group Coloring for AI metrics  is now available. ","slug":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring","image":"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"\\n\\n[Aim](https://aimstack.io/)\xa01.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\\n\\nCheck out the new features at\xa0[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=).\\n\\nHere are the more notable changes:\\n\\n### Enhanced Context Table\\n\\n\\n\\nThe context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\\n\\nSo we have made 3 changes to tackle this problem\\n\\n### New table groups view\\n\\nBelow is the new modified look of the table. Here is what’s changed:\\n\\n* The empty per group is removed\\n* In addition to the group details popover, there is an in-place list group config is available for instant lookup\\n\\n![](https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp \\"New improved Aim Context Table on Explore\\")\\n\\n\\n\\n* **\\\\[In Progress for next release]**\xa0Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\\n\\n### Column Management\\n\\n\\n\\nThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\\n\\nYou can also re-order the columns by dragging the mid-section up/down.\\n\\nThis is super-handy when dealing with 100s of columns on the table for parameters.\\n\\n![](https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp \\"Drag columns left and right to pin, search and disable less important ones\\")\\n\\n\\n\\n### Advanced Group Coloring\\n\\n\\n\\nThis is the latest iteration over the group coloring.\\n\\n* Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\\n* You can shuffle the colors in case they aren’t pick to your liking\\n* You can pick two different color palette for better distribution of the colors.\\n\\n  ![](https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp \\"Effective coloring of the groups is important!\\")\\n\\n  ### Thanks to\\n* [mahnerak](https://github.com/mahnerak), Lars,\xa0[Joe](https://github.com/jafioti)\xa0for detailed feedback — helped us immensely in this iteration.\\n\\n  ### Learn More\\n\\n  We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\\n\\n  Try out\xa0[Aim](https://aimstack.io/), join the\xa0[Aim community](https://slack.aimstack.io/), share your feedback.\\n\\n  We are building the next generation of democratized AI dev tools.\\n\\n  And don’t forget to leave\xa0[Aim](https://aimstack.io/)\xa0a star on GitHub for support \uD83D\uDE4C.","html":"<p><a href=\\"https://aimstack.io/\\">Aim</a>\xa01.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools</p>\\n<p>Check out the new features at\xa0<a href=\\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=\\">play.aimstack.io</a>.</p>\\n<p>Here are the more notable changes:</p>\\n<h3>Enhanced Context Table</h3>\\n<p>The context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).</p>\\n<p>So we have made 3 changes to tackle this problem</p>\\n<h3>New table groups view</h3>\\n<p>Below is the new modified look of the table. Here is what’s changed:</p>\\n<ul>\\n<li>The empty per group is removed</li>\\n<li>In addition to the group details popover, there is an in-place list group config is available for instant lookup</li>\\n</ul>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp\\" alt=\\"\\" title=\\"New improved Aim Context Table on Explore\\"></p>\\n<ul>\\n<li><strong>[In Progress for next release]</strong>\xa0Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.</li>\\n</ul>\\n<h3>Column Management</h3>\\n<p>This feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.</p>\\n<p>You can also re-order the columns by dragging the mid-section up/down.</p>\\n<p>This is super-handy when dealing with 100s of columns on the table for parameters.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp\\" alt=\\"\\" title=\\"Drag columns left and right to pin, search and disable less important ones\\"></p>\\n<h3>Advanced Group Coloring</h3>\\n<p>This is the latest iteration over the group coloring.</p>\\n<ul>\\n<li>\\n<p>Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.</p>\\n</li>\\n<li>\\n<p>You can shuffle the colors in case they aren’t pick to your liking</p>\\n</li>\\n<li>\\n<p>You can pick two different color palette for better distribution of the colors.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp\\" alt=\\"\\" title=\\"Effective coloring of the groups is important!\\"></p>\\n<h3>Thanks to</h3>\\n</li>\\n<li>\\n<p><a href=\\"https://github.com/mahnerak\\">mahnerak</a>, Lars,\xa0<a href=\\"https://github.com/jafioti\\">Joe</a>\xa0for detailed feedback — helped us immensely in this iteration.</p>\\n<h3>Learn More</h3>\\n<p>We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.</p>\\n<p>Try out\xa0<a href=\\"https://aimstack.io/\\">Aim</a>, join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>, share your feedback.</p>\\n<p>We are building the next generation of democratized AI dev tools.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://aimstack.io/\\">Aim</a>\xa0a star on GitHub for support \uD83D\uDE4C.</p>\\n</li>\\n</ul>"},"_id":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","_raw":{"sourceFilePath":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileName":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring"},"type":"Post"},{"title":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","date":"2021-05-18T13:58:29.035Z","author":"Gev Soghomonian","description":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","slug":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)\xa02.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\\n\\nCheck out the updated Aim at\xa0[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\\n\\nFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes\xa0[here](https://github.com/aimhubio/aim/milestone/6?closed=1).\\n\\n## XGBoost Inegration\\n\\n\\n\\nTo begin with, \xa0`aim.callback`\xa0is now available that exports\xa0`AimCallback`\xa0to be passed to the\xa0`xgb.train`\xa0as a callback to log the experiments.\\n\\nCheck out this\xa0[blogpost](https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/)\xa0for additional details on how to integrate Aim to your XGBoost code.\\n\\n## Confidence Interval as the aggregation method\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/confidence_interval.png)\\n\\nNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.\xa0\\n\\n## Learn More\\n\\n\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nCheck out the latest\xa0[Aim integration](https://aimstack.io/aim-v2-2-0-hugging-face-integration/)!\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa02.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.</p>\\n<p>Check out the updated Aim at\xa0<a href=\\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\\">play.aimstack.io</a>.</p>\\n<p>For this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes\xa0<a href=\\"https://github.com/aimhubio/aim/milestone/6?closed=1\\">here</a>.</p>\\n<h2>XGBoost Inegration</h2>\\n<p>To begin with, \xa0<code>aim.callback</code>\xa0is now available that exports\xa0<code>AimCallback</code>\xa0to be passed to the\xa0<code>xgb.train</code>\xa0as a callback to log the experiments.</p>\\n<p>Check out this\xa0<a href=\\"https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/\\">blogpost</a>\xa0for additional details on how to integrate Aim to your XGBoost code.</p>\\n<h2>Confidence Interval as the aggregation method</h2>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/confidence_interval.png\\" alt=\\"\\"></p>\\n<p>Now you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.\xa0</p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Check out the latest\xa0<a href=\\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\\">Aim integration</a>!</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","_raw":{"sourceFilePath":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileName":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements"},"type":"Post"},{"title":"Aim 3.1 — Images Tracker and Images Explorer","date":"2021-11-25T14:57:05.232Z","author":"Gev Soghomonian","description":"Excited to share with you the Aim 3.1! \uD83D\uDE0A A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community","slug":"aim-3-1-—-images-tracker-and-images-explorer","image":"https://aimstack.io/wp-content/uploads/2021/08/aim-scaled.jpeg","draft":false,"categories":["New Releases"],"body":{"raw":"Excited to share with you the\xa0Aim 3.1!\xa0\\n\\nA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\\n\\nWe have added a new demo to showcase the new Explore Images feature. We have also forked the\xa0[lightweight-gan from lucidrains](https://github.com/lucidrains/lightweight-gan)\xa0and added the Aim integration.\\n\\nDemo code:\xa0<https://github.com/aimhubio/lightweight-gan>\\\\\\nDemo site:\xa0[play.aimstack.io:10002](http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq&select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS&images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK)\\n\\n**The notable changes in the Aim 3.1.0:**\\n\\n* Images tracker and explorer\\n* Runs navigation from the single run page\\n* Performance improvements and tests\\n\\nThanks to srikanth,\xa0Mohamad\xa0Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\\n\\n## Images Tracker and Explorer\\n\\nFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\\n\\nAll the regular grouping and search goodies are available on Images Explorer too\xa0\\n\\nAdd an image tracker in your training code like this:\\n\\n```\\nrun.track(Image(tensor_or_pil, caption), name=\'gen\', step=step, context={ \\"subset\\": \\"train\\" })\\n```\\n\\nOnce you run your training, the images will be saved and become available on the Images Explorer tab.\\n\\nOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\\n\\nHere is a quick promo video of what it can do. The rest of the details can be found in the docs[\xa0here](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html).\\n\\n![](https://aimstack.io/wp-content/uploads/2021/08/1_3IkmtAOLGy-e3eJneuv2bA.png)\\n\\nA quick demo video.\\n\\n<https://www.youtube.com/watch?v=4mtUFV8yG_o&feature=youtu.be>\\n\\nThe Images view on the single run page is coming soon …\\n\\n## Runs Navigation\\n\\nOne of the regressions from the Aim 2.x to\xa0[3.0.0](https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/)\xa0was the ability to navigate between the runs of the same experiment from the single run page.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/08/111.png)\\n\\nWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\\n\\nSorry for the regression\xa0\\n\\n## Performance improvements and assurances\\n\\nTwo groups of performance improvements have been made— on the UI and on the SDK side.\\n\\n### **Column virtualization on Aim UI Table**\\n\\nThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\\n\\nWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem — and this is a key step towards it.\xa0\\n\\n### **Storage performance tests**\\n\\nOne of the unique challenges in building Aim is to make sure each newly added data type won’t impact the overall performance. We have added continuously add more performance tests to ensure the new releases don’t impact the performance.\\n\\nIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\\n\\nPerformance reliability is key for Aim and this is a major iteration towards that!\\n\\n## Learn more\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<p>Excited to share with you the\xa0Aim 3.1!\xa0</p>\\n<p>A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.</p>\\n<p>We have added a new demo to showcase the new Explore Images feature. We have also forked the\xa0<a href=\\"https://github.com/lucidrains/lightweight-gan\\">lightweight-gan from lucidrains</a>\xa0and added the Aim integration.</p>\\n<p>Demo code:\xa0<a href=\\"https://github.com/aimhubio/lightweight-gan\\">https://github.com/aimhubio/lightweight-gan</a><br>\\nDemo site:\xa0<a href=\\"http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq&#x26;select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS&#x26;images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK\\">play.aimstack.io:10002</a></p>\\n<p><strong>The notable changes in the Aim 3.1.0:</strong></p>\\n<ul>\\n<li>Images tracker and explorer</li>\\n<li>Runs navigation from the single run page</li>\\n<li>Performance improvements and tests</li>\\n</ul>\\n<p>Thanks to srikanth,\xa0Mohamad\xa0Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.</p>\\n<h2>Images Tracker and Explorer</h2>\\n<p>Finally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.</p>\\n<p>All the regular grouping and search goodies are available on Images Explorer too\xa0</p>\\n<p>Add an image tracker in your training code like this:</p>\\n<pre><code>run.track(Image(tensor_or_pil, caption), name=\'gen\', step=step, context={ \\"subset\\": \\"train\\" })\\n</code></pre>\\n<p>Once you run your training, the images will be saved and become available on the Images Explorer tab.</p>\\n<p>On Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.</p>\\n<p>Here is a quick promo video of what it can do. The rest of the details can be found in the docs<a href=\\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\\">\xa0here</a>.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/08/1_3IkmtAOLGy-e3eJneuv2bA.png\\" alt=\\"\\"></p>\\n<p>A quick demo video.</p>\\n<p><a href=\\"https://www.youtube.com/watch?v=4mtUFV8yG_o&#x26;feature=youtu.be\\">https://www.youtube.com/watch?v=4mtUFV8yG_o&#x26;feature=youtu.be</a></p>\\n<p>The Images view on the single run page is coming soon …</p>\\n<h2>Runs Navigation</h2>\\n<p>One of the regressions from the Aim 2.x to\xa0<a href=\\"https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/\\">3.0.0</a>\xa0was the ability to navigate between the runs of the same experiment from the single run page.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/08/111.png\\" alt=\\"\\"></p>\\n<p>We have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.</p>\\n<p>Sorry for the regression\xa0</p>\\n<h2>Performance improvements and assurances</h2>\\n<p>Two groups of performance improvements have been made— on the UI and on the SDK side.</p>\\n<h3><strong>Column virtualization on Aim UI Table</strong></h3>\\n<p>The ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.</p>\\n<p>We are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem — and this is a key step towards it.\xa0</p>\\n<h3><strong>Storage performance tests</strong></h3>\\n<p>One of the unique challenges in building Aim is to make sure each newly added data type won’t impact the overall performance. We have added continuously add more performance tests to ensure the new releases don’t impact the performance.</p>\\n<p>In the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.</p>\\n<p>Performance reliability is key for Aim and this is a major iteration towards that!</p>\\n<h2>Learn more</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-3-1-—-images-tracker-and-images-explorer.md","_raw":{"sourceFilePath":"aim-3-1-—-images-tracker-and-images-explorer.md","sourceFileName":"aim-3-1-—-images-tracker-and-images-explorer.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-3-1-—-images-tracker-and-images-explorer"},"type":"Post"},{"title":"Aim 3.2 — Jupyter Notebook integration & Histograms","date":"2021-12-08T15:09:19.264Z","author":"Gev Soghomonian","description":"Aim 3.2 featuring Jupyter notebook integration is now available! \uD83D\uDE0A We are on a mission to democratize AI dev tools. ","slug":"aim-3-2-—-jupyter-notebook-integration-histograms","image":"https://aimstack.io/wp-content/uploads/2021/08/3.2.0.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.2 featuring Jupyter notebook integration is now available!\xa0\\n\\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\\n\\nWithin a week after the\xa0[v3.1](https://aimstack.io/aim3-1-images-tracker-and-images-explorer/)\xa0we are releasing the v3.2 with a number of key highly requested features in:\\n\\n* Aim Jupyter Notebook extension (Colab is coming soon…)\\n* Histogram tracking and visualization\\n* Full-size images, images resize and render modes\\n* Search autofill and suggest on the UI\\n\\n## Aim & Jupyter notebook\\n\\n\\n\\nNow you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with the\xa0`aim`\xa0package.\\n\\nYou don’t need to install anything other than Aim. Here are the\xa0[docs](https://aimstack.readthedocs.io/en/latest/index.html).\\\\\\nA quick demo of integrating Aim & Jupyter Notebook::\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1-3.gif \\"It’s that easy to use Aim on your Jupyter notebook!\\")\\n\\n## Histogram tracking and Visualization\\n\\nStarting from 3.2 you can track Distributions and visualize them as histograms on the run page.\\n\\nThis is how to track the weights distribution in your training code:\\n\\n```\\nfrom aim import Distribution\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Distribution(tensor), # Pass distribution\\n        name=\'gradients\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'type\': \'weights\',\\n        },\\n    )\\n```\\n\\nThe\xa0[demo](http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51),\xa0[docs](https://aimstack.readthedocs.io/en/latest/index.html)\xa0and\xa0[examples](https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py)\xa0\\n\\nAnd this is how it looks on the UI.\xa0Aim lets you visualize the distributions every layer at every tracked step.\xa0Super powerful!\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/0.gif \\"Visualize every layer and every tracked step.\\")\\n\\n## Full-size images, images resize and render modes\\n\\n\\n\\nNow you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.\\n\\nHere is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/2.gif \\"Easily resize the queried images\\")\\n\\nWe have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/3.gif \\"Toggle between pixelated and smooth images render modes\\")\\n\\nYou can also view full-size images by clicking on the top right corner magnifier on each images.\\n\\nBTW\xa0the smooth / pixelated config also translates to the full-size images.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/4.gif \\"full-size images: easy!\\")\\n\\n## Query Language autosuggest editor\\n\\nAnother highly requested feature.\\n\\nNow there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.\\n\\nHopefully\xa0this makes it much easier to get started with Aim.Here is a quick demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/5.gif)\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave Aim a star on GitHub for support.","html":"<p>Aim 3.2 featuring Jupyter notebook integration is now available!\xa0</p>\\n<p>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.</p>\\n<p>Within a week after the\xa0<a href=\\"https://aimstack.io/aim3-1-images-tracker-and-images-explorer/\\">v3.1</a>\xa0we are releasing the v3.2 with a number of key highly requested features in:</p>\\n<ul>\\n<li>Aim Jupyter Notebook extension (Colab is coming soon…)</li>\\n<li>Histogram tracking and visualization</li>\\n<li>Full-size images, images resize and render modes</li>\\n<li>Search autofill and suggest on the UI</li>\\n</ul>\\n<h2>Aim &#x26; Jupyter notebook</h2>\\n<p>Now you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with the\xa0<code>aim</code>\xa0package.</p>\\n<p>You don’t need to install anything other than Aim. Here are the\xa0<a href=\\"https://aimstack.readthedocs.io/en/latest/index.html\\">docs</a>.<br>\\nA quick demo of integrating Aim &#x26; Jupyter Notebook::</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1-3.gif\\" alt=\\"\\" title=\\"It’s that easy to use Aim on your Jupyter notebook!\\"></p>\\n<h2>Histogram tracking and Visualization</h2>\\n<p>Starting from 3.2 you can track Distributions and visualize them as histograms on the run page.</p>\\n<p>This is how to track the weights distribution in your training code:</p>\\n<pre><code>from aim import Distribution\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Distribution(tensor), # Pass distribution\\n        name=\'gradients\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'type\': \'weights\',\\n        },\\n    )\\n</code></pre>\\n<p>The\xa0<a href=\\"http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51\\">demo</a>,\xa0<a href=\\"https://aimstack.readthedocs.io/en/latest/index.html\\">docs</a>\xa0and\xa0<a href=\\"https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py\\">examples</a>\xa0</p>\\n<p>And this is how it looks on the UI.\xa0Aim lets you visualize the distributions every layer at every tracked step.\xa0Super powerful!</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/0.gif\\" alt=\\"\\" title=\\"Visualize every layer and every tracked step.\\"></p>\\n<h2>Full-size images, images resize and render modes</h2>\\n<p>Now you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.</p>\\n<p>Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/2.gif\\" alt=\\"\\" title=\\"Easily resize the queried images\\"></p>\\n<p>We have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/3.gif\\" alt=\\"\\" title=\\"Toggle between pixelated and smooth images render modes\\"></p>\\n<p>You can also view full-size images by clicking on the top right corner magnifier on each images.</p>\\n<p>BTW\xa0the smooth / pixelated config also translates to the full-size images.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/4.gif\\" alt=\\"\\" title=\\"full-size images: easy!\\"></p>\\n<h2>Query Language autosuggest editor</h2>\\n<p>Another highly requested feature.</p>\\n<p>Now there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.</p>\\n<p>Hopefully\xa0this makes it much easier to get started with Aim.Here is a quick demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/5.gif\\" alt=\\"\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://aimstack.readthedocs.io/en/latest/overview.html\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave Aim a star on GitHub for support.</p>"},"_id":"aim-3-2-—-jupyter-notebook-integration-histograms.md","_raw":{"sourceFilePath":"aim-3-2-—-jupyter-notebook-integration-histograms.md","sourceFileName":"aim-3-2-—-jupyter-notebook-integration-histograms.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-3-2-—-jupyter-notebook-integration-histograms"},"type":"Post"},{"title":"Aim 3.3 — Audio & Text tracking, Plotly & Colab integrations","date":"2022-01-13T15:33:59.865Z","author":"Gev Soghomonian","description":"Hey community, Aim 3.3 is now available! \uD83D\uDE0A We are on a mission to democratize AI dev tools.  ","slug":"aim-3-3-—-audio-text-tracking-plotly-colab-integrations","image":"https://aimstack.io/wp-content/uploads/2021/08/aim-3.3.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, Aim 3.3 is now available! \uD83D\uDE0A\\n\\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.\\n\\nAim 3.3 is full of highly requested features. Lots of items scratched from the public Aim\xa0[roadmap](https://github.com/aimhubio/aim#roadmap).\\n\\n* Audio tracking and exploration\\n* Text tracking and visualization\\n* Scatter plot explorer\\n* Colab integration\\n* Plotly integration\\n* Images visualization on run details page\\n\\nNew Demo:\xa0[play.aimstack.io:10004](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a)\\n\\nDemo Code:\xa0[github.com/osoblanco/FastSpeech2](https://github.com/osoblanco/FastSpeech2/blob/master/train.py)\\n\\n\\n\\nSpecial thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.\\n\\n## Audio tracking and exploration\\n\\n\\n\\nNow you can track audio files with Aim during your speech-to-text or other audio-involving experiments.\\n\\nIt will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.\\n\\nJust like tracking the metrics, we have enabled a simple API to track the audio.\\n\\n```\\nfrom aim import Audio\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Audio(arr), # Pass audio file or numpy array\\n        name=\'outputs\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'subset\': \'train\',\\n        },\\n    )\\n\\n```\\n\\nHere is how it looks like on the UI.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1_LkeFigHPnQM1drADXJ3yZQ-1.gif)\\n\\n## **Text tracking and visualization**\\n\\nUse this to compare text inputs and outputs during training\\n\\nSimilarly, you can also track text with Aim during your NLP experiments.\\n\\nHere is how the code looks like:\\n\\n```\\nfrom aim import Text\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Text(string), # Pass a string you want to track\\n        name=\'outputs\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'subset\': \'train\',\\n        },\\n    )\\n```\\n\\nThis is the end result on the UI.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1_nkqUz9DecxaMmJoCUe_8Rw.gif \\"Training info tracked as a text\\")\\n\\n## Colab integration\\n\\nAfter we have integrated\xa0[Aim to Jupyter](https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms/), there were many requests to enable Aim on Colab too. Now it has arrived!\xa0\\n\\nWith fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!\\n\\nHere is an example\xa0[colab to get started with](https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing).\\n\\n> Note: Please make sure to run all the cells to be able to use the UI as well\\n\\nSo this is how it looks on your browser:\\n\\n![](https://aimstack.io/wp-content/uploads/2021/08/Run-Aim.gif)\\n\\n## Plotly integration\\n\\n\\n\\nNow you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.\\n\\nThis is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.\\n\\n```\\nfrom aim import Figure\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Figure(fig_obj), # Pass any plotly figure\\n        name=\'plotly_bars\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'subset\': \'train\',\\n        },\\n    )\\n```\\n\\nThe end-result on the Aim Web UI.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1_8xn3PH_Hzk6fotyZQqy-AQ.gif)\\n\\n## Images visualization on run details page\\n\\nAs we had launched the images tracking and visualization in[\xa03.1](https://aimstack.io/aim3-1-images-tracker-and-images-explorer/), we haven’t enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.\\n\\nHere is how it looks on the UI\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1_jnUogPgbzwX6GOmDaXHKMg.gif)\\n\\n## Learn More\\n\\n\\n\\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features and bugs.\\n\\nAnd don’t forget to leave Aim a star on GitHub for support.","html":"<p>Hey community, Aim 3.3 is now available! \uD83D\uDE0A</p>\\n<p>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.</p>\\n<p>Aim 3.3 is full of highly requested features. Lots of items scratched from the public Aim\xa0<a href=\\"https://github.com/aimhubio/aim#roadmap\\">roadmap</a>.</p>\\n<ul>\\n<li>Audio tracking and exploration</li>\\n<li>Text tracking and visualization</li>\\n<li>Scatter plot explorer</li>\\n<li>Colab integration</li>\\n<li>Plotly integration</li>\\n<li>Images visualization on run details page</li>\\n</ul>\\n<p>New Demo:\xa0<a href=\\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a\\">play.aimstack.io:10004</a></p>\\n<p>Demo Code:\xa0<a href=\\"https://github.com/osoblanco/FastSpeech2/blob/master/train.py\\">github.com/osoblanco/FastSpeech2</a></p>\\n<p>Special thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.</p>\\n<h2>Audio tracking and exploration</h2>\\n<p>Now you can track audio files with Aim during your speech-to-text or other audio-involving experiments.</p>\\n<p>It will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.</p>\\n<p>Just like tracking the metrics, we have enabled a simple API to track the audio.</p>\\n<pre><code>from aim import Audio\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Audio(arr), # Pass audio file or numpy array\\n        name=\'outputs\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'subset\': \'train\',\\n        },\\n    )\\n\\n</code></pre>\\n<p>Here is how it looks like on the UI.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1_LkeFigHPnQM1drADXJ3yZQ-1.gif\\" alt=\\"\\"></p>\\n<h2><strong>Text tracking and visualization</strong></h2>\\n<p>Use this to compare text inputs and outputs during training</p>\\n<p>Similarly, you can also track text with Aim during your NLP experiments.</p>\\n<p>Here is how the code looks like:</p>\\n<pre><code>from aim import Text\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Text(string), # Pass a string you want to track\\n        name=\'outputs\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'subset\': \'train\',\\n        },\\n    )\\n</code></pre>\\n<p>This is the end result on the UI.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1_nkqUz9DecxaMmJoCUe_8Rw.gif\\" alt=\\"\\" title=\\"Training info tracked as a text\\"></p>\\n<h2>Colab integration</h2>\\n<p>After we have integrated\xa0<a href=\\"https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms/\\">Aim to Jupyter</a>, there were many requests to enable Aim on Colab too. Now it has arrived!\xa0</p>\\n<p>With fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!</p>\\n<p>Here is an example\xa0<a href=\\"https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing\\">colab to get started with</a>.</p>\\n<blockquote>\\n<p>Note: Please make sure to run all the cells to be able to use the UI as well</p>\\n</blockquote>\\n<p>So this is how it looks on your browser:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/08/Run-Aim.gif\\" alt=\\"\\"></p>\\n<h2>Plotly integration</h2>\\n<p>Now you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.</p>\\n<p>This is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.</p>\\n<pre><code>from aim import Figure\\n\\nfor step in range(1000):\\n    my_run.track(\\n        Figure(fig_obj), # Pass any plotly figure\\n        name=\'plotly_bars\', # The name of distributions\\n        step=step,   # Step index (optional)\\n        epoch=0,     # Epoch (optional)\\n        context={    # Context (optional)\\n            \'subset\': \'train\',\\n        },\\n    )\\n</code></pre>\\n<p>The end-result on the Aim Web UI.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1_8xn3PH_Hzk6fotyZQqy-AQ.gif\\" alt=\\"\\"></p>\\n<h2>Images visualization on run details page</h2>\\n<p>As we had launched the images tracking and visualization in<a href=\\"https://aimstack.io/aim3-1-images-tracker-and-images-explorer/\\">\xa03.1</a>, we haven’t enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.</p>\\n<p>Here is how it looks on the UI</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1_jnUogPgbzwX6GOmDaXHKMg.gif\\" alt=\\"\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://aimstack.readthedocs.io/en/latest/overview.html\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features and bugs.</p>\\n<p>And don’t forget to leave Aim a star on GitHub for support.</p>"},"_id":"aim-3-3-—-audio-text-tracking-plotly-colab-integrations.md","_raw":{"sourceFilePath":"aim-3-3-—-audio-text-tracking-plotly-colab-integrations.md","sourceFileName":"aim-3-3-—-audio-text-tracking-plotly-colab-integrations.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-3-3-—-audio-text-tracking-plotly-colab-integrations"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\\n\\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\\n\\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\\n\\nModels are usually ran on validation subset after each epoch.\\\\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\\n\\nThere is a need to collect and effectively compare all these metrics.\\n\\nHere is how to do that on\xa0[Aim](https://github.com/aimhubio/aim)\\n\\n# Using context to track for different subsets?\\n\\nUse the\xa0[aim.track](https://github.com/aimhubio/aim#track)\xa0context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\\n\\n![](<script src=\\"https://gist.github.com/SGevorg/e08524b3538d3f71d14bf1857a7bc6e9.js\\"></script> \\"Here is how it looks like on the code\\")\\n\\nOnce the training is ran, execute\xa0`aim up`\xa0in your terminal and start the Aim UI.\\n\\n# Using subplots to compare test, val loss and bleu metrics\\n\\n> **\\\\*Note:**\xa0The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\\n\\nLet’s go step-by-step on how to break down lots of experiments using subplots.\\n\\n**Step 1.**\xa0Explore the runs, the context table, play with the query language.\\n\\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \\"Explore the training runs\\")\\n\\n**Step 2.**\xa0Add the\xa0`bleu`\xa0metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\\n\\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \\"Divide into subplots by metric\\")\\n\\n**Step 3.**\xa0Search by\xa0`context.subset`\xa0to show both\xa0`test`\xa0and\xa0`val`\xa0`loss`\xa0and\xa0`bleu`\xa0metrics. Divide into subplots further by\xa0`context.subset`\xa0too so Aim UI shows\xa0`test`\xa0and\xa0`val`\xa0metrics on different subplots for better comparison.\\n\\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \\"Divide into subplots by context / subset\\")\\n\\nNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\\n\\n# Summary\\n\\nHere is a full summary video on how to do it on the UI.\\n\\n![](https://www.youtube.com/watch?v=DGI8S7SUfEk)\\n\\n# Learn More\\n\\nIf you find Aim useful, support us and\xa0[star the project](https://github.com/aimhubio/aim)\xa0on GitHub. Join the\xa0[Aim community](https://slack.aimstack.io/)\xa0and share more about your use-cases and how we can improve Aim to suit them.","html":"<p>Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.</p>\\n<p>The model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.</p>\\n<p>The validation and test sets are used to test the model on additional unseen data to verify how well it generalise.</p>\\n<p>Models are usually ran on validation subset after each epoch.<br>\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.</p>\\n<p>There is a need to collect and effectively compare all these metrics.</p>\\n<p>Here is how to do that on\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a></p>\\n<h1>Using context to track for different subsets?</h1>\\n<p>Use the\xa0<a href=\\"https://github.com/aimhubio/aim#track\\">aim.track</a>\xa0context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.</p>\\n<p>![]( \\"Here is how it looks like on the code\\")</p>\\n<p>Once the training is ran, execute\xa0<code>aim up</code>\xa0in your terminal and start the Aim UI.</p>\\n<h1>Using subplots to compare test, val loss and bleu metrics</h1>\\n<blockquote>\\n<p><strong>*Note:</strong>\xa0The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*</p>\\n</blockquote>\\n<p>Let’s go step-by-step on how to break down lots of experiments using subplots.</p>\\n<p><strong>Step 1.</strong>\xa0Explore the runs, the context table, play with the query language.</p>\\n<p><img src=\\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\\" alt=\\"\\" title=\\"Explore the training runs\\"></p>\\n<p><strong>Step 2.</strong>\xa0Add the\xa0<code>bleu</code>\xa0metric to the Select input — query both metrics at the same time. Divide into subplots by metric.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\\" alt=\\"\\" title=\\"Divide into subplots by metric\\"></p>\\n<p><strong>Step 3.</strong>\xa0Search by\xa0<code>context.subset</code>\xa0to show both\xa0<code>test</code>\xa0and\xa0<code>val</code>\xa0<code>loss</code>\xa0and\xa0<code>bleu</code>\xa0metrics. Divide into subplots further by\xa0<code>context.subset</code>\xa0too so Aim UI shows\xa0<code>test</code>\xa0and\xa0<code>val</code>\xa0metrics on different subplots for better comparison.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\\" alt=\\"\\" title=\\"Divide into subplots by context / subset\\"></p>\\n<p>Not it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.</p>\\n<h1>Summary</h1>\\n<p>Here is a full summary video on how to do it on the UI.</p>\\n<p><img src=\\"https://www.youtube.com/watch?v=DGI8S7SUfEk\\" alt=\\"\\"></p>\\n<h1>Learn More</h1>\\n<p>If you find Aim useful, support us and\xa0<a href=\\"https://github.com/aimhubio/aim\\">star the project</a>\xa0on GitHub. Join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>\xa0and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim v2.2.0 — Hugging Face integration","date":"2021-03-24T13:20:24.937Z","author":"Gev Soghomonian","description":"Aim 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.","slug":"aim-v2-2-0-—-hugging-face-integration","image":"https://aimstack.io/wp-content/uploads/2022/02/HF.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)\xa02.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\\n\\nThanks to\xa0[siddk](https://github.com/siddk),\xa0[TommasoBendinelli](https://github.com/TommasoBendinelli)\xa0and\xa0[Khazhak](https://github.com/Khazhak)\xa0for their contribution to this release.\\n\\n> **Note on the Aim versioning:**\xa0The previous two release posts:\xa0[Aim 1.3.5](https://aimstack.io/blog/new-releases/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)\xa0and\xa0[Aim 1.3.8](https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring/)\xa0had used the version number of\xa0[AimUI](https://aimstack.readthedocs.io/en/latest/ui/overview.html)\xa0as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.\xa0[Check out the Aim CHANGELOG](https://github.com/aimhubio/aim/blob/main/CHANGELOG.md).\\n\\nWe have also added\xa0[milestones](https://github.com/aimhubio/aim/milestones)\xa0for each version. As well as the\xa0[Roadmap](https://github.com/aimhubio/aim#roadmap).\\n\\nCheck out the new features at\xa0[play.aimstack.io](http://play.aimstack.io:43900/dashboard).\\n\\n## Hugging Face integration\\n\\nHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\\n\\n```\\nfrom aim.hugging_face import AimCallback\\nfrom transformers import Trainer\\n\\naim_callback = AimCallback(repo=\'/log/dir/path\', experiment=\'your_experiment_name\')\\n\\ntrainer = Trainer(\\n       model=model,\\n       args=training_args,\\n       callbacks=[aim_callback]\\n    )\\n```\\n\\nHere is how it works:\xa0`aim_callback`\xa0will automatically open an Aim\xa0`Session`\xa0and close it when the trainer is done. When\xa0`trainer.train()`,\xa0`trainer.evaluate()`\xa0or\xa0`trainer.predict()`\xa0is called,\xa0`aim_callback`\xa0will automatically log the hyper-params and respective metrics.\\n\\nFind a full example\xa0[here](https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py).\\n\\n## Metric Visibility Control\\n\\nWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\\n\\nA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo.gif \\"Hide individual metrics as well as collectively\\")\\n\\n## Column resize\\n\\n\\n\\nWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.\xa0Resize is available both on Explore and Dashboard tables.Here is a quick demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo2.gif \\"Drag column edges back-and-forth to resize\\")\\n\\n## Hide columns with similar values\\n\\n\\n\\nMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\\n\\nThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo3.gif \\"Leave only different columns with a button click, if needed customize afterwards\\")\\n\\n## Logscale\\n\\n\\n\\nOne of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo4.gif \\"log-scale on Aim\\")\\n\\n## New methods for aggregation\\n\\n\\n\\nNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo5.gif)\\n\\n## Learn More\\n\\n[Aim is on the mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://slack.aimstack.io/), share your feedback.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support\xa0\uD83D\uDE4C.","html":"<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa02.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.</p>\\n<p>Thanks to\xa0<a href=\\"https://github.com/siddk\\">siddk</a>,\xa0<a href=\\"https://github.com/TommasoBendinelli\\">TommasoBendinelli</a>\xa0and\xa0<a href=\\"https://github.com/Khazhak\\">Khazhak</a>\xa0for their contribution to this release.</p>\\n<blockquote>\\n<p><strong>Note on the Aim versioning:</strong>\xa0The previous two release posts:\xa0<a href=\\"https://aimstack.io/blog/new-releases/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\\">Aim 1.3.5</a>\xa0and\xa0<a href=\\"https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring/\\">Aim 1.3.8</a>\xa0had used the version number of\xa0<a href=\\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\\">AimUI</a>\xa0as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.\xa0<a href=\\"https://github.com/aimhubio/aim/blob/main/CHANGELOG.md\\">Check out the Aim CHANGELOG</a>.</p>\\n</blockquote>\\n<p>We have also added\xa0<a href=\\"https://github.com/aimhubio/aim/milestones\\">milestones</a>\xa0for each version. As well as the\xa0<a href=\\"https://github.com/aimhubio/aim#roadmap\\">Roadmap</a>.</p>\\n<p>Check out the new features at\xa0<a href=\\"http://play.aimstack.io:43900/dashboard\\">play.aimstack.io</a>.</p>\\n<h2>Hugging Face integration</h2>\\n<p>Hugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.</p>\\n<pre><code>from aim.hugging_face import AimCallback\\nfrom transformers import Trainer\\n\\naim_callback = AimCallback(repo=\'/log/dir/path\', experiment=\'your_experiment_name\')\\n\\ntrainer = Trainer(\\n       model=model,\\n       args=training_args,\\n       callbacks=[aim_callback]\\n    )\\n</code></pre>\\n<p>Here is how it works:\xa0<code>aim_callback</code>\xa0will automatically open an Aim\xa0<code>Session</code>\xa0and close it when the trainer is done. When\xa0<code>trainer.train()</code>,\xa0<code>trainer.evaluate()</code>\xa0or\xa0<code>trainer.predict()</code>\xa0is called,\xa0<code>aim_callback</code>\xa0will automatically log the hyper-params and respective metrics.</p>\\n<p>Find a full example\xa0<a href=\\"https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py\\">here</a>.</p>\\n<h2>Metric Visibility Control</h2>\\n<p>When dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.</p>\\n<p>A toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo.gif\\" alt=\\"\\" title=\\"Hide individual metrics as well as collectively\\"></p>\\n<h2>Column resize</h2>\\n<p>With lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.\xa0Resize is available both on Explore and Dashboard tables.Here is a quick demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo2.gif\\" alt=\\"\\" title=\\"Drag column edges back-and-forth to resize\\"></p>\\n<h2>Hide columns with similar values</h2>\\n<p>More often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.</p>\\n<p>This feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo3.gif\\" alt=\\"\\" title=\\"Leave only different columns with a button click, if needed customize afterwards\\"></p>\\n<h2>Logscale</h2>\\n<p>One of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo4.gif\\" alt=\\"\\" title=\\"log-scale on Aim\\"></p>\\n<h2>New methods for aggregation</h2>\\n<p>Now you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo5.gif\\" alt=\\"\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on the mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>, share your feedback.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support\xa0\uD83D\uDE4C.</p>"},"_id":"aim-v2-2-0-—-hugging-face-integration.md","_raw":{"sourceFilePath":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileName":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-2-0-—-hugging-face-integration"},"type":"Post"},{"title":"Aim v2.3.0 — System Resource Usage and Reverse Grouping","date":"2021-04-20T13:39:39.516Z","author":"Gev Soghomonian","description":"Aim 2.3.0 allowing System Resource Usage optimization is out! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out...","slug":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping","image":"https://aimstack.io/wp-content/uploads/2021/04/2.3.0-1.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)\xa02.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\\n\\nCheck out the updated Aim at\xa0[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\\n\\nBelow are the highlight features of the update. Find the full list of changes\xa0[here](https://github.com/aimhubio/aim/milestone/3?closed=1).\\n\\n## System Resource Usage\\n\\n\\n\\nNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with\xa0`system_tracking_interval==0`.\\n\\nOnce you run the training, you will see the following. Here is a demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo-1.gif \\"Aim automatic system resource tracking demo\\")\\n\\nOf course you can also\xa0[query all those new metrics](https://aimstack.io/wp-admin/post.php?post=706&action=edit)\xa0in the Explore page and compare, group, aggregate with the rest of the metrics!\\n\\n## Reverse grouping (“against” the param)\\n\\n\\n\\nAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a\xa0Reverse Grouping. Here is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2021/04/reverse-grouping-1024x566.gif \\"Aim Reverse grouping demo\\")\\n\\nUse the toggle next to switch between grouping modes.\\n\\n## Line Chart Smoothing\\n\\n\\n\\nNow you can apply smoothing on metrics. Here is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/metric-smoothing.gif \\"Aim metric smoothing\\")\\n\\nThere are two type of smoothing options available:\xa0`Exponential Moving Average`\xa0and\xa0`Central Moving Average`. Further info of\xa0[how](https://en.wikipedia.org/wiki/Moving_average)\xa0it’s calculated.\\n\\n## New aggregation modes\\n\\n\\n\\nWe have additionally added two more aggregation modes:\xa0`standard error`\xa0and\xa0`standard deviation`\xa0. Here is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/aggr-modes.gif)\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa02.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.</p>\\n<p>Check out the updated Aim at\xa0<a href=\\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\\">play.aimstack.io</a>.</p>\\n<p>Below are the highlight features of the update. Find the full list of changes\xa0<a href=\\"https://github.com/aimhubio/aim/milestone/3?closed=1\\">here</a>.</p>\\n<h2>System Resource Usage</h2>\\n<p>Now you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with\xa0<code>system_tracking_interval==0</code>.</p>\\n<p>Once you run the training, you will see the following. Here is a demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo-1.gif\\" alt=\\"\\" title=\\"Aim automatic system resource tracking demo\\"></p>\\n<p>Of course you can also\xa0<a href=\\"https://aimstack.io/wp-admin/post.php?post=706&#x26;action=edit\\">query all those new metrics</a>\xa0in the Explore page and compare, group, aggregate with the rest of the metrics!</p>\\n<h2>Reverse grouping (“against” the param)</h2>\\n<p>As much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a\xa0Reverse Grouping. Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/04/reverse-grouping-1024x566.gif\\" alt=\\"\\" title=\\"Aim Reverse grouping demo\\"></p>\\n<p>Use the toggle next to switch between grouping modes.</p>\\n<h2>Line Chart Smoothing</h2>\\n<p>Now you can apply smoothing on metrics. Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/metric-smoothing.gif\\" alt=\\"\\" title=\\"Aim metric smoothing\\"></p>\\n<p>There are two type of smoothing options available:\xa0<code>Exponential Moving Average</code>\xa0and\xa0<code>Central Moving Average</code>. Further info of\xa0<a href=\\"https://en.wikipedia.org/wiki/Moving_average\\">how</a>\xa0it’s calculated.</p>\\n<h2>New aggregation modes</h2>\\n<p>We have additionally added two more aggregation modes:\xa0<code>standard error</code>\xa0and\xa0<code>standard deviation</code>\xa0. Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/aggr-modes.gif\\" alt=\\"\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","_raw":{"sourceFilePath":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileName":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping"},"type":"Post"},{"title":"Aim v2.6.0 — Docker requirement removed and Metric as x-axis","date":"2021-06-24T14:25:34.071Z","author":"Gev Soghomonian","description":"Before and after Aim v2.5.0","slug":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis","image":"https://aimstack.io/wp-content/uploads/2021/06/2.6.0.png","draft":false,"categories":["New Releases"],"body":{"raw":"## Metric as an alternative x-axis\\n\\nSometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.\\n\\nFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared —\xa0`loss`\xa0and`bleu`. One of the main indicators of a performant model in NMT, is when higher values of\xa0`bleu`\xa0correlates with the lower values of\xa0`loss`.\\n\\nJust plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use\xa0`loss`\xa0as the x-axis to directly see the correlation between the\xa0`loss`and the`bleu`\xa0metrics for two groups of runs (\xa0`max_k==3`and\xa0`max_k == 1`).\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1_Y0wAL9nZ7IhgLOUr-cCRRQ.gif \\"Demo for metric as alternative x-axis\\")\\n\\nUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\\n\\n> **Note**: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<h2>Metric as an alternative x-axis</h2>\\n<p>Sometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.</p>\\n<p>For instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared —\xa0<code>loss</code>\xa0and<code>bleu</code>. One of the main indicators of a performant model in NMT, is when higher values of\xa0<code>bleu</code>\xa0correlates with the lower values of\xa0<code>loss</code>.</p>\\n<p>Just plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use\xa0<code>loss</code>\xa0as the x-axis to directly see the correlation between the\xa0<code>loss</code>and the<code>bleu</code>\xa0metrics for two groups of runs (\xa0<code>max_k==3</code>and\xa0<code>max_k == 1</code>).</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1_Y0wAL9nZ7IhgLOUr-cCRRQ.gif\\" alt=\\"\\" title=\\"Demo for metric as alternative x-axis\\"></p>\\n<p>Using a different metric as the x-axis helps to see the correlation between two metrics of training runs better.</p>\\n<blockquote>\\n<p><strong>Note</strong>: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).</p>\\n</blockquote>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","_raw":{"sourceFilePath":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileName":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis"},"type":"Post"},{"title":"Aim v2.7.1 — Table export and Explore view bookmarks","date":"2021-07-15T14:31:32.317Z","author":"Gev Soghomonian","description":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback! Check out the updated","slug":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks","image":"https://aimstack.io/wp-content/uploads/2021/07/v2.7.1.gif","draft":false,"categories":["New Releases"],"body":{"raw":"Aim v2.7.1\xa0is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\\n\\nCheck out the updated Aim demo at\xa0[play.aimstack.io](http://play.aimstack.io:43900/dashboard).\\n\\nBelow are the two main highlights of\xa0Aim v2.7.1.\\n\\n## Aim Table CSV export\\n\\nNow you can export both tables from\xa0Explore\xa0and\xa0Dashboard\xa0to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with\xa0[Pandas](https://pandas.pydata.org/)) for your MLOps needs.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/07/csv-export.gif \\"Exporting CSV from the Explore table\\")\\n\\n## Bookmark/Save the Explore State \\\\[experimental]\\n\\nThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you’d like to revisit.\\n\\nIn the future\xa0[releases](https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/), we will build on top of this feature to potentially add notes, fully manage and share them.\xa0*For now, this feature is in experimental mode*. Please feel free to share more feedback and how we can improve its experience.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/07/bookmark.gif \\"Use bookmarks to save important explore states\\")\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize MLOps tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support\xa0.","html":"<p>Aim v2.7.1\xa0is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!</p>\\n<p>Check out the updated Aim demo at\xa0<a href=\\"http://play.aimstack.io:43900/dashboard\\">play.aimstack.io</a>.</p>\\n<p>Below are the two main highlights of\xa0Aim v2.7.1.</p>\\n<h2>Aim Table CSV export</h2>\\n<p>Now you can export both tables from\xa0Explore\xa0and\xa0Dashboard\xa0to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with\xa0<a href=\\"https://pandas.pydata.org/\\">Pandas</a>) for your MLOps needs.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/07/csv-export.gif\\" alt=\\"\\" title=\\"Exporting CSV from the Explore table\\"></p>\\n<h2>Bookmark/Save the Explore State [experimental]</h2>\\n<p>This new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you’d like to revisit.</p>\\n<p>In the future\xa0<a href=\\"https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/\\">releases</a>, we will build on top of this feature to potentially add notes, fully manage and share them.\xa0<em>For now, this feature is in experimental mode</em>. Please feel free to share more feedback and how we can improve its experience.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/07/bookmark.gif\\" alt=\\"\\" title=\\"Use bookmarks to save important explore states\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize MLOps tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support\xa0.</p>"},"_id":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","_raw":{"sourceFilePath":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","sourceFileName":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks"},"type":"Post"},{"title":"Aim’s foundations & why we’re building a Tensorboard alternative","date":"2021-10-22T14:48:48.452Z","author":"Gev Soghomonian","description":"The origins of Aim In the fateful summer of 2020, our friend mahnerak – a researcher at a non-profit lab was hitting the limits of Tensorboard. He wasn’t","slug":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative","image":"https://aimstack.io/wp-content/uploads/2021/08/3.0.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The origins of Aim\\n\\nIn the fateful summer of 2020, our friend\xa0[mahnerak](https://twitter.com/mahnerak)\xa0– a researcher at\xa0[a non-profit lab](https://yerevann.com/)\xa0was hitting the limits of Tensorboard. He wasn’t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That’s how we decided to build a Tensorboard alternative.\\n\\n[Gor](https://github.com/gorarakelyan)\xa0and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\\n\\nBy fall 2020,\xa0[Aim 2.0](https://aimstack.io/aim-v2-2-0-hugging-face-integration/)\xa0launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even\xa0[r/MachineLearning loved it](https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/).\\n\\n By spring 2021, mahnerak co-authoerd a paper [WARP](https://aclanthology.org/2021.acl-long.381/)\xa0([code](https://github.com/yerevann/warp)): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\\n\\n## A scale problem\\n\\nBut Aim’s power users — who often do 5K+ runs —were hitting issues.\\n\\nAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests.\xa0*Live updates, image tracking, distribution tracking*… and Aim 2.0 was hitting the limits of Aim 1.0’s design.\\n\\nIn order to support the future, we had to make changes to the foundation now.\\n\\n## Launching Aim 3.0.0\\n\\nAn additional\xa0[317 pull requests](https://github.com/aimhubio/aim/milestone/13?closed=1)\xa0later, we are excited to launch\xa0Aim v3.0.0\xa0!!!\\n\\nAs a result, the most important changes include:\\n\\n**A completely revamped UI**\\n\\n* Home page and run detail page\\n* Runs, metrics and params explorers\\n* Bookmarks and Tags\\n\\n**A completely revamped Aim Python SDK**\\n\\n* New and much more intuitive (but still quite vanilla) API to track your training runs\\n* New and 10x faster embedded storage based on\xa0[Rocksdb](http://rocksdb.org/). This will allow us to store virtually any type of AI metadata. On the contrary,\xa0[AimRecords](https://github.com/aimhubio/aimrecords)\xa0was designed for metrics and hyperparams only.\\n\\nEnjoy the changes!\\n\\n![](https://aimstack.io/wp-content/uploads/2021/08/changes.gif)\\n\\n## Performance improvements\\n\\n* Average run query execution time on ~2000 runs: 0.784s.\\n* Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\\n* New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (*for comparison, v2 was performant with limitation for only 100 metrics*).\\n\\n## Comparisons to familiar tools\\n\\n### Tensorboard\\n\\n**Training run comparison**\\n\\n* The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\\n* With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params.\xa0After all,\xa0TensorBoard doesn’t have features to group, aggregate the metrics\\n\\n**Scalability**\\n\\n* Aim can handle 1000s of training runs both on the backend and on the UI.\\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\\n\\nAim will have the beloved TB visualizations\\n\\n* Embedding projector.\\n* Neural network visualization.\\n\\n### [](https://github.com/aimhubio/aim#mlflow)MLFlow\\n\\nMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\\n\\n**Run comparison**\\n\\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\\n* MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\\n\\n**UI Scalability**\\n\\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\\n\\n### [](https://github.com/aimhubio/aim#weights-and-biases)Weights and Biases\\n\\n**Hosted vs self-hosted**\\n\\n* Weights and Biases is a hosted closed-source MLOps platform.\\n* Aim is self-hosted, free and open-source experiment tracking tool.\\n\\n## Aim Roadmap[](https://github.com/aimhubio/aim#tensorboard)\\n\\nWith this version we are also publishing the Aim\xa0[roadmap](https://github.com/aimhubio/aim#roadmap)\xa0for the next 3 months.\\n\\nThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\\n\\nWe are also\xa0[inviting community contributors](https://github.com/aimhubio/aim#community)\xa0to help us get there faster!\\n\\n## Why are we building Aim?\\n\\nWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\\n\\nExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\\n\\nHowever we see lots of improvements to be made. Especially around issues like:\\n\\n* ability to handle 1000s of large-scale experiments\\n* actionable, beautiful and performant visualizations\\n* extensibility — how easy are the apis for extension/democratization?\\n\\nWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\\n\\nOur mission…\\n\\nAim’s mission is to democratize AI dev tools. We believe that the best AI tools need to be:\\n\\n* open-source, open-data-format, community-driven\\n* have great UI/UX, CLI and other interfaces for automation\\n* performant both on UI and data\\n* extensible — enable ways to build around for so many use-cases\\n\\n## Thanks to\\n\\n[Ruben Karapetyan](https://twitter.com/roubkar)\xa0for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\\n\\n[Mahnerak](https://twitter.com/mahnerak)\xa0for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\\n\\nAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\\n\\nThe contributors who have been relentlessly iterating over the course of the summer.\\n\\nOn to the next generation of ML tools!!\\n\\n## Join Us!\\n\\nJoin the\xa0[Aim community](https://slack.aimstack.io/), test Aim out, ask questions, help us build the future of AI tooling!\\n\\nIf you find Aim useful, drop by and star the repo ⭐","html":"<h2>The origins of Aim</h2>\\n<p>In the fateful summer of 2020, our friend\xa0<a href=\\"https://twitter.com/mahnerak\\">mahnerak</a>\xa0– a researcher at\xa0<a href=\\"https://yerevann.com/\\">a non-profit lab</a>\xa0was hitting the limits of Tensorboard. He wasn’t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That’s how we decided to build a Tensorboard alternative.</p>\\n<p><a href=\\"https://github.com/gorarakelyan\\">Gor</a>\xa0and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.</p>\\n<p>By fall 2020,\xa0<a href=\\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\\">Aim 2.0</a>\xa0launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even\xa0<a href=\\"https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/\\">r/MachineLearning loved it</a>.</p>\\n<p>By spring 2021, mahnerak co-authoerd a paper <a href=\\"https://aclanthology.org/2021.acl-long.381/\\">WARP</a>\xa0(<a href=\\"https://github.com/yerevann/warp\\">code</a>): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.</p>\\n<h2>A scale problem</h2>\\n<p>But Aim’s power users — who often do 5K+ runs —were hitting issues.</p>\\n<p>After over 250 pull requests, 1.2K GitHub stars and 200 feature requests.\xa0<em>Live updates, image tracking, distribution tracking</em>… and Aim 2.0 was hitting the limits of Aim 1.0’s design.</p>\\n<p>In order to support the future, we had to make changes to the foundation now.</p>\\n<h2>Launching Aim 3.0.0</h2>\\n<p>An additional\xa0<a href=\\"https://github.com/aimhubio/aim/milestone/13?closed=1\\">317 pull requests</a>\xa0later, we are excited to launch\xa0Aim v3.0.0\xa0!!!</p>\\n<p>As a result, the most important changes include:</p>\\n<p><strong>A completely revamped UI</strong></p>\\n<ul>\\n<li>Home page and run detail page</li>\\n<li>Runs, metrics and params explorers</li>\\n<li>Bookmarks and Tags</li>\\n</ul>\\n<p><strong>A completely revamped Aim Python SDK</strong></p>\\n<ul>\\n<li>New and much more intuitive (but still quite vanilla) API to track your training runs</li>\\n<li>New and 10x faster embedded storage based on\xa0<a href=\\"http://rocksdb.org/\\">Rocksdb</a>. This will allow us to store virtually any type of AI metadata. On the contrary,\xa0<a href=\\"https://github.com/aimhubio/aimrecords\\">AimRecords</a>\xa0was designed for metrics and hyperparams only.</li>\\n</ul>\\n<p>Enjoy the changes!</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/08/changes.gif\\" alt=\\"\\"></p>\\n<h2>Performance improvements</h2>\\n<ul>\\n<li>Average run query execution time on ~2000 runs: 0.784s.</li>\\n<li>Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.</li>\\n<li>New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (<em>for comparison, v2 was performant with limitation for only 100 metrics</em>).</li>\\n</ul>\\n<h2>Comparisons to familiar tools</h2>\\n<h3>Tensorboard</h3>\\n<p><strong>Training run comparison</strong></p>\\n<ul>\\n<li>The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.</li>\\n<li>With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params.\xa0After all,\xa0TensorBoard doesn’t have features to group, aggregate the metrics</li>\\n</ul>\\n<p><strong>Scalability</strong></p>\\n<ul>\\n<li>Aim can handle 1000s of training runs both on the backend and on the UI.</li>\\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\\n</ul>\\n<p>Aim will have the beloved TB visualizations</p>\\n<ul>\\n<li>Embedding projector.</li>\\n<li>Neural network visualization.</li>\\n</ul>\\n<h3><a href=\\"https://github.com/aimhubio/aim#mlflow\\"></a>MLFlow</h3>\\n<p>MLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\\n<p><strong>Run comparison</strong></p>\\n<ul>\\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.</li>\\n<li>MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .</li>\\n</ul>\\n<p><strong>UI Scalability</strong></p>\\n<ul>\\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\\n</ul>\\n<h3><a href=\\"https://github.com/aimhubio/aim#weights-and-biases\\"></a>Weights and Biases</h3>\\n<p><strong>Hosted vs self-hosted</strong></p>\\n<ul>\\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\\n</ul>\\n<h2>Aim Roadmap<a href=\\"https://github.com/aimhubio/aim#tensorboard\\"></a></h2>\\n<p>With this version we are also publishing the Aim\xa0<a href=\\"https://github.com/aimhubio/aim#roadmap\\">roadmap</a>\xa0for the next 3 months.</p>\\n<p>This is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.</p>\\n<p>We are also\xa0<a href=\\"https://github.com/aimhubio/aim#community\\">inviting community contributors</a>\xa0to help us get there faster!</p>\\n<h2>Why are we building Aim?</h2>\\n<p>We have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.</p>\\n<p>Existing open-source tools (TensorBoard, MLFlow) are super-inspiring for us.</p>\\n<p>However we see lots of improvements to be made. Especially around issues like:</p>\\n<ul>\\n<li>ability to handle 1000s of large-scale experiments</li>\\n<li>actionable, beautiful and performant visualizations</li>\\n<li>extensibility — how easy are the apis for extension/democratization?</li>\\n</ul>\\n<p>With this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.</p>\\n<p>Our mission…</p>\\n<p>Aim’s mission is to democratize AI dev tools. We believe that the best AI tools need to be:</p>\\n<ul>\\n<li>open-source, open-data-format, community-driven</li>\\n<li>have great UI/UX, CLI and other interfaces for automation</li>\\n<li>performant both on UI and data</li>\\n<li>extensible — enable ways to build around for so many use-cases</li>\\n</ul>\\n<h2>Thanks to</h2>\\n<p><a href=\\"https://twitter.com/roubkar\\">Ruben Karapetyan</a>\xa0for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.</p>\\n<p><a href=\\"https://twitter.com/mahnerak\\">Mahnerak</a>\xa0for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.</p>\\n<p>Aim users Mohammad Elgaar, Vopani for continuous feedback on our work.</p>\\n<p>The contributors who have been relentlessly iterating over the course of the summer.</p>\\n<p>On to the next generation of ML tools!!</p>\\n<h2>Join Us!</h2>\\n<p>Join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>, test Aim out, ask questions, help us build the future of AI tooling!</p>\\n<p>If you find Aim useful, drop by and star the repo ⭐</p>"},"_id":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","_raw":{"sourceFilePath":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","sourceFileName":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":" Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out […]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger used… What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\\n\\n[Aim](https://github.com/aimhubio/aim)\xa0is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\\n\\n## What is XGBoost?\\n\\n[XGBoost](https://github.com/dmlc/xgboost)\xa0is an optimized gradient boosting library with highly\xa0\xa0*efficient*,\xa0\xa0*flexible,*\xa0 and\xa0\xa0*portable*\xa0design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\\n\\n## How to use Aim with XGBoost?\\n\\nCheck out end-to-end\xa0[Aim integration](https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/)\xa0examples with multiple frameworks\xa0[here](https://github.com/aimhubio/aim/tree/main/examples). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\\n\\n```\\n# You should download and extract the data beforehand. Simply by doing this: \\n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\\n\\nfrom __future__ import division\\n\\nimport numpy as np\\nimport xgboost as xgb\\nfrom aim.xgboost import AimCallback\\n\\n# label need to be 0 to num_class -1\\ndata = np.loadtxt(\'./dermatology.data\', delimiter=\',\',\\n        converters={33: lambda x:int(x == \'?\'), 34: lambda x:int(x) - 1})\\nsz = data.shape\\n\\ntrain = data[:int(sz[0] * 0.7), :]\\ntest = data[int(sz[0] * 0.7):, :]\\n\\ntrain_X = train[:, :33]\\ntrain_Y = train[:, 34]\\n\\ntest_X = test[:, :33]\\ntest_Y = test[:, 34]\\nprint(len(train_X))\\n\\nxg_train = xgb.DMatrix(train_X, label=train_Y)\\nxg_test = xgb.DMatrix(test_X, label=test_Y)\\n# setup parameters for xgboost\\nparam = {}\\n# use softmax multi-class classification\\nparam[\'objective\'] = \'multi:softmax\'\\n# scale weight of positive examples\\nparam[\'eta\'] = 0.1\\nparam[\'max_depth\'] = 6\\nparam[\'nthread\'] = 4\\nparam[\'num_class\'] = 6\\n\\nwatchlist = [(xg_train, \'train\'), (xg_test, \'test\')]\\nnum_round = 50\\nbst = xgb.train(param, xg_train, num_round, watchlist)\\n# get prediction\\npred = bst.predict(xg_test)\\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softmax = {}\'.format(error_rate))\\n\\n# do the same thing again, but output probabilities\\nparam[\'objective\'] = \'multi:softprob\'\\nbst = xgb.train(param, xg_train, num_round, watchlist, \\n                callbacks=[AimCallback(repo=\'.\', experiment=\'xgboost_test\')])\\n# Note: this convention has been changed since xgboost-unity\\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\\npred_label = np.argmax(pred_prob, axis=1)\\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softprob = {}\'.format(error_rate))\\n```\\n\\nAs you can see on line 49,\xa0AimCallback\xa0is imported from\xa0`aim.xgboost`\xa0and passed to\xa0`xgb.train`\xa0as one of the callbacks. Aim session can open and close by the AimCallback and the\xa0metrics and hparamsstore by XGBoost. In addition to that, thesystem measures\xa0pass to Aim as well.\\n\\n## What it looks like?\\n\\nAfter you run the experiment and the\xa0`aim up`\xa0command in the\xa0`aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/05/dashboard.png \\"Aim UI dashboard page\\")\\n\\nTo explore the run, we should:\\n\\n* Choose the\xa0`xgboost_test`experiment.\\n* Select the metrics to explore.\\n* Divide into charts by metrics.\\n\\nFor example, the gif below illustrates the steps above.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/05/1.gif)\\n\\n\\\\\\n So this is what the final result looks like.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/05/2.png)\\n\\nIn short, we can easily analyze the runs and the system usage.\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nIf you find Aim useful, support us and star\xa0[the project](https://github.com/aimhubio/aim)\xa0on GitHub. Also, join the\xa0[Aim community](https://aimstack.slack.com/ssb/redirect)\xa0and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is Aim?</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.</p>\\n<h2>What is XGBoost?</h2>\\n<p><a href=\\"https://github.com/dmlc/xgboost\\">XGBoost</a>\xa0is an optimized gradient boosting library with highly\xa0\xa0<em>efficient</em>,\xa0\xa0<em>flexible,</em>\xa0 and\xa0\xa0<em>portable</em>\xa0design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.</p>\\n<h2>How to use Aim with XGBoost?</h2>\\n<p>Check out end-to-end\xa0<a href=\\"https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/\\">Aim integration</a>\xa0examples with multiple frameworks\xa0<a href=\\"https://github.com/aimhubio/aim/tree/main/examples\\">here</a>. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.</p>\\n<pre><code># You should download and extract the data beforehand. Simply by doing this: \\n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\\n\\nfrom __future__ import division\\n\\nimport numpy as np\\nimport xgboost as xgb\\nfrom aim.xgboost import AimCallback\\n\\n# label need to be 0 to num_class -1\\ndata = np.loadtxt(\'./dermatology.data\', delimiter=\',\',\\n        converters={33: lambda x:int(x == \'?\'), 34: lambda x:int(x) - 1})\\nsz = data.shape\\n\\ntrain = data[:int(sz[0] * 0.7), :]\\ntest = data[int(sz[0] * 0.7):, :]\\n\\ntrain_X = train[:, :33]\\ntrain_Y = train[:, 34]\\n\\ntest_X = test[:, :33]\\ntest_Y = test[:, 34]\\nprint(len(train_X))\\n\\nxg_train = xgb.DMatrix(train_X, label=train_Y)\\nxg_test = xgb.DMatrix(test_X, label=test_Y)\\n# setup parameters for xgboost\\nparam = {}\\n# use softmax multi-class classification\\nparam[\'objective\'] = \'multi:softmax\'\\n# scale weight of positive examples\\nparam[\'eta\'] = 0.1\\nparam[\'max_depth\'] = 6\\nparam[\'nthread\'] = 4\\nparam[\'num_class\'] = 6\\n\\nwatchlist = [(xg_train, \'train\'), (xg_test, \'test\')]\\nnum_round = 50\\nbst = xgb.train(param, xg_train, num_round, watchlist)\\n# get prediction\\npred = bst.predict(xg_test)\\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softmax = {}\'.format(error_rate))\\n\\n# do the same thing again, but output probabilities\\nparam[\'objective\'] = \'multi:softprob\'\\nbst = xgb.train(param, xg_train, num_round, watchlist, \\n                callbacks=[AimCallback(repo=\'.\', experiment=\'xgboost_test\')])\\n# Note: this convention has been changed since xgboost-unity\\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\\npred_label = np.argmax(pred_prob, axis=1)\\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softprob = {}\'.format(error_rate))\\n</code></pre>\\n<p>As you can see on line 49,\xa0AimCallback\xa0is imported from\xa0<code>aim.xgboost</code>\xa0and passed to\xa0<code>xgb.train</code>\xa0as one of the callbacks. Aim session can open and close by the AimCallback and the\xa0metrics and hparamsstore by XGBoost. In addition to that, thesystem measures\xa0pass to Aim as well.</p>\\n<h2>What it looks like?</h2>\\n<p>After you run the experiment and the\xa0<code>aim up</code>\xa0command in the\xa0<code>aim_logs</code>directory, Aim UI will be running. When first opened, the dashboard page will come up.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/05/dashboard.png\\" alt=\\"\\" title=\\"Aim UI dashboard page\\"></p>\\n<p>To explore the run, we should:</p>\\n<ul>\\n<li>Choose the\xa0<code>xgboost_test</code>experiment.</li>\\n<li>Select the metrics to explore.</li>\\n<li>Divide into charts by metrics.</li>\\n</ul>\\n<p>For example, the gif below illustrates the steps above.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/05/1.gif\\" alt=\\"\\"></p>\\n<p><br>\\nSo this is what the final result looks like.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/05/2.png\\" alt=\\"\\"></p>\\n<p>In short, we can easily analyze the runs and the system usage.</p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>If you find Aim useful, support us and star\xa0<a href=\\"https://github.com/aimhubio/aim\\">the project</a>\xa0on GitHub. Also, join the\xa0<a href=\\"https://aimstack.slack.com/ssb/redirect\\">Aim community</a>\xa0and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://aimstack.io/wp-content/uploads/2021/03/pytorch.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\\n\\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\\n\\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\\n\\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.\xa0One thing you do not need when tuning hyper-parameters is variance.\\n\\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\\n\\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using\xa0[Aim](https://github.com/aimhubio/aim).\\n\\n## **How to fix the seed in PyTorch Lightning**\\n\\n\\n\\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\\n\\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use\xa0`pl.seed_everything(seed)`\xa0. See the docs\xa0[here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\\n\\n> Note: in other libraries you would use something like:\xa0`np.random.seed()`\xa0or\xa0`torch.manual_seed()`\xa0\\n\\n## **Implementation**\\n\\n\\n\\nFind the full code for this and other tutorials\xa0[here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\\n\\n```\\nimport torch\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import CIFAR10\\nfrom torchvision.models import resnet18\\nfrom torchvision import transforms\\nimport pytorch_lightning as pl\\nfrom aim.pytorch_lightning import AimLogger\\n\\nclass ImageClassifierModel(pl.LightningModule):\\n\\n    def __init__(self, seed, lr, optimizer):\\n        super(ImageClassifierModel, self).__init__()\\n        pl.seed_everything(seed)\\n        # This fixes a seed for all the modules used by pytorch-lightning\\n        # Note: using random.seed(seed) is not enough, there are multiple\\n        # other seeds like hash seed, seed for numpy etc.\\n        self.lr = lr\\n        self.optimizer = optimizer\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        self.model = resnet18(pretrained = True, progress = True)\\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\\n        # changing the last layer from 1000 out_features to 10 because the model\\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\\n        self.model.to(\'cuda\') # moving the model to cuda\\n\\n    def train_dataloader(self):\\n        # makes the training dataloader\\n        train_ds = CIFAR10(\'.\', train = True, transform = transforms.ToTensor(), download = True)\\n        train_loader = DataLoader(train_ds, batch_size=32)\\n        return train_loader\\n        \\n    def val_dataloader(self):\\n        # makes the validation dataloader\\n        val_ds = CIFAR10(\'.\', train = False, transform = transforms.ToTensor(), download = True)\\n        val_loader = DataLoader(val_ds, batch_size=32)\\n        return val_loader\\n\\n    def forward(self, x):\\n        return self.model.forward(x)\\n\\n    def training_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.log(\'train_loss\', loss)\\n        # logging the loss with \\"train_\\" prefix\\n        return loss\\n\\n    def validation_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.total_classified += y.shape[0]\\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\\n        # Calculating total and correctly classified images to determine the accuracy later\\n        self.log(\'val_loss\', loss) \\n        # logging the loss with \\"val_\\" prefix\\n        return loss\\n\\n    def validation_epoch_end(self, results):\\n        accuracy = self.correctly_classified / self.total_classified\\n        self.log(\'val_accuracy\', accuracy)\\n        # logging accuracy\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        return accuracy\\n\\n    def configure_optimizers(self):\\n        # Choose an optimizer and set up a learning rate according to hyperparameters\\n        if self.optimizer == \'Adam\':\\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\\n        elif self.optimizer == \'SGD\':\\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\\n        else:\\n            raise NotImplementedError\\n\\nif __name__ == \\"__main__\\":\\n\\n    seeds = [47, 881, 123456789]\\n    lrs = [0.1, 0.01]\\n    optimizers = [\'SGD\', \'Adam\']\\n\\n    for seed in seeds:\\n        for optimizer in optimizers:\\n            for lr in lrs:\\n                # choosing one set of hyperparaameters from the ones above\\n            \\n                model = ImageClassifierModel(seed, lr, optimizer)\\n                # initializing the model we will train with the chosen hyperparameters\\n\\n                aim_logger = AimLogger(\\n                    experiment=\'resnet18_classification\',\\n                    train_metric_prefix=\'train_\',\\n                    val_metric_prefix=\'val_\',\\n                )\\n                aim_logger.log_hyperparams({\\n                    \'lr\': lr,\\n                    \'optimizer\': optimizer,\\n                    \'seed\': seed\\n                })\\n                # initializing the aim logger and logging the hyperparameters\\n\\n                trainer = pl.Trainer(\\n                    logger=aim_logger,\\n                    gpus=1,\\n                    max_epochs=5,\\n                    progress_bar_refresh_rate=1,\\n                    log_every_n_steps=10,\\n                    check_val_every_n_epoch=1)\\n                # making the pytorch-lightning trainer\\n\\n                trainer.fit(model)\\n                # training the model\\n```\\n\\n\\n\\n## Analyzing the Training Runs\\n\\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after\xa0[Aim 1.3.5 release](https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)) and have multiple charts of different metrics on the same screen.\\n\\nDo the following steps to see the different effects of the optimizers\\n\\n* Go to dashboard, explore by experiment\\n* Add loss to\xa0`SELECT`\xa0and divide into subplots by metric\\n* Group by experiment to make all metrics of similar color\\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\\n\\nHere is how it looks on Aim:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/10.gif)\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/11.png)\\n\\n\\n\\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\\n\\nIf you apply the same settings to the learning rate, this is the result:\\n\\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\\n\\n\\n\\n* Remove both previous groupings\\n* Group by color by learning rate\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/12.gif)\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/13.png)\\n\\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\\n\\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\\n\\nOn top of this, if we also add grouping by style by optimizer:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/24.gif)\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/25.png)\\n\\nNow, it is obvious that the the runs with SGD optimizer and\xa0`lr=0.01`\xa0(green, broken lines) are the best choices for all the seeds we have tried.\\n\\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\\n\\n## Learn More\\n\\n\\n\\nIf you find Aim useful, support us and\xa0[star the project](https://github.com/aimhubio/aim)\xa0on GitHub. Join the\xa0[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect)\xa0and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is a random seed and how is it important?</h2>\\n<p>The random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:</p>\\n<ul>\\n<li>Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.</li>\\n<li>Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.</li>\\n<li>Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).</li>\\n</ul>\\n<p>As you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.\xa0One thing you do not need when tuning hyper-parameters is variance.</p>\\n<p>The purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.</p>\\n<p>Later in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>.</p>\\n<h2><strong>How to fix the seed in PyTorch Lightning</strong></h2>\\n<p>Fixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python<code>random.seed(seed)</code>, but it is not enough for PL.</p>\\n<p>Pytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use\xa0<code>pl.seed_everything(seed)</code>\xa0. See the docs\xa0<a href=\\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\\">here</a>.</p>\\n<blockquote>\\n<p>Note: in other libraries you would use something like:\xa0<code>np.random.seed()</code>\xa0or\xa0<code>torch.manual_seed()</code>\xa0</p>\\n</blockquote>\\n<h2><strong>Implementation</strong></h2>\\n<p>Find the full code for this and other tutorials\xa0<a href=\\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\\">here</a>.</p>\\n<pre><code>import torch\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import CIFAR10\\nfrom torchvision.models import resnet18\\nfrom torchvision import transforms\\nimport pytorch_lightning as pl\\nfrom aim.pytorch_lightning import AimLogger\\n\\nclass ImageClassifierModel(pl.LightningModule):\\n\\n    def __init__(self, seed, lr, optimizer):\\n        super(ImageClassifierModel, self).__init__()\\n        pl.seed_everything(seed)\\n        # This fixes a seed for all the modules used by pytorch-lightning\\n        # Note: using random.seed(seed) is not enough, there are multiple\\n        # other seeds like hash seed, seed for numpy etc.\\n        self.lr = lr\\n        self.optimizer = optimizer\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        self.model = resnet18(pretrained = True, progress = True)\\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\\n        # changing the last layer from 1000 out_features to 10 because the model\\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\\n        self.model.to(\'cuda\') # moving the model to cuda\\n\\n    def train_dataloader(self):\\n        # makes the training dataloader\\n        train_ds = CIFAR10(\'.\', train = True, transform = transforms.ToTensor(), download = True)\\n        train_loader = DataLoader(train_ds, batch_size=32)\\n        return train_loader\\n        \\n    def val_dataloader(self):\\n        # makes the validation dataloader\\n        val_ds = CIFAR10(\'.\', train = False, transform = transforms.ToTensor(), download = True)\\n        val_loader = DataLoader(val_ds, batch_size=32)\\n        return val_loader\\n\\n    def forward(self, x):\\n        return self.model.forward(x)\\n\\n    def training_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.log(\'train_loss\', loss)\\n        # logging the loss with \\"train_\\" prefix\\n        return loss\\n\\n    def validation_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.total_classified += y.shape[0]\\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\\n        # Calculating total and correctly classified images to determine the accuracy later\\n        self.log(\'val_loss\', loss) \\n        # logging the loss with \\"val_\\" prefix\\n        return loss\\n\\n    def validation_epoch_end(self, results):\\n        accuracy = self.correctly_classified / self.total_classified\\n        self.log(\'val_accuracy\', accuracy)\\n        # logging accuracy\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        return accuracy\\n\\n    def configure_optimizers(self):\\n        # Choose an optimizer and set up a learning rate according to hyperparameters\\n        if self.optimizer == \'Adam\':\\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\\n        elif self.optimizer == \'SGD\':\\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\\n        else:\\n            raise NotImplementedError\\n\\nif __name__ == \\"__main__\\":\\n\\n    seeds = [47, 881, 123456789]\\n    lrs = [0.1, 0.01]\\n    optimizers = [\'SGD\', \'Adam\']\\n\\n    for seed in seeds:\\n        for optimizer in optimizers:\\n            for lr in lrs:\\n                # choosing one set of hyperparaameters from the ones above\\n            \\n                model = ImageClassifierModel(seed, lr, optimizer)\\n                # initializing the model we will train with the chosen hyperparameters\\n\\n                aim_logger = AimLogger(\\n                    experiment=\'resnet18_classification\',\\n                    train_metric_prefix=\'train_\',\\n                    val_metric_prefix=\'val_\',\\n                )\\n                aim_logger.log_hyperparams({\\n                    \'lr\': lr,\\n                    \'optimizer\': optimizer,\\n                    \'seed\': seed\\n                })\\n                # initializing the aim logger and logging the hyperparameters\\n\\n                trainer = pl.Trainer(\\n                    logger=aim_logger,\\n                    gpus=1,\\n                    max_epochs=5,\\n                    progress_bar_refresh_rate=1,\\n                    log_every_n_steps=10,\\n                    check_val_every_n_epoch=1)\\n                # making the pytorch-lightning trainer\\n\\n                trainer.fit(model)\\n                # training the model\\n</code></pre>\\n<h2>Analyzing the Training Runs</h2>\\n<p>After each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after\xa0<a href=\\"https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\\">Aim 1.3.5 release</a>) and have multiple charts of different metrics on the same screen.</p>\\n<p>Do the following steps to see the different effects of the optimizers</p>\\n<ul>\\n<li>Go to dashboard, explore by experiment</li>\\n<li>Add loss to\xa0<code>SELECT</code>\xa0and divide into subplots by metric</li>\\n<li>Group by experiment to make all metrics of similar color</li>\\n<li>Group by style by optimizer to see different optimizers on loss and accuracy and its effects</li>\\n</ul>\\n<p>Here is how it looks on Aim:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/10.gif\\" alt=\\"\\"></p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/11.png\\" alt=\\"\\"></p>\\n<p>From the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.</p>\\n<p>If you apply the same settings to the learning rate, this is the result:</p>\\n<h2>For the next step to analyze how learning rate affects the experiments, do the following steps:</h2>\\n<ul>\\n<li>Remove both previous groupings</li>\\n<li>Group by color by learning rate</li>\\n</ul>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/12.gif\\" alt=\\"\\"></p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/13.png\\" alt=\\"\\"></p>\\n<p>As you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.</p>\\n<p>We showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.</p>\\n<p>On top of this, if we also add grouping by style by optimizer:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/24.gif\\" alt=\\"\\"></p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/25.png\\" alt=\\"\\"></p>\\n<p>Now, it is obvious that the the runs with SGD optimizer and\xa0<code>lr=0.01</code>\xa0(green, broken lines) are the best choices for all the seeds we have tried.</p>\\n<p>Fixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.</p>\\n<h2>Learn More</h2>\\n<p>If you find Aim useful, support us and\xa0<a href=\\"https://github.com/aimhubio/aim\\">star the project</a>\xa0on GitHub. Join the\xa0<a href=\\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\\">Aim community</a>\xa0and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"}]');[..._index_namespaceObject];var Pagnation=__webpack_require__(5065),next_router=__webpack_require__(1163),blog={title:"MLOps Blog | AimStack",description:"AimStack",image:"",path:"blog"};function Blog(){let router=(0,next_router.useRouter)(),params=router.query,pathname=router.pathname,page=Number(params.page)||1,posts=_index_namespaceObject.map(post=>(0,dist.ei)(post,["title","date","slug","description","draft","image","categories"])),totalPostCount=(0,utils.mC)(_index_namespaceObject.length),totalPosts=(0,react.useCallback)(()=>(0,utils.eU)(posts,page),[page,posts]);return(0,jsx_runtime.jsxs)(jsx_runtime.Fragment,{children:[(0,jsx_runtime.jsx)(SEO.Z,{...blog}),(0,jsx_runtime.jsxs)(foundations.W2,{children:[(0,jsx_runtime.jsx)(foundations.xv,{as:"h1",size:6,className:"title",css:{textAlign:"center",my:"$10"},children:"Recent Articles"}),(0,jsx_runtime.jsx)(BlogList.Z,{blogList:totalPosts()}),(0,jsx_runtime.jsx)(Pagnation.Z,{pathname:pathname,currentPage:page,totalPostCount:totalPostCount})]})]})}},558:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{Bi:function(){return formattedDate},Qs:function(){return titleCase},eU:function(){return getTotalPosts},ef:function(){return ImageUrl},mC:function(){return pageCount}});var _config__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(3346);function sortByDate(a,b){return new Date(b.date)-new Date(a.date)}function pageCount(number){return Math.ceil(number/_config__WEBPACK_IMPORTED_MODULE_0__.ou)}function ImageUrl(url){return _config__WEBPACK_IMPORTED_MODULE_0__.ZP+url}function titleCase(str){return str.replace(/(^\w)/g,g=>g[0].toUpperCase()).replace(/([-_]\w)/g,g=>" "+g[1].toUpperCase()).trim()}function formattedDate(date){return new Date(date).toLocaleDateString("en-US",{day:"numeric",month:"short",year:"numeric"})}function getTotalPosts(posts,page){let postSortByDate=posts.sort(sortByDate),publish=postSortByDate.filter((post,i)=>!post.draft),totalPosts=publish.slice(0,_config__WEBPACK_IMPORTED_MODULE_0__.ou);return 2===page&&(totalPosts=publish.slice(_config__WEBPACK_IMPORTED_MODULE_0__.ou,_config__WEBPACK_IMPORTED_MODULE_0__.ou*page)),page>2&&(totalPosts=publish.slice(_config__WEBPACK_IMPORTED_MODULE_0__.ou*page-_config__WEBPACK_IMPORTED_MODULE_0__.ou,_config__WEBPACK_IMPORTED_MODULE_0__.ou*page)),totalPosts}}},function(__webpack_require__){__webpack_require__.O(0,[962,774,888,179],function(){return __webpack_require__(__webpack_require__.s=3986)}),_N_E=__webpack_require__.O()}]);