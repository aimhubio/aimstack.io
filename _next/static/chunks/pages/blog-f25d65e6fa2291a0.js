(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[195],{3301:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{ei:function(){return pick}});let pick=(obj,keys)=>keys.reduce((acc,key)=>(acc[key]=obj[key],acc),{})},3986:function(__unused_webpack_module,__unused_webpack_exports,__webpack_require__){(window.__NEXT_P=window.__NEXT_P||[]).push(["/blog",function(){return __webpack_require__(693)}])},4213:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{Z:function(){return BlogList_BlogList}});var jsx_runtime=__webpack_require__(5893);__webpack_require__(7294);var styles=__webpack_require__(5270);let BlogListStyle=(0,styles.zo)("ul",{display:"flex",flexWrap:"wrap",marginLeft:"-40px","@bp1":{marginLeft:"-54px"},"@bp3":{marginLeft:"0"}}),BlogItem=(0,styles.zo)("li",{width:"calc((100% / 3) - 40px)",marginLeft:"40px",marginBottom:"40px","@bp1":{width:"calc((100% / 2) - 54px)",marginLeft:"54px",marginBottom:"54px"},"@bp3":{width:"100%",marginLeft:"0",marginBottom:"48px"}});var Card_style=__webpack_require__(5349),foundations=__webpack_require__(9923),dist_image=__webpack_require__(8782),image_default=__webpack_require__.n(dist_image),UIkit=__webpack_require__(8561),next_link=__webpack_require__(1664),link_default=__webpack_require__.n(next_link);let Card=props=>(0,jsx_runtime.jsxs)(Card_style.UE,{children:[(0,jsx_runtime.jsx)(Card_style.fb,{children:(0,jsx_runtime.jsx)(link_default(),{href:"/blog/".concat(props.slug),children:(0,jsx_runtime.jsx)(image_default(),{src:props.image,alt:props.title,layout:"fill",objectFit:"cover",objectPosition:"top"})})}),(0,jsx_runtime.jsxs)(Card_style.aY,{children:[(0,jsx_runtime.jsx)(Card_style.WD,{children:(0,jsx_runtime.jsx)(link_default(),{href:"/category/".concat(props.categories[0].toLowerCase()),children:(0,jsx_runtime.jsxs)(foundations.xv,{size:1,css:{display:"flex",alignItems:"center"},children:[(0,jsx_runtime.jsx)(UIkit.JO,{name:"folder",size:14}),props.categories[0]]})})}),(0,jsx_runtime.jsx)(foundations.xv,{as:"h3",size:6,className:"title",truncate:!0,children:(0,jsx_runtime.jsx)(link_default(),{href:"/blog/".concat(props.slug),children:props.title})}),(0,jsx_runtime.jsx)(foundations.xv,{size:2,className:"title",lineClamp:!0,css:{my:"$6",$$lineClamp:3,fontFamily:"$Lora"},children:(0,jsx_runtime.jsx)(link_default(),{href:"/blog/".concat(props.slug),children:props.description})}),(0,jsx_runtime.jsx)(Card_style.eW,{children:props.views&&(0,jsx_runtime.jsx)(foundations.xv,{size:1,link:"secondary",children:(0,jsx_runtime.jsxs)(link_default(),{href:"/blog/".concat(props.slug),children:[(0,jsx_runtime.jsx)(UIkit.JO,{name:"eye",size:14}),props.views,"K"]})})})]})]}),BlogList=param=>{let{blogList}=param;return(0,jsx_runtime.jsx)(BlogListStyle,{children:blogList.map(blog=>(0,jsx_runtime.jsx)(BlogItem,{children:(0,jsx_runtime.jsx)(Card,{...blog},blog.slug)},blog.slug))})};var BlogList_BlogList=BlogList},5349:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{UE:function(){return CardStyle},WD:function(){return Category},aY:function(){return CardContent},eW:function(){return CardFooter},fb:function(){return ImageWrapper}});var styles__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(5270);let CardStyle=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{}),ImageWrapper=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{position:"relative",height:"0",paddingBottom:"65%"}),CardContent=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{paddingTop:"$6"}),Category=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{marginBottom:"$2",display:"flex",alignItems:"center",color:"$grey",transition:"$main",".icon":{marginRight:"$1",fill:"$grey",transition:"$main"},"&:hover":{color:"$darkGray",".icon":{fill:"$darkGray"}}}),CardFooter=(0,styles__WEBPACK_IMPORTED_MODULE_0__.zo)("div",{display:"flex",alignItems:"center",justifyContent:"flex-end",".icon":{marginRight:"$1",fill:"$grey"}})},5065:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{Z:function(){return Pagnation}});var jsx_runtime=__webpack_require__(5893);__webpack_require__(7294);var next_link=__webpack_require__(1664),link_default=__webpack_require__.n(next_link),styles=__webpack_require__(5270);let PaginationStyle=(0,styles.zo)("nav",{my:"$10"}),PaginationList=(0,styles.zo)("ul",{display:"flex",alignItems:"center",justifyContent:"center",li:{height:"44px",width:"44px",fontSize:"$1",fontWeight:"$4",textAlign:"center",marginRight:"$1",a:{display:"block",lineHeight:"44px",transition:"$main","&.active":{backgroundColor:"$primary",color:"$white"},"&:hover":{"&:not(.active)":{color:"$primary"},".icon":{fill:"$primary"}}}}});var UIkit=__webpack_require__(8561);let Pagination=param=>{let{currentPage,totalPostCount,pathname}=param,pageIntoArray=Array.from(Array(totalPostCount).keys());return pageIntoArray.length<=1?null:(0,jsx_runtime.jsx)(PaginationStyle,{children:(0,jsx_runtime.jsxs)(PaginationList,{children:[currentPage>1&&(0,jsx_runtime.jsx)("li",{children:(0,jsx_runtime.jsx)(link_default(),{href:{pathname:"/".concat(pathname),query:{page:currentPage-1}},children:(0,jsx_runtime.jsx)(UIkit.JO,{name:"chevron-left"})})}),pageIntoArray.map(page=>(0,jsx_runtime.jsx)("li",{children:(0,jsx_runtime.jsx)(link_default(),{href:{pathname:"/".concat(pathname),query:{page:page+1}},className:currentPage===page+1?"active":"",children:page+1})},page)),currentPage<pageIntoArray.length&&(0,jsx_runtime.jsx)("li",{children:(0,jsx_runtime.jsx)(link_default(),{href:{pathname:"/".concat(pathname),query:{page:currentPage+1}},children:(0,jsx_runtime.jsx)(UIkit.JO,{name:"chevron-right"})})})]})})};var Pagnation=Pagination},233:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";var react_jsx_runtime__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(5893);__webpack_require__(7294);var next_seo__WEBPACK_IMPORTED_MODULE_2__=__webpack_require__(2962),config__WEBPACK_IMPORTED_MODULE_3__=__webpack_require__(3346);let Seo=props=>(0,react_jsx_runtime__WEBPACK_IMPORTED_MODULE_0__.jsx)(next_seo__WEBPACK_IMPORTED_MODULE_2__.PB,{title:props.title,description:props.description,openGraph:{url:"".concat(config__WEBPACK_IMPORTED_MODULE_3__.ZP).concat(props.path),title:props.title,description:props.description,type:props.type||"website",locale:"en_US",article:{publishedTime:props.date,authors:["".concat(config__WEBPACK_IMPORTED_MODULE_3__.ZP,"about-us")]},images:[{url:props.image,width:1224,height:724,alt:props.title,type:"image/jpeg"}],site_name:"Aimstack"},twitter:{cardType:"summary_large_image"}});__webpack_exports__.Z=Seo},3346:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{c7:function(){return GITHUB_API},ou:function(){return show_per_page}});let GITHUB_API="https://api.github.com/repos/aimhubio/aim/",show_per_page=9;__webpack_exports__.ZP="https://v4.aimstack.io/"},693:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.r(__webpack_exports__),__webpack_require__.d(__webpack_exports__,{default:function(){return Blog}});var jsx_runtime=__webpack_require__(5893),react=__webpack_require__(7294),foundations=__webpack_require__(9923),BlogList=__webpack_require__(4213),SEO=__webpack_require__(233),utils=__webpack_require__(558),dist=__webpack_require__(3301),_index_namespaceObject=JSON.parse('[{"title":"Aim 1.3.5 — Activity View and X-axis alignment","date":"2021-02-05T11:16:41.748Z","author":"Gev Soghomonian","description":"Aim v1.3.5 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open source MLOps tools. Check out the new features […]","slug":"aim-1-3-5-—-activity-view-and-x-axis-alignment","image":"/images/dynamic/1st.gif","draft":false,"categories":["New Releases"],"body":{"raw":"\\n\\n## Aim Release — Activity View and X-axis alignment\\n\\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is now available. Thanks to the incredible [Aim community](https://discord.com/invite/zXq2NfVdtF) for the feedback and support on building democratized open source AI dev tools.\\n\\nCheck out the new features at play.aimstack.io\\n\\nHere are the highlights of the features:\\n\\n## Activity View on the Dashboard\\n\\nThe **Activity View** shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away\\n\\n**Statistics** displays the overall count of [Experiments and Runs. ](https://github.com/aimhubio/aim#concepts)\\n\\n![](/images/dynamic/1st.gif \\"The new dashboard in action!\\")\\n\\n## **X-axis alignment by epoch**\\n\\nX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.\\n\\n![](/images/dynamic/2nd.gif \\"No more misaligned metrics!\\")\\n\\n## **Ordering runs both on Explore and on Dashboard**\\n\\n\\n\\nNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\\n\\nOn Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\\n\\n![](/images/dynamic/3.gif \\"Sort the runs both on Dashboard and Explore!\\")\\n\\n## Learn More\\n\\n\\n\\nWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\\n\\nWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month\xa0\uD83D\uDE0A). Try out Aim, join the\xa0[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support\xa0\uD83D\uDE4C.","html":"<h2>Aim Release — Activity View and X-axis alignment</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a> v1.3.5 is now available. Thanks to the incredible <a href=\\"https://discord.com/invite/zXq2NfVdtF\\">Aim community</a> for the feedback and support on building democratized open source AI dev tools.</p>\\n<p>Check out the new features at play.aimstack.io</p>\\n<p>Here are the highlights of the features:</p>\\n<h2>Activity View on the Dashboard</h2>\\n<p>The <strong>Activity View</strong> shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away</p>\\n<p><strong>Statistics</strong> displays the overall count of <a href=\\"https://github.com/aimhubio/aim#concepts\\">Experiments and Runs. </a></p>\\n<p><img src=\\"/images/dynamic/1st.gif\\" alt=\\"\\" title=\\"The new dashboard in action!\\"></p>\\n<h2><strong>X-axis alignment by epoch</strong></h2>\\n<p>X-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.</p>\\n<p><img src=\\"/images/dynamic/2nd.gif\\" alt=\\"\\" title=\\"No more misaligned metrics!\\"></p>\\n<h2><strong>Ordering runs both on Explore and on Dashboard</strong></h2>\\n<p>Now you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.</p>\\n<p>On Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.</p>\\n<p><img src=\\"/images/dynamic/3.gif\\" alt=\\"\\" title=\\"Sort the runs both on Dashboard and Explore!\\"></p>\\n<h2>Learn More</h2>\\n<p>We have been working on Aim for the past 6 months and excited to see this much interest from the community.</p>\\n<p>We are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month\xa0\uD83D\uDE0A). Try out Aim, join the\xa0<a href=\\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\\">Aim community</a>, share your feedback.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support\xa0\uD83D\uDE4C.</p>"},"_id":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","_raw":{"sourceFilePath":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileName":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-1-3-5-—-activity-view-and-x-axis-alignment"},"type":"Post"},{"title":"Aim 1.3.8 — Enhanced Context Table and Advanced Group Coloring","date":"2021-03-01T11:09:06.858Z","author":"Gev Soghomonian","description":"Aim 1.3.8 featuring Advanced Group Coloring for AI metrics  is now available. ","slug":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring","image":"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"\\n\\n[Aim](https://aimstack.io/)\xa01.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\\n\\nCheck out the new features at\xa0[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=).\\n\\nHere are the more notable changes:\\n\\n### Enhanced Context Table\\n\\n\\n\\nThe context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\\n\\nSo we have made 3 changes to tackle this problem\\n\\n### New table groups view\\n\\nBelow is the new modified look of the table. Here is what’s changed:\\n\\n* The empty per group is removed\\n* In addition to the group details popover, there is an in-place list group config is available for instant lookup\\n\\n![](https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp \\"New improved Aim Context Table on Explore\\")\\n\\n\\n\\n* **\\\\[In Progress for next release]**\xa0Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\\n\\n### Column Management\\n\\n\\n\\nThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\\n\\nYou can also re-order the columns by dragging the mid-section up/down.\\n\\nThis is super-handy when dealing with 100s of columns on the table for parameters.\\n\\n![](https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp \\"Drag columns left and right to pin, search and disable less important ones\\")\\n\\n\\n\\n### Advanced Group Coloring\\n\\n\\n\\nThis is the latest iteration over the group coloring.\\n\\n* Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\\n* You can shuffle the colors in case they aren’t pick to your liking\\n* You can pick two different color palette for better distribution of the colors.\\n\\n  ![](https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp \\"Effective coloring of the groups is important!\\")\\n\\n  ### Thanks to\\n* [mahnerak](https://github.com/mahnerak), Lars,\xa0[Joe](https://github.com/jafioti)\xa0for detailed feedback — helped us immensely in this iteration.\\n\\n  ### Learn More\\n\\n  We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\\n\\n  Try out\xa0[Aim](https://aimstack.io/), join the\xa0[Aim community](https://slack.aimstack.io/), share your feedback.\\n\\n  We are building the next generation of democratized AI dev tools.\\n\\n  And don’t forget to leave\xa0[Aim](https://aimstack.io/)\xa0a star on GitHub for support \uD83D\uDE4C.","html":"<p><a href=\\"https://aimstack.io/\\">Aim</a>\xa01.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools</p>\\n<p>Check out the new features at\xa0<a href=\\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=\\">play.aimstack.io</a>.</p>\\n<p>Here are the more notable changes:</p>\\n<h3>Enhanced Context Table</h3>\\n<p>The context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).</p>\\n<p>So we have made 3 changes to tackle this problem</p>\\n<h3>New table groups view</h3>\\n<p>Below is the new modified look of the table. Here is what’s changed:</p>\\n<ul>\\n<li>The empty per group is removed</li>\\n<li>In addition to the group details popover, there is an in-place list group config is available for instant lookup</li>\\n</ul>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp\\" alt=\\"\\" title=\\"New improved Aim Context Table on Explore\\"></p>\\n<ul>\\n<li><strong>[In Progress for next release]</strong>\xa0Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.</li>\\n</ul>\\n<h3>Column Management</h3>\\n<p>This feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.</p>\\n<p>You can also re-order the columns by dragging the mid-section up/down.</p>\\n<p>This is super-handy when dealing with 100s of columns on the table for parameters.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp\\" alt=\\"\\" title=\\"Drag columns left and right to pin, search and disable less important ones\\"></p>\\n<h3>Advanced Group Coloring</h3>\\n<p>This is the latest iteration over the group coloring.</p>\\n<ul>\\n<li>\\n<p>Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.</p>\\n</li>\\n<li>\\n<p>You can shuffle the colors in case they aren’t pick to your liking</p>\\n</li>\\n<li>\\n<p>You can pick two different color palette for better distribution of the colors.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp\\" alt=\\"\\" title=\\"Effective coloring of the groups is important!\\"></p>\\n<h3>Thanks to</h3>\\n</li>\\n<li>\\n<p><a href=\\"https://github.com/mahnerak\\">mahnerak</a>, Lars,\xa0<a href=\\"https://github.com/jafioti\\">Joe</a>\xa0for detailed feedback — helped us immensely in this iteration.</p>\\n<h3>Learn More</h3>\\n<p>We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.</p>\\n<p>Try out\xa0<a href=\\"https://aimstack.io/\\">Aim</a>, join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>, share your feedback.</p>\\n<p>We are building the next generation of democratized AI dev tools.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://aimstack.io/\\">Aim</a>\xa0a star on GitHub for support \uD83D\uDE4C.</p>\\n</li>\\n</ul>"},"_id":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","_raw":{"sourceFilePath":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileName":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring"},"type":"Post"},{"title":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","date":"2021-05-18T13:58:29.035Z","author":"Gev Soghomonian","description":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","slug":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)\xa02.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\\n\\nCheck out the updated Aim at\xa0[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\\n\\nFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes\xa0[here](https://github.com/aimhubio/aim/milestone/6?closed=1).\\n\\n## XGBoost Inegration\\n\\n\\n\\nTo begin with, \xa0`aim.callback`\xa0is now available that exports\xa0`AimCallback`\xa0to be passed to the\xa0`xgb.train`\xa0as a callback to log the experiments.\\n\\nCheck out this\xa0[blogpost](https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/)\xa0for additional details on how to integrate Aim to your XGBoost code.\\n\\n## Confidence Interval as the aggregation method\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/confidence_interval.png)\\n\\nNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.\xa0\\n\\n## Learn More\\n\\n\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nCheck out the latest\xa0[Aim integration](https://aimstack.io/aim-v2-2-0-hugging-face-integration/)!\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa02.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.</p>\\n<p>Check out the updated Aim at\xa0<a href=\\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\\">play.aimstack.io</a>.</p>\\n<p>For this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes\xa0<a href=\\"https://github.com/aimhubio/aim/milestone/6?closed=1\\">here</a>.</p>\\n<h2>XGBoost Inegration</h2>\\n<p>To begin with, \xa0<code>aim.callback</code>\xa0is now available that exports\xa0<code>AimCallback</code>\xa0to be passed to the\xa0<code>xgb.train</code>\xa0as a callback to log the experiments.</p>\\n<p>Check out this\xa0<a href=\\"https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/\\">blogpost</a>\xa0for additional details on how to integrate Aim to your XGBoost code.</p>\\n<h2>Confidence Interval as the aggregation method</h2>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/confidence_interval.png\\" alt=\\"\\"></p>\\n<p>Now you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.\xa0</p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Check out the latest\xa0<a href=\\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\\">Aim integration</a>!</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","_raw":{"sourceFilePath":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileName":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\\n\\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\\n\\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\\n\\nModels are usually ran on validation subset after each epoch.\\\\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\\n\\nThere is a need to collect and effectively compare all these metrics.\\n\\nHere is how to do that on\xa0[Aim](https://github.com/aimhubio/aim)\\n\\n# Using context to track for different subsets?\\n\\nUse the\xa0[aim.track](https://github.com/aimhubio/aim#track)\xa0context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\\n\\n![](<script src=\\"https://gist.github.com/SGevorg/e08524b3538d3f71d14bf1857a7bc6e9.js\\"></script> \\"Here is how it looks like on the code\\")\\n\\nOnce the training is ran, execute\xa0`aim up`\xa0in your terminal and start the Aim UI.\\n\\n# Using subplots to compare test, val loss and bleu metrics\\n\\n> **\\\\*Note:**\xa0The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\\n\\nLet’s go step-by-step on how to break down lots of experiments using subplots.\\n\\n**Step 1.**\xa0Explore the runs, the context table, play with the query language.\\n\\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \\"Explore the training runs\\")\\n\\n**Step 2.**\xa0Add the\xa0`bleu`\xa0metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\\n\\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \\"Divide into subplots by metric\\")\\n\\n**Step 3.**\xa0Search by\xa0`context.subset`\xa0to show both\xa0`test`\xa0and\xa0`val`\xa0`loss`\xa0and\xa0`bleu`\xa0metrics. Divide into subplots further by\xa0`context.subset`\xa0too so Aim UI shows\xa0`test`\xa0and\xa0`val`\xa0metrics on different subplots for better comparison.\\n\\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \\"Divide into subplots by context / subset\\")\\n\\nNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\\n\\n# Summary\\n\\nHere is a full summary video on how to do it on the UI.\\n\\n![](https://www.youtube.com/watch?v=DGI8S7SUfEk)\\n\\n# Learn More\\n\\nIf you find Aim useful, support us and\xa0[star the project](https://github.com/aimhubio/aim)\xa0on GitHub. Join the\xa0[Aim community](https://slack.aimstack.io/)\xa0and share more about your use-cases and how we can improve Aim to suit them.","html":"<p>Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.</p>\\n<p>The model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.</p>\\n<p>The validation and test sets are used to test the model on additional unseen data to verify how well it generalise.</p>\\n<p>Models are usually ran on validation subset after each epoch.<br>\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.</p>\\n<p>There is a need to collect and effectively compare all these metrics.</p>\\n<p>Here is how to do that on\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a></p>\\n<h1>Using context to track for different subsets?</h1>\\n<p>Use the\xa0<a href=\\"https://github.com/aimhubio/aim#track\\">aim.track</a>\xa0context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.</p>\\n<p>![]( \\"Here is how it looks like on the code\\")</p>\\n<p>Once the training is ran, execute\xa0<code>aim up</code>\xa0in your terminal and start the Aim UI.</p>\\n<h1>Using subplots to compare test, val loss and bleu metrics</h1>\\n<blockquote>\\n<p><strong>*Note:</strong>\xa0The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*</p>\\n</blockquote>\\n<p>Let’s go step-by-step on how to break down lots of experiments using subplots.</p>\\n<p><strong>Step 1.</strong>\xa0Explore the runs, the context table, play with the query language.</p>\\n<p><img src=\\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\\" alt=\\"\\" title=\\"Explore the training runs\\"></p>\\n<p><strong>Step 2.</strong>\xa0Add the\xa0<code>bleu</code>\xa0metric to the Select input — query both metrics at the same time. Divide into subplots by metric.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\\" alt=\\"\\" title=\\"Divide into subplots by metric\\"></p>\\n<p><strong>Step 3.</strong>\xa0Search by\xa0<code>context.subset</code>\xa0to show both\xa0<code>test</code>\xa0and\xa0<code>val</code>\xa0<code>loss</code>\xa0and\xa0<code>bleu</code>\xa0metrics. Divide into subplots further by\xa0<code>context.subset</code>\xa0too so Aim UI shows\xa0<code>test</code>\xa0and\xa0<code>val</code>\xa0metrics on different subplots for better comparison.</p>\\n<p><img src=\\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\\" alt=\\"\\" title=\\"Divide into subplots by context / subset\\"></p>\\n<p>Not it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.</p>\\n<h1>Summary</h1>\\n<p>Here is a full summary video on how to do it on the UI.</p>\\n<p><img src=\\"https://www.youtube.com/watch?v=DGI8S7SUfEk\\" alt=\\"\\"></p>\\n<h1>Learn More</h1>\\n<p>If you find Aim useful, support us and\xa0<a href=\\"https://github.com/aimhubio/aim\\">star the project</a>\xa0on GitHub. Join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>\xa0and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim v2.2.0 — Hugging Face integration","date":"2021-03-24T13:20:24.937Z","author":"Gev Soghomonian","description":"Aim 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.","slug":"aim-v2-2-0-—-hugging-face-integration","image":"https://aimstack.io/wp-content/uploads/2022/02/HF.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)\xa02.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\\n\\nThanks to\xa0[siddk](https://github.com/siddk),\xa0[TommasoBendinelli](https://github.com/TommasoBendinelli)\xa0and\xa0[Khazhak](https://github.com/Khazhak)\xa0for their contribution to this release.\\n\\n> **Note on the Aim versioning:**\xa0The previous two release posts:\xa0[Aim 1.3.5](https://aimstack.io/blog/new-releases/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)\xa0and\xa0[Aim 1.3.8](https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring/)\xa0had used the version number of\xa0[AimUI](https://aimstack.readthedocs.io/en/latest/ui/overview.html)\xa0as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.\xa0[Check out the Aim CHANGELOG](https://github.com/aimhubio/aim/blob/main/CHANGELOG.md).\\n\\nWe have also added\xa0[milestones](https://github.com/aimhubio/aim/milestones)\xa0for each version. As well as the\xa0[Roadmap](https://github.com/aimhubio/aim#roadmap).\\n\\nCheck out the new features at\xa0[play.aimstack.io](http://play.aimstack.io:43900/dashboard).\\n\\n## Hugging Face integration\\n\\nHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\\n\\n```\\nfrom aim.hugging_face import AimCallback\\nfrom transformers import Trainer\\n\\naim_callback = AimCallback(repo=\'/log/dir/path\', experiment=\'your_experiment_name\')\\n\\ntrainer = Trainer(\\n       model=model,\\n       args=training_args,\\n       callbacks=[aim_callback]\\n    )\\n```\\n\\nHere is how it works:\xa0`aim_callback`\xa0will automatically open an Aim\xa0`Session`\xa0and close it when the trainer is done. When\xa0`trainer.train()`,\xa0`trainer.evaluate()`\xa0or\xa0`trainer.predict()`\xa0is called,\xa0`aim_callback`\xa0will automatically log the hyper-params and respective metrics.\\n\\nFind a full example\xa0[here](https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py).\\n\\n## Metric Visibility Control\\n\\nWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\\n\\nA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo.gif \\"Hide individual metrics as well as collectively\\")\\n\\n## Column resize\\n\\n\\n\\nWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.\xa0Resize is available both on Explore and Dashboard tables.Here is a quick demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo2.gif \\"Drag column edges back-and-forth to resize\\")\\n\\n## Hide columns with similar values\\n\\n\\n\\nMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\\n\\nThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo3.gif \\"Leave only different columns with a button click, if needed customize afterwards\\")\\n\\n## Logscale\\n\\n\\n\\nOne of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo4.gif \\"log-scale on Aim\\")\\n\\n## New methods for aggregation\\n\\n\\n\\nNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo5.gif)\\n\\n## Learn More\\n\\n[Aim is on the mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://slack.aimstack.io/), share your feedback.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support\xa0\uD83D\uDE4C.","html":"<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa02.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.</p>\\n<p>Thanks to\xa0<a href=\\"https://github.com/siddk\\">siddk</a>,\xa0<a href=\\"https://github.com/TommasoBendinelli\\">TommasoBendinelli</a>\xa0and\xa0<a href=\\"https://github.com/Khazhak\\">Khazhak</a>\xa0for their contribution to this release.</p>\\n<blockquote>\\n<p><strong>Note on the Aim versioning:</strong>\xa0The previous two release posts:\xa0<a href=\\"https://aimstack.io/blog/new-releases/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\\">Aim 1.3.5</a>\xa0and\xa0<a href=\\"https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring/\\">Aim 1.3.8</a>\xa0had used the version number of\xa0<a href=\\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\\">AimUI</a>\xa0as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.\xa0<a href=\\"https://github.com/aimhubio/aim/blob/main/CHANGELOG.md\\">Check out the Aim CHANGELOG</a>.</p>\\n</blockquote>\\n<p>We have also added\xa0<a href=\\"https://github.com/aimhubio/aim/milestones\\">milestones</a>\xa0for each version. As well as the\xa0<a href=\\"https://github.com/aimhubio/aim#roadmap\\">Roadmap</a>.</p>\\n<p>Check out the new features at\xa0<a href=\\"http://play.aimstack.io:43900/dashboard\\">play.aimstack.io</a>.</p>\\n<h2>Hugging Face integration</h2>\\n<p>Hugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.</p>\\n<pre><code>from aim.hugging_face import AimCallback\\nfrom transformers import Trainer\\n\\naim_callback = AimCallback(repo=\'/log/dir/path\', experiment=\'your_experiment_name\')\\n\\ntrainer = Trainer(\\n       model=model,\\n       args=training_args,\\n       callbacks=[aim_callback]\\n    )\\n</code></pre>\\n<p>Here is how it works:\xa0<code>aim_callback</code>\xa0will automatically open an Aim\xa0<code>Session</code>\xa0and close it when the trainer is done. When\xa0<code>trainer.train()</code>,\xa0<code>trainer.evaluate()</code>\xa0or\xa0<code>trainer.predict()</code>\xa0is called,\xa0<code>aim_callback</code>\xa0will automatically log the hyper-params and respective metrics.</p>\\n<p>Find a full example\xa0<a href=\\"https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py\\">here</a>.</p>\\n<h2>Metric Visibility Control</h2>\\n<p>When dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.</p>\\n<p>A toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo.gif\\" alt=\\"\\" title=\\"Hide individual metrics as well as collectively\\"></p>\\n<h2>Column resize</h2>\\n<p>With lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.\xa0Resize is available both on Explore and Dashboard tables.Here is a quick demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo2.gif\\" alt=\\"\\" title=\\"Drag column edges back-and-forth to resize\\"></p>\\n<h2>Hide columns with similar values</h2>\\n<p>More often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.</p>\\n<p>This feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo3.gif\\" alt=\\"\\" title=\\"Leave only different columns with a button click, if needed customize afterwards\\"></p>\\n<h2>Logscale</h2>\\n<p>One of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo4.gif\\" alt=\\"\\" title=\\"log-scale on Aim\\"></p>\\n<h2>New methods for aggregation</h2>\\n<p>Now you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo5.gif\\" alt=\\"\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on the mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://slack.aimstack.io/\\">Aim community</a>, share your feedback.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support\xa0\uD83D\uDE4C.</p>"},"_id":"aim-v2-2-0-—-hugging-face-integration.md","_raw":{"sourceFilePath":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileName":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-2-0-—-hugging-face-integration"},"type":"Post"},{"title":"Aim v2.3.0 — System Resource Usage and Reverse Grouping","date":"2021-04-20T13:39:39.516Z","author":"Gev Soghomonian","description":"Aim 2.3.0 allowing System Resource Usage optimization is out! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out...","slug":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping","image":"https://aimstack.io/wp-content/uploads/2021/04/2.3.0-1.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)\xa02.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\\n\\nCheck out the updated Aim at\xa0[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\\n\\nBelow are the highlight features of the update. Find the full list of changes\xa0[here](https://github.com/aimhubio/aim/milestone/3?closed=1).\\n\\n## System Resource Usage\\n\\n\\n\\nNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with\xa0`system_tracking_interval==0`.\\n\\nOnce you run the training, you will see the following. Here is a demo:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/demo-1.gif \\"Aim automatic system resource tracking demo\\")\\n\\nOf course you can also\xa0[query all those new metrics](https://aimstack.io/wp-admin/post.php?post=706&action=edit)\xa0in the Explore page and compare, group, aggregate with the rest of the metrics!\\n\\n## Reverse grouping (“against” the param)\\n\\n\\n\\nAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a\xa0Reverse Grouping. Here is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2021/04/reverse-grouping-1024x566.gif \\"Aim Reverse grouping demo\\")\\n\\nUse the toggle next to switch between grouping modes.\\n\\n## Line Chart Smoothing\\n\\n\\n\\nNow you can apply smoothing on metrics. Here is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/metric-smoothing.gif \\"Aim metric smoothing\\")\\n\\nThere are two type of smoothing options available:\xa0`Exponential Moving Average`\xa0and\xa0`Central Moving Average`. Further info of\xa0[how](https://en.wikipedia.org/wiki/Moving_average)\xa0it’s calculated.\\n\\n## New aggregation modes\\n\\n\\n\\nWe have additionally added two more aggregation modes:\xa0`standard error`\xa0and\xa0`standard deviation`\xa0. Here is how it works:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/aggr-modes.gif)\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa02.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.</p>\\n<p>Check out the updated Aim at\xa0<a href=\\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\\">play.aimstack.io</a>.</p>\\n<p>Below are the highlight features of the update. Find the full list of changes\xa0<a href=\\"https://github.com/aimhubio/aim/milestone/3?closed=1\\">here</a>.</p>\\n<h2>System Resource Usage</h2>\\n<p>Now you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with\xa0<code>system_tracking_interval==0</code>.</p>\\n<p>Once you run the training, you will see the following. Here is a demo:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/demo-1.gif\\" alt=\\"\\" title=\\"Aim automatic system resource tracking demo\\"></p>\\n<p>Of course you can also\xa0<a href=\\"https://aimstack.io/wp-admin/post.php?post=706&#x26;action=edit\\">query all those new metrics</a>\xa0in the Explore page and compare, group, aggregate with the rest of the metrics!</p>\\n<h2>Reverse grouping (“against” the param)</h2>\\n<p>As much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a\xa0Reverse Grouping. Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/04/reverse-grouping-1024x566.gif\\" alt=\\"\\" title=\\"Aim Reverse grouping demo\\"></p>\\n<p>Use the toggle next to switch between grouping modes.</p>\\n<h2>Line Chart Smoothing</h2>\\n<p>Now you can apply smoothing on metrics. Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/metric-smoothing.gif\\" alt=\\"\\" title=\\"Aim metric smoothing\\"></p>\\n<p>There are two type of smoothing options available:\xa0<code>Exponential Moving Average</code>\xa0and\xa0<code>Central Moving Average</code>. Further info of\xa0<a href=\\"https://en.wikipedia.org/wiki/Moving_average\\">how</a>\xa0it’s calculated.</p>\\n<h2>New aggregation modes</h2>\\n<p>We have additionally added two more aggregation modes:\xa0<code>standard error</code>\xa0and\xa0<code>standard deviation</code>\xa0. Here is how it works:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/aggr-modes.gif\\" alt=\\"\\"></p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","_raw":{"sourceFilePath":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileName":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping"},"type":"Post"},{"title":"Aim v2.6.0 — Docker requirement removed and Metric as x-axis","date":"2021-06-24T14:25:34.071Z","author":"Gev Soghomonian","description":"Before and after Aim v2.5.0","slug":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis","image":"https://aimstack.io/wp-content/uploads/2021/06/2.6.0.png","draft":false,"categories":["New Releases"],"body":{"raw":"## Metric as an alternative x-axis\\n\\nSometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.\\n\\nFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared —\xa0`loss`\xa0and`bleu`. One of the main indicators of a performant model in NMT, is when higher values of\xa0`bleu`\xa0correlates with the lower values of\xa0`loss`.\\n\\nJust plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use\xa0`loss`\xa0as the x-axis to directly see the correlation between the\xa0`loss`and the`bleu`\xa0metrics for two groups of runs (\xa0`max_k==3`and\xa0`max_k == 1`).\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/1_Y0wAL9nZ7IhgLOUr-cCRRQ.gif \\"Demo for metric as alternative x-axis\\")\\n\\nUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\\n\\n> **Note**: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\\n\\nTry out\xa0[Aim](https://github.com/aimhubio/aim), join the\xa0[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\\n\\nAnd don’t forget to leave\xa0[Aim](https://github.com/aimhubio/aim)\xa0a star on GitHub for support.","html":"<h2>Metric as an alternative x-axis</h2>\\n<p>Sometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.</p>\\n<p>For instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared —\xa0<code>loss</code>\xa0and<code>bleu</code>. One of the main indicators of a performant model in NMT, is when higher values of\xa0<code>bleu</code>\xa0correlates with the lower values of\xa0<code>loss</code>.</p>\\n<p>Just plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use\xa0<code>loss</code>\xa0as the x-axis to directly see the correlation between the\xa0<code>loss</code>and the<code>bleu</code>\xa0metrics for two groups of runs (\xa0<code>max_k==3</code>and\xa0<code>max_k == 1</code>).</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/1_Y0wAL9nZ7IhgLOUr-cCRRQ.gif\\" alt=\\"\\" title=\\"Demo for metric as alternative x-axis\\"></p>\\n<p>Using a different metric as the x-axis helps to see the correlation between two metrics of training runs better.</p>\\n<blockquote>\\n<p><strong>Note</strong>: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).</p>\\n</blockquote>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.</p>\\n<p>Try out\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>, join the\xa0<a href=\\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\\n<p>And don’t forget to leave\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0a star on GitHub for support.</p>"},"_id":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","_raw":{"sourceFilePath":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileName":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":" Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out […]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger used… What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\\n\\n[Aim](https://github.com/aimhubio/aim)\xa0is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\\n\\n## What is XGBoost?\\n\\n[XGBoost](https://github.com/dmlc/xgboost)\xa0is an optimized gradient boosting library with highly\xa0\xa0*efficient*,\xa0\xa0*flexible,*\xa0 and\xa0\xa0*portable*\xa0design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\\n\\n## How to use Aim with XGBoost?\\n\\nCheck out end-to-end\xa0[Aim integration](https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/)\xa0examples with multiple frameworks\xa0[here](https://github.com/aimhubio/aim/tree/main/examples). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\\n\\n```\\n# You should download and extract the data beforehand. Simply by doing this: \\n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\\n\\nfrom __future__ import division\\n\\nimport numpy as np\\nimport xgboost as xgb\\nfrom aim.xgboost import AimCallback\\n\\n# label need to be 0 to num_class -1\\ndata = np.loadtxt(\'./dermatology.data\', delimiter=\',\',\\n        converters={33: lambda x:int(x == \'?\'), 34: lambda x:int(x) - 1})\\nsz = data.shape\\n\\ntrain = data[:int(sz[0] * 0.7), :]\\ntest = data[int(sz[0] * 0.7):, :]\\n\\ntrain_X = train[:, :33]\\ntrain_Y = train[:, 34]\\n\\ntest_X = test[:, :33]\\ntest_Y = test[:, 34]\\nprint(len(train_X))\\n\\nxg_train = xgb.DMatrix(train_X, label=train_Y)\\nxg_test = xgb.DMatrix(test_X, label=test_Y)\\n# setup parameters for xgboost\\nparam = {}\\n# use softmax multi-class classification\\nparam[\'objective\'] = \'multi:softmax\'\\n# scale weight of positive examples\\nparam[\'eta\'] = 0.1\\nparam[\'max_depth\'] = 6\\nparam[\'nthread\'] = 4\\nparam[\'num_class\'] = 6\\n\\nwatchlist = [(xg_train, \'train\'), (xg_test, \'test\')]\\nnum_round = 50\\nbst = xgb.train(param, xg_train, num_round, watchlist)\\n# get prediction\\npred = bst.predict(xg_test)\\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softmax = {}\'.format(error_rate))\\n\\n# do the same thing again, but output probabilities\\nparam[\'objective\'] = \'multi:softprob\'\\nbst = xgb.train(param, xg_train, num_round, watchlist, \\n                callbacks=[AimCallback(repo=\'.\', experiment=\'xgboost_test\')])\\n# Note: this convention has been changed since xgboost-unity\\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\\npred_label = np.argmax(pred_prob, axis=1)\\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softprob = {}\'.format(error_rate))\\n```\\n\\nAs you can see on line 49,\xa0AimCallback\xa0is imported from\xa0`aim.xgboost`\xa0and passed to\xa0`xgb.train`\xa0as one of the callbacks. Aim session can open and close by the AimCallback and the\xa0metrics and hparamsstore by XGBoost. In addition to that, thesystem measures\xa0pass to Aim as well.\\n\\n## What it looks like?\\n\\nAfter you run the experiment and the\xa0`aim up`\xa0command in the\xa0`aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/05/dashboard.png \\"Aim UI dashboard page\\")\\n\\nTo explore the run, we should:\\n\\n* Choose the\xa0`xgboost_test`experiment.\\n* Select the metrics to explore.\\n* Divide into charts by metrics.\\n\\nFor example, the gif below illustrates the steps above.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/05/1.gif)\\n\\n\\\\\\n So this is what the final result looks like.\\n\\n![](https://aimstack.io/wp-content/uploads/2021/05/2.png)\\n\\nIn short, we can easily analyze the runs and the system usage.\\n\\n## Learn More\\n\\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\\n\\nIf you find Aim useful, support us and star\xa0[the project](https://github.com/aimhubio/aim)\xa0on GitHub. Also, join the\xa0[Aim community](https://aimstack.slack.com/ssb/redirect)\xa0and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is Aim?</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim\\">Aim</a>\xa0is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.</p>\\n<h2>What is XGBoost?</h2>\\n<p><a href=\\"https://github.com/dmlc/xgboost\\">XGBoost</a>\xa0is an optimized gradient boosting library with highly\xa0\xa0<em>efficient</em>,\xa0\xa0<em>flexible,</em>\xa0 and\xa0\xa0<em>portable</em>\xa0design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.</p>\\n<h2>How to use Aim with XGBoost?</h2>\\n<p>Check out end-to-end\xa0<a href=\\"https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/\\">Aim integration</a>\xa0examples with multiple frameworks\xa0<a href=\\"https://github.com/aimhubio/aim/tree/main/examples\\">here</a>. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.</p>\\n<pre><code># You should download and extract the data beforehand. Simply by doing this: \\n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\\n\\nfrom __future__ import division\\n\\nimport numpy as np\\nimport xgboost as xgb\\nfrom aim.xgboost import AimCallback\\n\\n# label need to be 0 to num_class -1\\ndata = np.loadtxt(\'./dermatology.data\', delimiter=\',\',\\n        converters={33: lambda x:int(x == \'?\'), 34: lambda x:int(x) - 1})\\nsz = data.shape\\n\\ntrain = data[:int(sz[0] * 0.7), :]\\ntest = data[int(sz[0] * 0.7):, :]\\n\\ntrain_X = train[:, :33]\\ntrain_Y = train[:, 34]\\n\\ntest_X = test[:, :33]\\ntest_Y = test[:, 34]\\nprint(len(train_X))\\n\\nxg_train = xgb.DMatrix(train_X, label=train_Y)\\nxg_test = xgb.DMatrix(test_X, label=test_Y)\\n# setup parameters for xgboost\\nparam = {}\\n# use softmax multi-class classification\\nparam[\'objective\'] = \'multi:softmax\'\\n# scale weight of positive examples\\nparam[\'eta\'] = 0.1\\nparam[\'max_depth\'] = 6\\nparam[\'nthread\'] = 4\\nparam[\'num_class\'] = 6\\n\\nwatchlist = [(xg_train, \'train\'), (xg_test, \'test\')]\\nnum_round = 50\\nbst = xgb.train(param, xg_train, num_round, watchlist)\\n# get prediction\\npred = bst.predict(xg_test)\\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softmax = {}\'.format(error_rate))\\n\\n# do the same thing again, but output probabilities\\nparam[\'objective\'] = \'multi:softprob\'\\nbst = xgb.train(param, xg_train, num_round, watchlist, \\n                callbacks=[AimCallback(repo=\'.\', experiment=\'xgboost_test\')])\\n# Note: this convention has been changed since xgboost-unity\\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\\npred_label = np.argmax(pred_prob, axis=1)\\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\\nprint(\'Test error using softprob = {}\'.format(error_rate))\\n</code></pre>\\n<p>As you can see on line 49,\xa0AimCallback\xa0is imported from\xa0<code>aim.xgboost</code>\xa0and passed to\xa0<code>xgb.train</code>\xa0as one of the callbacks. Aim session can open and close by the AimCallback and the\xa0metrics and hparamsstore by XGBoost. In addition to that, thesystem measures\xa0pass to Aim as well.</p>\\n<h2>What it looks like?</h2>\\n<p>After you run the experiment and the\xa0<code>aim up</code>\xa0command in the\xa0<code>aim_logs</code>directory, Aim UI will be running. When first opened, the dashboard page will come up.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/05/dashboard.png\\" alt=\\"\\" title=\\"Aim UI dashboard page\\"></p>\\n<p>To explore the run, we should:</p>\\n<ul>\\n<li>Choose the\xa0<code>xgboost_test</code>experiment.</li>\\n<li>Select the metrics to explore.</li>\\n<li>Divide into charts by metrics.</li>\\n</ul>\\n<p>For example, the gif below illustrates the steps above.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/05/1.gif\\" alt=\\"\\"></p>\\n<p><br>\\nSo this is what the final result looks like.</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2021/05/2.png\\" alt=\\"\\"></p>\\n<p>In short, we can easily analyze the runs and the system usage.</p>\\n<h2>Learn More</h2>\\n<p><a href=\\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\\">Aim is on a mission to democratize AI dev tools.</a></p>\\n<p>If you find Aim useful, support us and star\xa0<a href=\\"https://github.com/aimhubio/aim\\">the project</a>\xa0on GitHub. Also, join the\xa0<a href=\\"https://aimstack.slack.com/ssb/redirect\\">Aim community</a>\xa0and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://aimstack.io/wp-content/uploads/2021/03/pytorch.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\\n\\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\\n\\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\\n\\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.\xa0One thing you do not need when tuning hyper-parameters is variance.\\n\\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\\n\\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using\xa0[Aim](https://github.com/aimhubio/aim).\\n\\n## **How to fix the seed in PyTorch Lightning**\\n\\n\\n\\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\\n\\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use\xa0`pl.seed_everything(seed)`\xa0. See the docs\xa0[here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\\n\\n> Note: in other libraries you would use something like:\xa0`np.random.seed()`\xa0or\xa0`torch.manual_seed()`\xa0\\n\\n## **Implementation**\\n\\n\\n\\nFind the full code for this and other tutorials\xa0[here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\\n\\n```\\nimport torch\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import CIFAR10\\nfrom torchvision.models import resnet18\\nfrom torchvision import transforms\\nimport pytorch_lightning as pl\\nfrom aim.pytorch_lightning import AimLogger\\n\\nclass ImageClassifierModel(pl.LightningModule):\\n\\n    def __init__(self, seed, lr, optimizer):\\n        super(ImageClassifierModel, self).__init__()\\n        pl.seed_everything(seed)\\n        # This fixes a seed for all the modules used by pytorch-lightning\\n        # Note: using random.seed(seed) is not enough, there are multiple\\n        # other seeds like hash seed, seed for numpy etc.\\n        self.lr = lr\\n        self.optimizer = optimizer\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        self.model = resnet18(pretrained = True, progress = True)\\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\\n        # changing the last layer from 1000 out_features to 10 because the model\\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\\n        self.model.to(\'cuda\') # moving the model to cuda\\n\\n    def train_dataloader(self):\\n        # makes the training dataloader\\n        train_ds = CIFAR10(\'.\', train = True, transform = transforms.ToTensor(), download = True)\\n        train_loader = DataLoader(train_ds, batch_size=32)\\n        return train_loader\\n        \\n    def val_dataloader(self):\\n        # makes the validation dataloader\\n        val_ds = CIFAR10(\'.\', train = False, transform = transforms.ToTensor(), download = True)\\n        val_loader = DataLoader(val_ds, batch_size=32)\\n        return val_loader\\n\\n    def forward(self, x):\\n        return self.model.forward(x)\\n\\n    def training_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.log(\'train_loss\', loss)\\n        # logging the loss with \\"train_\\" prefix\\n        return loss\\n\\n    def validation_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.total_classified += y.shape[0]\\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\\n        # Calculating total and correctly classified images to determine the accuracy later\\n        self.log(\'val_loss\', loss) \\n        # logging the loss with \\"val_\\" prefix\\n        return loss\\n\\n    def validation_epoch_end(self, results):\\n        accuracy = self.correctly_classified / self.total_classified\\n        self.log(\'val_accuracy\', accuracy)\\n        # logging accuracy\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        return accuracy\\n\\n    def configure_optimizers(self):\\n        # Choose an optimizer and set up a learning rate according to hyperparameters\\n        if self.optimizer == \'Adam\':\\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\\n        elif self.optimizer == \'SGD\':\\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\\n        else:\\n            raise NotImplementedError\\n\\nif __name__ == \\"__main__\\":\\n\\n    seeds = [47, 881, 123456789]\\n    lrs = [0.1, 0.01]\\n    optimizers = [\'SGD\', \'Adam\']\\n\\n    for seed in seeds:\\n        for optimizer in optimizers:\\n            for lr in lrs:\\n                # choosing one set of hyperparaameters from the ones above\\n            \\n                model = ImageClassifierModel(seed, lr, optimizer)\\n                # initializing the model we will train with the chosen hyperparameters\\n\\n                aim_logger = AimLogger(\\n                    experiment=\'resnet18_classification\',\\n                    train_metric_prefix=\'train_\',\\n                    val_metric_prefix=\'val_\',\\n                )\\n                aim_logger.log_hyperparams({\\n                    \'lr\': lr,\\n                    \'optimizer\': optimizer,\\n                    \'seed\': seed\\n                })\\n                # initializing the aim logger and logging the hyperparameters\\n\\n                trainer = pl.Trainer(\\n                    logger=aim_logger,\\n                    gpus=1,\\n                    max_epochs=5,\\n                    progress_bar_refresh_rate=1,\\n                    log_every_n_steps=10,\\n                    check_val_every_n_epoch=1)\\n                # making the pytorch-lightning trainer\\n\\n                trainer.fit(model)\\n                # training the model\\n```\\n\\n\\n\\n## Analyzing the Training Runs\\n\\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after\xa0[Aim 1.3.5 release](https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)) and have multiple charts of different metrics on the same screen.\\n\\nDo the following steps to see the different effects of the optimizers\\n\\n* Go to dashboard, explore by experiment\\n* Add loss to\xa0`SELECT`\xa0and divide into subplots by metric\\n* Group by experiment to make all metrics of similar color\\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\\n\\nHere is how it looks on Aim:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/10.gif)\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/11.png)\\n\\n\\n\\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\\n\\nIf you apply the same settings to the learning rate, this is the result:\\n\\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\\n\\n\\n\\n* Remove both previous groupings\\n* Group by color by learning rate\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/12.gif)\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/13.png)\\n\\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\\n\\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\\n\\nOn top of this, if we also add grouping by style by optimizer:\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/24.gif)\\n\\n![](https://aimstack.io/wp-content/uploads/2022/02/25.png)\\n\\nNow, it is obvious that the the runs with SGD optimizer and\xa0`lr=0.01`\xa0(green, broken lines) are the best choices for all the seeds we have tried.\\n\\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\\n\\n## Learn More\\n\\n\\n\\nIf you find Aim useful, support us and\xa0[star the project](https://github.com/aimhubio/aim)\xa0on GitHub. Join the\xa0[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect)\xa0and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is a random seed and how is it important?</h2>\\n<p>The random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:</p>\\n<ul>\\n<li>Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.</li>\\n<li>Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.</li>\\n<li>Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).</li>\\n</ul>\\n<p>As you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.\xa0One thing you do not need when tuning hyper-parameters is variance.</p>\\n<p>The purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.</p>\\n<p>Later in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using\xa0<a href=\\"https://github.com/aimhubio/aim\\">Aim</a>.</p>\\n<h2><strong>How to fix the seed in PyTorch Lightning</strong></h2>\\n<p>Fixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python<code>random.seed(seed)</code>, but it is not enough for PL.</p>\\n<p>Pytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use\xa0<code>pl.seed_everything(seed)</code>\xa0. See the docs\xa0<a href=\\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\\">here</a>.</p>\\n<blockquote>\\n<p>Note: in other libraries you would use something like:\xa0<code>np.random.seed()</code>\xa0or\xa0<code>torch.manual_seed()</code>\xa0</p>\\n</blockquote>\\n<h2><strong>Implementation</strong></h2>\\n<p>Find the full code for this and other tutorials\xa0<a href=\\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\\">here</a>.</p>\\n<pre><code>import torch\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import CIFAR10\\nfrom torchvision.models import resnet18\\nfrom torchvision import transforms\\nimport pytorch_lightning as pl\\nfrom aim.pytorch_lightning import AimLogger\\n\\nclass ImageClassifierModel(pl.LightningModule):\\n\\n    def __init__(self, seed, lr, optimizer):\\n        super(ImageClassifierModel, self).__init__()\\n        pl.seed_everything(seed)\\n        # This fixes a seed for all the modules used by pytorch-lightning\\n        # Note: using random.seed(seed) is not enough, there are multiple\\n        # other seeds like hash seed, seed for numpy etc.\\n        self.lr = lr\\n        self.optimizer = optimizer\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        self.model = resnet18(pretrained = True, progress = True)\\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\\n        # changing the last layer from 1000 out_features to 10 because the model\\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\\n        self.model.to(\'cuda\') # moving the model to cuda\\n\\n    def train_dataloader(self):\\n        # makes the training dataloader\\n        train_ds = CIFAR10(\'.\', train = True, transform = transforms.ToTensor(), download = True)\\n        train_loader = DataLoader(train_ds, batch_size=32)\\n        return train_loader\\n        \\n    def val_dataloader(self):\\n        # makes the validation dataloader\\n        val_ds = CIFAR10(\'.\', train = False, transform = transforms.ToTensor(), download = True)\\n        val_loader = DataLoader(val_ds, batch_size=32)\\n        return val_loader\\n\\n    def forward(self, x):\\n        return self.model.forward(x)\\n\\n    def training_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.log(\'train_loss\', loss)\\n        # logging the loss with \\"train_\\" prefix\\n        return loss\\n\\n    def validation_step(self, batch, batch_nb):\\n        x, y = batch\\n        y_hat = self.model(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        # calculating the cross entropy loss on the result\\n        self.total_classified += y.shape[0]\\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\\n        # Calculating total and correctly classified images to determine the accuracy later\\n        self.log(\'val_loss\', loss) \\n        # logging the loss with \\"val_\\" prefix\\n        return loss\\n\\n    def validation_epoch_end(self, results):\\n        accuracy = self.correctly_classified / self.total_classified\\n        self.log(\'val_accuracy\', accuracy)\\n        # logging accuracy\\n        self.total_classified = 0\\n        self.correctly_classified = 0\\n        return accuracy\\n\\n    def configure_optimizers(self):\\n        # Choose an optimizer and set up a learning rate according to hyperparameters\\n        if self.optimizer == \'Adam\':\\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\\n        elif self.optimizer == \'SGD\':\\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\\n        else:\\n            raise NotImplementedError\\n\\nif __name__ == \\"__main__\\":\\n\\n    seeds = [47, 881, 123456789]\\n    lrs = [0.1, 0.01]\\n    optimizers = [\'SGD\', \'Adam\']\\n\\n    for seed in seeds:\\n        for optimizer in optimizers:\\n            for lr in lrs:\\n                # choosing one set of hyperparaameters from the ones above\\n            \\n                model = ImageClassifierModel(seed, lr, optimizer)\\n                # initializing the model we will train with the chosen hyperparameters\\n\\n                aim_logger = AimLogger(\\n                    experiment=\'resnet18_classification\',\\n                    train_metric_prefix=\'train_\',\\n                    val_metric_prefix=\'val_\',\\n                )\\n                aim_logger.log_hyperparams({\\n                    \'lr\': lr,\\n                    \'optimizer\': optimizer,\\n                    \'seed\': seed\\n                })\\n                # initializing the aim logger and logging the hyperparameters\\n\\n                trainer = pl.Trainer(\\n                    logger=aim_logger,\\n                    gpus=1,\\n                    max_epochs=5,\\n                    progress_bar_refresh_rate=1,\\n                    log_every_n_steps=10,\\n                    check_val_every_n_epoch=1)\\n                # making the pytorch-lightning trainer\\n\\n                trainer.fit(model)\\n                # training the model\\n</code></pre>\\n<h2>Analyzing the Training Runs</h2>\\n<p>After each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after\xa0<a href=\\"https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\\">Aim 1.3.5 release</a>) and have multiple charts of different metrics on the same screen.</p>\\n<p>Do the following steps to see the different effects of the optimizers</p>\\n<ul>\\n<li>Go to dashboard, explore by experiment</li>\\n<li>Add loss to\xa0<code>SELECT</code>\xa0and divide into subplots by metric</li>\\n<li>Group by experiment to make all metrics of similar color</li>\\n<li>Group by style by optimizer to see different optimizers on loss and accuracy and its effects</li>\\n</ul>\\n<p>Here is how it looks on Aim:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/10.gif\\" alt=\\"\\"></p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/11.png\\" alt=\\"\\"></p>\\n<p>From the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.</p>\\n<p>If you apply the same settings to the learning rate, this is the result:</p>\\n<h2>For the next step to analyze how learning rate affects the experiments, do the following steps:</h2>\\n<ul>\\n<li>Remove both previous groupings</li>\\n<li>Group by color by learning rate</li>\\n</ul>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/12.gif\\" alt=\\"\\"></p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/13.png\\" alt=\\"\\"></p>\\n<p>As you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.</p>\\n<p>We showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.</p>\\n<p>On top of this, if we also add grouping by style by optimizer:</p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/24.gif\\" alt=\\"\\"></p>\\n<p><img src=\\"https://aimstack.io/wp-content/uploads/2022/02/25.png\\" alt=\\"\\"></p>\\n<p>Now, it is obvious that the the runs with SGD optimizer and\xa0<code>lr=0.01</code>\xa0(green, broken lines) are the best choices for all the seeds we have tried.</p>\\n<p>Fixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.</p>\\n<h2>Learn More</h2>\\n<p>If you find Aim useful, support us and\xa0<a href=\\"https://github.com/aimhubio/aim\\">star the project</a>\xa0on GitHub. Join the\xa0<a href=\\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\\">Aim community</a>\xa0and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"}]');[..._index_namespaceObject];var Pagnation=__webpack_require__(5065),next_router=__webpack_require__(1163),blog={title:"MLOps Blog | AimStack",description:"AimStack",image:"",path:"blog"};function Blog(){let router=(0,next_router.useRouter)(),params=router.query,pathname=router.pathname,page=Number(params.page)||1,posts=_index_namespaceObject.map(post=>(0,dist.ei)(post,["title","date","slug","description","draft","image","categories"])),totalPostCount=(0,utils.mC)(_index_namespaceObject.length),totalPosts=(0,react.useCallback)(()=>(0,utils.eU)(posts,page),[page,posts]);return(0,jsx_runtime.jsxs)(jsx_runtime.Fragment,{children:[(0,jsx_runtime.jsx)(SEO.Z,{...blog}),(0,jsx_runtime.jsxs)(foundations.W2,{children:[(0,jsx_runtime.jsx)(foundations.xv,{as:"h1",size:6,className:"title",css:{textAlign:"center",my:"$10"},children:"Recent Articles"}),(0,jsx_runtime.jsx)(BlogList.Z,{blogList:totalPosts()}),(0,jsx_runtime.jsx)(Pagnation.Z,{pathname:pathname,currentPage:page,totalPostCount:totalPostCount})]})]})}},558:function(__unused_webpack_module,__webpack_exports__,__webpack_require__){"use strict";__webpack_require__.d(__webpack_exports__,{Bi:function(){return formattedDate},Qs:function(){return titleCase},eU:function(){return getTotalPosts},ef:function(){return ImageUrl},mC:function(){return pageCount}});var _config__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(3346);function sortByDate(a,b){return new Date(b.date)-new Date(a.date)}function pageCount(number){return Math.ceil(number/_config__WEBPACK_IMPORTED_MODULE_0__.ou)}function ImageUrl(url){return _config__WEBPACK_IMPORTED_MODULE_0__.ZP+url}function titleCase(str){return str.replace(/(^\w)/g,g=>g[0].toUpperCase()).replace(/([-_]\w)/g,g=>" "+g[1].toUpperCase()).trim()}function formattedDate(date){return new Date(date).toLocaleDateString("en-US",{day:"numeric",month:"short",year:"numeric"})}function getTotalPosts(posts,page){let postSortByDate=posts.sort(sortByDate),publish=postSortByDate.filter((post,i)=>!post.draft),totalPosts=publish.slice(0,_config__WEBPACK_IMPORTED_MODULE_0__.ou);return 2===page&&(totalPosts=publish.slice(_config__WEBPACK_IMPORTED_MODULE_0__.ou,_config__WEBPACK_IMPORTED_MODULE_0__.ou*page)),page>2&&(totalPosts=publish.slice(_config__WEBPACK_IMPORTED_MODULE_0__.ou*page-_config__WEBPACK_IMPORTED_MODULE_0__.ou,_config__WEBPACK_IMPORTED_MODULE_0__.ou*page)),totalPosts}}},function(__webpack_require__){__webpack_require__.O(0,[962,774,888,179],function(){return __webpack_require__(__webpack_require__.s=3986)}),_N_E=__webpack_require__.O()}]);