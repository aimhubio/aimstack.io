{"pageProps":{"posts":[{"title":"LangChain + Aim: Building and Debugging AI Systems Made EASY!","date":"2023-04-06T17:51:14.014Z","author":"Gor Arakelyan","description":"As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial. Use Aim to easily trace complex AI systems built with LangChain.","slug":"langchain-aim-building-and-debugging-ai-systems-made-easy","image":"/images/dynamic/langchain_header.jpg","draft":false,"categories":["Integrations"],"body":{"raw":"\n\n# The Rise of Complex AI Systems\n\nWith the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.\n\nBut the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.\n\n**As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.** Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.\n\nIn this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:\n\n* track all inputs and outputs of chains,\n* visualize and explore individual chains,\n* compare several chains side-by-side.\n\n# LangChain: Building AI Systems with LLMs\n\nLangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.\n\nIt provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!\n\n# **Aim:** Upgraded Debugging Experience for AI Systems\n\nMonitoring and debugging AI systems requires more than just scanning output logs on a terminal.\n\n**Introducing Aim!**\n\nAim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.\n\nImportantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.\n\nIn that way, Aim makes debugging, monitoring, comparing different executions a breeze.\n\n**Experience the ultimate control with Aim!**\n\nCheck out Aim on GitHub: **[github.com/aimhubio/aim](http://github.com/aimhubio/aim)**\n\n![](/images/dynamic/explorer.jpg)\n\n# Aim + LangChain = üöÄ\n\nWith the release of LangChain **[v0.0.127](https://github.com/hwchase17/langchain/releases/tag/v0.0.127)**, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! **All you need to do is configure the Aim callback and run your executions as usual.**\n\nAim does the rest for you by tracking tools and LLMs‚Äô inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.\n\n![](/images/dynamic/langchain_aim.jpg)\n\nLet's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.\n\n# Hands-On Example: Building a Multi-Task AI Agent\n\n## Setting up the agent and the Aim callback\n\nLet‚Äôs build an agent equipped with two tools:\n\n* the SerpApi tool to access Google search results,\n* the LLM-math tool to perform required mathematical operations.\n\nIn this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:\n\n```python\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n```\n\nNow that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.\n\n```python\nfrom langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n```\n\n> **Aim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.**\n\nFind the full script and more examples in the official LangChain docs: [](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html)**<https://python.langchain.com/en/latest/ecosystem/aim_tracking.html>**\n\n## **Executing the agent and running Aim**\n\nBefore executing the agent, ensure that Aim is installed by executing the following command:\n\n```shell\npip install aim\n```\n\nNow, let's run multiple executions and launch the Aim UI to visualize and explore the results:\n\n1. execute the script by running `python example.py`,\n2. then, start the UI with `aim up` command.\n\nWith the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.\n\n## Exploring executions via Aim\n\n### Home page\n\nOn the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.\n\n![](/images/dynamic/home.jpg)\n\n\n\n### Deep dive into a single execution\n\nWhen navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:\n\n* CLI command and arguments,\n* Environment variables,\n* Packages,\n* Git information,\n* System resource usage,\n* and other relevant information about an individual execution.\n\n![](/images/dynamic/individual_exec.jpg)\n\nAim automatically captures terminal outputs during execution. Access these logs in the ‚ÄúLogs‚Äù tab to easily keep track of the progress of your AI system and identify issues.\n\n![](/images/dynamic/logs.jpg)\n\nIn the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.\n\n![](/images/dynamic/text_tab.jpg)\n\nWith Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.\n\n![](/images/dynamic/explorer.jpg)\n\nFor instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.\n\n**With Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.**\n\n# Wrapping Up\n\nIn conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!\n\n# Learn more\n\nCheck out the Aim + LangChain integration docs¬†[here](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html).\n\nLangChain repo:¬†[](https://github.com/hwchase17/langchain)<https://github.com/hwchase17/langchain>\n\nAim repo:¬†[](https://github.com/aimhubio/aim)<https://github.com/aimhubio/aim>\n\nIf you have questions, join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\n\nDrop a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<h1>The Rise of Complex AI Systems</h1>\n<p>With the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.</p>\n<p>But the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.</p>\n<p><strong>As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.</strong> Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.</p>\n<p>In this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:</p>\n<ul>\n<li>track all inputs and outputs of chains,</li>\n<li>visualize and explore individual chains,</li>\n<li>compare several chains side-by-side.</li>\n</ul>\n<h1>LangChain: Building AI Systems with LLMs</h1>\n<p>LangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.</p>\n<p>It provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!</p>\n<h1><strong>Aim:</strong> Upgraded Debugging Experience for AI Systems</h1>\n<p>Monitoring and debugging AI systems requires more than just scanning output logs on a terminal.</p>\n<p><strong>Introducing Aim!</strong></p>\n<p>Aim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.</p>\n<p>Importantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.</p>\n<p>In that way, Aim makes debugging, monitoring, comparing different executions a breeze.</p>\n<p><strong>Experience the ultimate control with Aim!</strong></p>\n<p>Check out Aim on GitHub: <strong><a href=\"http://github.com/aimhubio/aim\">github.com/aimhubio/aim</a></strong></p>\n<p><img src=\"/images/dynamic/explorer.jpg\" alt=\"\"></p>\n<h1>Aim + LangChain = üöÄ</h1>\n<p>With the release of LangChain <strong><a href=\"https://github.com/hwchase17/langchain/releases/tag/v0.0.127\">v0.0.127</a></strong>, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! <strong>All you need to do is configure the Aim callback and run your executions as usual.</strong></p>\n<p>Aim does the rest for you by tracking tools and LLMs‚Äô inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.</p>\n<p><img src=\"/images/dynamic/langchain_aim.jpg\" alt=\"\"></p>\n<p>Let's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.</p>\n<h1>Hands-On Example: Building a Multi-Task AI Agent</h1>\n<h2>Setting up the agent and the Aim callback</h2>\n<p>Let‚Äôs build an agent equipped with two tools:</p>\n<ul>\n<li>the SerpApi tool to access Google search results,</li>\n<li>the LLM-math tool to perform required mathematical operations.</li>\n</ul>\n<p>In this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:</p>\n<pre><code class=\"language-python\">tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n</code></pre>\n<p>Now that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.</p>\n<pre><code class=\"language-python\">from langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n</code></pre>\n<blockquote>\n<p><strong>Aim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.</strong></p>\n</blockquote>\n<p>Find the full script and more examples in the official LangChain docs: <a href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"></a><strong><a href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\">https://python.langchain.com/en/latest/ecosystem/aim_tracking.html</a></strong></p>\n<h2><strong>Executing the agent and running Aim</strong></h2>\n<p>Before executing the agent, ensure that Aim is installed by executing the following command:</p>\n<pre><code class=\"language-shell\">pip install aim\n</code></pre>\n<p>Now, let's run multiple executions and launch the Aim UI to visualize and explore the results:</p>\n<ol>\n<li>execute the script by running <code>python example.py</code>,</li>\n<li>then, start the UI with <code>aim up</code> command.</li>\n</ol>\n<p>With the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.</p>\n<h2>Exploring executions via Aim</h2>\n<h3>Home page</h3>\n<p>On the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.</p>\n<p><img src=\"/images/dynamic/home.jpg\" alt=\"\"></p>\n<h3>Deep dive into a single execution</h3>\n<p>When navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:</p>\n<ul>\n<li>CLI command and arguments,</li>\n<li>Environment variables,</li>\n<li>Packages,</li>\n<li>Git information,</li>\n<li>System resource usage,</li>\n<li>and other relevant information about an individual execution.</li>\n</ul>\n<p><img src=\"/images/dynamic/individual_exec.jpg\" alt=\"\"></p>\n<p>Aim automatically captures terminal outputs during execution. Access these logs in the ‚ÄúLogs‚Äù tab to easily keep track of the progress of your AI system and identify issues.</p>\n<p><img src=\"/images/dynamic/logs.jpg\" alt=\"\"></p>\n<p>In the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.</p>\n<p><img src=\"/images/dynamic/text_tab.jpg\" alt=\"\"></p>\n<p>With Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.</p>\n<p><img src=\"/images/dynamic/explorer.jpg\" alt=\"\"></p>\n<p>For instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.</p>\n<p><strong>With Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.</strong></p>\n<h1>Wrapping Up</h1>\n<p>In conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!</p>\n<h1>Learn more</h1>\n<p>Check out the Aim + LangChain integration docs¬†<a href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\">here</a>.</p>\n<p>LangChain repo:¬†<a href=\"https://github.com/hwchase17/langchain\"></a><a href=\"https://github.com/hwchase17/langchain\">https://github.com/hwchase17/langchain</a></p>\n<p>Aim repo:¬†<a href=\"https://github.com/aimhubio/aim\"></a><a href=\"https://github.com/aimhubio/aim\">https://github.com/aimhubio/aim</a></p>\n<p>If you have questions, join the¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå</p>\n<p>Drop a ‚≠êÔ∏è on¬†<a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","_raw":{"sourceFilePath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileName":"langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy"},"type":"Post"},{"title":"Launching Aim on Hugging Face Spaces","date":"2023-04-19T13:41:34.213Z","author":"Gor Arakelyan","description":"Deploy Aim on Hugging Face Spaces within seconds using the Docker template!","slug":"launching-aim-on-hugging-face-spaces","image":"/images/dynamic/hf_space_header.png","draft":false,"categories":["Integrations"],"body":{"raw":"We are excited to announce the launch of Aim on Hugging Face Spaces! üöÄ\n\nWith just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.\n\n![](/images/dynamic/hf_space_me.png)\n\nAim is an open-source, self-hosted AI Metadata tracking tool.\\\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata ‚Äî perfect for automations and Jupyter Notebook analysis.\n\nIn this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Let‚Äôs dive in and get started!\n\nLearn more about Aim on the GitHub repository: **[github.com/aimhubio/aim](https://github.com/aimhubio/aim)**\n\n## Deploy Aim on Hugging Face Spaces within seconds using the Docker template\n\nTo get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the ‚ÄúCreate new Space‚Äù button, or open the page directly by the following link: [](https://huggingface.co/new-space?template=aimstack/aim)**<https://huggingface.co/new-space?template=aimstack/aim>**\n\n![](/images/dynamic/hf_space_deploy.png)\n\nSet up your Aim Space in no time:\n\n1. Choose a name for your Space.\n2. Adjust Space hardware and the visibility mode.\n3. Submit your Space!\n\nAfter submitting the Space, you'll be able to monitor its progress through the building status:\n\n![](/images/dynamic/hf_space_building.png)\n\nOnce it transitions to ‚ÄúRunning‚Äù, your space is ready to go!\n\n![](/images/dynamic/hf_space_running.png)\n\n**Ta-da! üéâ You're all set to start using Aim on Hugging Face.**\n\nBy pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:\n\n1. Run the following bash command to compress `.aim` directory:\n\n   ```shell\n   tar -czvf aim_repo.tar.gz .aim\n   ```\n2. Commit and push files to your Space.\n\nThat's it! Now open the App section of your Space, and Aim will display your training logs.\n\nUpdating Spaces is incredibly convenient ‚Äì you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. üî•\n\n## See Aim in Action with Existing Demos on the Hub\n\nLet‚Äôs explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.\n\n* Neural machine translation task: [](https://huggingface.co/spaces/aimstack/nmt)<https://huggingface.co/spaces/aimstack/nmt>\n* Simple handwritten digits recognition task: [](https://huggingface.co/spaces/aimstack/digit-recognition)<https://huggingface.co/spaces/aimstack/digit-recognition>\n* Image generation task with lightweight GAN implementation: [](https://huggingface.co/spaces/aimstack/image-generation)<https://huggingface.co/spaces/aimstack/image-generation>\n\n![](/images/dynamic/hf_space_demos.png)\n\nWhen navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.\n\n![Aim homepage](/images/dynamic/hf_space_overview.png \"Aim homepage\")\n\nOpen the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.\n\n![Individual run page](/images/dynamic/hf_space_individual_run.png \"Individual run page\")\n\nTake your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.\n\nMetrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.\n\n![Metrics Explorer](/images/dynamic/hf_space_me.png \"Metrics Explorer\")\n\nExplorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.\n\n![Pythonic search](/images/dynamic/hf_space_pythonic_search.png \"Pythonic search\")\n\nIn addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.\n\n![Images Explorer](/images/dynamic/hf_space_media.png \"Images Explorer\")\n\nUse Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!\n\n## One more thing‚Ä¶ üëÄ\n\nHaving Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.\n\nTo embed your Space, construct the following link based on Space owner and Space name: **`https://owner-space-name.hf.space`**. This link can be used to embed your Space in any website or notebook using the following HTML code:\n\n```html\n%%html\n<iframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n>\n</iframe>\n```\n\n## Next steps\n\nWe are going to continuously iterate over Aim Space onboarding and usability, including:\n\n* the ability to read logs directly from Hugging Face Hub model repos,\n* automatic conversion of TensorBoard logs to Aim format,\n* Aim HF Space-specific onboarding steps.\n\nMuch more coming soon... stay tuned for the updates!\n\n## Learn more\n\nCheck out Aim Space documentation [here](https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html)\n\nAim repo on GitHub: [github.com/aimhubio/aim](http://github.com/aimhubio/aim)\n\nIf you have questions, join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\n\nDrop a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<p>We are excited to announce the launch of Aim on Hugging Face Spaces! üöÄ</p>\n<p>With just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.</p>\n<p><img src=\"/images/dynamic/hf_space_me.png\" alt=\"\"></p>\n<p>Aim is an open-source, self-hosted AI Metadata tracking tool.<br>\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata ‚Äî perfect for automations and Jupyter Notebook analysis.</p>\n<p>In this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Let‚Äôs dive in and get started!</p>\n<p>Learn more about Aim on the GitHub repository: <strong><a href=\"https://github.com/aimhubio/aim\">github.com/aimhubio/aim</a></strong></p>\n<h2>Deploy Aim on Hugging Face Spaces within seconds using the Docker template</h2>\n<p>To get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the ‚ÄúCreate new Space‚Äù button, or open the page directly by the following link: <a href=\"https://huggingface.co/new-space?template=aimstack/aim\"></a><strong><a href=\"https://huggingface.co/new-space?template=aimstack/aim\">https://huggingface.co/new-space?template=aimstack/aim</a></strong></p>\n<p><img src=\"/images/dynamic/hf_space_deploy.png\" alt=\"\"></p>\n<p>Set up your Aim Space in no time:</p>\n<ol>\n<li>Choose a name for your Space.</li>\n<li>Adjust Space hardware and the visibility mode.</li>\n<li>Submit your Space!</li>\n</ol>\n<p>After submitting the Space, you'll be able to monitor its progress through the building status:</p>\n<p><img src=\"/images/dynamic/hf_space_building.png\" alt=\"\"></p>\n<p>Once it transitions to ‚ÄúRunning‚Äù, your space is ready to go!</p>\n<p><img src=\"/images/dynamic/hf_space_running.png\" alt=\"\"></p>\n<p><strong>Ta-da! üéâ You're all set to start using Aim on Hugging Face.</strong></p>\n<p>By pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:</p>\n<ol>\n<li>\n<p>Run the following bash command to compress <code>.aim</code> directory:</p>\n<pre><code class=\"language-shell\">tar -czvf aim_repo.tar.gz .aim\n</code></pre>\n</li>\n<li>\n<p>Commit and push files to your Space.</p>\n</li>\n</ol>\n<p>That's it! Now open the App section of your Space, and Aim will display your training logs.</p>\n<p>Updating Spaces is incredibly convenient ‚Äì you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. üî•</p>\n<h2>See Aim in Action with Existing Demos on the Hub</h2>\n<p>Let‚Äôs explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.</p>\n<ul>\n<li>Neural machine translation task: <a href=\"https://huggingface.co/spaces/aimstack/nmt\"></a><a href=\"https://huggingface.co/spaces/aimstack/nmt\">https://huggingface.co/spaces/aimstack/nmt</a></li>\n<li>Simple handwritten digits recognition task: <a href=\"https://huggingface.co/spaces/aimstack/digit-recognition\"></a><a href=\"https://huggingface.co/spaces/aimstack/digit-recognition\">https://huggingface.co/spaces/aimstack/digit-recognition</a></li>\n<li>Image generation task with lightweight GAN implementation: <a href=\"https://huggingface.co/spaces/aimstack/image-generation\"></a><a href=\"https://huggingface.co/spaces/aimstack/image-generation\">https://huggingface.co/spaces/aimstack/image-generation</a></li>\n</ul>\n<p><img src=\"/images/dynamic/hf_space_demos.png\" alt=\"\"></p>\n<p>When navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.</p>\n<p><img src=\"/images/dynamic/hf_space_overview.png\" alt=\"Aim homepage\" title=\"Aim homepage\"></p>\n<p>Open the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.</p>\n<p><img src=\"/images/dynamic/hf_space_individual_run.png\" alt=\"Individual run page\" title=\"Individual run page\"></p>\n<p>Take your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.</p>\n<p>Metrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.</p>\n<p><img src=\"/images/dynamic/hf_space_me.png\" alt=\"Metrics Explorer\" title=\"Metrics Explorer\"></p>\n<p>Explorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.</p>\n<p><img src=\"/images/dynamic/hf_space_pythonic_search.png\" alt=\"Pythonic search\" title=\"Pythonic search\"></p>\n<p>In addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.</p>\n<p><img src=\"/images/dynamic/hf_space_media.png\" alt=\"Images Explorer\" title=\"Images Explorer\"></p>\n<p>Use Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!</p>\n<h2>One more thing‚Ä¶ üëÄ</h2>\n<p>Having Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.</p>\n<p>To embed your Space, construct the following link based on Space owner and Space name: <strong><code>https://owner-space-name.hf.space</code></strong>. This link can be used to embed your Space in any website or notebook using the following HTML code:</p>\n<pre><code class=\"language-html\">%%html\n&#x3C;iframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n>\n&#x3C;/iframe>\n</code></pre>\n<h2>Next steps</h2>\n<p>We are going to continuously iterate over Aim Space onboarding and usability, including:</p>\n<ul>\n<li>the ability to read logs directly from Hugging Face Hub model repos,</li>\n<li>automatic conversion of TensorBoard logs to Aim format,</li>\n<li>Aim HF Space-specific onboarding steps.</li>\n</ul>\n<p>Much more coming soon... stay tuned for the updates!</p>\n<h2>Learn more</h2>\n<p>Check out Aim Space documentation <a href=\"https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html\">here</a></p>\n<p>Aim repo on GitHub: <a href=\"http://github.com/aimhubio/aim\">github.com/aimhubio/aim</a></p>\n<p>If you have questions, join the¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå</p>\n<p>Drop a ‚≠êÔ∏è on¬†<a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/launching-aim-on-hugging-face-spaces.md","_raw":{"sourceFilePath":"posts/launching-aim-on-hugging-face-spaces.md","sourceFileName":"launching-aim-on-hugging-face-spaces.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/launching-aim-on-hugging-face-spaces"},"type":"Post"}]},"__N_SSG":true}