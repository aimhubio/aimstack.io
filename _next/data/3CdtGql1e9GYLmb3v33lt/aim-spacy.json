{"pageProps":{"post":{"title":"aim-spacy","author":"AimStack","logo":"https://user-images.githubusercontent.com/13848158/216383642-788e1287-11cf-4022-961a-1e374fdbe902.png","org_name":"AimStack","org_link":"https://github.com/aimhubio","repo_name":"aim-spacy","repo_link":"https://github.com/aimhubio/aim-spacy","version":"0.2.0","installation":"pip install aim-spacy","about":"Aim-based spaCy experiment tracker","badges_body":"[![PyPI Package](https://img.shields.io/pypi/v/aim-spacy?color=yellow)](https://pypi.org/project/aim-spacy/)\n[![PyPI Downloads](https://img.shields.io/pypi/dw/aim-spacy?color=green)](https://pypi.org/project/aim-spacy/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim-spacy)](https://pypi.org/project/aim-spacy/)\n[![Issues](https://img.shields.io/github/issues/aimhubio/aim-spacy)](http://github.com/aimhubio/aim-spacy/issues)\n[![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)","body":{"raw":"# Aim-spaCy\n\n<br />\n\n[Aim-spaCy](https://aimstack.io/spacy) is an [Aim](https://github.com/aimhubio/aim)-based spaCy experiment tracker. Its mission is to help AI researchers compare their spaCy metadata dramatically faster at scale.\n\n## About\nWhen running spaCy for training and inference a variety of metadata gets generated (hyperparams, metrics, visualizations etc). The runs/inferences need to be compared to do the next iterations quickly.\nAim is the most advanced open-source self-hosted AI experiment / metadata comparison tool.\n\nAim-spaCy helps to easily collect, store and explore training logs for spaCy, including:\n- metrics\n- hyper-parameters\n- displaCy visualizations\n\nMore about Aim: https://github.com/aimhubio/aim\n\nMore about spaCy: https://github.com/explosion/spaCy\n\n## Examples\n\n- A quick example on how to track metrics, hparams and displaCy outputs with AimLogger\n  - https://github.com/aimhubio/aim-spacy/tree/master/examples/training_demo \n- Example of tracking displaCy visualizations with AimDisplaCy\n  - https://github.com/aimhubio/aim-spacy/tree/master/examples/image_tracking_demo\n\n## Getting Started\n\n### 1. Installation\n\nIn order to use aim for experimentation tracking, you must install it onto your system.\n\n```\npip install aim-spacy\n```\n\n### 2. Tracking metrics, hparams and displacy visualizations\n\n\n<details>\n  <summary>1. Tracking with AimLogger</summary>\n  \n`spacy.AimLogger.v1` allows the user to leverage [`Aim`](https://aimstack.io/) as the experiment tracker throughout the model development cycles. All of the training metrics will be tracked by the `Aim` through the training steps and can be easily accessed through Aim UI.\n\n\nIt is important to note that you can observe the changes in the metrics live throughout the training. Aim also supports tracking multiple experiments simultaneously. Aim will store all of the [training config](https://spacy.io/usage/training#config) hyperparameters that are used during experimentation along with [system-related information](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7) ranging from GPU/CPU usability to Disk IO. This comes in with an added benefit that you can search/filter across the experimentation rather granularly, using our pythonic search [AimQL](https://aimstack.readthedocs.io/en/latest/using/search.html?highlight=AimQL#searching), live from the UI. To access the Aim Logger one can simply add a config akin to this\n\n\nExample config:\n\n```ini\n[training.logger]\n@loggers = \"spacy.AimLogger.v1\"\nrepo = \"path/to/save/logs\"\nexperiment_name = \"your_experiment_name\"\n```\n\nThe complete overview of Aim Logger inputs looks like this.\n\n| Name                   | Type | Description|\n| ---------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `repo`         | `str`           | The path for saving the logs        |\n| `experiment_name` | `str`     | The name of the experiment to track(default: `None`, the experiment will be determined by hash).                                                                                                                                       |\n| `viz_path`   | `str` | The path of the dataset (in a .spacy format) that will be used for saving the visualisations during training. Usually, this is taken as the dev set (dev.spacy) For a standalone run, this option is not needed (default: `None`).               |\n| `model_log_interval`   | `int` |How often should the model log the experimentation results               |\n| `image_size`   | `str` | A string with a ',' separation designating the height and width of the image that is going to be saved during the training              |\n| `experiment_type`   | `str` | This flag designates what kind of an experiment is currently being tracked. If 'ner' or 'dep' is specified then the tracker will also use the `viz_path` and `image_size` for tracking displacy outputs during training. For a standalone run, this option is not needed (default: `None`).               ||\n\n</details>\n\n<details>\n  <summary>2. Tracking in any python script</summary>\n\nTrack metrics and hparams\n```py\nfrom aim import Run\n\nrun = Run()\n\n# Save inputs, hparams or any other `key: value` pairs\nrun['hparams'] = {\n    'learning_rate': 0.001,\n    'batch_size': 32,\n}\n\n# ...\nfor step in range(10):\n    # Log metrics to visualize performance\n    run.track(step, name='metric_name')\n# ...\n```\n\nTrack displacy visulizations\n```py\nfrom aim_displacy import AimDisplaCy\nfrom aim import Run\n\naim_run = Run()\n\naim_displacy = AimDisplaCy(image_size=(600, 200))\n\nfor index, text in tqdm(enumerate(data), total=len(data)):\n    doc = nlp(text)\n    aim_run.track(\n        aim_displacy(doc, style='dep', caption=f'Dependecy for data: {index}'),\n        name='Parsing',\n        context={'type': 'dependency'}\n    )\n    aim_run.track(\n        aim_displacy(doc, style='ent', caption=f'Entity for data: {index}'),\n        name='Parsing',\n        context={'type': 'entity'}\n    )\n```\n  \n</details>\n\n### 3. Browsing results with Aim UI\n\nExecute the below command to run Aim UI:\n```shell\naim up\n```\n\nYou should see the following output meaning Aim UI is up and running:\n```\nRunning Aim UI on repo `<Repo#-5930451821203570655 path=/.aim read_only=None>`\nOpen http://127.0.0.1:43800\nPress Ctrl+C to exit\n```\n\nOpen your browser and navigate to http://127.0.0.1:43800 \n\nYou should be able to see the home page of Aim UI!\n\n![homescreen_img](https://user-images.githubusercontent.com/8036160/165775219-35fb1e09-e2a9-4f26-a6f4-0cf5273d2ac8.png)\n\nUsing the full scale of `aim-spacy` integration it is possible to aggregate/group and filter your experiments with various levels of granularity\n\n<!-- ![](https://user-images.githubusercontent.com/8036160/165744936-424c96ce-fed2-4453-b903-7356dea0be0f.svg) -->\n\n![grouped_img](https://user-images.githubusercontent.com/8036160/165745270-231f6db5-6c43-4647-b226-30b22b8030f1.png)\n\nYou can also filter using all the  parametrs  with `Aim`s pythonic search engine `AimQL`\n\n![aimQL-img](https://uploads-ssl.webflow.com/62558278c40852a7dfff6c09/62558278c408525151ff6c5e_2.png)\n\nTracking NER's and dependecies during training is also integrated within the pipeline\n\n![ner_dep_img](https://uploads-ssl.webflow.com/623c6d08838e92013e0cc115/62596a317852acb967cbc2d2_2.png)\n\n\nGrouping images along various axis is also entirely possible\n\n![ner_img](https://uploads-ssl.webflow.com/623c6d08838e92013e0cc115/62596a506525a64a2e7e7eae_4.png)\n","html":"<h1>Aim-spaCy</h1>\n<p><a href=\"https://aimstack.io/spacy\">Aim-spaCy</a> is an <a href=\"https://github.com/aimhubio/aim\">Aim</a>-based spaCy experiment tracker. Its mission is to help AI researchers compare their spaCy metadata dramatically faster at scale.</p>\n<h2>About</h2>\n<p>When running spaCy for training and inference a variety of metadata gets generated (hyperparams, metrics, visualizations etc). The runs/inferences need to be compared to do the next iterations quickly.\nAim is the most advanced open-source self-hosted AI experiment / metadata comparison tool.</p>\n<p>Aim-spaCy helps to easily collect, store and explore training logs for spaCy, including:</p>\n<ul>\n<li>metrics</li>\n<li>hyper-parameters</li>\n<li>displaCy visualizations</li>\n</ul>\n<p>More about Aim: https://github.com/aimhubio/aim</p>\n<p>More about spaCy: https://github.com/explosion/spaCy</p>\n<h2>Examples</h2>\n<ul>\n<li>A quick example on how to track metrics, hparams and displaCy outputs with AimLogger\n<ul>\n<li>https://github.com/aimhubio/aim-spacy/tree/master/examples/training_demo</li>\n</ul>\n</li>\n<li>Example of tracking displaCy visualizations with AimDisplaCy\n<ul>\n<li>https://github.com/aimhubio/aim-spacy/tree/master/examples/image_tracking_demo</li>\n</ul>\n</li>\n</ul>\n<h2>Getting Started</h2>\n<h3>1. Installation</h3>\n<p>In order to use aim for experimentation tracking, you must install it onto your system.</p>\n<pre><code>pip install aim-spacy\n</code></pre>\n<h3>2. Tracking metrics, hparams and displacy visualizations</h3>\n<p><code>spacy.AimLogger.v1</code> allows the user to leverage <a href=\"https://aimstack.io/\"><code>Aim</code></a> as the experiment tracker throughout the model development cycles. All of the training metrics will be tracked by the <code>Aim</code> through the training steps and can be easily accessed through Aim UI.</p>\n<p>It is important to note that you can observe the changes in the metrics live throughout the training. Aim also supports tracking multiple experiments simultaneously. Aim will store all of the <a href=\"https://spacy.io/usage/training#config\">training config</a> hyperparameters that are used during experimentation along with <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\">system-related information</a> ranging from GPU/CPU usability to Disk IO. This comes in with an added benefit that you can search/filter across the experimentation rather granularly, using our pythonic search <a href=\"https://aimstack.readthedocs.io/en/latest/using/search.html?highlight=AimQL#searching\">AimQL</a>, live from the UI. To access the Aim Logger one can simply add a config akin to this</p>\n<p>Example config:</p>\n<pre><code class=\"language-ini\">[training.logger]\n@loggers = \"spacy.AimLogger.v1\"\nrepo = \"path/to/save/logs\"\nexperiment_name = \"your_experiment_name\"\n</code></pre>\n<p>The complete overview of Aim Logger inputs looks like this.</p>\n<p>| Name                   | Type | Description|\n| ---------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <code>repo</code>         | <code>str</code>           | The path for saving the logs        |\n| <code>experiment_name</code> | <code>str</code>     | The name of the experiment to track(default: <code>None</code>, the experiment will be determined by hash).                                                                                                                                       |\n| <code>viz_path</code>   | <code>str</code> | The path of the dataset (in a .spacy format) that will be used for saving the visualisations during training. Usually, this is taken as the dev set (dev.spacy) For a standalone run, this option is not needed (default: <code>None</code>).               |\n| <code>model_log_interval</code>   | <code>int</code> |How often should the model log the experimentation results               |\n| <code>image_size</code>   | <code>str</code> | A string with a ',' separation designating the height and width of the image that is going to be saved during the training              |\n| <code>experiment_type</code>   | <code>str</code> | This flag designates what kind of an experiment is currently being tracked. If 'ner' or 'dep' is specified then the tracker will also use the <code>viz_path</code> and <code>image_size</code> for tracking displacy outputs during training. For a standalone run, this option is not needed (default: <code>None</code>).               ||</p>\n<p>Track metrics and hparams</p>\n<pre><code class=\"language-py\">from aim import Run\n\nrun = Run()\n\n# Save inputs, hparams or any other `key: value` pairs\nrun['hparams'] = {\n    'learning_rate': 0.001,\n    'batch_size': 32,\n}\n\n# ...\nfor step in range(10):\n    # Log metrics to visualize performance\n    run.track(step, name='metric_name')\n# ...\n</code></pre>\n<p>Track displacy visulizations</p>\n<pre><code class=\"language-py\">from aim_displacy import AimDisplaCy\nfrom aim import Run\n\naim_run = Run()\n\naim_displacy = AimDisplaCy(image_size=(600, 200))\n\nfor index, text in tqdm(enumerate(data), total=len(data)):\n    doc = nlp(text)\n    aim_run.track(\n        aim_displacy(doc, style='dep', caption=f'Dependecy for data: {index}'),\n        name='Parsing',\n        context={'type': 'dependency'}\n    )\n    aim_run.track(\n        aim_displacy(doc, style='ent', caption=f'Entity for data: {index}'),\n        name='Parsing',\n        context={'type': 'entity'}\n    )\n</code></pre>\n<h3>3. Browsing results with Aim UI</h3>\n<p>Execute the below command to run Aim UI:</p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p>You should see the following output meaning Aim UI is up and running:</p>\n<pre><code>Running Aim UI on repo `&#x3C;Repo#-5930451821203570655 path=/.aim read_only=None>`\nOpen http://127.0.0.1:43800\nPress Ctrl+C to exit\n</code></pre>\n<p>Open your browser and navigate to http://127.0.0.1:43800</p>\n<p>You should be able to see the home page of Aim UI!</p>\n<p><img src=\"https://user-images.githubusercontent.com/8036160/165775219-35fb1e09-e2a9-4f26-a6f4-0cf5273d2ac8.png\" alt=\"homescreen_img\"></p>\n<p>Using the full scale of <code>aim-spacy</code> integration it is possible to aggregate/group and filter your experiments with various levels of granularity</p>\n<p><img src=\"https://user-images.githubusercontent.com/8036160/165745270-231f6db5-6c43-4647-b226-30b22b8030f1.png\" alt=\"grouped_img\"></p>\n<p>You can also filter using all the  parametrs  with <code>Aim</code>s pythonic search engine <code>AimQL</code></p>\n<p><img src=\"https://uploads-ssl.webflow.com/62558278c40852a7dfff6c09/62558278c408525151ff6c5e_2.png\" alt=\"aimQL-img\"></p>\n<p>Tracking NER's and dependecies during training is also integrated within the pipeline</p>\n<p><img src=\"https://uploads-ssl.webflow.com/623c6d08838e92013e0cc115/62596a317852acb967cbc2d2_2.png\" alt=\"ner_dep_img\"></p>\n<p>Grouping images along various axis is also entirely possible</p>\n<p><img src=\"https://uploads-ssl.webflow.com/623c6d08838e92013e0cc115/62596a506525a64a2e7e7eae_4.png\" alt=\"ner_img\"></p>"},"_id":"packages/aim-spacy.md","_raw":{"sourceFilePath":"packages/aim-spacy.md","sourceFileName":"aim-spacy.md","sourceFileDir":"packages","contentType":"markdown","flattenedPath":"packages/aim-spacy"},"type":"Package","slug":"aim-spacy"}},"__N_SSG":true}