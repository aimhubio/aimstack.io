{"pageProps":{"post":{"title":"Aim Community is Moving to Discord!! ðŸŽ‰","date":"2022-12-21T21:45:47.434Z","author":"Gev Soghomonian","description":"Aim community is officially moving to Discord!! ðŸŽ‰","slug":"aim-community-is-moving-to-discord-ðŸŽ‰","image":"https://miro.medium.com/max/1400/1*c_ovadkKTTkZOokR3WiUTg.webp","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)Â community is officially moving to Discord!! ðŸŽ‰\n\n[Join us on Discord](https://community.aimstack.io/), letâ€™s rock and build great things together!!\n\nIn this new space we are going to\n\n* be very proactive in sharing next steps, areas of focus and plans\n* share weekly roadmap / commitments\n* share more community-specific content\n* of course continue answering the user questions\n* work with the users to build the best open-source experiment tracking tools.\n\n***The slack workspace is going to become deprecated by the end of 2022.***\n\nI also asked a friend on the technical reasons for moving to Discord. Here is what he responded. \n\n![](https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp)\n\nAim High!! ðŸš€","html":"<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â community is officially moving to Discord!! ðŸŽ‰</p>\n<p><a href=\"https://community.aimstack.io/\">Join us on Discord</a>, letâ€™s rock and build great things together!!</p>\n<p>In this new space we are going to</p>\n<ul>\n<li>be very proactive in sharing next steps, areas of focus and plans</li>\n<li>share weekly roadmap / commitments</li>\n<li>share more community-specific content</li>\n<li>of course continue answering the user questions</li>\n<li>work with the users to build the best open-source experiment tracking tools.</li>\n</ul>\n<p><em><strong>The slack workspace is going to become deprecated by the end of 2022.</strong></em></p>\n<p>I also asked a friend on the technical reasons for moving to Discord. Here is what he responded.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp\" alt=\"\"></p>\n<p>Aim High!! ðŸš€</p>"},"_id":"posts/aim-community-is-moving-to-discord-ðŸŽ‰.md","_raw":{"sourceFilePath":"posts/aim-community-is-moving-to-discord-ðŸŽ‰.md","sourceFileName":"aim-community-is-moving-to-discord-ðŸŽ‰.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-community-is-moving-to-discord-ðŸŽ‰"},"type":"Post"},"posts":[{"title":"3D Spleen Segmentation with MONAI and Aim","date":"2022-06-27T20:55:12.121Z","author":"Gev Soghomonian","description":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from http://medicaldecathlon.com/. This is a","slug":"3d-spleen-segmentation-with-monai-and-aim","image":"/images/dynamic/image-7-.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset fromÂ [](http://medicaldecathlon.com/)<http://medicaldecathlon.com/>.\n\nThis is a longer form of theÂ [3D spleen segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb)Â on the MONAI tutorials repo.\n\nFor a complete in-depth overview of the code and the methodology please followÂ [this tutorial](https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb)Â directly or our very ownÂ [Aim from Zero to Hero tutorial](https://aimstack.io/aim-from-zero-to-hero-track-machine-learning-experiments/).\n\n## ***Tracking the basics***\n\nAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\n\n### ***Transformations Matter***\n\nThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the`Single Run Page`.\n\n![](/images/dynamic/image-10-.png)\n\nYou can further search/filter through the runs based on these transformations using Aimsâ€™ very own pythonic search languageÂ `AimQL`. An Advanced usage looks like this\n\n![](/images/dynamic/image-11-.png)\n\n### *Lets talk about models and optimizers*\n\nSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonicÂ `dict`-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from`AimQL`\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n```\n\n![](/images/dynamic/image-12-.png)\n\n## What about Losses?\n\nMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\n\nFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image-5-.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view theÂ [complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html)Â or simply play around in ourÂ [interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&chart=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&select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n## *A picture is worth a thousand words*\n\nIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution atÂ `Aim`. You canÂ [track images](https://aimstack.io/aim-monai-tutorial/(%3Chttps://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#%3E))Â as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\n\n```\naim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n```\n\nAll the grouping/filtering/aggregation functional presented above are also available for Images.\n\n![](/images/dynamic/image-6-.jpeg)\n\n![](/images/dynamic/image-7-.jpeg)\n\n## *A figure is worth a thousand pictures*\n\nWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\n\n```\naim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n```\n\nWithinÂ *`Aim`*Â you can access all the tracked Figures from within theÂ `Single Run Page`\n\n``\n\n![](/images/dynamic/image-2-.gif)\n\nAnother interesting thing that one canÂ [track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\n## ***The curtain falls***\n\nIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research withÂ *`1-2`*Â lines of code. You can also play around with the completed MONAI integration results with ourÂ [interactive demo](http://play.aimstack.io:10005/).\n\nIf you have any bug reports please follow up on ourÂ [github repository](https://github.com/aimhubio/aim/issues/new/choose). Feel free to join our growingÂ [Slack community](https://slack.aimstack.io/)Â and ask questions directly to the developers and seasoned users.\n\nIf you find Aim useful,Â [please stop by](https://github.com/aimhubio/aim)Â and drop us a star.","html":"<p>In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset fromÂ <a href=\"http://medicaldecathlon.com/\"></a><a href=\"http://medicaldecathlon.com/\">http://medicaldecathlon.com/</a>.</p>\n<p>This is a longer form of theÂ <a href=\"https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb\">3D spleen segmentation tutorial</a>Â on the MONAI tutorials repo.</p>\n<p>For a complete in-depth overview of the code and the methodology please followÂ <a href=\"https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb\">this tutorial</a>Â directly or our very ownÂ <a href=\"https://aimstack.io/aim-from-zero-to-hero-track-machine-learning-experiments/\">Aim from Zero to Hero tutorial</a>.</p>\n<h2><em><strong>Tracking the basics</strong></em></h2>\n<p>As we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.</p>\n<h3><em><strong>Transformations Matter</strong></em></h3>\n<p>The way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the<code>Single Run Page</code>.</p>\n<p><img src=\"/images/dynamic/image-10-.png\" alt=\"\"></p>\n<p>You can further search/filter through the runs based on these transformations using Aimsâ€™ very own pythonic search languageÂ <code>AimQL</code>. An Advanced usage looks like this</p>\n<p><img src=\"/images/dynamic/image-11-.png\" alt=\"\"></p>\n<h3><em>Lets talk about models and optimizers</em></h3>\n<p>Similar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonicÂ <code>dict</code>-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from<code>AimQL</code></p>\n<pre><code>aim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n</code></pre>\n<p><img src=\"/images/dynamic/image-12-.png\" alt=\"\"></p>\n<h2>What about Losses?</h2>\n<p>Majority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).</p>\n<p>Furthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion</p>\n<pre><code>aim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n</code></pre>\n<p>After grouping, we end up with a visualization akin to this.</p>\n<p><img src=\"/images/dynamic/image-5-.jpeg\" alt=\"\"></p>\n<p>Aggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\">complete documentation</a>Â or simply play around in ourÂ <a href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&#x26;chart=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&#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\">interactive Demo</a>.</p>\n<h2><em>A picture is worth a thousand words</em></h2>\n<p>In the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution atÂ <code>Aim</code>. You canÂ <a href=\"https://aimstack.io/aim-monai-tutorial/(%3Chttps://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#%3E)\">track images</a>Â as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.</p>\n<pre><code>aim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n</code></pre>\n<p>All the grouping/filtering/aggregation functional presented above are also available for Images.</p>\n<p><img src=\"/images/dynamic/image-6-.jpeg\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-7-.jpeg\" alt=\"\"></p>\n<h2><em>A figure is worth a thousand pictures</em></h2>\n<p>We decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always</p>\n<pre><code>aim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n</code></pre>\n<p>WithinÂ <em><code>Aim</code></em>Â you can access all the tracked Figures from within theÂ <code>Single Run Page</code></p>\n<p>``</p>\n<p><img src=\"/images/dynamic/image-2-.gif\" alt=\"\"></p>\n<p>Another interesting thing that one canÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\">track is distributions</a>. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.</p>\n<pre><code>from aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n</code></pre>\n<p>Distributions can be accessed from the Single Run Page as well.</p>\n<p><img src=\"/images/dynamic/image-6-.png\" alt=\"\"></p>\n<h2><em><strong>The curtain falls</strong></em></h2>\n<p>In this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research withÂ <em><code>1-2</code></em>Â lines of code. You can also play around with the completed MONAI integration results with ourÂ <a href=\"http://play.aimstack.io:10005/\">interactive demo</a>.</p>\n<p>If you have any bug reports please follow up on ourÂ <a href=\"https://github.com/aimhubio/aim/issues/new/choose\">github repository</a>. Feel free to join our growingÂ <a href=\"https://slack.aimstack.io/\">Slack community</a>Â and ask questions directly to the developers and seasoned users.</p>\n<p>If you find Aim useful,Â <a href=\"https://github.com/aimhubio/aim\">please stop by</a>Â and drop us a star.</p>"},"_id":"posts/3d-spleen-segmentation-with-monai-and-aim.md","_raw":{"sourceFilePath":"posts/3d-spleen-segmentation-with-monai-and-aim.md","sourceFileName":"3d-spleen-segmentation-with-monai-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/3d-spleen-segmentation-with-monai-and-aim"},"type":"Post"},{"title":"Aim 1.3.5 â€” Activity View and X-axis alignment","date":"2021-02-05T11:16:41.748Z","author":"Gev Soghomonian","description":"Aim v1.3.5 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open source MLOps tools. Check out the new features [â€¦]","slug":"aim-1-3-5-â€”-activity-view-and-x-axis-alignment","image":"/images/dynamic/1st.gif","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n## Aim Release â€” Activity View and X-axis alignment\n\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is now available. Thanks to the incredible [Aim community](https://discord.com/invite/zXq2NfVdtF) for the feedback and support on building democratized open source AI dev tools.\n\nCheck out the new features at play.aimstack.io\n\nHere are the highlights of the features:\n\n## Activity View on the Dashboard\n\nThe **Activity View** shows the daily experiment runs. With one click you can search each dayâ€™s runs and explore them straight away\n\n**Statistics** displays the overall count of [Experiments and Runs. ](https://github.com/aimhubio/aim#concepts)\n\n![](/images/dynamic/1st.gif \"The new dashboard in action!\")\n\n## **X-axis alignment by epoch**\n\nX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss â€” 1000 steps, validation loss â€” 15 steps) then you can align them by epoch, relative time or absolute time.\n\n![](/images/dynamic/2nd.gif \"No more misaligned metrics!\")\n\n## **Ordering runs both on Explore and on Dashboard**\n\n\n\nNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\n\nOn Explore, when sorted by a hyperparam thatâ€™s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\n\n![](/images/dynamic/3.gif \"Sort the runs both on Dashboard and Explore!\")\n\n## Learn More\n\n\n\nWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\n\nWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every monthÂ ðŸ˜Š). Try out Aim, join theÂ [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for supportÂ ðŸ™Œ.","html":"<h2>Aim Release â€” Activity View and X-axis alignment</h2>\n<p><a href=\"https://github.com/aimhubio/aim\">Aim</a> v1.3.5 is now available. Thanks to the incredible <a href=\"https://discord.com/invite/zXq2NfVdtF\">Aim community</a> for the feedback and support on building democratized open source AI dev tools.</p>\n<p>Check out the new features at play.aimstack.io</p>\n<p>Here are the highlights of the features:</p>\n<h2>Activity View on the Dashboard</h2>\n<p>The <strong>Activity View</strong> shows the daily experiment runs. With one click you can search each dayâ€™s runs and explore them straight away</p>\n<p><strong>Statistics</strong> displays the overall count of <a href=\"https://github.com/aimhubio/aim#concepts\">Experiments and Runs. </a></p>\n<p><img src=\"/images/dynamic/1st.gif\" alt=\"\" title=\"The new dashboard in action!\"></p>\n<h2><strong>X-axis alignment by epoch</strong></h2>\n<p>X-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss â€” 1000 steps, validation loss â€” 15 steps) then you can align them by epoch, relative time or absolute time.</p>\n<p><img src=\"/images/dynamic/2nd.gif\" alt=\"\" title=\"No more misaligned metrics!\"></p>\n<h2><strong>Ordering runs both on Explore and on Dashboard</strong></h2>\n<p>Now you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.</p>\n<p>On Explore, when sorted by a hyperparam thatâ€™s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.</p>\n<p><img src=\"/images/dynamic/3.gif\" alt=\"\" title=\"Sort the runs both on Dashboard and Explore!\"></p>\n<h2>Learn More</h2>\n<p>We have been working on Aim for the past 6 months and excited to see this much interest from the community.</p>\n<p>We are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every monthÂ ðŸ˜Š). Try out Aim, join theÂ <a href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\">Aim community</a>, share your feedback.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for supportÂ ðŸ™Œ.</p>"},"_id":"posts/aim-1-3-5-â€”-activity-view-and-x-axis-alignment.md","_raw":{"sourceFilePath":"posts/aim-1-3-5-â€”-activity-view-and-x-axis-alignment.md","sourceFileName":"aim-1-3-5-â€”-activity-view-and-x-axis-alignment.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-1-3-5-â€”-activity-view-and-x-axis-alignment"},"type":"Post"},{"title":"Aim 1.3.8 â€” Enhanced Context Table and Advanced Group Coloring","date":"2021-03-01T11:09:06.858Z","author":"Gev Soghomonian","description":"Aim 1.3.8 featuring Advanced Group Coloring for AI metrics  is now available. ","slug":"aim-1-3-8-â€”-enhanced-context-table-and-advanced-group-coloring","image":"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n[Aim](https://aimstack.io/)Â 1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\n\nCheck out the new features atÂ [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=).\n\nHere are the more notable changes:\n\n### Enhanced Context Table\n\n\n\nThe context table used to not use the screen real-estate effectively â€” an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\n\nSo we have made 3 changes to tackle this problem\n\n### New table groups view\n\nBelow is the new modified look of the table. Here is whatâ€™s changed:\n\n* The empty per group is removed\n* In addition to the group details popover, there is an in-place list group config is available for instant lookup\n\n![](https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp \"New improved Aim Context Table on Explore\")\n\n\n\n* **\\[In Progress for next release]**Â Column resize â€” this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\n\n### Column Management\n\n\n\nThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\n\nYou can also re-order the columns by dragging the mid-section up/down.\n\nThis is super-handy when dealing with 100s of columns on the table for parameters.\n\n![](https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp \"Drag columns left and right to pin, search and disable less important ones\")\n\n\n\n### Advanced Group Coloring\n\n\n\nThis is the latest iteration over the group coloring.\n\n* Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\n* You can shuffle the colors in case they arenâ€™t pick to your liking\n* You can pick two different color palette for better distribution of the colors.\n\n  ![](https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp \"Effective coloring of the groups is important!\")\n\n  ### Thanks to\n* [mahnerak](https://github.com/mahnerak), Lars,Â [Joe](https://github.com/jafioti)Â for detailed feedback â€” helped us immensely in this iteration.\n\n  ### Learn More\n\n  We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\n\n  Try outÂ [Aim](https://aimstack.io/), join theÂ [Aim community](https://slack.aimstack.io/), share your feedback.\n\n  We are building the next generation of democratized AI dev tools.\n\n  And donâ€™t forget to leaveÂ [Aim](https://aimstack.io/)Â a star on GitHub for support ðŸ™Œ.","html":"<p><a href=\"https://aimstack.io/\">Aim</a>Â 1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools</p>\n<p>Check out the new features atÂ <a href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=\">play.aimstack.io</a>.</p>\n<p>Here are the more notable changes:</p>\n<h3>Enhanced Context Table</h3>\n<p>The context table used to not use the screen real-estate effectively â€” an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).</p>\n<p>So we have made 3 changes to tackle this problem</p>\n<h3>New table groups view</h3>\n<p>Below is the new modified look of the table. Here is whatâ€™s changed:</p>\n<ul>\n<li>The empty per group is removed</li>\n<li>In addition to the group details popover, there is an in-place list group config is available for instant lookup</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp\" alt=\"\" title=\"New improved Aim Context Table on Explore\"></p>\n<ul>\n<li><strong>[In Progress for next release]</strong>Â Column resize â€” this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.</li>\n</ul>\n<h3>Column Management</h3>\n<p>This feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.</p>\n<p>You can also re-order the columns by dragging the mid-section up/down.</p>\n<p>This is super-handy when dealing with 100s of columns on the table for parameters.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp\" alt=\"\" title=\"Drag columns left and right to pin, search and disable less important ones\"></p>\n<h3>Advanced Group Coloring</h3>\n<p>This is the latest iteration over the group coloring.</p>\n<ul>\n<li>\n<p>Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.</p>\n</li>\n<li>\n<p>You can shuffle the colors in case they arenâ€™t pick to your liking</p>\n</li>\n<li>\n<p>You can pick two different color palette for better distribution of the colors.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp\" alt=\"\" title=\"Effective coloring of the groups is important!\"></p>\n<h3>Thanks to</h3>\n</li>\n<li>\n<p><a href=\"https://github.com/mahnerak\">mahnerak</a>, Lars,Â <a href=\"https://github.com/jafioti\">Joe</a>Â for detailed feedback â€” helped us immensely in this iteration.</p>\n<h3>Learn More</h3>\n<p>We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.</p>\n<p>Try outÂ <a href=\"https://aimstack.io/\">Aim</a>, join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>, share your feedback.</p>\n<p>We are building the next generation of democratized AI dev tools.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://aimstack.io/\">Aim</a>Â a star on GitHub for support ðŸ™Œ.</p>\n</li>\n</ul>"},"_id":"posts/aim-1-3-8-â€”-enhanced-context-table-and-advanced-group-coloring.md","_raw":{"sourceFilePath":"posts/aim-1-3-8-â€”-enhanced-context-table-and-advanced-group-coloring.md","sourceFileName":"aim-1-3-8-â€”-enhanced-context-table-and-advanced-group-coloring.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-1-3-8-â€”-enhanced-context-table-and-advanced-group-coloring"},"type":"Post"},{"title":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","date":"2021-05-18T13:58:29.035Z","author":"Gev Soghomonian","description":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","slug":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)Â 2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\n\nCheck out the updated Aim atÂ [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\n\nFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changesÂ [here](https://github.com/aimhubio/aim/milestone/6?closed=1).\n\n## XGBoost Inegration\n\nTo begin with, Â `aim.callback`Â is now available that exportsÂ `AimCallback`Â to be passed to theÂ `xgb.train`Â as a callback to log the experiments.\n\nCheck out thisÂ [blogpost](https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/)Â for additional details on how to integrate Aim to your XGBoost code.\n\n## Confidence Interval as the aggregation method\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyjcE9k4-cZGl5g03F3Mjg.png)\n\nNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.Â \n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nCheck out the latestÂ [Aim integration](https://aimstack.io/aim-v2-2-0-hugging-face-integration/)!\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for support.","html":"<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â 2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.</p>\n<p>Check out the updated Aim atÂ <a href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\">play.aimstack.io</a>.</p>\n<p>For this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changesÂ <a href=\"https://github.com/aimhubio/aim/milestone/6?closed=1\">here</a>.</p>\n<h2>XGBoost Inegration</h2>\n<p>To begin with, Â <code>aim.callback</code>Â is now available that exportsÂ <code>AimCallback</code>Â to be passed to theÂ <code>xgb.train</code>Â as a callback to log the experiments.</p>\n<p>Check out thisÂ <a href=\"https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/\">blogpost</a>Â for additional details on how to integrate Aim to your XGBoost code.</p>\n<h2>Confidence Interval as the aggregation method</h2>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyjcE9k4-cZGl5g03F3Mjg.png\" alt=\"\"></p>\n<p>Now you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.Â </p>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Check out the latestÂ <a href=\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\">Aim integration</a>!</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support.</p>"},"_id":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","_raw":{"sourceFilePath":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileName":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements"},"type":"Post"},{"title":"Aim 2022 community report","date":"2023-01-05T21:48:26.367Z","author":"Gev Soghomonian","description":"Itâ€™s time to review the 2022. Happy New Year! ðŸŽ† ","slug":"aim-2022-community-report","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OvP3KCeUaBbqWj_jq4c5xg.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey lovely Aimers, Happy New Year! ðŸŽ†\n\nNow that the 2022 is over, we are excited to share what has happened in 2022. We have achieved a lot together!\n\n# TLDR â€” Highlights\n\nIn 2022 we have taken Aim to completely different level. It has become from a cool open-source project into a full-blown product. An inspiring community-led effort!!\n\n* We have shippedÂ [12 new Aim versions](https://github.com/aimhubio/aim/releaseshttps://github.com/aimhubio/aim/releases)Â and countless other patches\n* Almost every week for the past 80+ weeks a new Aim version has been shipped â€”Â **a blistering pace of innovation!! ðŸ˜…**\n* Aim contributors have doubled to 48 in 2022\n* ~1300 GitHub issues have been created in 2022 and over 1100 of them have been resolved.\n* We have reached 3K stars on GitHub. â­ï¸ (If you like our work,Â [drop us a star!](https://github.com/aimhubio/aim)Â )\n* WeÂ [launched](https://medium.com/aimstack/aim-community-is-moving-to-discord-da24a52169db)Â our community onÂ [discord](https://community.aimstack.io/).\n\n# Product\n\nWe have shipped 100s of major features in 2022. It would take several chapters to talk about all of them.\n\nHowever, here are two fundamental features that we havenâ€™t talked much about yet.\n\n## Base Explorer\n\n* A foundational work to help compare any type of renderable ML metadata at mass.\n* This is the first experimental step we have made to turn Aim into a one-stop-shop for multidimensional metadata analysis.\n\nThe Figures explorer runs on Base Explorer\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRbl_iVzy8FwRfj6eZWffA.png)\n\n## Aim Callbacks\n\nA surgical intervention into your ML runs. For everything!!\n\n* Aim Users are using the callbacks to create policies to automatically manage trainings. A major step towards reproducibility and true training management. Hours of GPU-time saved at the worst case scenario!!\n\nWe are going to talk a lot about the callbacks too! ðŸ™Œ\n\nMore about callbacksÂ [here](https://aimstack.readthedocs.io/en/latest/using/callbacks.html#)\n\n## Aim Roadmap\n\nHere is how the public Aim roadmap looks like now. This is just the subset of the additions made. A monumental work!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rYmolGfB_bEjGHFb4TB5Q.png)\n\n# Integrations\n\nAim is heavily integrated with the AI ecosystem now.\n\n* in 2022 Aim ecosystem integrations have grown 3x\n* ~10 more integrations are also in progress right now â¤ï¸\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EMYy1Eh6qYqrLZo2i6UPw.png)\n\n\n\n* Lots of impactful repos have adopted Aim as well. Including these ones:Â [HF Accelerate](https://github.com/huggingface/accelerate),Â [FAIR Metaseq](https://github.com/facebookresearch/metaseqhttps://github.com/facebookresearch/metaseq)Â (default tracker),Â [FAIR Fairseq](https://github.com/facebookresearch/fairseq),Â [Ludwig](https://github.com/ludwig-ai/ludwig)Â etc\n* Community-requested Aim converters for Mlflow, TensorBoard andÂ [Wandb](https://medium.com/aimstack/aim-tutorial-for-weights-and-biases-users-bfbdde76f21e?source=collection_home---4------0-----------------------).\n\n# Onwards to 2023\n\nIn 2023 we are going to work hard to put Aim into the hands of as many researchers and MLOps engineers as possible!\n\nSoon we will publish the Aim roadmap of 2023! Lots to share. :)\n\n**2023 here we come! ðŸ˜Š**","html":"<p>Hey lovely Aimers, Happy New Year! ðŸŽ†</p>\n<p>Now that the 2022 is over, we are excited to share what has happened in 2022. We have achieved a lot together!</p>\n<h1>TLDR â€” Highlights</h1>\n<p>In 2022 we have taken Aim to completely different level. It has become from a cool open-source project into a full-blown product. An inspiring community-led effort!!</p>\n<ul>\n<li>We have shippedÂ <a href=\"https://github.com/aimhubio/aim/releaseshttps://github.com/aimhubio/aim/releases\">12 new Aim versions</a>Â and countless other patches</li>\n<li>Almost every week for the past 80+ weeks a new Aim version has been shipped â€”Â <strong>a blistering pace of innovation!! ðŸ˜…</strong></li>\n<li>Aim contributors have doubled to 48 in 2022</li>\n<li>~1300 GitHub issues have been created in 2022 and over 1100 of them have been resolved.</li>\n<li>We have reached 3K stars on GitHub. â­ï¸ (If you like our work,Â <a href=\"https://github.com/aimhubio/aim\">drop us a star!</a>Â )</li>\n<li>WeÂ <a href=\"https://medium.com/aimstack/aim-community-is-moving-to-discord-da24a52169db\">launched</a>Â our community onÂ <a href=\"https://community.aimstack.io/\">discord</a>.</li>\n</ul>\n<h1>Product</h1>\n<p>We have shipped 100s of major features in 2022. It would take several chapters to talk about all of them.</p>\n<p>However, here are two fundamental features that we havenâ€™t talked much about yet.</p>\n<h2>Base Explorer</h2>\n<ul>\n<li>A foundational work to help compare any type of renderable ML metadata at mass.</li>\n<li>This is the first experimental step we have made to turn Aim into a one-stop-shop for multidimensional metadata analysis.</li>\n</ul>\n<p>The Figures explorer runs on Base Explorer</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRbl_iVzy8FwRfj6eZWffA.png\" alt=\"\"></p>\n<h2>Aim Callbacks</h2>\n<p>A surgical intervention into your ML runs. For everything!!</p>\n<ul>\n<li>Aim Users are using the callbacks to create policies to automatically manage trainings. A major step towards reproducibility and true training management. Hours of GPU-time saved at the worst case scenario!!</li>\n</ul>\n<p>We are going to talk a lot about the callbacks too! ðŸ™Œ</p>\n<p>More about callbacksÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/callbacks.html#\">here</a></p>\n<h2>Aim Roadmap</h2>\n<p>Here is how the public Aim roadmap looks like now. This is just the subset of the additions made. A monumental work!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rYmolGfB_bEjGHFb4TB5Q.png\" alt=\"\"></p>\n<h1>Integrations</h1>\n<p>Aim is heavily integrated with the AI ecosystem now.</p>\n<ul>\n<li>in 2022 Aim ecosystem integrations have grown 3x</li>\n<li>~10 more integrations are also in progress right now â¤ï¸</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EMYy1Eh6qYqrLZo2i6UPw.png\" alt=\"\"></p>\n<ul>\n<li>Lots of impactful repos have adopted Aim as well. Including these ones:Â <a href=\"https://github.com/huggingface/accelerate\">HF Accelerate</a>,Â <a href=\"https://github.com/facebookresearch/metaseqhttps://github.com/facebookresearch/metaseq\">FAIR Metaseq</a>Â (default tracker),Â <a href=\"https://github.com/facebookresearch/fairseq\">FAIR Fairseq</a>,Â <a href=\"https://github.com/ludwig-ai/ludwig\">Ludwig</a>Â etc</li>\n<li>Community-requested Aim converters for Mlflow, TensorBoard andÂ <a href=\"https://medium.com/aimstack/aim-tutorial-for-weights-and-biases-users-bfbdde76f21e?source=collection_home---4------0-----------------------\">Wandb</a>.</li>\n</ul>\n<h1>Onwards to 2023</h1>\n<p>In 2023 we are going to work hard to put Aim into the hands of as many researchers and MLOps engineers as possible!</p>\n<p>Soon we will publish the Aim roadmap of 2023! Lots to share. :)</p>\n<p><strong>2023 here we come! ðŸ˜Š</strong></p>"},"_id":"posts/aim-2022-community-report.md","_raw":{"sourceFilePath":"posts/aim-2022-community-report.md","sourceFileName":"aim-2022-community-report.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2022-community-report"},"type":"Post"},{"title":"Aim 2022; Product Recap","date":"2023-01-20T06:16:21.056Z","author":"Gor Arakelyan","description":"A retrospective look at the past year!","slug":"aim-2022-product-recap","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mpgFSGReVaycf5644hM6xw.png","draft":false,"categories":["New Releases"],"body":{"raw":"# A retrospective look at the past year\n\n\n\nAim underwent significant improvements in the past year. Over the course of 50 releases, including 12 minor releases and 38 patch releases, Aim received contributions from 35 total contributors, including 18 new external contributors. These contributions included 769 merged pull requests and 487 submitted issues!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhiLjCXeyDNrMIuZU5_SXg.png \"Aim contributions pulse\")\n\nThere was a significant progress in various areas of the Aim, including adding more supported platforms, taking the UI to the whole new level, adding remote tracking capabilities, revamping the CLI, extending the QL and moving towards active monitoring for optimizing large-scale trainings.\n\nIn addition to adding new features and extending Aim, improvements in performance and usability were applied to enhance the overall user experience. Furthermore, new integrations with machine learning tools were implemented to make it easier to use Aim in a broader technology stack\n\nOverall, these improvements greatly enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.\n\n# Aim Community\n\n![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GitHotOjlY9WPsvQmWaqwQ.png \"Aim contributors\")\n\n\n\n> We are on a mission to democratize AI dev tools!\n\nNumerous external contributions were pushed to â€œmainâ€ in 2022. Without community support, Aim would not be where it is today.\n\nCongratulations toÂ [Sharathmk99](https://github.com/Sharathmk99),Â [YodaEmbedding](https://github.com/YodaEmbedding),Â [hjoonjang](https://github.com/hjoonjang),Â [jangop](https://github.com/jangop),Â [djwessel](https://github.com/djwessel),Â [GeeeekExplorer](https://github.com/GeeeekExplorer),Â [dsblank](https://github.com/dsblank),Â [timokau](https://github.com/timokau),Â [kumarshreshtha](https://github.com/kumarshreshtha),Â [kage08](https://github.com/kage08),Â [karan2801](https://github.com/karan2801),Â [hendriks73](https://github.com/hendriks73),Â [yeghiakoronian](https://github.com/yeghiakoronian),Â [uduse](https://github.com/uduse),Â [arnauddhaene](https://github.com/arnauddhaene),Â [lukoucky](https://github.com/lukoucky),Â [Arvind644](https://github.com/Arvind644),Â [shrinandj](https://github.com/shrinandj)Â for their first contributions over the past year! ðŸŽ‰\n\nThese contributions have covered a range of areas, including documentation, user guides, the SDK and the UI!\n\n# Highlights\n\n* Supported platforms were expanded to include Docker, Apple M1, Conda, Google Colab and Jupyter Notebooks.\n* The UI was given a makeover, with upgraded home and run single pages, as well as brand new experiment page. The new audio and figure explorers now allow for the exploration of more types of metadata.\n* Aim remote tracking server was rolled out to enable tracking from multi-host environments.\n* Active monitoring capabilities were enabled, such as notifications for stalled runs, and the ability to programmatically define training heuristics using callbacks and notifications.\n* Aim was integrated with 9 different machine learning frameworks alongside with 3 convertors that help to migrate from other experiment trackers.\n* Last but not least, the documentation was completely revamped, including almost 40 pages of guides towards helping in effectively using and understanding Aim.\n\n# **Supported Platforms**\n\n\n\nImprovements were made to ensure Aim works seamlessly with widely used environments and platforms in ML/MLOps field, including adding support for new platforms.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYG-SSawh8Q8jte6BjyNMQ.png)\n\n\n\n* Support for running Aim on Apple M1 devices.\n* Support for using Aim inside Google Colab and Jupyter notebooks.\n* Docker images for Aim UI and server.\n* Support for running Aim in Kubernetes clusters.\n* Aim became available in Conda.\n\n# User Interface\n\n\n\nThe UI is one of key interfaces to interact with the ML training runs tracked by Aim. Itâ€™s super-powerful for comparing large number of trainings. The UI was significantly improved in the past year with new pages and explorers to help to explore more metadata types.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odqgX33xLVzYBI6sfRYRWQ.png \"Home Page\")\n\n\n\n* Revamped home page to see projectâ€™s contributions and overall statistics.\n* Revamped run page to deep-dive into an individual run.\n* Brand new experiment page to view experimentâ€™s details, attached runs, etc.\n* New explorers, such as the â€œFigures Explorerâ€ and â€œAudio Explorerâ€, to allow a cross-run comparison of Plotly figures and audio objects respectively.\n* â€œMetrics Explorerâ€ key enhancements, including displaying chart legends and the ability of exporting charts as images.\n\n\n\n# Command Line Interface\n\n\n\nAim CLI offers a simple interface to easily manage tracked trainings. Two key groups of commands were added:\n\n* **Runs management**Â to enable the base operations for managing trainings â€” list, copy, remove, move, upload, close.\n* **Storage management**Â to help to manage Aim data storage, such as reindexing, pruning, managing data format versions (advanced usage).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZFkXFzmz5BxGYofUgWdTg.png)\n\n# Query Language\n\nAim enables a powerful query language to filter through all the stored metadata using python expressions. A few additions were done providing a more friendly and pythonic interface for working with date-time expressions, as well as querying runs based on its metrics results.\n\n![](https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6J9gouDEt9Mkhv9Cds4dNw.png)\n\n# Remote Tracking\n\nThe Aim remote tracking server, which allows running trainings in a multi-host environment and collect tracked data in a centralized location, used to be one of the most requested features.\n\nIt has been gradually rolled out from its experimental phase to a stable version that can scale up and handle an increasing number of parallel trainings.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CizfTBF8Nh4DZcun6TP8Pg.png)\n\nSwitching from a local environment to a centralized server is as easy as pie, as zero code changes are required.âš¡\\\nThis is because the interfaces are completely compatible!\n\n# Active Monitoring\n\nAim made it first steps towards optimizing conducting long and large scale trainings, such as training or tuning GPT-like models. It reduces human-hours spent monitoring/restarting runs, improves reproducibility and ramps up time for people who need to train these models:\n\n* Ability to notify on stalled/stuck runs.\n* Callbacks and notifications to define training heuristics programmatically.\n\nA quote from â€œScaling Laws for Generative Mixed-Modal Language Modelsâ€ (Aghajanyan et al., 2023,Â [arXiv:2301.03728](https://arxiv.org/abs/2301.03728)):\n\n> We tracked all experiments using the Aim experiment tracker (Arakelyan et al., 2020). To ensure consistent training strategies across our experiments, we implemented a model restart policy using the Aim experiment tracker and callbacks. Specifically, if training perplexities do not decrease after 500 million tokens, the training run is restarted with a reduced learning rate with a factor of 0.8 of the current time step. This policy helps remove variance in the scaling laws due to differences in training procedures and allows us to scale up the number of asynchronous experiments significantly. All experiments were conducted in a two-month time frame with a cluster of 768 80GB A100 GPUs. The majority of experiments used 64 GPUs at a time.\n\n# Key Performance and Usability Optimizations\n\n* Storage: long metrics sampling and retrieval (>1M steps).\n* Storage: robust locking mechanism and automatic background indexing.\n* UI: The v3.12 milestone was released, addressing over 30 usability issues.\n* UI: virtualizations in various places across the UI to improve responsiveness when displaying large amount of data.\n* UI: optimizations in stream decoding and data encoding to enhance overall performance.\n* UI: live update optimizations to effectively update and display real-time data.\n\n# Integrations\n\nAim easily integrates with a large number of widely adopted machine learning frameworks and tools, reducing barriers to get started with Aim. During the past year, a number of integrations were added:\n\n* **ML Frameworks**Â â€” Pytorch Ignite, CatBoost, LightGBM, KerasTuner, fastai, MXNet, Optuna, PaddlePaddle\n* **Convertors**Â to easily migrate from other toolsÂ **â€”**Â TensorBoard to Aim, MLFlow to Aim, WandB to Aim.\n* **Tools that integrated Aim**Â â€” Metaâ€™s fairseq and metaseq frameworks, HuggingFaceâ€™s accelerate, Ludwig and Kedro.\n\n![](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*YT4Yf1sJUqQLWRtiF8fJgQ.png)\n\n# Learn More\n\n\n\nIf you have any questions joinÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. ðŸ™Œ\n\nShow some love by dropping a â­ï¸ onÂ [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<h1>A retrospective look at the past year</h1>\n<p>Aim underwent significant improvements in the past year. Over the course of 50 releases, including 12 minor releases and 38 patch releases, Aim received contributions from 35 total contributors, including 18 new external contributors. These contributions included 769 merged pull requests and 487 submitted issues!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhiLjCXeyDNrMIuZU5_SXg.png\" alt=\"\" title=\"Aim contributions pulse\"></p>\n<p>There was a significant progress in various areas of the Aim, including adding more supported platforms, taking the UI to the whole new level, adding remote tracking capabilities, revamping the CLI, extending the QL and moving towards active monitoring for optimizing large-scale trainings.</p>\n<p>In addition to adding new features and extending Aim, improvements in performance and usability were applied to enhance the overall user experience. Furthermore, new integrations with machine learning tools were implemented to make it easier to use Aim in a broader technology stack</p>\n<p>Overall, these improvements greatly enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.</p>\n<h1>Aim Community</h1>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GitHotOjlY9WPsvQmWaqwQ.png\" alt=\"\" title=\"Aim contributors\"></p>\n<blockquote>\n<p>We are on a mission to democratize AI dev tools!</p>\n</blockquote>\n<p>Numerous external contributions were pushed to â€œmainâ€ in 2022. Without community support, Aim would not be where it is today.</p>\n<p>Congratulations toÂ <a href=\"https://github.com/Sharathmk99\">Sharathmk99</a>,Â <a href=\"https://github.com/YodaEmbedding\">YodaEmbedding</a>,Â <a href=\"https://github.com/hjoonjang\">hjoonjang</a>,Â <a href=\"https://github.com/jangop\">jangop</a>,Â <a href=\"https://github.com/djwessel\">djwessel</a>,Â <a href=\"https://github.com/GeeeekExplorer\">GeeeekExplorer</a>,Â <a href=\"https://github.com/dsblank\">dsblank</a>,Â <a href=\"https://github.com/timokau\">timokau</a>,Â <a href=\"https://github.com/kumarshreshtha\">kumarshreshtha</a>,Â <a href=\"https://github.com/kage08\">kage08</a>,Â <a href=\"https://github.com/karan2801\">karan2801</a>,Â <a href=\"https://github.com/hendriks73\">hendriks73</a>,Â <a href=\"https://github.com/yeghiakoronian\">yeghiakoronian</a>,Â <a href=\"https://github.com/uduse\">uduse</a>,Â <a href=\"https://github.com/arnauddhaene\">arnauddhaene</a>,Â <a href=\"https://github.com/lukoucky\">lukoucky</a>,Â <a href=\"https://github.com/Arvind644\">Arvind644</a>,Â <a href=\"https://github.com/shrinandj\">shrinandj</a>Â for their first contributions over the past year! ðŸŽ‰</p>\n<p>These contributions have covered a range of areas, including documentation, user guides, the SDK and the UI!</p>\n<h1>Highlights</h1>\n<ul>\n<li>Supported platforms were expanded to include Docker, Apple M1, Conda, Google Colab and Jupyter Notebooks.</li>\n<li>The UI was given a makeover, with upgraded home and run single pages, as well as brand new experiment page. The new audio and figure explorers now allow for the exploration of more types of metadata.</li>\n<li>Aim remote tracking server was rolled out to enable tracking from multi-host environments.</li>\n<li>Active monitoring capabilities were enabled, such as notifications for stalled runs, and the ability to programmatically define training heuristics using callbacks and notifications.</li>\n<li>Aim was integrated with 9 different machine learning frameworks alongside with 3 convertors that help to migrate from other experiment trackers.</li>\n<li>Last but not least, the documentation was completely revamped, including almost 40 pages of guides towards helping in effectively using and understanding Aim.</li>\n</ul>\n<h1><strong>Supported Platforms</strong></h1>\n<p>Improvements were made to ensure Aim works seamlessly with widely used environments and platforms in ML/MLOps field, including adding support for new platforms.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYG-SSawh8Q8jte6BjyNMQ.png\" alt=\"\"></p>\n<ul>\n<li>Support for running Aim on Apple M1 devices.</li>\n<li>Support for using Aim inside Google Colab and Jupyter notebooks.</li>\n<li>Docker images for Aim UI and server.</li>\n<li>Support for running Aim in Kubernetes clusters.</li>\n<li>Aim became available in Conda.</li>\n</ul>\n<h1>User Interface</h1>\n<p>The UI is one of key interfaces to interact with the ML training runs tracked by Aim. Itâ€™s super-powerful for comparing large number of trainings. The UI was significantly improved in the past year with new pages and explorers to help to explore more metadata types.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odqgX33xLVzYBI6sfRYRWQ.png\" alt=\"\" title=\"Home Page\"></p>\n<ul>\n<li>Revamped home page to see projectâ€™s contributions and overall statistics.</li>\n<li>Revamped run page to deep-dive into an individual run.</li>\n<li>Brand new experiment page to view experimentâ€™s details, attached runs, etc.</li>\n<li>New explorers, such as the â€œFigures Explorerâ€ and â€œAudio Explorerâ€, to allow a cross-run comparison of Plotly figures and audio objects respectively.</li>\n<li>â€œMetrics Explorerâ€ key enhancements, including displaying chart legends and the ability of exporting charts as images.</li>\n</ul>\n<h1>Command Line Interface</h1>\n<p>Aim CLI offers a simple interface to easily manage tracked trainings. Two key groups of commands were added:</p>\n<ul>\n<li><strong>Runs management</strong>Â to enable the base operations for managing trainings â€” list, copy, remove, move, upload, close.</li>\n<li><strong>Storage management</strong>Â to help to manage Aim data storage, such as reindexing, pruning, managing data format versions (advanced usage).</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZFkXFzmz5BxGYofUgWdTg.png\" alt=\"\"></p>\n<h1>Query Language</h1>\n<p>Aim enables a powerful query language to filter through all the stored metadata using python expressions. A few additions were done providing a more friendly and pythonic interface for working with date-time expressions, as well as querying runs based on its metrics results.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6J9gouDEt9Mkhv9Cds4dNw.png\" alt=\"\"></p>\n<h1>Remote Tracking</h1>\n<p>The Aim remote tracking server, which allows running trainings in a multi-host environment and collect tracked data in a centralized location, used to be one of the most requested features.</p>\n<p>It has been gradually rolled out from its experimental phase to a stable version that can scale up and handle an increasing number of parallel trainings.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CizfTBF8Nh4DZcun6TP8Pg.png\" alt=\"\"></p>\n<p>Switching from a local environment to a centralized server is as easy as pie, as zero code changes are required.âš¡<br>\nThis is because the interfaces are completely compatible!</p>\n<h1>Active Monitoring</h1>\n<p>Aim made it first steps towards optimizing conducting long and large scale trainings, such as training or tuning GPT-like models. It reduces human-hours spent monitoring/restarting runs, improves reproducibility and ramps up time for people who need to train these models:</p>\n<ul>\n<li>Ability to notify on stalled/stuck runs.</li>\n<li>Callbacks and notifications to define training heuristics programmatically.</li>\n</ul>\n<p>A quote from â€œScaling Laws for Generative Mixed-Modal Language Modelsâ€ (Aghajanyan et al., 2023,Â <a href=\"https://arxiv.org/abs/2301.03728\">arXiv:2301.03728</a>):</p>\n<blockquote>\n<p>We tracked all experiments using the Aim experiment tracker (Arakelyan et al., 2020). To ensure consistent training strategies across our experiments, we implemented a model restart policy using the Aim experiment tracker and callbacks. Specifically, if training perplexities do not decrease after 500 million tokens, the training run is restarted with a reduced learning rate with a factor of 0.8 of the current time step. This policy helps remove variance in the scaling laws due to differences in training procedures and allows us to scale up the number of asynchronous experiments significantly. All experiments were conducted in a two-month time frame with a cluster of 768 80GB A100 GPUs. The majority of experiments used 64 GPUs at a time.</p>\n</blockquote>\n<h1>Key Performance and Usability Optimizations</h1>\n<ul>\n<li>Storage: long metrics sampling and retrieval (>1M steps).</li>\n<li>Storage: robust locking mechanism and automatic background indexing.</li>\n<li>UI: The v3.12 milestone was released, addressing over 30 usability issues.</li>\n<li>UI: virtualizations in various places across the UI to improve responsiveness when displaying large amount of data.</li>\n<li>UI: optimizations in stream decoding and data encoding to enhance overall performance.</li>\n<li>UI: live update optimizations to effectively update and display real-time data.</li>\n</ul>\n<h1>Integrations</h1>\n<p>Aim easily integrates with a large number of widely adopted machine learning frameworks and tools, reducing barriers to get started with Aim. During the past year, a number of integrations were added:</p>\n<ul>\n<li><strong>ML Frameworks</strong>Â â€” Pytorch Ignite, CatBoost, LightGBM, KerasTuner, fastai, MXNet, Optuna, PaddlePaddle</li>\n<li><strong>Convertors</strong>Â to easily migrate from other toolsÂ <strong>â€”</strong>Â TensorBoard to Aim, MLFlow to Aim, WandB to Aim.</li>\n<li><strong>Tools that integrated Aim</strong>Â â€” Metaâ€™s fairseq and metaseq frameworks, HuggingFaceâ€™s accelerate, Ludwig and Kedro.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1358/format:webp/1*YT4Yf1sJUqQLWRtiF8fJgQ.png\" alt=\"\"></p>\n<h1>Learn More</h1>\n<p>If you have any questions joinÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. ðŸ™Œ</p>\n<p>Show some love by dropping a â­ï¸ onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/aim-2022-product-recap.md","_raw":{"sourceFilePath":"posts/aim-2022-product-recap.md","sourceFileName":"aim-2022-product-recap.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2022-product-recap"},"type":"Post"},{"title":"Aim 3.1 â€” Images Tracker and Images Explorer","date":"2021-11-25T14:57:05.232Z","author":"Gev Soghomonian","description":"Excited to share with you the Aim 3.1! ðŸ˜Š A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community","slug":"aim-3-1-â€”-images-tracker-and-images-explorer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Excited to share with you theÂ Aim 3.1!Â \n\nA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWe have added a new demo to showcase the new Explore Images feature. We have also forked theÂ [lightweight-gan from lucidrains](https://github.com/lucidrains/lightweight-gan)Â and added the Aim integration.\n\nDemo code:Â <https://github.com/aimhubio/lightweight-gan>\\\nDemo site:Â [play.aimstack.io:10002](http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq&select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS&images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK)\n\n**The notable changes in the Aim 3.1.0:**\n\n* Images tracker and explorer\n* Runs navigation from the single run page\n* Performance improvements and tests\n\nThanks to srikanth,Â MohamadÂ Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\n\n## Images Tracker and Explorer\n\nFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\n\nAll the regular grouping and search goodies are available on Images Explorer tooÂ \n\nAdd an image tracker in your training code like this:\n\n```\nrun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n```\n\nOnce you run your training, the images will be saved and become available on the Images Explorer tab.\n\nOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\n\nHere is a quick promo video of what it can do. The rest of the details can be found in the docs[Â here](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png)\n\nA quick demo video.\n\n![](https://youtu.be/4mtUFV8yG_o)\n\nThe Images view on the single run page is coming soon â€¦\n\n## Runs Navigation\n\nOne of the regressions from the Aim 2.x toÂ [3.0.0](https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/)Â was the ability to navigate between the runs of the same experiment from the single run page.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63UGGl2EDquqGHIY03VpQQ.png)\n\nWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\n\nSorry for the regressionÂ \n\n## Performance improvements and assurances\n\nTwo groups of performance improvements have been madeâ€” on the UI and on the SDK side.\n\n### **Column virtualization on Aim UI Table**\n\nThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\n\nWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem â€” and this is a key step towards Â it. ðŸ»\n\n### **Storage performance tests**\n\nOne of the unique challenges in building Aim is to make sure each newly added data type wonâ€™t impact the overall performance. We have added continuously add more performance tests to ensure the new releases donâ€™t impact the performance.\n\nIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\n\nPerformance reliability is key for Aim and this is a major iteration towards that!\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for support.","html":"<p>Excited to share with you theÂ Aim 3.1!Â </p>\n<p>A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.</p>\n<p>We have added a new demo to showcase the new Explore Images feature. We have also forked theÂ <a href=\"https://github.com/lucidrains/lightweight-gan\">lightweight-gan from lucidrains</a>Â and added the Aim integration.</p>\n<p>Demo code:Â <a href=\"https://github.com/aimhubio/lightweight-gan\">https://github.com/aimhubio/lightweight-gan</a><br>\nDemo site:Â <a href=\"http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq&#x26;select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS&#x26;images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK\">play.aimstack.io:10002</a></p>\n<p><strong>The notable changes in the Aim 3.1.0:</strong></p>\n<ul>\n<li>Images tracker and explorer</li>\n<li>Runs navigation from the single run page</li>\n<li>Performance improvements and tests</li>\n</ul>\n<p>Thanks to srikanth,Â MohamadÂ Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.</p>\n<h2>Images Tracker and Explorer</h2>\n<p>Finally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.</p>\n<p>All the regular grouping and search goodies are available on Images Explorer tooÂ </p>\n<p>Add an image tracker in your training code like this:</p>\n<pre><code>run.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n</code></pre>\n<p>Once you run your training, the images will be saved and become available on the Images Explorer tab.</p>\n<p>On Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.</p>\n<p>Here is a quick promo video of what it can do. The rest of the details can be found in the docs<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\">Â here</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png\" alt=\"\"></p>\n<p>A quick demo video.</p>\n<p><img src=\"https://youtu.be/4mtUFV8yG_o\" alt=\"\"></p>\n<p>The Images view on the single run page is coming soon â€¦</p>\n<h2>Runs Navigation</h2>\n<p>One of the regressions from the Aim 2.x toÂ <a href=\"https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/\">3.0.0</a>Â was the ability to navigate between the runs of the same experiment from the single run page.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63UGGl2EDquqGHIY03VpQQ.png\" alt=\"\"></p>\n<p>We have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.</p>\n<p>Sorry for the regressionÂ </p>\n<h2>Performance improvements and assurances</h2>\n<p>Two groups of performance improvements have been madeâ€” on the UI and on the SDK side.</p>\n<h3><strong>Column virtualization on Aim UI Table</strong></h3>\n<p>The ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.</p>\n<p>We are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem â€” and this is a key step towards Â it. ðŸ»</p>\n<h3><strong>Storage performance tests</strong></h3>\n<p>One of the unique challenges in building Aim is to make sure each newly added data type wonâ€™t impact the overall performance. We have added continuously add more performance tests to ensure the new releases donâ€™t impact the performance.</p>\n<p>In the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.</p>\n<p>Performance reliability is key for Aim and this is a major iteration towards that!</p>\n<h2>Learn more</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support.</p>"},"_id":"posts/aim-3-1-â€”-images-tracker-and-images-explorer.md","_raw":{"sourceFilePath":"posts/aim-3-1-â€”-images-tracker-and-images-explorer.md","sourceFileName":"aim-3-1-â€”-images-tracker-and-images-explorer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-1-â€”-images-tracker-and-images-explorer"},"type":"Post"},{"title":"Aim 3.10 â€” Visualize terminal logs, M1 support & better query autocomplete","date":"2022-05-25T20:49:49.499Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.10 is now available! We are on a mission to democratize AI dev tools.","slug":"aim-3-10-â€”-visualize-terminal-logs-m1-support-better-query-autocomplete","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5HbX-z8qa0ORFDRKArmFw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.10 is now available!\n\nWe are on a mission toÂ [democratize AI dev tools](https://aimstack.io/blog/new-releases/aim%E2%80%99s-foundations-why-we%E2%80%99re-building-a-tensorboard-alternative). Thanks to the awesome Aim community for the help and contributions.\n\nHere is whatâ€™s new in Aim 3.10:\n\n* Visualize terminal logs\n* M1 support\n* Better autocomplete experience\n* Aim citation available\n* CatBoost integration\n* LightGBM integration\n\nCheck out the 3.10 releaseÂ [milestone](https://github.com/aimhubio/aim/milestone/33)Â and theÂ [GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.10.0)\n\n> *Shout out to Daniel Wessel,Â [arnauddhaene](https://github.com/arnauddhaene),[Â uduse](https://github.com/uduse),Â [lukoucky](https://github.com/lukoucky), Armen Aghajanyan and others for contributions and feedback. We really appreciate it.*\n\n## Visualize terminal logs\n\nWhen it comes to automating training of multiple runs with job schedulers or workload managers on a cluster, it becomes hard to track theÂ **terminal**Â **logs**Â of the runs.\n\nNow Aim automatically streams the terminal logs to the UI. Near-real-time.\n\nThe terminal logs can be turned off if the run instance is created with the following flag in place:\n\n```\naim_run = Run(capture_terminal_logs=False)\n```\n\nCheck out more about this featureÂ [in Aim docs](https://aimstack.readthedocs.io/en/latest/using/configure_runs.html?highlight=terminal%20logs#capturing-terminal-logs).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K60XrQq0iH6NlO0NFz44EQ.png)\n\n## M1 support\n\n[With the awesome work by the PyTorch team](https://twitter.com/PyTorch/status/1526944876478144512)Â on enabling support for GPU-accelerated PyTorch training on Mac, there has been a huge demand to enable aim on M1 as well.\n\nAim now supports M1 too. Now you can use Aim withÂ [PyTorch](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)Â on Mac to track and deeply compare your experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp5LKw55RgE4gnPXC1cWIg.png)\n\n## Better autocomplete experience\n\nWe have integrated a rich code autocomplete system as the Aim community loves to deeply query their training runs. This has been a highly requested improvement and a huge productivity booster. Expect more improvements here.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*vEQnAJSks5YGNCXHllDnNA.gif)\n\n## Aim citation available\n\nNow you can cite Aim from your paper if you are using Aim to compare your experiments.Â [The citation file](https://github.com/aimhubio/aim/blob/main/CITATION.cff).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLHweR2at8tbpHiTkelbfA.png)\n\n[Thanks to the researchers from Meta AI Research for the incentive and initiative.](https://arxiv.org/abs/2205.10770)Â Â ðŸ™Œ\n\n## CatBoost integration\n\n[CatBoost](https://catboost.ai/)Â is one of the fastest Gradient Boosting on Decision Trees library. Aim now supports CatBoost out of the box. Check out the[Â Aim CatBoost docs here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost).\n\nHere is a shortÂ [example](https://github.com/aimhubio/aim/blob/main/examples/catboost_track.py)Â that shows how to use AimLogger for CatBoost.\n\n## LightGBM integration\n\n[LightGBM](https://lightgbm.readthedocs.io/en/latest/)Â is one of the most widely-used and battle-tested gradient boosting frameworks. Due to high-demand we have also added an out-of-the-box Aim LightGBM integration.\n\n[Here is a short code example](https://github.com/aimhubio/aim/blob/main/examples/lightgbm_track.py)Â andÂ [the docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm)Â on how to use the integration.\n\n- - -\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.io/blog/new-releases/aim%E2%80%99s-foundations-why-we%E2%80%99re-building-a-tensorboard-alternative)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n* [](https://twitter.com/share?url=https://aimstack.io/aim-3-10-release-catboost-integration/&text=Aim%203.10%20%E2%80%94%20Visualize%20terminal%20logs%2C%20M1%20support%20%26%23038%3B%20better%20query%20autocomplete)","html":"<p>Hey team, Aim 3.10 is now available!</p>\n<p>We are on a mission toÂ <a href=\"https://aimstack.io/blog/new-releases/aim%E2%80%99s-foundations-why-we%E2%80%99re-building-a-tensorboard-alternative\">democratize AI dev tools</a>. Thanks to the awesome Aim community for the help and contributions.</p>\n<p>Here is whatâ€™s new in Aim 3.10:</p>\n<ul>\n<li>Visualize terminal logs</li>\n<li>M1 support</li>\n<li>Better autocomplete experience</li>\n<li>Aim citation available</li>\n<li>CatBoost integration</li>\n<li>LightGBM integration</li>\n</ul>\n<p>Check out the 3.10 releaseÂ <a href=\"https://github.com/aimhubio/aim/milestone/33\">milestone</a>Â and theÂ <a href=\"https://github.com/aimhubio/aim/releases/tag/v3.10.0\">GitHub Release Notes</a></p>\n<blockquote>\n<p><em>Shout out to Daniel Wessel,Â <a href=\"https://github.com/arnauddhaene\">arnauddhaene</a>,<a href=\"https://github.com/uduse\">Â uduse</a>,Â <a href=\"https://github.com/lukoucky\">lukoucky</a>, Armen Aghajanyan and others for contributions and feedback. We really appreciate it.</em></p>\n</blockquote>\n<h2>Visualize terminal logs</h2>\n<p>When it comes to automating training of multiple runs with job schedulers or workload managers on a cluster, it becomes hard to track theÂ <strong>terminal</strong>Â <strong>logs</strong>Â of the runs.</p>\n<p>Now Aim automatically streams the terminal logs to the UI. Near-real-time.</p>\n<p>The terminal logs can be turned off if the run instance is created with the following flag in place:</p>\n<pre><code>aim_run = Run(capture_terminal_logs=False)\n</code></pre>\n<p>Check out more about this featureÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/configure_runs.html?highlight=terminal%20logs#capturing-terminal-logs\">in Aim docs</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K60XrQq0iH6NlO0NFz44EQ.png\" alt=\"\"></p>\n<h2>M1 support</h2>\n<p><a href=\"https://twitter.com/PyTorch/status/1526944876478144512\">With the awesome work by the PyTorch team</a>Â on enabling support for GPU-accelerated PyTorch training on Mac, there has been a huge demand to enable aim on M1 as well.</p>\n<p>Aim now supports M1 too. Now you can use Aim withÂ <a href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\">PyTorch</a>Â on Mac to track and deeply compare your experiments.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp5LKw55RgE4gnPXC1cWIg.png\" alt=\"\"></p>\n<h2>Better autocomplete experience</h2>\n<p>We have integrated a rich code autocomplete system as the Aim community loves to deeply query their training runs. This has been a highly requested improvement and a huge productivity booster. Expect more improvements here.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*vEQnAJSks5YGNCXHllDnNA.gif\" alt=\"\"></p>\n<h2>Aim citation available</h2>\n<p>Now you can cite Aim from your paper if you are using Aim to compare your experiments.Â <a href=\"https://github.com/aimhubio/aim/blob/main/CITATION.cff\">The citation file</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLHweR2at8tbpHiTkelbfA.png\" alt=\"\"></p>\n<p><a href=\"https://arxiv.org/abs/2205.10770\">Thanks to the researchers from Meta AI Research for the incentive and initiative.</a>Â Â ðŸ™Œ</p>\n<h2>CatBoost integration</h2>\n<p><a href=\"https://catboost.ai/\">CatBoost</a>Â is one of the fastest Gradient Boosting on Decision Trees library. Aim now supports CatBoost out of the box. Check out the<a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">Â Aim CatBoost docs here</a>.</p>\n<p>Here is a shortÂ <a href=\"https://github.com/aimhubio/aim/blob/main/examples/catboost_track.py\">example</a>Â that shows how to use AimLogger for CatBoost.</p>\n<h2>LightGBM integration</h2>\n<p><a href=\"https://lightgbm.readthedocs.io/en/latest/\">LightGBM</a>Â is one of the most widely-used and battle-tested gradient boosting frameworks. Due to high-demand we have also added an out-of-the-box Aim LightGBM integration.</p>\n<p><a href=\"https://github.com/aimhubio/aim/blob/main/examples/lightgbm_track.py\">Here is a short code example</a>Â andÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">the docs</a>Â on how to use the integration.</p>\n<hr>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.io/blog/new-releases/aim%E2%80%99s-foundations-why-we%E2%80%99re-building-a-tensorboard-alternative\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<ul>\n<li><a href=\"https://twitter.com/share?url=https://aimstack.io/aim-3-10-release-catboost-integration/&#x26;text=Aim%203.10%20%E2%80%94%20Visualize%20terminal%20logs%2C%20M1%20support%20%26%23038%3B%20better%20query%20autocomplete\"></a></li>\n</ul>"},"_id":"posts/aim-3-10-â€”-visualize-terminal-logs-m1-support-better-query-autocomplete.md","_raw":{"sourceFilePath":"posts/aim-3-10-â€”-visualize-terminal-logs-m1-support-better-query-autocomplete.md","sourceFileName":"aim-3-10-â€”-visualize-terminal-logs-m1-support-better-query-autocomplete.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-10-â€”-visualize-terminal-logs-m1-support-better-query-autocomplete"},"type":"Post"},{"title":"Aim 3.13â€“Figures Explorer and notifications on stalled runs","date":"2022-08-24T21:08:50.210Z","author":"Gev Soghomonian","description":"In this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we havenâ€™t had the chance to talk about much. ","slug":"aim-3-13â€“figures-explorer-and-notifications-on-stalled-runs","image":"https://miro.medium.com/max/1400/1*5hUcOciwufupoI4-nBbcyA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"# Hey team, Aim 3.13 is now available!\n\nIn this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we havenâ€™t had the chance to talk about much. Stay tuned on theÂ [AimStack](https://twitter.com/aimstackio)Â twitter where we will share them and more use-cases.\n\n**Aim is evolving really quickly! ðŸš€**\n\n> *We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.*\n>\n> *Thanks toÂ [Sharathmk99 and](https://github.com/Sharathmk99)Â [tmynn](https://github.com/tmynn)Â for their first contributions!! ðŸ”¥ðŸ”¥ðŸ”¥*\n\n**TLDR;**\n\nAim 3.13 is full of impactful changes. Here are the main highlights!\n\n* **Figures Explorer**\\\n  Explore and compare 100s of Plotly figures within a few clicks.\n* **Notify about stalled runs**\\\n  Configure Aim to notify your team when the run is stalled (slack, email, workspace are available)\n* **Stable Aim Remote Server**\n* **KerasTuner integration**\n* **Weights and Biases log converter**\n\nCheck out the Aim 3.13 releaseÂ [milestone](https://github.com/aimhubio/aim/issues?q=is%3Aissue+milestone%3Av3.13.0+is%3Aopen)Â and theÂ [GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.13.0)Â for more information.\n\n# Figures Explorer\n\nFigures Explorer is a new addition to the mighty Aim explorers. With this feature now you can compare 1000s of Plotly figures within a few clicks.\n\nThis is just the first iteration ðŸ˜Š. Expect lots of improvements of the experience over time. This is how the figures explorer works:\n\n![](https://miro.medium.com/max/1400/1*WKwbKRBJPK8fErSKVItf6Q.gif)\n\n# Notify about stalled runs\n\nOne of the ways the precious training time is prolonged / wasted when for some reasons the runs stall. The training runs can stall in so many different ways:\n\n* Hardware issues\n* exception in the code\n* driver issues\n\nAnd many other things we never anticipate. This feature allows Aim users to configure notifications to their preferred channel (slack, workspace) when the run stalls.\n\nCheck out theÂ [docs](https://aimstack.readthedocs.io/en/latest/using/run_status_notifications.html#)Â for more info. Here is how it looks on your slack:\n\n![](https://miro.medium.com/max/1400/1*OFZh37yiEa_vo-14gv7UFw.webp)\n\n# Stable Aim Remote Server\n\nAim Remote allows to set up a remote Aim tracking server so the tracked logs can be sent to a centralized location.\n\nItâ€™s been a while since Aim Remote Server was an experimental feature. We have been lucky as the users have started using it and shared lots of feedbacks, issues that we have fixed.\n\nNow we are excited to announce that the Aim Remote Server is stable\n\n![](https://miro.medium.com/max/1400/1*zC-crWCgWncraYJFQRAd3g.webp)\n\n# KerasTuner integration\n\nA highly requested integration with KerasTuner.\n\nHere are theÂ [docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tunerhttps://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tuner)Â and a short code snippet. Super-easy to integrate Aim with your KerasTuner project now.\n\n```\nfrom aim.keras_tuner import AimCallback\n\n...\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n```\n\n# Weights and Biases log converter\n\nThe W&B users who also wanted to use Aim in their work, have been asking about this feature for a while now.\n\nExcited to share that now you can easily convert your W&B logs to Aim.\n\nHere is how it works:\n\n```\ncd project-directory\n# Init the Aim repo\naim init \n# Run the converter to migrate logs from WandB to Aim\naim convert wandb --entity 'my_team' --project 'my_project'\n```\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)Â ðŸ™Œ\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDonâ€™t forget to leave us a star onÂ [GitHub](https://github.com/aimhubio/aim)Â if you think Aim is useful â­ï¸","html":"<h1>Hey team, Aim 3.13 is now available!</h1>\n<p>In this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we havenâ€™t had the chance to talk about much. Stay tuned on theÂ <a href=\"https://twitter.com/aimstackio\">AimStack</a>Â twitter where we will share them and more use-cases.</p>\n<p><strong>Aim is evolving really quickly! ðŸš€</strong></p>\n<blockquote>\n<p><em>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.</em></p>\n<p><em>Thanks toÂ <a href=\"https://github.com/Sharathmk99\">Sharathmk99 and</a>Â <a href=\"https://github.com/tmynn\">tmynn</a>Â for their first contributions!! ðŸ”¥ðŸ”¥ðŸ”¥</em></p>\n</blockquote>\n<p><strong>TLDR;</strong></p>\n<p>Aim 3.13 is full of impactful changes. Here are the main highlights!</p>\n<ul>\n<li><strong>Figures Explorer</strong><br>\nExplore and compare 100s of Plotly figures within a few clicks.</li>\n<li><strong>Notify about stalled runs</strong><br>\nConfigure Aim to notify your team when the run is stalled (slack, email, workspace are available)</li>\n<li><strong>Stable Aim Remote Server</strong></li>\n<li><strong>KerasTuner integration</strong></li>\n<li><strong>Weights and Biases log converter</strong></li>\n</ul>\n<p>Check out the Aim 3.13 releaseÂ <a href=\"https://github.com/aimhubio/aim/issues?q=is%3Aissue+milestone%3Av3.13.0+is%3Aopen\">milestone</a>Â and theÂ <a href=\"https://github.com/aimhubio/aim/releases/tag/v3.13.0\">GitHub Release Notes</a>Â for more information.</p>\n<h1>Figures Explorer</h1>\n<p>Figures Explorer is a new addition to the mighty Aim explorers. With this feature now you can compare 1000s of Plotly figures within a few clicks.</p>\n<p>This is just the first iteration ðŸ˜Š. Expect lots of improvements of the experience over time. This is how the figures explorer works:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*WKwbKRBJPK8fErSKVItf6Q.gif\" alt=\"\"></p>\n<h1>Notify about stalled runs</h1>\n<p>One of the ways the precious training time is prolonged / wasted when for some reasons the runs stall. The training runs can stall in so many different ways:</p>\n<ul>\n<li>Hardware issues</li>\n<li>exception in the code</li>\n<li>driver issues</li>\n</ul>\n<p>And many other things we never anticipate. This feature allows Aim users to configure notifications to their preferred channel (slack, workspace) when the run stalls.</p>\n<p>Check out theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/run_status_notifications.html#\">docs</a>Â for more info. Here is how it looks on your slack:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*OFZh37yiEa_vo-14gv7UFw.webp\" alt=\"\"></p>\n<h1>Stable Aim Remote Server</h1>\n<p>Aim Remote allows to set up a remote Aim tracking server so the tracked logs can be sent to a centralized location.</p>\n<p>Itâ€™s been a while since Aim Remote Server was an experimental feature. We have been lucky as the users have started using it and shared lots of feedbacks, issues that we have fixed.</p>\n<p>Now we are excited to announce that the Aim Remote Server is stable</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*zC-crWCgWncraYJFQRAd3g.webp\" alt=\"\"></p>\n<h1>KerasTuner integration</h1>\n<p>A highly requested integration with KerasTuner.</p>\n<p>Here are theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tunerhttps://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tuner\">docs</a>Â and a short code snippet. Super-easy to integrate Aim with your KerasTuner project now.</p>\n<pre><code>from aim.keras_tuner import AimCallback\n\n...\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n</code></pre>\n<h1>Weights and Biases log converter</h1>\n<p>The W&#x26;B users who also wanted to use Aim in their work, have been asking about this feature for a while now.</p>\n<p>Excited to share that now you can easily convert your W&#x26;B logs to Aim.</p>\n<p>Here is how it works:</p>\n<pre><code>cd project-directory\n# Init the Aim repo\naim init \n# Run the converter to migrate logs from WandB to Aim\naim convert wandb --entity 'my_team' --project 'my_project'\n</code></pre>\n<h1>Learn More</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a>Â ðŸ™Œ</p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>Donâ€™t forget to leave us a star onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>Â if you think Aim is useful â­ï¸</p>"},"_id":"posts/aim-3-13â€“figures-explorer-and-notifications-on-stalled-runs.md","_raw":{"sourceFilePath":"posts/aim-3-13â€“figures-explorer-and-notifications-on-stalled-runs.md","sourceFileName":"aim-3-13â€“figures-explorer-and-notifications-on-stalled-runs.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-13â€“figures-explorer-and-notifications-on-stalled-runs"},"type":"Post"},{"title":"Aim 3.14 â€” Brand new Home Page!","date":"2022-11-03T21:31:15.756Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.14 is now available! ","slug":"aim-3-14-â€”-brand-new-home-page","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEJ0_GtGyXC4sv2sZrSCRA.png","draft":false,"categories":["New Releases"],"body":{"raw":"# Hey team, Aim 3.14 is now available!\n\nWe are releasing a new Aim version every 4â€“5 weeks with community-contributed features and fixes.\n\n**A brand new Aim release every few weeks**. With new features and fixes! ðŸš€\n\n> We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n>\n> Thanks toÂ [djwessel](https://github.com/djwessel)Â andÂ [Vahram-aimhub](https://github.com/Vahram-aimhub)Â for their first contributionsðŸ”¥ðŸ”¥ðŸ”¥\n\n# Aim 3.14 Overview\n\nHere is whatâ€™s new in the latest release:\n\n* **Brand new Home Page**\n* **MXNet and FastAI loggers**\n* **New tooltip positioning modes**\n* **Ability to edit tags on the runs table**\n\nCheck out theÂ [Aim 3.14 release milestone](https://github.com/aimhubio/aim/milestone/41)Â and theÂ [GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.14.0)Â for more information.\n\n> Note: at the time of writing this, Aim is atÂ [3.14.3](https://github.com/aimhubio/aim/releases)Â â€” check those intermediate versions too.\n\n# Brand new Home Page\n\nAfter months of requests from the community to build a quick access page, improve the onboarding, we are excited to introduce new Home page.\n\nThe home page is pretty self-descriptive. Itâ€™s an overview of whatever actions have been taken using Aim.\n\nThis is in no way the final version, we continue to iterate on it. Would love to hear your feedbacks.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhWVuIETasWSsUc3oEYdfQ.png)\n\n# MXNet and fastai loggers\n\nNow there are loggers available both for MxNet and FastAI.\n\nIt takes only a few lines to incorporate Aim to your code.\n\nHere is how it looks for MXNet in your code. MoreÂ [MXNet integration details here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-mxnet)\n\n\n\n```\nfrom aim.mxnet import AimLoggingHandler\n\naim_log_handler = AimLoggingHandler(repo='.', experiment_name='mxnet_example',\n                                    log_interval=1, metrics=[train_acc, train_loss, val_acc])\n\nest.fit(train_data=train_data_loader, val_data=val_data_loader,\n        epochs=num_epochs, event_handlers=[aim_log_handler])\n```\n\nThis is the code for fastai Aim integrations.Â [More details here.](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)\n\n```\nfrom aim.fastai import AimCallback\n\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_example'))\n```\n\n## New tooltip positioning modes\n\nWe have added three buttons to the Explorers tooltip on click.\n\nThese buttons allow to fix where youâ€™d want to see these tooltips appear when hovering over the metrics.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71V5hEnGIOg2-QIi01ok1A.png)\n\nIn this case we have chosen to place the tooltip at the top of the page.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vDsrHdRa42jxpTcG66nkg.png)\n\n# Ability to edit tags on the runs table\n\nTags have become one of the widely used features lately and we are iterating so you can make the best out of them.\n\nNow you can add tags to your runs from within the Runs Explorer.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vk0DJyFCGdw9A_TPzGDMRg.png)\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)Â ðŸ™Œ\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDonâ€™t forget to leave us a star onÂ [GitHub](https://github.com/aimhubio/aim)Â if you think Aim is useful â­ï¸.","html":"<h1>Hey team, Aim 3.14 is now available!</h1>\n<p>We are releasing a new Aim version every 4â€“5 weeks with community-contributed features and fixes.</p>\n<p><strong>A brand new Aim release every few weeks</strong>. With new features and fixes! ðŸš€</p>\n<blockquote>\n<p>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.</p>\n<p>Thanks toÂ <a href=\"https://github.com/djwessel\">djwessel</a>Â andÂ <a href=\"https://github.com/Vahram-aimhub\">Vahram-aimhub</a>Â for their first contributionsðŸ”¥ðŸ”¥ðŸ”¥</p>\n</blockquote>\n<h1>Aim 3.14 Overview</h1>\n<p>Here is whatâ€™s new in the latest release:</p>\n<ul>\n<li><strong>Brand new Home Page</strong></li>\n<li><strong>MXNet and FastAI loggers</strong></li>\n<li><strong>New tooltip positioning modes</strong></li>\n<li><strong>Ability to edit tags on the runs table</strong></li>\n</ul>\n<p>Check out theÂ <a href=\"https://github.com/aimhubio/aim/milestone/41\">Aim 3.14 release milestone</a>Â and theÂ <a href=\"https://github.com/aimhubio/aim/releases/tag/v3.14.0\">GitHub Release Notes</a>Â for more information.</p>\n<blockquote>\n<p>Note: at the time of writing this, Aim is atÂ <a href=\"https://github.com/aimhubio/aim/releases\">3.14.3</a>Â â€” check those intermediate versions too.</p>\n</blockquote>\n<h1>Brand new Home Page</h1>\n<p>After months of requests from the community to build a quick access page, improve the onboarding, we are excited to introduce new Home page.</p>\n<p>The home page is pretty self-descriptive. Itâ€™s an overview of whatever actions have been taken using Aim.</p>\n<p>This is in no way the final version, we continue to iterate on it. Would love to hear your feedbacks.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhWVuIETasWSsUc3oEYdfQ.png\" alt=\"\"></p>\n<h1>MXNet and fastai loggers</h1>\n<p>Now there are loggers available both for MxNet and FastAI.</p>\n<p>It takes only a few lines to incorporate Aim to your code.</p>\n<p>Here is how it looks for MXNet in your code. MoreÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-mxnet\">MXNet integration details here</a></p>\n<pre><code>from aim.mxnet import AimLoggingHandler\n\naim_log_handler = AimLoggingHandler(repo='.', experiment_name='mxnet_example',\n                                    log_interval=1, metrics=[train_acc, train_loss, val_acc])\n\nest.fit(train_data=train_data_loader, val_data=val_data_loader,\n        epochs=num_epochs, event_handlers=[aim_log_handler])\n</code></pre>\n<p>This is the code for fastai Aim integrations.Â <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">More details here.</a></p>\n<pre><code>from aim.fastai import AimCallback\n\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_example'))\n</code></pre>\n<h2>New tooltip positioning modes</h2>\n<p>We have added three buttons to the Explorers tooltip on click.</p>\n<p>These buttons allow to fix where youâ€™d want to see these tooltips appear when hovering over the metrics.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71V5hEnGIOg2-QIi01ok1A.png\" alt=\"\"></p>\n<p>In this case we have chosen to place the tooltip at the top of the page.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vDsrHdRa42jxpTcG66nkg.png\" alt=\"\"></p>\n<h1>Ability to edit tags on the runs table</h1>\n<p>Tags have become one of the widely used features lately and we are iterating so you can make the best out of them.</p>\n<p>Now you can add tags to your runs from within the Runs Explorer.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vk0DJyFCGdw9A_TPzGDMRg.png\" alt=\"\"></p>\n<h1>Learn More</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a>Â ðŸ™Œ</p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>Donâ€™t forget to leave us a star onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>Â if you think Aim is useful â­ï¸.</p>"},"_id":"posts/aim-3-14-â€”-brand-new-home-page.md","_raw":{"sourceFilePath":"posts/aim-3-14-â€”-brand-new-home-page.md","sourceFileName":"aim-3-14-â€”-brand-new-home-page.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-14-â€”-brand-new-home-page"},"type":"Post"},{"title":"Aim 3.15 â€” Callbacks, logging and notifications, line chart legends, experiment page, Audios Explorer","date":"2022-12-06T21:40:50.164Z","author":"Gor Arakelyan","description":"Aim 3.15 is out!","slug":"aim-3-15-â€”-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer","image":"https://miro.medium.com/max/1400/1*wQWwnwDeCSVQ-YbZapbHlQ.webp","draft":false,"categories":["New Releases"],"body":{"raw":"## Hey team, get ready for a major upgrade!\n\nWe are thrilled to announce the release of Aim v3.15, packed with new features and improvements.\n\n> *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Without your help, Aim would not be where it is today.*\n>\n> *Congratulations toÂ [jangop](https://github.com/jangop),Â [hjoonjang](https://github.com/hjoonjang)Â andÂ [kumarshreshtha](https://github.com/kumarshreshtha)*Â *for their first contributions.*\n\n**TL;DR**Â The new version includes lots of significant and highly requested enhancements, as well as key improvements. Here are the highlights:\n\n* **Callbacks**\n* **Logging and notifications**\n* **Line chart legends**\n* **Audios Explorer**\n* **Experiment page**\n* **Robust locking mechanism**\n* **Integrations with PaddlePaddle and Optuna**\n\n# Aim SDK\n\n## **Callbacks**\n\nMany things can go wrong during ML training that could result in wasted GPUs and time. The Aim callbacks API helps to define custom callbacks to be executed at any point during ML training.\n\nFind API docs and examplesÂ [here](https://aimstack.readthedocs.io/en/latest/using/callbacks.html).\n\nAn end-to-end toy script below:\n\n*Send a notification and exits the process, when the loss explodes.*\n\n```\nimport random\nimport time\n\nfrom aim import Run\nfrom aim import TrainingFlow\nfrom aim.sdk.callbacks import events\n\n\n# Define the callbacks\nclass MyCallbacks:\n    @events.on.training_metrics_collected\n    def check_loss_explosion(self, metrics, run, **kwargs):\n        if metrics.get('loss') > 1:\n            # Log warning message and send the corresponding notification\n            run.log_warning(f'loss exploded: {metrics[\"loss\"]}')\n            # Exit the process\n            exit()\n\n\n# Initialize Aim run and register callbacks\nrun = Run()\ntraining_flow = TrainingFlow(run=run)\ntraining_flow.register(MyCallbacks())\n\n\n# Training simulation\nfor _ in range(100):\n    loss = random.random() * 2\n    print(loss)\n    run.track(loss, name='loss', context={'subset': 'train'})\n    training_flow.training_metrics_collected(\n        metrics={'loss': loss},\n        run=run\n    )\n    time.sleep(1)\n```\n\n## Logging and notifications\n\nDuring the training many key events happen. Use new Aim logging API to log any text message and send the corresponding notification to slack or workplace. It is as easy as callingÂ `aim_run.log_error(message)`.\n\nIt perfectly fits with callbacks API and enables programmatically defining training heuristics and including it as part of the code. A simple example is â€œlet me know when the training run starts to overfitâ€.\n\nRead more in theÂ [docs](https://aimstack.readthedocs.io/en/latest/using/logging.html).\n\n# Aim UI\n\n## Chart legends\n\nSince the early days of Aimâ€™s launch, one of the most requested features has been displaying chart legends. And finally it is here! The legends section makes it easy to learn metrics corresponding fields and hyper-params at a glance.\n\n![](https://miro.medium.com/max/1400/1*TFNCZjjwXFg7JZv1F1qSpg.webp)\n\n## **Audios Explorer**\n\nTo track the performance of ML models that process speech data, it is important to track audio objects.\n\nThe new Audios Explorer makes it super easy to query and play tracked audio objects across the runs. Flexible grouping options enable arranging them into rows and columns to make exploration even easier.\n\n![](https://miro.medium.com/max/1400/1*mnP4qbmlQuJE-nGqe-36lw.webp)\n\n\n\n## Experiment page\n\nThe experiment page provides an holistic view of the results of the trainings in order to identify trends and patterns that can help inform further experimentation and optimization.\n\nExperiment page consists from the following tabs:\n\n* ðŸ Â **Overview**Â â€” Experiment overview, number of active/finished runs, timeline of the runs.\n* ðŸƒÂ **Runs**Â â€”Holistic view of the experiment runs.\n* ðŸ““Â **Notes**Â â€” Experiment notes and insights with notion-like rich editor.\n* âš™ï¸Â **Settings**Â â€” Experiment settings, such as name and description.\n\n![](https://miro.medium.com/max/1400/1*h-DLE3XnBM5uGKBDRcu_iA.webp)\n\n# Integrations\n\n## Integration with PaddlePaddle\n\nWith Aim v3.15 it takes just two steps to integrate Aim into your PaddlePaddle training script.\n\n```\n# Step 1: Explicitly import the AimCallback for tracking training metadata\nfrom aim.paddle import AimCallback\n\n...\n\n# Step 2: Pass the callback to callbacks list upon initiating your training\ncallback = AimCallback(repo='.', experiment='paddle_test')\nmodel.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)\n\n```\n\nSee docsÂ [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-paddlepaddle).\n\n## Integration with Optuna\n\nSmoothly integrate Aim into your Optuna project.\n\nHere are theÂ [docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-optuna)Â and a short code snippet.\n\n```\nfrom aim.optuna import AimCallback\n\n...\naim_callback = AimCallback(experiment_name=\"optuna_single_run\")\nstudy.optimize(objective, n_trials=10, callbacks=[aim_callback])\n```\n\n# *Key optimizations and fixes*\n\n\n\n## Runs locking mechanism and finished runs detection\n\nNumber of issues were reported w.r.t runs locking. More robust locking mechanism is applied starting from Aim v3.15.\n\n## Remote server scaling\n\nNow it is possible to spin up several workers to achieve higher processing speed for remote tracking server. Just run the server passing â€œworkersâ€ argument:Â `aim server --workers=N`\n\n# Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)Â ðŸ™Œ\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://discord.gg/F5TpzUeY), share your feedback, open issues for new features, bugs.\n\nDonâ€™t forget to leave us a star onÂ [GitHub](https://github.com/aimhubio/aim)Â if you think Aim is useful â­ï¸","html":"<h2>Hey team, get ready for a major upgrade!</h2>\n<p>We are thrilled to announce the release of Aim v3.15, packed with new features and improvements.</p>\n<blockquote>\n<p><em>We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Without your help, Aim would not be where it is today.</em></p>\n<p><em>Congratulations toÂ <a href=\"https://github.com/jangop\">jangop</a>,Â <a href=\"https://github.com/hjoonjang\">hjoonjang</a>Â andÂ <a href=\"https://github.com/kumarshreshtha\">kumarshreshtha</a></em>Â <em>for their first contributions.</em></p>\n</blockquote>\n<p><strong>TL;DR</strong>Â The new version includes lots of significant and highly requested enhancements, as well as key improvements. Here are the highlights:</p>\n<ul>\n<li><strong>Callbacks</strong></li>\n<li><strong>Logging and notifications</strong></li>\n<li><strong>Line chart legends</strong></li>\n<li><strong>Audios Explorer</strong></li>\n<li><strong>Experiment page</strong></li>\n<li><strong>Robust locking mechanism</strong></li>\n<li><strong>Integrations with PaddlePaddle and Optuna</strong></li>\n</ul>\n<h1>Aim SDK</h1>\n<h2><strong>Callbacks</strong></h2>\n<p>Many things can go wrong during ML training that could result in wasted GPUs and time. The Aim callbacks API helps to define custom callbacks to be executed at any point during ML training.</p>\n<p>Find API docs and examplesÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/callbacks.html\">here</a>.</p>\n<p>An end-to-end toy script below:</p>\n<p><em>Send a notification and exits the process, when the loss explodes.</em></p>\n<pre><code>import random\nimport time\n\nfrom aim import Run\nfrom aim import TrainingFlow\nfrom aim.sdk.callbacks import events\n\n\n# Define the callbacks\nclass MyCallbacks:\n    @events.on.training_metrics_collected\n    def check_loss_explosion(self, metrics, run, **kwargs):\n        if metrics.get('loss') > 1:\n            # Log warning message and send the corresponding notification\n            run.log_warning(f'loss exploded: {metrics[\"loss\"]}')\n            # Exit the process\n            exit()\n\n\n# Initialize Aim run and register callbacks\nrun = Run()\ntraining_flow = TrainingFlow(run=run)\ntraining_flow.register(MyCallbacks())\n\n\n# Training simulation\nfor _ in range(100):\n    loss = random.random() * 2\n    print(loss)\n    run.track(loss, name='loss', context={'subset': 'train'})\n    training_flow.training_metrics_collected(\n        metrics={'loss': loss},\n        run=run\n    )\n    time.sleep(1)\n</code></pre>\n<h2>Logging and notifications</h2>\n<p>During the training many key events happen. Use new Aim logging API to log any text message and send the corresponding notification to slack or workplace. It is as easy as callingÂ <code>aim_run.log_error(message)</code>.</p>\n<p>It perfectly fits with callbacks API and enables programmatically defining training heuristics and including it as part of the code. A simple example is â€œlet me know when the training run starts to overfitâ€.</p>\n<p>Read more in theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\">docs</a>.</p>\n<h1>Aim UI</h1>\n<h2>Chart legends</h2>\n<p>Since the early days of Aimâ€™s launch, one of the most requested features has been displaying chart legends. And finally it is here! The legends section makes it easy to learn metrics corresponding fields and hyper-params at a glance.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*TFNCZjjwXFg7JZv1F1qSpg.webp\" alt=\"\"></p>\n<h2><strong>Audios Explorer</strong></h2>\n<p>To track the performance of ML models that process speech data, it is important to track audio objects.</p>\n<p>The new Audios Explorer makes it super easy to query and play tracked audio objects across the runs. Flexible grouping options enable arranging them into rows and columns to make exploration even easier.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*mnP4qbmlQuJE-nGqe-36lw.webp\" alt=\"\"></p>\n<h2>Experiment page</h2>\n<p>The experiment page provides an holistic view of the results of the trainings in order to identify trends and patterns that can help inform further experimentation and optimization.</p>\n<p>Experiment page consists from the following tabs:</p>\n<ul>\n<li>ðŸ Â <strong>Overview</strong>Â â€” Experiment overview, number of active/finished runs, timeline of the runs.</li>\n<li>ðŸƒÂ <strong>Runs</strong>Â â€”Holistic view of the experiment runs.</li>\n<li>ðŸ““Â <strong>Notes</strong>Â â€” Experiment notes and insights with notion-like rich editor.</li>\n<li>âš™ï¸Â <strong>Settings</strong>Â â€” Experiment settings, such as name and description.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*h-DLE3XnBM5uGKBDRcu_iA.webp\" alt=\"\"></p>\n<h1>Integrations</h1>\n<h2>Integration with PaddlePaddle</h2>\n<p>With Aim v3.15 it takes just two steps to integrate Aim into your PaddlePaddle training script.</p>\n<pre><code># Step 1: Explicitly import the AimCallback for tracking training metadata\nfrom aim.paddle import AimCallback\n\n...\n\n# Step 2: Pass the callback to callbacks list upon initiating your training\ncallback = AimCallback(repo='.', experiment='paddle_test')\nmodel.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)\n\n</code></pre>\n<p>See docsÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-paddlepaddle\">here</a>.</p>\n<h2>Integration with Optuna</h2>\n<p>Smoothly integrate Aim into your Optuna project.</p>\n<p>Here are theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-optuna\">docs</a>Â and a short code snippet.</p>\n<pre><code>from aim.optuna import AimCallback\n\n...\naim_callback = AimCallback(experiment_name=\"optuna_single_run\")\nstudy.optimize(objective, n_trials=10, callbacks=[aim_callback])\n</code></pre>\n<h1><em>Key optimizations and fixes</em></h1>\n<h2>Runs locking mechanism and finished runs detection</h2>\n<p>Number of issues were reported w.r.t runs locking. More robust locking mechanism is applied starting from Aim v3.15.</p>\n<h2>Remote server scaling</h2>\n<p>Now it is possible to spin up several workers to achieve higher processing speed for remote tracking server. Just run the server passing â€œworkersâ€ argument:Â <code>aim server --workers=N</code></p>\n<h1>Learn more</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a>Â ðŸ™Œ</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://discord.gg/F5TpzUeY\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>Donâ€™t forget to leave us a star onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>Â if you think Aim is useful â­ï¸</p>"},"_id":"posts/aim-3-15-â€”-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","_raw":{"sourceFilePath":"posts/aim-3-15-â€”-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","sourceFileName":"aim-3-15-â€”-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-15-â€”-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer"},"type":"Post"},{"title":"Aim 3.2 â€” Jupyter Notebook integration & Histograms","date":"2021-12-08T15:09:19.264Z","author":"Gev Soghomonian","description":"Aim 3.2 featuring Jupyter notebook integration is now available! ðŸ˜Š We are on a mission to democratize AI dev tools. ","slug":"aim-3-2-â€”-jupyter-notebook-integration-histograms","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQCHweFLuEHppnFsQOaUnw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.2 featuring Jupyter notebook integration is now available!Â \n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWithin a week after theÂ [v3.1](https://aimstack.io/aim3-1-images-tracker-and-images-explorer/)Â we are releasing the v3.2 with a number of key highly requested features in:\n\n* Aim Jupyter Notebook extension (Colab is coming soonâ€¦)\n* Histogram tracking and visualization\n* Full-size images, images resize and render modes\n* Search autofill and suggest on the UI\n\n## Aim & Jupyter notebook\n\nNow you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with theÂ `aim`Â package.\n\nYou donâ€™t need to install anything other than Aim. Here are theÂ [docs](https://aimstack.readthedocs.io/en/latest/index.html).\\\nA quick demo of integrating Aim & Jupyter Notebook::\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*7Xqp7esswOPWZpRoDqSawg.gif \"Itâ€™s that easy to use Aim on your Jupyter notebook!\")\n\n## Histogram tracking and Visualization\n\nStarting from 3.2 you can track Distributions and visualize them as histograms on the run page.\n\nThis is how to track the weights distribution in your training code:\n\n```\nfrom aim import Distribution\n\nfor step in range(1000):\n    my_run.track(\n        Distribution(tensor), # Pass distribution\n        name='gradients', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'type': 'weights',\n        },\n    )\n```\n\nTheÂ [demo](http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51),Â [docs](https://aimstack.readthedocs.io/en/latest/index.html)Â andÂ [examples](https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py)Â \n\nAnd this is how it looks on the UI.Â Aim lets you visualize the distributions every layer at every tracked step.Â Super powerful!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Il1FjcQVpYHyuKHUVAtS0A.gif \"Visualize every layer and every tracked step.\")\n\n## Full-size images, images resize and render modes\n\nNow you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.\n\nHere is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*zYuodUJ1ykDhrFD9rdYC_w.gif \"Easily resize the queried images\")\n\nWe have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*pTG1Z34twqOOo7x6yoo1YQ.gif \"Toggle between pixelated and smooth images render modes\")\n\nYou can also view full-size images by clicking on the top right corner magnifier on each images.\n\nBTWÂ the smooth / pixelated config also translates to the full-size images.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qvhjgoBWWCaT0Hm_6eXzOw.gif \"full-size images: easy!\")\n\n## Query Language autosuggest editor\n\nAnother highly requested feature.\n\nNow there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.\n\nHopefullyÂ this makes it much easier to get started with Aim.Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*eHTlgK8CdxpUsWC_edw1rw.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leave Aim a star on GitHub for support.","html":"<p>Aim 3.2 featuring Jupyter notebook integration is now available!Â </p>\n<p>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.</p>\n<p>Within a week after theÂ <a href=\"https://aimstack.io/aim3-1-images-tracker-and-images-explorer/\">v3.1</a>Â we are releasing the v3.2 with a number of key highly requested features in:</p>\n<ul>\n<li>Aim Jupyter Notebook extension (Colab is coming soonâ€¦)</li>\n<li>Histogram tracking and visualization</li>\n<li>Full-size images, images resize and render modes</li>\n<li>Search autofill and suggest on the UI</li>\n</ul>\n<h2>Aim &#x26; Jupyter notebook</h2>\n<p>Now you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with theÂ <code>aim</code>Â package.</p>\n<p>You donâ€™t need to install anything other than Aim. Here are theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/index.html\">docs</a>.<br>\nA quick demo of integrating Aim &#x26; Jupyter Notebook::</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*7Xqp7esswOPWZpRoDqSawg.gif\" alt=\"\" title=\"Itâ€™s that easy to use Aim on your Jupyter notebook!\"></p>\n<h2>Histogram tracking and Visualization</h2>\n<p>Starting from 3.2 you can track Distributions and visualize them as histograms on the run page.</p>\n<p>This is how to track the weights distribution in your training code:</p>\n<pre><code>from aim import Distribution\n\nfor step in range(1000):\n    my_run.track(\n        Distribution(tensor), # Pass distribution\n        name='gradients', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'type': 'weights',\n        },\n    )\n</code></pre>\n<p>TheÂ <a href=\"http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51\">demo</a>,Â <a href=\"https://aimstack.readthedocs.io/en/latest/index.html\">docs</a>Â andÂ <a href=\"https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py\">examples</a>Â </p>\n<p>And this is how it looks on the UI.Â Aim lets you visualize the distributions every layer at every tracked step.Â Super powerful!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Il1FjcQVpYHyuKHUVAtS0A.gif\" alt=\"\" title=\"Visualize every layer and every tracked step.\"></p>\n<h2>Full-size images, images resize and render modes</h2>\n<p>Now you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.</p>\n<p>Here is how it works:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*zYuodUJ1ykDhrFD9rdYC_w.gif\" alt=\"\" title=\"Easily resize the queried images\"></p>\n<p>We have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pTG1Z34twqOOo7x6yoo1YQ.gif\" alt=\"\" title=\"Toggle between pixelated and smooth images render modes\"></p>\n<p>You can also view full-size images by clicking on the top right corner magnifier on each images.</p>\n<p>BTWÂ the smooth / pixelated config also translates to the full-size images.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qvhjgoBWWCaT0Hm_6eXzOw.gif\" alt=\"\" title=\"full-size images: easy!\"></p>\n<h2>Query Language autosuggest editor</h2>\n<p>Another highly requested feature.</p>\n<p>Now there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.</p>\n<p>HopefullyÂ this makes it much easier to get started with Aim.Here is a quick demo:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*eHTlgK8CdxpUsWC_edw1rw.gif\" alt=\"\"></p>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leave Aim a star on GitHub for support.</p>"},"_id":"posts/aim-3-2-â€”-jupyter-notebook-integration-histograms.md","_raw":{"sourceFilePath":"posts/aim-3-2-â€”-jupyter-notebook-integration-histograms.md","sourceFileName":"aim-3-2-â€”-jupyter-notebook-integration-histograms.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-2-â€”-jupyter-notebook-integration-histograms"},"type":"Post"},{"title":"Aim 3.3 â€” Audio & Text tracking, Plotly & Colab integrations","date":"2022-01-13T15:33:59.865Z","author":"Gev Soghomonian","description":"Hey community, Aim 3.3 is now available! ðŸ˜Š We are on a mission to democratize AI dev tools.  ","slug":"aim-3-3-â€”-audio-text-tracking-plotly-colab-integrations","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*300ZXHcgylKBit_O2KDeMA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, Aim 3.3 is now available! ðŸ˜Š\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.\n\nAim 3.3 is full of highly requested features. Lots of items scratched from the public AimÂ [roadmap](https://github.com/aimhubio/aim#roadmap).\n\n* Audio tracking and exploration\n* Text tracking and visualization\n* Scatter plot explorer\n* Colab integration\n* Plotly integration\n* Images visualization on run details page\n\nNew Demo:Â [play.aimstack.io:10004](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a)\n\nDemo Code:Â [github.com/osoblanco/FastSpeech2](https://github.com/osoblanco/FastSpeech2/blob/master/train.py)\n\nSpecial thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.\n\n## Audio tracking and exploration\n\nNow you can track audio files with Aim during your speech-to-text or other audio-involving experiments.\n\nIt will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.\n\nJust like tracking the metrics, we have enabled a simple API to track the audio.\n\n```\nfrom aim import Audio\n\nfor step in range(1000):\n    my_run.track(\n        Audio(arr), # Pass audio file or numpy array\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nHere is how it looks like on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*LkeFigHPnQM1drADXJ3yZQ.gif)\n\n## **Text tracking and visualization**\n\nUse this to compare text inputs and outputs during training\n\nSimilarly, you can also track text with Aim during your NLP experiments.\n\nHere is how the code looks like:\n\n```\nfrom aim import Text\n\nfor step in range(1000):\n    my_run.track(\n        Text(string), # Pass a string you want to track\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nThis is the end result on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*nkqUz9DecxaMmJoCUe_8Rw.gif \"Training info tracked as a text\")\n\n## Colab integration\n\nAfter we have integratedÂ [Aim to Jupyter](https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms/), there were many requests to enable Aim on Colab too. Now it has arrived!Â \n\nWith fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!\n\nHere is an exampleÂ [colab to get started with](https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing).\n\n> Note: Please make sure to run all the cells to be able to use the UI as well\n\nSo this is how it looks on your browser:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*SjYF_Kq4WzWOodVzsjemvA.gif)\n\n## Plotly integration\n\nNow you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.\n\nThis is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.\n\n```\nfrom aim import Figure\n\nfor step in range(1000):\n    my_run.track(\n        Figure(fig_obj), # Pass any plotly figure\n        name='plotly_bars', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nThe end-result on the Aim Web UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*8xn3PH_Hzk6fotyZQqy-AQ.gif)\n\n## Images visualization on run details page\n\nAs we had launched the images tracking and visualization in[Â 3.1](https://aimstack.io/aim3-1-images-tracker-and-images-explorer/), we havenâ€™t enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.\n\nHere is how it looks on the UI\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*jnUogPgbzwX6GOmDaXHKMg.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features and bugs.\n\nAnd donâ€™t forget to leave Aim a star on GitHub for support.","html":"<p>Hey community, Aim 3.3 is now available! ðŸ˜Š</p>\n<p>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.</p>\n<p>Aim 3.3 is full of highly requested features. Lots of items scratched from the public AimÂ <a href=\"https://github.com/aimhubio/aim#roadmap\">roadmap</a>.</p>\n<ul>\n<li>Audio tracking and exploration</li>\n<li>Text tracking and visualization</li>\n<li>Scatter plot explorer</li>\n<li>Colab integration</li>\n<li>Plotly integration</li>\n<li>Images visualization on run details page</li>\n</ul>\n<p>New Demo:Â <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a\">play.aimstack.io:10004</a></p>\n<p>Demo Code:Â <a href=\"https://github.com/osoblanco/FastSpeech2/blob/master/train.py\">github.com/osoblanco/FastSpeech2</a></p>\n<p>Special thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.</p>\n<h2>Audio tracking and exploration</h2>\n<p>Now you can track audio files with Aim during your speech-to-text or other audio-involving experiments.</p>\n<p>It will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.</p>\n<p>Just like tracking the metrics, we have enabled a simple API to track the audio.</p>\n<pre><code>from aim import Audio\n\nfor step in range(1000):\n    my_run.track(\n        Audio(arr), # Pass audio file or numpy array\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n</code></pre>\n<p>Here is how it looks like on the UI.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*LkeFigHPnQM1drADXJ3yZQ.gif\" alt=\"\"></p>\n<h2><strong>Text tracking and visualization</strong></h2>\n<p>Use this to compare text inputs and outputs during training</p>\n<p>Similarly, you can also track text with Aim during your NLP experiments.</p>\n<p>Here is how the code looks like:</p>\n<pre><code>from aim import Text\n\nfor step in range(1000):\n    my_run.track(\n        Text(string), # Pass a string you want to track\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n</code></pre>\n<p>This is the end result on the UI.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*nkqUz9DecxaMmJoCUe_8Rw.gif\" alt=\"\" title=\"Training info tracked as a text\"></p>\n<h2>Colab integration</h2>\n<p>After we have integratedÂ <a href=\"https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms/\">Aim to Jupyter</a>, there were many requests to enable Aim on Colab too. Now it has arrived!Â </p>\n<p>With fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!</p>\n<p>Here is an exampleÂ <a href=\"https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing\">colab to get started with</a>.</p>\n<blockquote>\n<p>Note: Please make sure to run all the cells to be able to use the UI as well</p>\n</blockquote>\n<p>So this is how it looks on your browser:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*SjYF_Kq4WzWOodVzsjemvA.gif\" alt=\"\"></p>\n<h2>Plotly integration</h2>\n<p>Now you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.</p>\n<p>This is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.</p>\n<pre><code>from aim import Figure\n\nfor step in range(1000):\n    my_run.track(\n        Figure(fig_obj), # Pass any plotly figure\n        name='plotly_bars', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n</code></pre>\n<p>The end-result on the Aim Web UI.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*8xn3PH_Hzk6fotyZQqy-AQ.gif\" alt=\"\"></p>\n<h2>Images visualization on run details page</h2>\n<p>As we had launched the images tracking and visualization in<a href=\"https://aimstack.io/aim3-1-images-tracker-and-images-explorer/\">Â 3.1</a>, we havenâ€™t enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.</p>\n<p>Here is how it looks on the UI</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jnUogPgbzwX6GOmDaXHKMg.gif\" alt=\"\"></p>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features and bugs.</p>\n<p>And donâ€™t forget to leave Aim a star on GitHub for support.</p>"},"_id":"posts/aim-3-3-â€”-audio-text-tracking-plotly-colab-integrations.md","_raw":{"sourceFilePath":"posts/aim-3-3-â€”-audio-text-tracking-plotly-colab-integrations.md","sourceFileName":"aim-3-3-â€”-audio-text-tracking-plotly-colab-integrations.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-3-â€”-audio-text-tracking-plotly-colab-integrations"},"type":"Post"},{"title":"Aim 3.4 â€“ Remote Tracking Alpha, Sorting & deleting runs","date":"2022-01-24T16:06:51.383Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.4  featuring remote-tracking is now available! ðŸ˜Š Aim is an open source AI experiment tracking tool.","slug":"aim-3-4-â€“-remote-tracking-alpha-sorting-deleting-runs","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-Ubp6reeVEAahicQEz-SQ.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.4Â  featuring remote-tracking is now available!Â \n\nAim is an open source AI experiment tracking tool. We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\nThis is a milestone release with lots of anticipated new features and one thatâ€™s super-anticipated (remote-tracking!). Brace yourselves, the collaborative Aim is here one commit at a time!!!\n\n## Here is whatâ€™s new:\n\n* Remote tracking \\[experimental]\n* Run delete and archive: batch and single\n* Ability to stack images on theÂ [Images Explorer](https://aimstack.io/aim3-1-images-tracker-and-images-explorer/)\n* Text filtering via regexp\n* Trendline on scatterplots\n* More images features: display images by original size, align by width, images reordering\n\nSpecial thanks to Geoffrey Chen, gormat, sjakkampudi, osoblanco, Sennevs, gloryVine, krstp and others for continuous feedback and help.\n\n## Remote tracking \\[experimental]\n\nThis is a milestone release with lots of anticipated new features and one thatâ€™s super-anticipated. Brace yourselves, the collaborative Aim is here one commit at a time!!!\n\nAim Remote Tracking is very simple and easy to get started withÂ \n\nHere are the steps to make it work. For more details pls check outÂ [the end-to-end guide on Aim docs](https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html).\n\n````\n### Ensure Aim version 3.4.x\n```sh\n$ pip install \"aim>=3.4.0\"\n```\n\n### Initialize the remote server\n```sh\n$ aim init # (optional)\n$ aim server --repo <REPO_PATH>\n```\nOutput:\n```\n> Server is mounted on 0.0.0.0:53800\n> Press Ctrl+C to exit\n```\n### Start the Aim UI\n```\n$ aim up --repo <REPO_PATH>\n```\n\n### Change only 1 line in your training code\nPoint your repo to the remote location\n```py\naim_run = Run(repo='aim://172.3.66.145:53800')# replace example IP with your tracking server IP/hostname\n```\n````\n\n## Run, delete and archive: Batch and single\n\nOne of the very highly anticipated features by the Aim community. Now you can archive or hard delete your premature, interrupted runs from all the explorer pages as well as Run Details Settings tab.\n\nItâ€™s really easy to use. just select the run and theÂ `Delete`Â /Â `Archive`Â buttons will appear.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qu9172RudbYfvFaVXVvNg.png)\n\n## Ability to stack images on the Images Explorer\n\nThis feature allows to not only view the tracked images sequentially, but also stack them by the last grouped param.\n\nOnce stacked, a slider appears that will allow you to navigate between the layers of stacked images. This feature will allow to closely observe the evolution of output images (GANs, wave outputs as img etc)\n\nHere is a quick demoÂ [link](http://play.aimstack.io:10002/images?grouping=7sKkRob88jEDScwcb7RLVkxZMAGNj8QPqfbeDBN11Hau1HzW4sr7FGSESSWwWH7qPYX4NvhSCtkgWXMTkHXEixu6soDSnPEYZ6CCa5fqUhbpWZ17EGMYDCknV4JErUVLaxyDtt2VjKEVSgEfGdWghLKLL5euM3YAF34MwNEtRVJxuXGAreiKEtXYbwbMHrs5JJ5kZ&select=gTSaqzN7Mb8GdSizQfqNj9qCzd61EVTqUzAWmhAMFt1NVE2ABWM3PcYXbvEoHXrHBBwG51ytmWtG2AXtTVbUr8PVynYCpDzJqg4ZFEGauDS7PFw91DQ7nnxPzstQYFwbxWBQTR8x2eiUdScMgXh5XvkpPpn1HQpTbKz7ZgU1GXuezCrjcVXN9KGatx2S5iJAwNS4pLWpuE6HthgD9ww8sNzQzDBJuDHHHUSgkXDV2CoJqzAEE6RKf8gsdrWwP9GLovkHk43oMqPzwcmXCnUG4TFG7oL2HGaqXk2MSAN44RAMDbhwBH9nwV4KAyCHZD6kcrvdyBdadH92iPJMsW4cDQD26XaMsRxN3mWAqkvZPvAznnppWaVNFHv4V8q4kmemmoTNRi5jCD2r4b1ZgpuVN639cNsniPdQxPHm38bw56&images=JV4T9kjLib9QU1Y8yG9XzRSB2pQtueUv7vj8dnnFN5YWxuyyRLbSPPQ5MJEGfgzssBbWPsgdT3jMaGndWtj9uiqCnWqbzQ6ZUfK31hTgPKaDZXMWsyNnjjoTbSwU5E4Y2TVQ1m8yThVhtdQZ7Uf6zf1tpx7JgdXmeBdgzEFiYz63qFrj5wabvD6xvanu9X7giWsgPwB9aA4pMRNG4KuKSpDW5EaqVZzNoRxUPmMiL42kjrB7mEu9rKdbRxTcfDKv4ziN3fkJ2Euhq4YD2nmmsnsktJbjrLPyVjsrJfzeGALh6C37tW4C215en6yLhVp6kzh9EnHKLQwA3pgpnfDRi4NMgBmkQrHX4RMpZWWN9YKDUpKWc1bajxEU6wmzNiLD4FfVkT8Wt5uyQZe2mY81fiAgmh2nDHdP8MXAU77j569wpwcR2B68iHPqdCFQGyyyMN9YFt7u9hWvw1hca85Xmeo7gsKP5prhseh2QsFHjG3BgRMnaJRgn9hKktoFiXbxcwrdWr4CVYehuRTu48ixf1ivDx1uKDLTr1MTyDnVtZ55Hh7zkgEFKQXsXHesxUBJ9hyciPUYhsDU5NG4ib5PUw2g326JHMpnDcRpMLy464GdBj891zYaw1PvU2DAFchHjJHLiiDdEF5tfLSm4jkG7ARWF5yUisVarAaCkfYhcXkAL7Wfg7RGUnhfmkRyd7DXhskCDvjPAdt98gTZJGsTbSFFrpKXTSAiBqhbY9Z9a6n29wxsffGC4hntQZMCqFr1YAd16A6LBrfR4NTPLZoLySjqq3ZdirCxf2E8pTah2ty76vt17VpQA6DbRrFKDKJUTiGL7WskgMHpKT2d2Fd881ALiQKnRgtQTKh34JNaKi3UAF2ZvLg9hQVxaqCD5bNdhuFHtnKM926dt5v8dJLxxTsJzgeysaHNsnMvGv3Ej6r3AjVfdL7w2mjT8vDUJ7wp5JXrWp915NjDYYMXJMHJ67P3bnr2eLZtvrowfNZYe4DTvSoGmYgqVNeNiK3ErNR3npX6cv2hNDFWmooMShc61yHB4GC77n2UMf7pmjmQm5Y3M55cuFcYPAmwiRfvFmtd4Y53ADe6n8kFHAPrscSQ7hYWMDeoSQyNQESFUmQMNjR5HjrU5SVvTrMeVCiiyYTmzhAxiz3D6iV1HyTvkCREjPEUyvWNb9vMwBk32uFUmqzbEZn2XGQLCakqNrf4y3uV7EPiz6pTZSmiRUMWQm1U9HSoDRoqcEysgbr8FDeXgtYZ6tyvCtJFHibfydW2wvtpRwaWXtvjxfbpcxWtr73VC9Amk45mkKMdq5QKGZR5Da9S9wATBT4ipA18Xrmv7kSQSMeHD4QJRuQtLgrG77VmcmtybH9ofqQALQ5wFN24JV1f3Jo9QdwTNBdfe7rQ5X7CEfCb7RgEM29sA4bZjHND7Ppos9ygV2F17YCekfuKZwXzo9k6y94SYcPvh7BBiTMG3HEJvbfHQDUjSv3eR8zYfdts5gbQWrV8aUmocu8HxYBj4J814o6gC1CfLaRvrRYSF3d7xxBTimFdg5BNJ3Xx4vVfRgfiZxJxfYUpVMpXK8kXi2F6t9Zgu1Gt8xBbidPuHS2nqWbEjAVnEKp8a6h8b6rQZUzAWfsJfmNzZTdXvAnm15GfifuZwhAj5SPaYdB5VhQVBoZ2MXsjt6kVxYPdvqhhUDufaebSba9ytYeuoR9iezz4SyAL5zGYeKm9gayQPdzPeZd5e4NToj2hN16tM47FevssgF5nR2pGjXtDUkbKhAZjcThphgE7XAVKc244pDDWgnqjFdjjJe1byVDpS2sVExmanrrGueWdidKxHSzjwKgvrhP4egLEF5y7joDgVrUHVPoraBYaTWzcGRFs2jCjHvfCs22nGvznFgQ9tnFkedou71d6kfmXXudtgzWPCp22zDQ3EiMT5tPq8oRjpcJNoWdQ1xBe1ZFK2dxfwr1ZPW1r9mqaeunxUM251vG9DgxmT6Nm67VU5bNuVeV3Z97KfXQVaPcgw7BrKmJRNk6fFsCSBjTpae3Xu68DHKexwf1cc7JBgKh34XJo2PgsWRMRTs6rK4CGN92ZVen1kKGZoD1QK9QVjZghpKRimnuiibNN7ehZFjhBVNQ6x6Mc6YgjkH98QUtUPhAsT9pKA5WfP6UL3BtqvGvJRvZsPnZDnVkn6xqsvwaPAZDDAMjukw8h1KayDbpaPJQFRoM2GnLWVD2uE2xRRxginm15h5Sm9UT9avk6Dh5N7V8PV6iyWbSXwMDmUQqRHBbW1b2mGiYY84fniRnfPU8qPw6rfdGYYPBZeE5uqgHH8rDSfZ8ZaJKW7TU4R4woSgyVjvFAuTi1RPaoRDCRDUhKRncQy8DSe8gcHFGz8NsrHSy5To7PF7APQ4BE4zquXfmUzvos1CEQ3F1ZwMUFX2tNZNhRePQpQi8SdS2456UaZzRS3L3cmo9sZV3nH9y3zDuT1DGtih4K44Q5H5QiP793V5DBXxKmX6icM2Z9LKtm3CxEBa9tkDHcqN9Mvku2YTsQHA1V3iJooFpvnAyuXjNud4asecrumfCuLUjqDAKeLL9gWC7Mesv6u8hEy8Lob5vh2758KccuNbpuY3Siv3whk7Cdvp5rw2KPkxcVNQAU8xSqwxictcEsnPPawyjdBMFTEtJbsWhxP9HLpSMgkvpK1t3V4rFFvJKrg4oUZgEuVWPuZkj7v8UwhPGVxqQ6Xvhk6F2bovsx9KofwZQaYMYA8KHzED7KFAhkG3JKBcfoiXiwGx5JaYJsPc43u6eMPZzKSaQQksnUhXHPwZN2HivK9MAABiT5tPUZaedmnzmdZQL5jJ19yFVsccpGSxxPVtd3QE1Jz6bgWX6K3o9JnwymB7Xt2vziQvnntqxZNCtHYtPG1DjVq2GYYFoxT3KCEMFbfoinsApNSG3jNyNxXxeikciuFYqk8hNCAEGoYfNMFPmNkHUDMFYpBERxbGSmw1JtZGfJLPyWHk8puUeQ1S1G29aysDaDuD9dGGp89J2MxM4oXmek8eUQzA83AW45oY5Vh7vLctQNcjXpXBPt69rhJjRriE27MyToawf48jerCApZVjUW9Sy24hykpjMKv6ttsqWrECy6bRBwWnTgUh11jMn8fcwUxwMkJEe5dNf5UDPqJ17fg72pF6aCHM7bp5Mm5GTVrjgmiTwdf9F5jTk9evz3nXdgMRPmJFFYApKpE947UKRJyzT5MpQtwQGEDHT9VaTkhgmAPYFeuK2Uznz7trJ1QtewkxJ3Sth72M7i9JPDEBnnmjaxsZ4FADBRsz4RBLntJndpWCvYJgxD96LxSwGFpb3FRooTvtG1Z8pRfHHjcowUr2vFaSpRyPhV6)Â to try out. Here is a glimpse of the UI:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdJPM1ggTMtdcri1htvZdg.png)\n\n## Text Search\n\nNow a comprehensive text searching is available to be able to outline the desired text. Regular Expressions are available among others.\n\nHere is a link for a quickÂ [demo](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/texts). And a glimpse of the UI:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt8sjFzpbhomdqRDwKb-Mw.png)\n\n## Trendline on scatterplots\n\nNow you can render a trendline over your scatterplots on Scatters Explorer. We have enabled linear and Loess trendline.\n\nThanks toÂ [sjakkampudi](https://github.com/aimhubio/aim/issues/1160)Â for the help on this one.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pndAq3SIevuTckBZ20wwqg.png)\n\n## More Images Features\n\nWe have added a number of new tools to the Images Explorer\n\n## Display Images by original size\n\nNow you can view & compare the tracked images by their original size. This has been a key missing ingredient for effective images comparison.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXgyBHiVPAV_8l6-im_P3A.png)\n\n## Align by width\n\nYou can also align images by their width. This is especially handy when long images are tracked, such as sound spectograms (40px / 1000px). This feature will allow to effectively compare such images.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHHMux9rYd1sLRX4OjLCeQ.png)\n\n## Images Sorting\n\nYou can also apply sorting of images by any tracked available parameter. In this case I have enabled sorting by step in descending order to see the later generated images in my GAN experiments. A lot smoother than the ones above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4gA2EO7b0AI39YjcpLwIA.png)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leave Aim a star on GitHub for support.","html":"<p>Hey team, Aim 3.4Â  featuring remote-tracking is now available!Â </p>\n<p>Aim is an open source AI experiment tracking tool. We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.</p>\n<p>This is a milestone release with lots of anticipated new features and one thatâ€™s super-anticipated (remote-tracking!). Brace yourselves, the collaborative Aim is here one commit at a time!!!</p>\n<h2>Here is whatâ€™s new:</h2>\n<ul>\n<li>Remote tracking [experimental]</li>\n<li>Run delete and archive: batch and single</li>\n<li>Ability to stack images on theÂ <a href=\"https://aimstack.io/aim3-1-images-tracker-and-images-explorer/\">Images Explorer</a></li>\n<li>Text filtering via regexp</li>\n<li>Trendline on scatterplots</li>\n<li>More images features: display images by original size, align by width, images reordering</li>\n</ul>\n<p>Special thanks to Geoffrey Chen, gormat, sjakkampudi, osoblanco, Sennevs, gloryVine, krstp and others for continuous feedback and help.</p>\n<h2>Remote tracking [experimental]</h2>\n<p>This is a milestone release with lots of anticipated new features and one thatâ€™s super-anticipated. Brace yourselves, the collaborative Aim is here one commit at a time!!!</p>\n<p>Aim Remote Tracking is very simple and easy to get started withÂ </p>\n<p>Here are the steps to make it work. For more details pls check outÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html\">the end-to-end guide on Aim docs</a>.</p>\n<pre><code>### Ensure Aim version 3.4.x\n```sh\n$ pip install \"aim>=3.4.0\"\n```\n\n### Initialize the remote server\n```sh\n$ aim init # (optional)\n$ aim server --repo &#x3C;REPO_PATH>\n```\nOutput:\n```\n> Server is mounted on 0.0.0.0:53800\n> Press Ctrl+C to exit\n```\n### Start the Aim UI\n```\n$ aim up --repo &#x3C;REPO_PATH>\n```\n\n### Change only 1 line in your training code\nPoint your repo to the remote location\n```py\naim_run = Run(repo='aim://172.3.66.145:53800')# replace example IP with your tracking server IP/hostname\n```\n</code></pre>\n<h2>Run, delete and archive: Batch and single</h2>\n<p>One of the very highly anticipated features by the Aim community. Now you can archive or hard delete your premature, interrupted runs from all the explorer pages as well as Run Details Settings tab.</p>\n<p>Itâ€™s really easy to use. just select the run and theÂ <code>Delete</code>Â /Â <code>Archive</code>Â buttons will appear.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qu9172RudbYfvFaVXVvNg.png\" alt=\"\"></p>\n<h2>Ability to stack images on the Images Explorer</h2>\n<p>This feature allows to not only view the tracked images sequentially, but also stack them by the last grouped param.</p>\n<p>Once stacked, a slider appears that will allow you to navigate between the layers of stacked images. This feature will allow to closely observe the evolution of output images (GANs, wave outputs as img etc)</p>\n<p>Here is a quick demoÂ <a href=\"http://play.aimstack.io:10002/images?grouping=7sKkRob88jEDScwcb7RLVkxZMAGNj8QPqfbeDBN11Hau1HzW4sr7FGSESSWwWH7qPYX4NvhSCtkgWXMTkHXEixu6soDSnPEYZ6CCa5fqUhbpWZ17EGMYDCknV4JErUVLaxyDtt2VjKEVSgEfGdWghLKLL5euM3YAF34MwNEtRVJxuXGAreiKEtXYbwbMHrs5JJ5kZ&#x26;select=gTSaqzN7Mb8GdSizQfqNj9qCzd61EVTqUzAWmhAMFt1NVE2ABWM3PcYXbvEoHXrHBBwG51ytmWtG2AXtTVbUr8PVynYCpDzJqg4ZFEGauDS7PFw91DQ7nnxPzstQYFwbxWBQTR8x2eiUdScMgXh5XvkpPpn1HQpTbKz7ZgU1GXuezCrjcVXN9KGatx2S5iJAwNS4pLWpuE6HthgD9ww8sNzQzDBJuDHHHUSgkXDV2CoJqzAEE6RKf8gsdrWwP9GLovkHk43oMqPzwcmXCnUG4TFG7oL2HGaqXk2MSAN44RAMDbhwBH9nwV4KAyCHZD6kcrvdyBdadH92iPJMsW4cDQD26XaMsRxN3mWAqkvZPvAznnppWaVNFHv4V8q4kmemmoTNRi5jCD2r4b1ZgpuVN639cNsniPdQxPHm38bw56&#x26;images=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\">link</a>Â to try out. Here is a glimpse of the UI:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdJPM1ggTMtdcri1htvZdg.png\" alt=\"\"></p>\n<h2>Text Search</h2>\n<p>Now a comprehensive text searching is available to be able to outline the desired text. Regular Expressions are available among others.</p>\n<p>Here is a link for a quickÂ <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/texts\">demo</a>. And a glimpse of the UI:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt8sjFzpbhomdqRDwKb-Mw.png\" alt=\"\"></p>\n<h2>Trendline on scatterplots</h2>\n<p>Now you can render a trendline over your scatterplots on Scatters Explorer. We have enabled linear and Loess trendline.</p>\n<p>Thanks toÂ <a href=\"https://github.com/aimhubio/aim/issues/1160\">sjakkampudi</a>Â for the help on this one.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pndAq3SIevuTckBZ20wwqg.png\" alt=\"\"></p>\n<h2>More Images Features</h2>\n<p>We have added a number of new tools to the Images Explorer</p>\n<h2>Display Images by original size</h2>\n<p>Now you can view &#x26; compare the tracked images by their original size. This has been a key missing ingredient for effective images comparison.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXgyBHiVPAV_8l6-im_P3A.png\" alt=\"\"></p>\n<h2>Align by width</h2>\n<p>You can also align images by their width. This is especially handy when long images are tracked, such as sound spectograms (40px / 1000px). This feature will allow to effectively compare such images.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHHMux9rYd1sLRX4OjLCeQ.png\" alt=\"\"></p>\n<h2>Images Sorting</h2>\n<p>You can also apply sorting of images by any tracked available parameter. In this case I have enabled sorting by step in descending order to see the later generated images in my GAN experiments. A lot smoother than the ones above.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4gA2EO7b0AI39YjcpLwIA.png\" alt=\"\"></p>\n<h2>Learn more</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leave Aim a star on GitHub for support.</p>"},"_id":"posts/aim-3-4-â€“-remote-tracking-alpha-sorting-deleting-runs.md","_raw":{"sourceFilePath":"posts/aim-3-4-â€“-remote-tracking-alpha-sorting-deleting-runs.md","sourceFileName":"aim-3-4-â€“-remote-tracking-alpha-sorting-deleting-runs.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-4-â€“-remote-tracking-alpha-sorting-deleting-runs"},"type":"Post"},{"title":"Aim 3.5 â€” TensorBoard logs support, Matplotlib integration & System Params logging","date":"2022-02-10T16:15:32.942Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.5 featuring TensorBoard logs support is now available!! We are on a mission to democratize MLOps tools. ","slug":"aim-3-5-â€”-tensorboard-logs-support-matplotlib-integration-system-params-logging","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xQnlTBk3BmsoMgsaE_0oQ.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.5 featuring TensorBoard logs support is now available!!Â We are on a mission to democratize MLOps tools.Â Thanks to the awesome Aim community for the help and contributions.\n\nHere is whatâ€™s new:\n\n* Aim supports TensorBoard logs\n* Track system params, CLI ,Env, Executable, Git, Installed packages\n* Matplotlib figure tracking and visualization\n* Ability move runs between Aim repos\n\n> Special thanks toÂ [gloryVine](https://github.com/gloryVine),Â [hughperkins](https://github.com/hughperkins),Â [Ssamdav](https://github.com/SSamDav),Â [ptaejoon](https://github.com/ptaejoon), mahnerak, gormat andÂ [Mike](https://github.com/mikel-brostrom)l for feedback, issues and help.\n\n## TensorBoard logs support\n\n\n\nItâ€™s been one of the most highly requested features by the Aim community. This feature was available for Aim 2.x and folks used to love this it. However, we had to drop it due to the backend changes.\n\nNow its back! Better than it was before and this is how it works:\n\n```\n$ cd /path/to/.aim\n$ aim convert tf --logdir ~/tensorflow/logdir\n```\n\nThis command willÂ  scan then convert theÂ `scalar`Â andÂ `image`Â type logs from your directory into Aim runs.\n\nRead more about how it worksÂ [here](https://aimstack.readthedocs.io/en/latest/guides/integrations/basic_aim_tensorflow_event_conversion.html).\n\n## Tracking Env info, git info with Aim\n\nTracking your ENV variables, CLI argument, git info, etc could be a lot of details to care about.\n\nNow there is a way to enable Aim to track the environment info automatically and they will be available as params.\n\nIt takes a small tweak to enable that:\n\n```\nrun = Run(log_system_params=True)\n```\n\nThen once tracked, you can search experiments based on these values too:\n\n```\nrun.__system_params.git_info.branch == 'feature/testing'\n```\n\nMore on this feature find outÂ [here](https://aimstack.readthedocs.io/en/latest/guides/training_reproducibility.html).\n\n> A special UI for these tracked data is to be shipped with the next version.\n\n## Tracking Matplotlib figures\n\n\n\nStarting Aim 3.5 you can also trackÂ [Matplotlib](https://matplotlib.org/)Â figures with Aim. During research (especially with Jupyter Notebooks) Matplotlib is very helpful in rendering intermediate images for analysis.\n\nNow you can track all such figures on Aim (both as Matplotlib figure and as an image). When tracking as an image, you can query and compare them too at scale on theÂ [Images Explorer.](https://aimstack.io/blog/new-releases/aim3-1-images-tracker-and-images-explorer/)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYdPka8XdPLAOxLdITESAw.png)\n\n[Here are the docs for more details](https://aimstack.readthedocs.io/en/latest/guides/basic_aim_matplotlib.html).\n\n## Moving Runs between Aim repos\n\nWe have added a CLI command to move runs between folders.\n\nThis will allow to easily move your best runs from a draft scratch project to your main one with one command. Here is how it works:\n\n```\nim mv --destination /new/path/to/.aim <my_run_hash_1> ...\n```\n\n\\\nFor more info, check the runs subcommand docsÂ [here](https://aimstack.readthedocs.io/en/latest/refs/cli.html#runs).\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humblingÂ  and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for support.","html":"<p>Hey team, Aim 3.5 featuring TensorBoard logs support is now available!!Â We are on a mission to democratize MLOps tools.Â Thanks to the awesome Aim community for the help and contributions.</p>\n<p>Here is whatâ€™s new:</p>\n<ul>\n<li>Aim supports TensorBoard logs</li>\n<li>Track system params, CLI ,Env, Executable, Git, Installed packages</li>\n<li>Matplotlib figure tracking and visualization</li>\n<li>Ability move runs between Aim repos</li>\n</ul>\n<blockquote>\n<p>Special thanks toÂ <a href=\"https://github.com/gloryVine\">gloryVine</a>,Â <a href=\"https://github.com/hughperkins\">hughperkins</a>,Â <a href=\"https://github.com/SSamDav\">Ssamdav</a>,Â <a href=\"https://github.com/ptaejoon\">ptaejoon</a>, mahnerak, gormat andÂ <a href=\"https://github.com/mikel-brostrom\">Mike</a>l for feedback, issues and help.</p>\n</blockquote>\n<h2>TensorBoard logs support</h2>\n<p>Itâ€™s been one of the most highly requested features by the Aim community. This feature was available for Aim 2.x and folks used to love this it. However, we had to drop it due to the backend changes.</p>\n<p>Now its back! Better than it was before and this is how it works:</p>\n<pre><code>$ cd /path/to/.aim\n$ aim convert tf --logdir ~/tensorflow/logdir\n</code></pre>\n<p>This command willÂ  scan then convert theÂ <code>scalar</code>Â andÂ <code>image</code>Â type logs from your directory into Aim runs.</p>\n<p>Read more about how it worksÂ <a href=\"https://aimstack.readthedocs.io/en/latest/guides/integrations/basic_aim_tensorflow_event_conversion.html\">here</a>.</p>\n<h2>Tracking Env info, git info with Aim</h2>\n<p>Tracking your ENV variables, CLI argument, git info, etc could be a lot of details to care about.</p>\n<p>Now there is a way to enable Aim to track the environment info automatically and they will be available as params.</p>\n<p>It takes a small tweak to enable that:</p>\n<pre><code>run = Run(log_system_params=True)\n</code></pre>\n<p>Then once tracked, you can search experiments based on these values too:</p>\n<pre><code>run.__system_params.git_info.branch == 'feature/testing'\n</code></pre>\n<p>More on this feature find outÂ <a href=\"https://aimstack.readthedocs.io/en/latest/guides/training_reproducibility.html\">here</a>.</p>\n<blockquote>\n<p>A special UI for these tracked data is to be shipped with the next version.</p>\n</blockquote>\n<h2>Tracking Matplotlib figures</h2>\n<p>Starting Aim 3.5 you can also trackÂ <a href=\"https://matplotlib.org/\">Matplotlib</a>Â figures with Aim. During research (especially with Jupyter Notebooks) Matplotlib is very helpful in rendering intermediate images for analysis.</p>\n<p>Now you can track all such figures on Aim (both as Matplotlib figure and as an image). When tracking as an image, you can query and compare them too at scale on theÂ <a href=\"https://aimstack.io/blog/new-releases/aim3-1-images-tracker-and-images-explorer/\">Images Explorer.</a></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYdPka8XdPLAOxLdITESAw.png\" alt=\"\"></p>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/guides/basic_aim_matplotlib.html\">Here are the docs for more details</a>.</p>\n<h2>Moving Runs between Aim repos</h2>\n<p>We have added a CLI command to move runs between folders.</p>\n<p>This will allow to easily move your best runs from a draft scratch project to your main one with one command. Here is how it works:</p>\n<pre><code>im mv --destination /new/path/to/.aim &#x3C;my_run_hash_1> ...\n</code></pre>\n<p><br>\nFor more info, check the runs subcommand docsÂ <a href=\"https://aimstack.readthedocs.io/en/latest/refs/cli.html#runs\">here</a>.</p>\n<h2>Learn more</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humblingÂ  and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support.</p>"},"_id":"posts/aim-3-5-â€”-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","_raw":{"sourceFilePath":"posts/aim-3-5-â€”-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","sourceFileName":"aim-3-5-â€”-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-5-â€”-tensorboard-logs-support-matplotlib-integration-system-params-logging"},"type":"Post"},{"title":"Aim 3.6 â€” Chart export, PyTorch Ignite & Activeloop Hub integrations","date":"2022-02-24T16:27:07.254Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite & Activeloop Hub integrations is now available!","slug":"aim-3-6-â€”-chart-export-pytorch-ignite-activeloop-hub-integrations","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-4kxMg5kF4ntVhW5889Xw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite & Activeloop Hub integrations is now available!\n\nWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is whatâ€™s new in addition toÂ [Aim 3.5](https://aimstack.io/blog/new-releases/aim-3-5-%E2%80%94-tensorboard-logs-support-matplotlib-integration-system-params-logging).\n\n* Export chart as image\n* MLflow to Aim logs converter\n* Pytorch Ignite integration\n* Activeloop Hub integration\n* Wildcard support forÂ *aim runs*Â subcommands\n\n*Special thanks toÂ [krstp](https://github.com/krstp),Â [ptaejoon](https://github.com/ptaejoon),Â [farizrahman4u](https://github.com/farizrahman4u)Â from Activeloop,Â [vfdev-5](https://github.com/vfdev-5)Â from PyTorch Ignite,Â [hughperkins](https://github.com/hughperkins),Â [Ssamdav](https://github.com/ssamdav)Â andÂ [mahnerak](https://github.com/mahnerak)Â for feedback, reviews and help.*\n\n## Export chart as image\n\nHey awesome Aim community, sorry for shippingÂ [this feature](https://github.com/aimhubio/aim/issues/339)Â so late. We know it has been so highly requested, just couldnâ€™t get our hands on it over and over again.\n\nNow the images export is here! You can export charts as JPEG, PNG and SVG so you can include the awesome Aim visuals in your reports and research papers.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0-dHK5OoYjUAWhyKfBWA.png)\n\nYou can control the width, height, the format of the downloadable image.\n\n> The ability to add / edit legends will be added in the coming versions.\n\nWould still love to hear feedback on how would you like us to tune this feature.\n\n## MLflow to Aim logs converter\n\nIts been alsoÂ [requested](https://github.com/aimhubio/aim/issues/409)Â across the board by many users. Now you can convert your MLflow logs to Aim so you can compare all of them regardless on where it was tracked.\n\nHere is how it works:\n\n```\n$ aim init\n$ aim convert mlflow --tracking_uri 'file:///Users/aim_user/mlruns'\n```\n\nBesides the metrics, You can convert Images, Texts and Sound/Audios. More on that inÂ [here](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-mlflow-logs-in-aim).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idkzKK4Ua8N3ATiokIx_rQ.png)\n\n## Pytorch Ignite integration\n\nFor all the Aim users who use PyTorch Ignite, there is now a native PI callback available so you can track your basic metrics automatically!\n\nHere is a code snippet on how to use the PI integration:\n\n```\n# call aim sdk designed for pytorch ignite\nfrom aim.pytorch_ignite import AimLogger\n\n# track experimential data by using Aim\naim_logger = AimLogger(\n    experiment='aim_on_pt_ignite',\n    train_metric_prefix='train_',\n    val_metric_prefix='val_',\n    test_metric_prefix='test_',\n)\n\n# track experimential data by using Aim\naim_logger.attach_output_handler(\n    train_evaluator,\n    event_name=Events.EPOCH_COMPLETED,\n    tag=\"train\",\n    metric_names=[\"nll\", \"accuracy\"],\n    global_step_transform=global_step_from_engine(trainer),\n)\n```\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKznHJq6W2XxmzyYpF17og.png)\n\n## Activeloop Hub integration\n\nExcited to share that thanks toÂ [an integration with Activeloop](https://activeloop.ai/), starting Aim 3.6 you can add yourÂ [Hub dataset](https://github.com/activeloopai/Hub)Â info as a run param and use them to search and compare your runs across all Explorers.\n\nHere is a code snippet to demonstrate the usage:\n\n```\nimport hub\n\nfrom aim.sdk.objects.plugins.hub_dataset import HubDataset\nfrom aim.sdk import Run\n\n# create dataset object\nds = hub.dataset('hub://activeloop/cifar100-test')\n\n# log dataset metadata\nrun = Run(system_tracking_interval=None)\nrun['hub_ds'] = HubDataset(ds)\n```\n\nThe following information becomes available among others:\n\n```\nrun.hub_ds.version\nrun.hub_ds.num_samples\nrun.hub_ds.tensors.name\nrun.hub_ds.tensors.compression_type\n```\n\nHub is an awesome tool to build, manage, query & visualize datasets for deep learning, as well as stream data real-time to PyTorch/TensorFlow & version-control it. Check out theÂ [Hub docs](https://docs.activeloop.ai/).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XlJrxPe3g9RYdqkvMaVUg.png)\n\nWildcard support for the \\`aim runs\\` command\n\nOnÂ [Aim 3.5](https://aimstack.io/blog/new-releases/aim-3-5-%E2%80%94-tensorboard-logs-support-matplotlib-integration-system-params-logging)Â we made available theÂ `run mv`Â command which allows to move single runs between folders.\n\nNow we have also added the wildcard to move the whole folder altogether\n\nHere is the CLI interface:\n\n```\n$ aim runs --repo <source_repo/.aim> mv * --destination <dest_repo/.aim>\n```\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](http://github.com/aimhubio/aim)Â a star on GitHub for support ðŸ™Œ.","html":"<p>Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite &#x26; Activeloop Hub integrations is now available!</p>\n<p>We are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.</p>\n<p>Here is whatâ€™s new in addition toÂ <a href=\"https://aimstack.io/blog/new-releases/aim-3-5-%E2%80%94-tensorboard-logs-support-matplotlib-integration-system-params-logging\">Aim 3.5</a>.</p>\n<ul>\n<li>Export chart as image</li>\n<li>MLflow to Aim logs converter</li>\n<li>Pytorch Ignite integration</li>\n<li>Activeloop Hub integration</li>\n<li>Wildcard support forÂ <em>aim runs</em>Â subcommands</li>\n</ul>\n<p><em>Special thanks toÂ <a href=\"https://github.com/krstp\">krstp</a>,Â <a href=\"https://github.com/ptaejoon\">ptaejoon</a>,Â <a href=\"https://github.com/farizrahman4u\">farizrahman4u</a>Â from Activeloop,Â <a href=\"https://github.com/vfdev-5\">vfdev-5</a>Â from PyTorch Ignite,Â <a href=\"https://github.com/hughperkins\">hughperkins</a>,Â <a href=\"https://github.com/ssamdav\">Ssamdav</a>Â andÂ <a href=\"https://github.com/mahnerak\">mahnerak</a>Â for feedback, reviews and help.</em></p>\n<h2>Export chart as image</h2>\n<p>Hey awesome Aim community, sorry for shippingÂ <a href=\"https://github.com/aimhubio/aim/issues/339\">this feature</a>Â so late. We know it has been so highly requested, just couldnâ€™t get our hands on it over and over again.</p>\n<p>Now the images export is here! You can export charts as JPEG, PNG and SVG so you can include the awesome Aim visuals in your reports and research papers.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0-dHK5OoYjUAWhyKfBWA.png\" alt=\"\"></p>\n<p>You can control the width, height, the format of the downloadable image.</p>\n<blockquote>\n<p>The ability to add / edit legends will be added in the coming versions.</p>\n</blockquote>\n<p>Would still love to hear feedback on how would you like us to tune this feature.</p>\n<h2>MLflow to Aim logs converter</h2>\n<p>Its been alsoÂ <a href=\"https://github.com/aimhubio/aim/issues/409\">requested</a>Â across the board by many users. Now you can convert your MLflow logs to Aim so you can compare all of them regardless on where it was tracked.</p>\n<p>Here is how it works:</p>\n<pre><code>$ aim init\n$ aim convert mlflow --tracking_uri 'file:///Users/aim_user/mlruns'\n</code></pre>\n<p>Besides the metrics, You can convert Images, Texts and Sound/Audios. More on that inÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-mlflow-logs-in-aim\">here</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idkzKK4Ua8N3ATiokIx_rQ.png\" alt=\"\"></p>\n<h2>Pytorch Ignite integration</h2>\n<p>For all the Aim users who use PyTorch Ignite, there is now a native PI callback available so you can track your basic metrics automatically!</p>\n<p>Here is a code snippet on how to use the PI integration:</p>\n<pre><code># call aim sdk designed for pytorch ignite\nfrom aim.pytorch_ignite import AimLogger\n\n# track experimential data by using Aim\naim_logger = AimLogger(\n    experiment='aim_on_pt_ignite',\n    train_metric_prefix='train_',\n    val_metric_prefix='val_',\n    test_metric_prefix='test_',\n)\n\n# track experimential data by using Aim\naim_logger.attach_output_handler(\n    train_evaluator,\n    event_name=Events.EPOCH_COMPLETED,\n    tag=\"train\",\n    metric_names=[\"nll\", \"accuracy\"],\n    global_step_transform=global_step_from_engine(trainer),\n)\n</code></pre>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKznHJq6W2XxmzyYpF17og.png\" alt=\"\"></p>\n<h2>Activeloop Hub integration</h2>\n<p>Excited to share that thanks toÂ <a href=\"https://activeloop.ai/\">an integration with Activeloop</a>, starting Aim 3.6 you can add yourÂ <a href=\"https://github.com/activeloopai/Hub\">Hub dataset</a>Â info as a run param and use them to search and compare your runs across all Explorers.</p>\n<p>Here is a code snippet to demonstrate the usage:</p>\n<pre><code>import hub\n\nfrom aim.sdk.objects.plugins.hub_dataset import HubDataset\nfrom aim.sdk import Run\n\n# create dataset object\nds = hub.dataset('hub://activeloop/cifar100-test')\n\n# log dataset metadata\nrun = Run(system_tracking_interval=None)\nrun['hub_ds'] = HubDataset(ds)\n</code></pre>\n<p>The following information becomes available among others:</p>\n<pre><code>run.hub_ds.version\nrun.hub_ds.num_samples\nrun.hub_ds.tensors.name\nrun.hub_ds.tensors.compression_type\n</code></pre>\n<p>Hub is an awesome tool to build, manage, query &#x26; visualize datasets for deep learning, as well as stream data real-time to PyTorch/TensorFlow &#x26; version-control it. Check out theÂ <a href=\"https://docs.activeloop.ai/\">Hub docs</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XlJrxPe3g9RYdqkvMaVUg.png\" alt=\"\"></p>\n<p>Wildcard support for the `aim runs` command</p>\n<p>OnÂ <a href=\"https://aimstack.io/blog/new-releases/aim-3-5-%E2%80%94-tensorboard-logs-support-matplotlib-integration-system-params-logging\">Aim 3.5</a>Â we made available theÂ <code>run mv</code>Â command which allows to move single runs between folders.</p>\n<p>Now we have also added the wildcard to move the whole folder altogether</p>\n<p>Here is the CLI interface:</p>\n<pre><code>$ aim runs --repo &#x3C;source_repo/.aim> mv * --destination &#x3C;dest_repo/.aim>\n</code></pre>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"http://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support ðŸ™Œ.</p>"},"_id":"posts/aim-3-6-â€”-chart-export-pytorch-ignite-activeloop-hub-integrations.md","_raw":{"sourceFilePath":"posts/aim-3-6-â€”-chart-export-pytorch-ignite-activeloop-hub-integrations.md","sourceFileName":"aim-3-6-â€”-chart-export-pytorch-ignite-activeloop-hub-integrations.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-6-â€”-chart-export-pytorch-ignite-activeloop-hub-integrations"},"type":"Post"},{"title":"Aim 3.7 â€” Revamped Run Single Page and Aim Docker Image","date":"2022-03-16T16:59:53.238Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.7 featuring Aim Docker Image is now available! We are on a mission to democratize MLOps tools.","slug":"aim-3-7-â€”-revamped-run-single-page-and-aim-docker-image","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INjZ3YZMiIgIc3EhLos_sw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.7 featuring Aim Docker Image is now available!\n\nWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\n[Aim](https://github.com/aimhubio/aim)Â is an open-source, self-hosted ML experiment tracking tool. Itâ€™s good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\n\nYou can use not only the great Aim UI but also its SDK to query your runsâ€™ metadata programmatically. Thatâ€™s especially useful for automations and additional analysis on a Jupyter Notebook.\n\nHere is whatâ€™s new in the V3.7 release:\n\n* Completely revamped Run Single Page\n* TF/Keras adaptors refactoring\n* Aim Docker Image\n\n> Special thanks to osoblanco for the help, insights and feedback.\n\n## **Run Single Page Overview**\n\nWe have created an overview section on the Run Single page to quickly observe the relevant info about your before getting deep into it.\n\n* The last values of metrics\n* Flattened list of key params\n* Overview information about your run\n* Tracked reproducibility fields (env and git info, etc)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQXh2J_Clq2SE1EDq_ooQ.png)\n\nAim Docker Image\n\nNow you can pull the Aim docker image from theÂ [docker hub](https://hub.docker.com/r/aimstack/aim)!\n\nFor the users who are trying to set up Aim on their clusters or server infrastructure via docker, now you can use the native container â€” released with each release.\n\nThe Aim docker container is pre-packaged Aim that can be mounted onÂ `.aim`Â directory and UI could be ran.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjSOM1o9Dk4WNHgDEcE64w.png \"Aimstack docker image\")\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAlso check the recent blog post onÂ [Aim 3.6](https://aimstack.io/aim-3-6-mlflow-chart-export-pytorch-ignite-activeloop-hub-integrations/)Â release featuring Mlflow logs converter & more!","html":"<p>Hey team, Aim 3.7 featuring Aim Docker Image is now available!</p>\n<p>We are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.</p>\n<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â is an open-source, self-hosted ML experiment tracking tool. Itâ€™s good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runsâ€™ metadata programmatically. Thatâ€™s especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Here is whatâ€™s new in the V3.7 release:</p>\n<ul>\n<li>Completely revamped Run Single Page</li>\n<li>TF/Keras adaptors refactoring</li>\n<li>Aim Docker Image</li>\n</ul>\n<blockquote>\n<p>Special thanks to osoblanco for the help, insights and feedback.</p>\n</blockquote>\n<h2><strong>Run Single Page Overview</strong></h2>\n<p>We have created an overview section on the Run Single page to quickly observe the relevant info about your before getting deep into it.</p>\n<ul>\n<li>The last values of metrics</li>\n<li>Flattened list of key params</li>\n<li>Overview information about your run</li>\n<li>Tracked reproducibility fields (env and git info, etc)</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQXh2J_Clq2SE1EDq_ooQ.png\" alt=\"\"></p>\n<p>Aim Docker Image</p>\n<p>Now you can pull the Aim docker image from theÂ <a href=\"https://hub.docker.com/r/aimstack/aim\">docker hub</a>!</p>\n<p>For the users who are trying to set up Aim on their clusters or server infrastructure via docker, now you can use the native container â€” released with each release.</p>\n<p>The Aim docker container is pre-packaged Aim that can be mounted onÂ <code>.aim</code>Â directory and UI could be ran.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjSOM1o9Dk4WNHgDEcE64w.png\" alt=\"\" title=\"Aimstack docker image\"></p>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>Also check the recent blog post onÂ <a href=\"https://aimstack.io/aim-3-6-mlflow-chart-export-pytorch-ignite-activeloop-hub-integrations/\">Aim 3.6</a>Â release featuring Mlflow logs converter &#x26; more!</p>"},"_id":"posts/aim-3-7-â€”-revamped-run-single-page-and-aim-docker-image.md","_raw":{"sourceFilePath":"posts/aim-3-7-â€”-revamped-run-single-page-and-aim-docker-image.md","sourceFileName":"aim-3-7-â€”-revamped-run-single-page-and-aim-docker-image.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-7-â€”-revamped-run-single-page-and-aim-docker-image"},"type":"Post"},{"title":"Aim 3.8 â€” DVC integration & Extensible HuggingFace callbacks","date":"2022-04-04T20:27:53.206Z","author":"Gev Soghomonian","description":"Aim 3.8 featuring extensible HuggingFace trainer callbacks is out ! We are on a mission to democratize AI dev tools. ","slug":"aim-3-8-â€”-dvc-integration-extensible-huggingface-callbacks","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIfbNRqB72W36j8gooKi4Q.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.8 featuring extensible HuggingFace trainer callbacks is out !\n\nWe are on a mission to[Â democratize AI dev tools](https://aimstack.io/aim-from-zero-to-hero-track-machine-learning-experiments/). Thanks to the awesome Aim community for the help and contributions.\n\nHere is whatâ€™s new:\n\n* Color scale of numeric values\n* DVC integration\n* Extensible HuggingFace and XGBoost callbacks\n\n> Special thanks to osoblanco, ashutoshsaboo and mohamadelgaar for the help and feedback\n\n\\\nColor scale of numeric values\n\nDue to popular demand an Aim 2.x feature is back!\n\nNow you can color-code the aggregated metric values on the Runs Explorer. The highest values will be in green and the lowest in yellow. The color changes from yellow to green at the median.\n\nWe are surely going to iterate over this feature. Further feedbacks are welcome!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOgBqVnIeq3fc_EduNAAfw.png \"Aimstack: Color scale of numeric values\")\n\n## DVC integration\n\nNow you can track the info about your DVC tracked files and datasets on Aim. With this we are iterating towards easy connection between Aim and DVC so you can connect your tracked artifacts with experiment tracking.\n\nShare more feedback with us. How would you like to see this evolve?\n\nThe code looks as simple as this:\n\n```\nfrom aim.sdk import Run\nfrom aim.sdk.objects.plugins.dvc_metadata import DvcData\n\nrun = Run(system_tracking_interval=None)\n\npath_to_dvc_repo = '.'\nrun['dvc_info'] = DvcData(path_to_dvc_repo)\n```\n\n## Extensible HuggingFace and XGBoost callbacks\n\nWhen using Aim with your favorite frameworks, the metadata is logged throughÂ `AimCallback`Â which is limited as it allows only specific group of logged metrics per framework.\n\nNow you can extend the AimCallback and log any other metadata made available by the framework.Â [Detailed docs here.](https://aimstack.readthedocs.io/en/latest/using/integration_guides.html)\n\nHere is an example on how to extend the HuggingFace callback to track texts with Aim:\n\n```\nfrom aim.hugging_face import AimCallback\nfrom aim import Text\n\n\nclass CustomCallback(AimCallback):\n    def on_log(self, args, state, control,\n               model=None, logs=None, **kwargs):\n        super().on_log(args, state, control, model, logs, **kwargs)\n\n        context = {\n            'subset': self._current_shift,\n        }\n        for log_name, log_value in logs.items():\n            if isinstance(log_value, str):\n                self.experiment.track(Text(log_value), name=log_name, context=context)\n```\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leave Aim a star on GitHub for support.","html":"<p>Aim 3.8 featuring extensible HuggingFace trainer callbacks is out !</p>\n<p>We are on a mission to<a href=\"https://aimstack.io/aim-from-zero-to-hero-track-machine-learning-experiments/\">Â democratize AI dev tools</a>. Thanks to the awesome Aim community for the help and contributions.</p>\n<p>Here is whatâ€™s new:</p>\n<ul>\n<li>Color scale of numeric values</li>\n<li>DVC integration</li>\n<li>Extensible HuggingFace and XGBoost callbacks</li>\n</ul>\n<blockquote>\n<p>Special thanks to osoblanco, ashutoshsaboo and mohamadelgaar for the help and feedback</p>\n</blockquote>\n<p><br>\nColor scale of numeric values</p>\n<p>Due to popular demand an Aim 2.x feature is back!</p>\n<p>Now you can color-code the aggregated metric values on the Runs Explorer. The highest values will be in green and the lowest in yellow. The color changes from yellow to green at the median.</p>\n<p>We are surely going to iterate over this feature. Further feedbacks are welcome!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOgBqVnIeq3fc_EduNAAfw.png\" alt=\"\" title=\"Aimstack: Color scale of numeric values\"></p>\n<h2>DVC integration</h2>\n<p>Now you can track the info about your DVC tracked files and datasets on Aim. With this we are iterating towards easy connection between Aim and DVC so you can connect your tracked artifacts with experiment tracking.</p>\n<p>Share more feedback with us. How would you like to see this evolve?</p>\n<p>The code looks as simple as this:</p>\n<pre><code>from aim.sdk import Run\nfrom aim.sdk.objects.plugins.dvc_metadata import DvcData\n\nrun = Run(system_tracking_interval=None)\n\npath_to_dvc_repo = '.'\nrun['dvc_info'] = DvcData(path_to_dvc_repo)\n</code></pre>\n<h2>Extensible HuggingFace and XGBoost callbacks</h2>\n<p>When using Aim with your favorite frameworks, the metadata is logged throughÂ <code>AimCallback</code>Â which is limited as it allows only specific group of logged metrics per framework.</p>\n<p>Now you can extend the AimCallback and log any other metadata made available by the framework.Â <a href=\"https://aimstack.readthedocs.io/en/latest/using/integration_guides.html\">Detailed docs here.</a></p>\n<p>Here is an example on how to extend the HuggingFace callback to track texts with Aim:</p>\n<pre><code>from aim.hugging_face import AimCallback\nfrom aim import Text\n\n\nclass CustomCallback(AimCallback):\n    def on_log(self, args, state, control,\n               model=None, logs=None, **kwargs):\n        super().on_log(args, state, control, model, logs, **kwargs)\n\n        context = {\n            'subset': self._current_shift,\n        }\n        for log_name, log_value in logs.items():\n            if isinstance(log_value, str):\n                self.experiment.track(Text(log_value), name=log_name, context=context)\n</code></pre>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leave Aim a star on GitHub for support.</p>"},"_id":"posts/aim-3-8-â€”-dvc-integration-extensible-huggingface-callbacks.md","_raw":{"sourceFilePath":"posts/aim-3-8-â€”-dvc-integration-extensible-huggingface-callbacks.md","sourceFileName":"aim-3-8-â€”-dvc-integration-extensible-huggingface-callbacks.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-8-â€”-dvc-integration-extensible-huggingface-callbacks"},"type":"Post"},{"title":"Aim 3.9 â€” Notes on training runs and upgrade to Pytorch Lightning 1.6 Pytorch Lightning","date":"2022-05-08T20:34:11.966Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.9 is now available! We are on a mission to democratize AI dev tools.","slug":"aim-3-9-â€”-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoVjVDwfOytJ-FyRqp_n0Q.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.9 is now available!\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is whatâ€™s new:\n\n* Notes on training runs\n* Upgrade to Pytorch Lightning 1.6\n\n> Special thanks to ashutoshsaboo, haritsahm and Justin Burton for the help and feedback.\n\n## Notes on the training runs\n\nNow there is an ability to add rich metadata edit (similar to notion) over the runs. Itâ€™s been a highly requested feature to be able to leave relevant code, result-related notes, next steps etc on the runs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghbd3OrbVatbwSwnwpIm-A.png)\n\n## Upgrade to Pytorch Lightning 1.6\n\nWe have upgraded the Aim Logger for Pytorch Lightning 1.6 updates.\n\nLearn more in the following thread:Â <https://github.com/aimhubio/aim/issues/1619>\n\nMeanwhile, check out an earlierÂ [tutorial](https://aimstack.io/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim/)Â on how to tune hyperparams with fixed seeds using PyTorch Lightning and Aim.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leave Aim a star on GitHub for support","html":"<p>Hey team, Aim 3.9 is now available!</p>\n<p>We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.</p>\n<p>Here is whatâ€™s new:</p>\n<ul>\n<li>Notes on training runs</li>\n<li>Upgrade to Pytorch Lightning 1.6</li>\n</ul>\n<blockquote>\n<p>Special thanks to ashutoshsaboo, haritsahm and Justin Burton for the help and feedback.</p>\n</blockquote>\n<h2>Notes on the training runs</h2>\n<p>Now there is an ability to add rich metadata edit (similar to notion) over the runs. Itâ€™s been a highly requested feature to be able to leave relevant code, result-related notes, next steps etc on the runs.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghbd3OrbVatbwSwnwpIm-A.png\" alt=\"\"></p>\n<h2>Upgrade to Pytorch Lightning 1.6</h2>\n<p>We have upgraded the Aim Logger for Pytorch Lightning 1.6 updates.</p>\n<p>Learn more in the following thread:Â <a href=\"https://github.com/aimhubio/aim/issues/1619\">https://github.com/aimhubio/aim/issues/1619</a></p>\n<p>Meanwhile, check out an earlierÂ <a href=\"https://aimstack.io/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim/\">tutorial</a>Â on how to tune hyperparams with fixed seeds using PyTorch Lightning and Aim.</p>\n<h2>Learn More</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leave Aim a star on GitHub for support</p>"},"_id":"posts/aim-3-9-â€”-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","_raw":{"sourceFilePath":"posts/aim-3-9-â€”-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","sourceFileName":"aim-3-9-â€”-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-9-â€”-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning"},"type":"Post"},{"title":"Aim and MLflow â€” Choosing Experiment Tracker for Zero-Shot Cross-Lingual Transfer","date":"2023-02-14T06:41:14.310Z","author":"Hovhannes Tamoyan","description":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user...","slug":"aim-and-mlflow-â€”-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v64PbdBn6kBvsH3t5bkv8w.png","draft":false,"categories":["Tutorials"],"body":{"raw":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.\n\nThe question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.\n\nIn this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.\n\n# Task Setup\n\nThe task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.\n\nParticularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.\n\nZero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.\n\nWe will explore two techniques in our experimentation:\n\n* Fine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.\n* Feature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.\n\nIn both techniques, we will undertake training utilizing theÂ `en`Â subset of theÂ `XNLI`Â dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from theÂ `XNLI`Â dataset, including English(`en`), German(`de`), French(`fr`), Spanish(`es`), Chinese(`zh`), and Arabic(`ar`).\n\n# The Datasets\n\nWe will utilize theÂ `XNLI`Â (cross-lingualÂ `NLI`) dataset, which is a selection of a few thousand examples from theÂ `MNLI`Â (multiÂ `NLI`) dataset, translated into 14 different languages, including some with limited resources.\n\nThe template of theÂ `NLI`Â task is as follows. Given a pair of sentences, aÂ `premise`Â and aÂ `hypothesis`Â need to determine whether aÂ `hypothesis`Â is true (entailment), false (contradiction), or undetermined (neutral) given aÂ `premise`.\n\nLetâ€™s take a look at a few samples to get hang of it. Say the given hypothesis isÂ `â€œIssues in Data Synthesis.â€`Â and the premise isÂ `â€œProblems in data synthesis.â€`. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.\n\nAnother example this time of a neutral pair of sentences is the following: the hypothesis isÂ `â€œShe was so happy she couldn't stop smiling.â€`Â and the premise isÂ `â€œShe smiled back.â€`. The first sentence doesnâ€™t imply the second one, however, it doesnâ€™t contradict it as well. Thus they are neutral.\n\nAn instance of contradiction, the hypothesis isÂ `â€œThe analysis proves that there is no link between PM and bronchitis.â€`Â and the premise isÂ `â€œThis analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.â€`. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.\n\nFor more examples please explore the HuggingFace Datasets page powered by Streamlit:Â <https://huggingface.co/datasets/viewer/>.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png)\n\n# The Models\n\n\n\nIn our experiments, we will utilize the following set of pre-trained multilingual language models:\n\n![](/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png)\n\nWe will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.\n\n# Setting up Training Environment\n\nBefore beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:\n\n\n\n```\naim-and-mlflow-usecase\nâ”œâ”€â”€ logs\nâ”‚   â”œâ”€â”€ aim_callback\nâ”‚   â”‚   â””â”€â”€ .aim\nâ”‚   â”œâ”€â”€ aimlflow\nâ”‚   â”‚   â””â”€â”€ .aim\nâ”‚   â”œâ”€â”€ checkpoints\nâ”‚   â””â”€â”€ mlruns\nâ””â”€â”€ main.py\n```\n\nLetâ€™s start off by creating the main directory, we named itÂ `aim-and-mlflow-usecase`, you can simply name anything you want. After which we need to download theÂ `main.py`Â from the following source:Â <https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer>. The code explanation and sample usage can be found in theÂ `README.md`Â file of the directory. We will be using this script to run our experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png)\n\nTheÂ `logs`Â directory as the name suggests stores the logs. In theÂ `checkpoints`Â folder, all the model states will be saved. TheÂ `mlruns`Â is the repository for MLflow experiments. TheÂ `aim_callback`Â will store the repository of Aim runs tracked using Aimâ€™s built-in callback for Hugging Face Transformers, meanwhile, theÂ `aimlflow`Â will store the runs converted from MLflow using the aimlflow tool.\n\n> *It is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.*\n\nIn order to start the training process, we will be using the following command. But first, letâ€™s navigate to the directory where our script is located (`aim-and-mlflow-usecase`Â in our case).\n\n```\npython main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nWhereÂ `{PATH_TO}`Â is the absolute path of theÂ `aim-and-mlflow-usecase`Â directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:\n\n```\npython main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nGo grab some snacks, trainings take a while ðŸ« â˜ºï¸.\n\n# Using aimlflow\n\nMeanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use theÂ `aimlflow`Â to simply convert and view our experiments live on Aim. Letâ€™s first show how this can be done after which tackle the question.\n\nInstalÂ `aimlflow`Â on your machine viaÂ `pip`, if it is not already installed:\n\n```\n$ pip install aimlflow\n```\n\n```\n$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n```\n\nThis command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.\n\nMore on how theÂ `aimlflow`Â can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:\n\n* **[Exploring MLflow experiments with a powerful UI](https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e)**\n* **[How to integrate aimlflow with your remote MLflow](https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf)**\n\nThis is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aimâ€™s built-in callback,Â `aim.hugging_face.AimCallback`, which is specifically designed for t`ransformers.Trainer`Â functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.\n\n# Unlocking the Power of Data Analysis\n\n\n\nOnce the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, letâ€™s launch both toolsâ€™ UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:\n\n```\n$ mlflow ui\n```\n\n```\n$ aim up\n```\n\n\n\n> *Note that for the best possible experience, we will be using the*Â `aim_callback/.aim`Â *repository in this demonstration, as it has deeper integration with the*Â `Trainer`*.*\n\nThe UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visitingÂ `http://127.0.0.1:5000`Â for MLflow andÂ `http://127.0.0.1:43800`Â for Aim.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png \"The user interface of MLflow on first look\")\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png \"The user interface of Aim on first look\")\n\nIn order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit theÂ `Compare Runs`Â page:\n\nBy selecting all the experiments, navigate to the comparison page by clicking theÂ `Compare`Â button.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png)\n\nTheÂ `Run details`Â section presents the run metadata, including the start and end time and duration of the run. TheÂ `Parameters`Â section displays the hyperparameters used for the run, such as the optimizer and architecture. TheÂ `Metrics`Â section showcases the latest values for each metric.\n\nHaving access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.\n\nTo organize the parameters into meaningful groups for our experiment, simply go to Aimâ€™sÂ `Metrics Explorer`Â page and follow a few straightforward steps:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif)\n\n\n\n\n\n# Gaining insights\n\n\n\nLetâ€™s examine the charts more closely and uncover valuable insights from our experiments.\n\nA quick examination of the charts reveals the following observations:\n\n* The fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.\n* The graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.\n\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png)\n\n\n\n* Even though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only byÂ `train_batch_size`. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png)\n\n\n\n* The charts unmistakably show that theÂ `bert-base-multilingual-cased`Â model achieved the best accuracy results, with the highest score observed for theÂ `en`Â subset, as the model was trained on that subset. Subsequently,Â `es`,Â `fr`,Â `de`,Â `zh`, andÂ `ar`Â followed. Unsurprisingly the scores for theÂ `zh`Â andÂ `ar`Â datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png)\n\n\n\n* Let us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.\n* ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png)\n\n\n\n  One can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. TheÂ `Parameter Explorer`Â will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how theÂ `Parameter Explorer`Â works, letâ€™s pick the following parameters:Â `train_batch_size`,Â `learning_rate`,`_name_or_path`,Â `loss`, and the accuracies ofÂ `sub_dataset`s. The following chart will be observed after clicking theÂ `Search`Â button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses theÂ `bert-base-multilingual-cased`Â model, with 5Â·10â»âµÂ `learning_rate`Â and theÂ `batch_size`Â is 8.\n\n  Taking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.\n\n  # Conclusion\n\n  This blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aimâ€™s powerful user interface to obtain valuable insights from the experiments.\n\n  We also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.","html":"<p>The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.</p>\n<p>The question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.</p>\n<p>In this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.</p>\n<h1>Task Setup</h1>\n<p>The task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.</p>\n<p>Particularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.</p>\n<p>Zero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.</p>\n<p>We will explore two techniques in our experimentation:</p>\n<ul>\n<li>Fine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.</li>\n<li>Feature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.</li>\n</ul>\n<p>In both techniques, we will undertake training utilizing theÂ <code>en</code>Â subset of theÂ <code>XNLI</code>Â dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from theÂ <code>XNLI</code>Â dataset, including English(<code>en</code>), German(<code>de</code>), French(<code>fr</code>), Spanish(<code>es</code>), Chinese(<code>zh</code>), and Arabic(<code>ar</code>).</p>\n<h1>The Datasets</h1>\n<p>We will utilize theÂ <code>XNLI</code>Â (cross-lingualÂ <code>NLI</code>) dataset, which is a selection of a few thousand examples from theÂ <code>MNLI</code>Â (multiÂ <code>NLI</code>) dataset, translated into 14 different languages, including some with limited resources.</p>\n<p>The template of theÂ <code>NLI</code>Â task is as follows. Given a pair of sentences, aÂ <code>premise</code>Â and aÂ <code>hypothesis</code>Â need to determine whether aÂ <code>hypothesis</code>Â is true (entailment), false (contradiction), or undetermined (neutral) given aÂ <code>premise</code>.</p>\n<p>Letâ€™s take a look at a few samples to get hang of it. Say the given hypothesis isÂ <code>â€œIssues in Data Synthesis.â€</code>Â and the premise isÂ <code>â€œProblems in data synthesis.â€</code>. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.</p>\n<p>Another example this time of a neutral pair of sentences is the following: the hypothesis isÂ <code>â€œShe was so happy she couldn't stop smiling.â€</code>Â and the premise isÂ <code>â€œShe smiled back.â€</code>. The first sentence doesnâ€™t imply the second one, however, it doesnâ€™t contradict it as well. Thus they are neutral.</p>\n<p>An instance of contradiction, the hypothesis isÂ <code>â€œThe analysis proves that there is no link between PM and bronchitis.â€</code>Â and the premise isÂ <code>â€œThis analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.â€</code>. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.</p>\n<p>For more examples please explore the HuggingFace Datasets page powered by Streamlit:Â <a href=\"https://huggingface.co/datasets/viewer/\">https://huggingface.co/datasets/viewer/</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png\" alt=\"\"></p>\n<h1>The Models</h1>\n<p>In our experiments, we will utilize the following set of pre-trained multilingual language models:</p>\n<p><img src=\"/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png\" alt=\"\"></p>\n<p>We will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.</p>\n<h1>Setting up Training Environment</h1>\n<p>Before beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:</p>\n<pre><code>aim-and-mlflow-usecase\nâ”œâ”€â”€ logs\nâ”‚   â”œâ”€â”€ aim_callback\nâ”‚   â”‚   â””â”€â”€ .aim\nâ”‚   â”œâ”€â”€ aimlflow\nâ”‚   â”‚   â””â”€â”€ .aim\nâ”‚   â”œâ”€â”€ checkpoints\nâ”‚   â””â”€â”€ mlruns\nâ””â”€â”€ main.py\n</code></pre>\n<p>Letâ€™s start off by creating the main directory, we named itÂ <code>aim-and-mlflow-usecase</code>, you can simply name anything you want. After which we need to download theÂ <code>main.py</code>Â from the following source:Â <a href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\">https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer</a>. The code explanation and sample usage can be found in theÂ <code>README.md</code>Â file of the directory. We will be using this script to run our experiments.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png\" alt=\"\"></p>\n<p>TheÂ <code>logs</code>Â directory as the name suggests stores the logs. In theÂ <code>checkpoints</code>Â folder, all the model states will be saved. TheÂ <code>mlruns</code>Â is the repository for MLflow experiments. TheÂ <code>aim_callback</code>Â will store the repository of Aim runs tracked using Aimâ€™s built-in callback for Hugging Face Transformers, meanwhile, theÂ <code>aimlflow</code>Â will store the runs converted from MLflow using the aimlflow tool.</p>\n<blockquote>\n<p><em>It is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.</em></p>\n</blockquote>\n<p>In order to start the training process, we will be using the following command. But first, letâ€™s navigate to the directory where our script is located (<code>aim-and-mlflow-usecase</code>Â in our case).</p>\n<pre><code>python main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n</code></pre>\n<p>WhereÂ <code>{PATH_TO}</code>Â is the absolute path of theÂ <code>aim-and-mlflow-usecase</code>Â directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:</p>\n<pre><code>python main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n</code></pre>\n<p>Go grab some snacks, trainings take a while ðŸ« â˜ºï¸.</p>\n<h1>Using aimlflow</h1>\n<p>Meanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use theÂ <code>aimlflow</code>Â to simply convert and view our experiments live on Aim. Letâ€™s first show how this can be done after which tackle the question.</p>\n<p>InstalÂ <code>aimlflow</code>Â on your machine viaÂ <code>pip</code>, if it is not already installed:</p>\n<pre><code>$ pip install aimlflow\n</code></pre>\n<pre><code>$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n</code></pre>\n<p>This command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.</p>\n<p>More on how theÂ <code>aimlflow</code>Â can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:</p>\n<ul>\n<li><strong><a href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\">Exploring MLflow experiments with a powerful UI</a></strong></li>\n<li><strong><a href=\"https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf\">How to integrate aimlflow with your remote MLflow</a></strong></li>\n</ul>\n<p>This is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aimâ€™s built-in callback,Â <code>aim.hugging_face.AimCallback</code>, which is specifically designed for t<code>ransformers.Trainer</code>Â functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.</p>\n<h1>Unlocking the Power of Data Analysis</h1>\n<p>Once the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, letâ€™s launch both toolsâ€™ UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:</p>\n<pre><code>$ mlflow ui\n</code></pre>\n<pre><code>$ aim up\n</code></pre>\n<blockquote>\n<p><em>Note that for the best possible experience, we will be using the</em>Â <code>aim_callback/.aim</code>Â <em>repository in this demonstration, as it has deeper integration with the</em>Â <code>Trainer</code><em>.</em></p>\n</blockquote>\n<p>The UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visitingÂ <code>http://127.0.0.1:5000</code>Â for MLflow andÂ <code>http://127.0.0.1:43800</code>Â for Aim.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png\" alt=\"\" title=\"The user interface of MLflow on first look\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png\" alt=\"\" title=\"The user interface of Aim on first look\"></p>\n<p>In order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit theÂ <code>Compare Runs</code>Â page:</p>\n<p>By selecting all the experiments, navigate to the comparison page by clicking theÂ <code>Compare</code>Â button.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png\" alt=\"\"></p>\n<p>TheÂ <code>Run details</code>Â section presents the run metadata, including the start and end time and duration of the run. TheÂ <code>Parameters</code>Â section displays the hyperparameters used for the run, such as the optimizer and architecture. TheÂ <code>Metrics</code>Â section showcases the latest values for each metric.</p>\n<p>Having access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.</p>\n<p>To organize the parameters into meaningful groups for our experiment, simply go to Aimâ€™sÂ <code>Metrics Explorer</code>Â page and follow a few straightforward steps:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif\" alt=\"\"></p>\n<h1>Gaining insights</h1>\n<p>Letâ€™s examine the charts more closely and uncover valuable insights from our experiments.</p>\n<p>A quick examination of the charts reveals the following observations:</p>\n<ul>\n<li>The fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.</li>\n<li>The graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png\" alt=\"\"></p>\n<ul>\n<li>Even though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only byÂ <code>train_batch_size</code>. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png\" alt=\"\"></p>\n<ul>\n<li>The charts unmistakably show that theÂ <code>bert-base-multilingual-cased</code>Â model achieved the best accuracy results, with the highest score observed for theÂ <code>en</code>Â subset, as the model was trained on that subset. Subsequently,Â <code>es</code>,Â <code>fr</code>,Â <code>de</code>,Â <code>zh</code>, andÂ <code>ar</code>Â followed. Unsurprisingly the scores for theÂ <code>zh</code>Â andÂ <code>ar</code>Â datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png\" alt=\"\"></p>\n<ul>\n<li>\n<p>Let us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.</p>\n</li>\n<li>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png\" alt=\"\"></p>\n<p>One can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. TheÂ <code>Parameter Explorer</code>Â will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how theÂ <code>Parameter Explorer</code>Â works, letâ€™s pick the following parameters:Â <code>train_batch_size</code>,Â <code>learning_rate</code>,<code>_name_or_path</code>,Â <code>loss</code>, and the accuracies ofÂ <code>sub_dataset</code>s. The following chart will be observed after clicking theÂ <code>Search</code>Â button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses theÂ <code>bert-base-multilingual-cased</code>Â model, with 5Â·10â»âµÂ <code>learning_rate</code>Â and theÂ <code>batch_size</code>Â is 8.</p>\n<p>Taking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.</p>\n<h1>Conclusion</h1>\n<p>This blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aimâ€™s powerful user interface to obtain valuable insights from the experiments.</p>\n<p>We also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.</p>\n</li>\n</ul>"},"_id":"posts/aim-and-mlflow-â€”-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","_raw":{"sourceFilePath":"posts/aim-and-mlflow-â€”-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileName":"aim-and-mlflow-â€”-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-and-mlflow-â€”-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets â€” train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets â€” train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that onÂ [Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\n\n\nUse theÂ [aim.track](https://github.com/aimhubio/aim#track)Â context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n```\nimport aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n```\n\nOnce the training is ran, executeÂ `aim up`Â in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n> **\\*Note:**Â The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLetâ€™s go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.**Â Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.**Â Add theÂ `bleu`Â metric to the Select input â€” query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.**Â Search byÂ `context.subset`Â to show bothÂ `test`Â andÂ `val`Â `loss`Â andÂ `bleu`Â metrics. Divide into subplots further byÂ `context.subset`Â too so Aim UI showsÂ `test`Â andÂ `val`Â metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot itâ€™s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\nHere is a full summary video on how to do it on the UI.\n\n![](https://youtu.be/jPNZ7JVkA-c)\n\n# Learn More\n\nIf you find Aim useful, support us andÂ [star the project](https://github.com/aimhubio/aim)Â on GitHub. Join theÂ [Aim community](https://slack.aimstack.io/)Â and share more about your use-cases and how we can improve Aim to suit them.","html":"<p>Researchers divide datasets into three subsets â€” train, validation and test so they can test their model performance at different levels.</p>\n<p>The model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.</p>\n<p>The validation and test sets are used to test the model on additional unseen data to verify how well it generalise.</p>\n<p>Models are usually ran on validation subset after each epoch.<br>\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.</p>\n<p>There is a need to collect and effectively compare all these metrics.</p>\n<p>Here is how to do that onÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a></p>\n<h1>Using context to track for different subsets?</h1>\n<p>Use theÂ <a href=\"https://github.com/aimhubio/aim#track\">aim.track</a>Â context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.</p>\n<pre><code>import aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n</code></pre>\n<p>Once the training is ran, executeÂ <code>aim up</code>Â in your terminal and start the Aim UI.</p>\n<h1>Using subplots to compare test, val loss and bleu metrics</h1>\n<blockquote>\n<p><strong>*Note:</strong>Â The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*</p>\n</blockquote>\n<p>Letâ€™s go step-by-step on how to break down lots of experiments using subplots.</p>\n<p><strong>Step 1.</strong>Â Explore the runs, the context table, play with the query language.</p>\n<p><img src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"></p>\n<p><strong>Step 2.</strong>Â Add theÂ <code>bleu</code>Â metric to the Select input â€” query both metrics at the same time. Divide into subplots by metric.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"></p>\n<p><strong>Step 3.</strong>Â Search byÂ <code>context.subset</code>Â to show bothÂ <code>test</code>Â andÂ <code>val</code>Â <code>loss</code>Â andÂ <code>bleu</code>Â metrics. Divide into subplots further byÂ <code>context.subset</code>Â too so Aim UI showsÂ <code>test</code>Â andÂ <code>val</code>Â metrics on different subplots for better comparison.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"></p>\n<p>Not itâ€™s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.</p>\n<h1>Summary</h1>\n<p>Here is a full summary video on how to do it on the UI.</p>\n<p><img src=\"https://youtu.be/jPNZ7JVkA-c\" alt=\"\"></p>\n<h1>Learn More</h1>\n<p>If you find Aim useful, support us andÂ <a href=\"https://github.com/aimhubio/aim\">star the project</a>Â on GitHub. Join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>Â and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim Community is Moving to Discord!! ðŸŽ‰","date":"2022-12-21T21:45:47.434Z","author":"Gev Soghomonian","description":"Aim community is officially moving to Discord!! ðŸŽ‰","slug":"aim-community-is-moving-to-discord-ðŸŽ‰","image":"https://miro.medium.com/max/1400/1*c_ovadkKTTkZOokR3WiUTg.webp","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)Â community is officially moving to Discord!! ðŸŽ‰\n\n[Join us on Discord](https://community.aimstack.io/), letâ€™s rock and build great things together!!\n\nIn this new space we are going to\n\n* be very proactive in sharing next steps, areas of focus and plans\n* share weekly roadmap / commitments\n* share more community-specific content\n* of course continue answering the user questions\n* work with the users to build the best open-source experiment tracking tools.\n\n***The slack workspace is going to become deprecated by the end of 2022.***\n\nI also asked a friend on the technical reasons for moving to Discord. Here is what he responded. \n\n![](https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp)\n\nAim High!! ðŸš€","html":"<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â community is officially moving to Discord!! ðŸŽ‰</p>\n<p><a href=\"https://community.aimstack.io/\">Join us on Discord</a>, letâ€™s rock and build great things together!!</p>\n<p>In this new space we are going to</p>\n<ul>\n<li>be very proactive in sharing next steps, areas of focus and plans</li>\n<li>share weekly roadmap / commitments</li>\n<li>share more community-specific content</li>\n<li>of course continue answering the user questions</li>\n<li>work with the users to build the best open-source experiment tracking tools.</li>\n</ul>\n<p><em><strong>The slack workspace is going to become deprecated by the end of 2022.</strong></em></p>\n<p>I also asked a friend on the technical reasons for moving to Discord. Here is what he responded.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp\" alt=\"\"></p>\n<p>Aim High!! ðŸš€</p>"},"_id":"posts/aim-community-is-moving-to-discord-ðŸŽ‰.md","_raw":{"sourceFilePath":"posts/aim-community-is-moving-to-discord-ðŸŽ‰.md","sourceFileName":"aim-community-is-moving-to-discord-ðŸŽ‰.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-community-is-moving-to-discord-ðŸŽ‰"},"type":"Post"},{"title":"Aim from Zero to Hero","date":"2022-03-16T16:39:52.780Z","author":"Gev Soghomonian","description":"In this blog post, we show how to use Aimâ€™s basic to highly advanced functionality in order to track your machine learning experiments with various","slug":"aim-from-zero-to-hero","image":"/images/dynamic/image.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"In this blog post, we show how to use Aimâ€™s basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\n\n## *Starting from basics: how Aim tracks machine learning experiments*\n\nMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training withÂ `matplotlib`. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\n\nTo track the losses, simply create an experiment run and add a tracking common like this\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n```\n\nYou are going to end up with a visualization of this kind.\n\n![](/images/dynamic/image-2-.png)\n\nA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view theÂ [complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html)Â or simply play around in ourÂ [interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&chart=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&select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n- - -\n\n## *Rising beyond watching losses*\n\nAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonicÂ dictÂ or dictionary-like object, i.e.\n\n```\naim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n```\n\nIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you canÂ [group with respect to the parameters/hyperparameters](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter)Â that you saved with aim.\n\nViewing theÂ [standalone parameters](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page)Â is fast for each separate Single Run\n\n![](/images/dynamic/image.png)\n\n[Filtering/grouping your runs using the selected trackable](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter)Â can be accessed and looked through in ourÂ [interactive tutorial](http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM&chart=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&select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n![](/images/dynamic/image-1-.png)\n\nFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query languageÂ [AimQL](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2), by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\n\n![](/images/dynamic/image-4-.png)\n\n## ***What if numbers are simply not enough?***\n\nVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\n\nAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\n\n```\naim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n```\n\nYou can find the complete guide to tracking in theÂ [official documentation](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#).\n\nAll the grouping/filtering/aggregation functional presented above is also available for Images.\n\n![](/images/dynamic/image-2-.jpeg)\n\n![](/images/dynamic/image-3-.jpeg)\n\n![](/images/dynamic/image-5-.png)\n\nThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\n\n![](/images/dynamic/image.gif)\n\nThe grouping is rather flexible with aÂ [multitude of options](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer).\n\nAnother interesting thing that one canÂ [track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\nYou can play around with the image and single run explorers and see all the trackables across our numerous Demos ([FS2](http://play.aimstack.io:10004/),Â [Spleen Segmentation](http://play.aimstack.io:10005/),Â [Lightweight GAN](http://play.aimstack.io:10002/),Â [Machine Translation](http://play.aimstack.io:10001/)).\n\n***One Run to rule them all***\n\nThere are times when a researcher would need to focus upon only aÂ [single run of the experiment](https://aimstack.io/aim-3-7-revamped-run-single-page-and-aim-docker-image/), where he can iterate through a complete list of all the things he tracked. Aim has a dedicatedÂ [Single Run Page](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page)Â for this very purpose.\n\nYou can view all the trackables in the following Tabs:\n\n* Parameters/ Hyperparametrs â€“ Everything tracked regarding the experiment\n* Metrics â€“ All the Tracked Metrics/Losses etc.\n* System â€“ All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\n* Distributions â€“ All the distributions tracked (i.e. Flowing gradients and weights)\n* Images â€“ All the Image objects saved\n* Audios â€“ All the Audio objects saved\n* Texts â€“ All the raw text tracked during experimentation\n* Figures â€“ All theÂ `matplotlin`/`Plotly`Â etc. Figures\n* Settings â€“ Settings that runs share\n\nLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\n\n![](/images/dynamic/image-7-.png)\n\n![](/images/dynamic/image-8-.png)\n\n![](/images/dynamic/image-9-.png)\n\nYou can look aroud the individual runs in one of our interactive Demos ([FS2](http://play.aimstack.io:10004/),Â [Spleen Segmentation](http://play.aimstack.io:10005/),Â [Lightweight GAN](http://play.aimstack.io:10002/),Â [Machine Translation](http://play.aimstack.io:10001/)).\n\n- - -\n\n## ***No More localhost. Track experiments remotely***\n\nAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\n\n```\naim init\n# Tringgering aim server\naim server --repo <REPO_PATH> --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo <REPO_PATH> --host 0.0.0.0 --port some_open_port\n```\n\nIntegrating this newly createdÂ [remote tracking server](https://aimstack.io/aim3-4-remote-tracking-alpha-sorting-deleting-runs/)Â within your experimentation is even easier.\n\n```\n# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n```\n\nAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\n\n![](/images/dynamic/image-4-.jpeg)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humblingÂ  and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for support.","html":"<p>In this blog post, we show how to use Aimâ€™s basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.</p>\n<h2><em>Starting from basics: how Aim tracks machine learning experiments</em></h2>\n<p>Most of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training withÂ <code>matplotlib</code>. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)</p>\n<p>To track the losses, simply create an experiment run and add a tracking common like this</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n</code></pre>\n<p>You are going to end up with a visualization of this kind.</p>\n<p><img src=\"/images/dynamic/image-2-.png\" alt=\"\"></p>\n<p>A natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n</code></pre>\n<p>After grouping, we end up with a visualization akin to this.</p>\n<p><img src=\"/images/dynamic/image.jpeg\" alt=\"\"></p>\n<p>Aggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\">complete documentation</a>Â or simply play around in ourÂ <a href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&#x26;chart=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&#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\">interactive Demo</a>.</p>\n<hr>\n<h2><em>Rising beyond watching losses</em></h2>\n<p>At one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonicÂ dictÂ or dictionary-like object, i.e.</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n</code></pre>\n<p>In the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you canÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\">group with respect to the parameters/hyperparameters</a>Â that you saved with aim.</p>\n<p>Viewing theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\">standalone parameters</a>Â is fast for each separate Single Run</p>\n<p><img src=\"/images/dynamic/image.png\" alt=\"\"></p>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\">Filtering/grouping your runs using the selected trackable</a>Â can be accessed and looked through in ourÂ <a href=\"http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM&#x26;chart=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&#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\">interactive tutorial</a>.</p>\n<p><img src=\"/images/dynamic/image-1-.png\" alt=\"\"></p>\n<p>For a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query languageÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2\">AimQL</a>, by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.</p>\n<p><img src=\"/images/dynamic/image-4-.png\" alt=\"\"></p>\n<h2><em><strong>What if numbers are simply not enough?</strong></em></h2>\n<p>Various machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.</p>\n<p>Aim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n</code></pre>\n<p>You can find the complete guide to tracking in theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#\">official documentation</a>.</p>\n<p>All the grouping/filtering/aggregation functional presented above is also available for Images.</p>\n<p><img src=\"/images/dynamic/image-2-.jpeg\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-3-.jpeg\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-5-.png\" alt=\"\"></p>\n<p>The Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.</p>\n<p><img src=\"/images/dynamic/image.gif\" alt=\"\"></p>\n<p>The grouping is rather flexible with aÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer\">multitude of options</a>.</p>\n<p>Another interesting thing that one canÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\">track is distributions</a>. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.</p>\n<pre><code>from aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n</code></pre>\n<p>Distributions can be accessed from the Single Run Page as well.</p>\n<p><img src=\"/images/dynamic/image-6-.png\" alt=\"\"></p>\n<p>You can play around with the image and single run explorers and see all the trackables across our numerous Demos (<a href=\"http://play.aimstack.io:10004/\">FS2</a>,Â <a href=\"http://play.aimstack.io:10005/\">Spleen Segmentation</a>,Â <a href=\"http://play.aimstack.io:10002/\">Lightweight GAN</a>,Â <a href=\"http://play.aimstack.io:10001/\">Machine Translation</a>).</p>\n<p><em><strong>One Run to rule them all</strong></em></p>\n<p>There are times when a researcher would need to focus upon only aÂ <a href=\"https://aimstack.io/aim-3-7-revamped-run-single-page-and-aim-docker-image/\">single run of the experiment</a>, where he can iterate through a complete list of all the things he tracked. Aim has a dedicatedÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\">Single Run Page</a>Â for this very purpose.</p>\n<p>You can view all the trackables in the following Tabs:</p>\n<ul>\n<li>Parameters/ Hyperparametrs â€“ Everything tracked regarding the experiment</li>\n<li>Metrics â€“ All the Tracked Metrics/Losses etc.</li>\n<li>System â€“ All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)</li>\n<li>Distributions â€“ All the distributions tracked (i.e. Flowing gradients and weights)</li>\n<li>Images â€“ All the Image objects saved</li>\n<li>Audios â€“ All the Audio objects saved</li>\n<li>Texts â€“ All the raw text tracked during experimentation</li>\n<li>Figures â€“ All theÂ <code>matplotlin</code>/<code>Plotly</code>Â etc. Figures</li>\n<li>Settings â€“ Settings that runs share</li>\n</ul>\n<p>Looking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form</p>\n<p><img src=\"/images/dynamic/image-7-.png\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-8-.png\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-9-.png\" alt=\"\"></p>\n<p>You can look aroud the individual runs in one of our interactive Demos (<a href=\"http://play.aimstack.io:10004/\">FS2</a>,Â <a href=\"http://play.aimstack.io:10005/\">Spleen Segmentation</a>,Â <a href=\"http://play.aimstack.io:10002/\">Lightweight GAN</a>,Â <a href=\"http://play.aimstack.io:10001/\">Machine Translation</a>).</p>\n<hr>\n<h2><em><strong>No More localhost. Track experiments remotely</strong></em></h2>\n<p>Aim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands</p>\n<pre><code>aim init\n# Tringgering aim server\naim server --repo &#x3C;REPO_PATH> --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo &#x3C;REPO_PATH> --host 0.0.0.0 --port some_open_port\n</code></pre>\n<p>Integrating this newly createdÂ <a href=\"https://aimstack.io/aim3-4-remote-tracking-alpha-sorting-deleting-runs/\">remote tracking server</a>Â within your experimentation is even easier.</p>\n<pre><code># This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n</code></pre>\n<p>After this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.</p>\n<p><img src=\"/images/dynamic/image-4-.jpeg\" alt=\"\"></p>\n<h2>Learn more</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humblingÂ  and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support.</p>"},"_id":"posts/aim-from-zero-to-hero.md","_raw":{"sourceFilePath":"posts/aim-from-zero-to-hero.md","sourceFileName":"aim-from-zero-to-hero.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-from-zero-to-hero"},"type":"Post"},{"title":"Aim tutorial for Weights and Biases users","date":"2022-09-07T21:13:32.862Z","author":"Hovhannes Tamoyan","description":"Using Aimâ€™s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights & Biases (wandb) runs into the native format and explore them via Aim UI. In this blog/post we will go over the steps required to migrate wandb logs to Aim. ","slug":"aim-tutorial-for-weights-and-biases-users","image":"https://miro.medium.com/max/1400/1*PBF_k6VevuUrquadJ86vfA.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\n\nI have been usingÂ [Aim](https://github.com/aimhubio/aim)Â for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\n\nThere are lots of converters already available to migrate or use along with other experiment trackers:Â [Aim Converters](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html).\n\nUsing Aimâ€™s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights & Biases (wandb) runs into the native format and explore them viaÂ [Aim UI](https://aimstack.readthedocs.io/en/latest/ui/overview.html).\n\nIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\n\n# Example Project Setup\n\nLetâ€™s take a look at a concrete example. We will be usingÂ `keras-tuner`Â to train aÂ `CNN`Â onÂ `Cifar10`Â dataset. Create a project and track the experiments using wandb. We will set the team name/entity to beÂ `sample-team`:\n\n```\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n```\n\nModel creation, data loading and other parts can be done as:\n\n```\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n```\n\nThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded letâ€™s convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\n\n> *Note: During this project I noticed that the*Â `wandb.keras.WandbCallback`Â *was tracking the metrics on epoch end. For better experience we recommend using the*Â `aim.keras_tuner.AimCallback`*: to track all the metrics on batch end.*\n\n# Converting runs from Wandb to Aim\n\nTo be able to explore Weights & Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\n\nPick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\n\n```\n$ aim init\n```\n\nTo start converting wandb experiments from entity/team:Â `sample-team`Â and project:Â `my-awesome-project`Â run:\n\n```\n$ aim convert wandb --entity sample-team --project my-awesome-project\n```\n\nThe converter will iterate over all the experiments in the projectÂ `my-awesome-project`Â and create a distinct Aim run for each experiment.\n\nFor more please see theÂ [â€œShow Weights and Biases logs in Aimâ€](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim)Â section inÂ [Aim docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nNow that we have our Aim repository initialized, we simply need to do:\n\n```\n$ aim up\n```\n\nto start the Aim UI.\n\n# Exploring the differences in Aim and wandb UIs\n\nTo see the configs, metadata and more on wandb we need to navigate to the â€œOverviewâ€ page:\n\n![](https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp)\n\nUnder the Config section, our logged configurations are shown. On the Summary page the metricsâ€™ latest results.\n\nThe wandb run â€œOverviewâ€ page counterpart on Aim UI is the individual run page, which can be accessed from the â€œRunsâ€ page:\n\n![](https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp)\n\nMore information can be found when opening an individual runâ€™s overview page.\n\n![](https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp)\n\nOn this page we can see the params, even theÂ `wandb_run_id`Â and theÂ `wandb_run_name`Â which are extracted from wandb runs during the conversion process.\n\nMore detailed information about the run can be found on each of the tabs, e.g. â€œRun Paramsâ€, â€œMetricsâ€, â€œSystemâ€ etc.\n\n# Filtering and Grouping the Metrics\n\nOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\n\nTo see the tracked metrics and compare them with other runs we need to open the â€œWorkspaceâ€ page on wandb:\n\n![](https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp)\n\nTo see the metrics on Aim you need to navigate to the â€œMetricsâ€ page. By default you will see an empty page with â€œno resultsâ€ message (which I hope they make this experience better soon), this is because we havenâ€™t selected any metrics to show. To do that we can use the â€œ+ Metricsâ€ button to open up the list of available metrics and select the desired metrics, e.g. â€œlossâ€ and â€œaccuracyâ€:\n\n![](https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp)\n\nThis is the view we will get after selecting to show the loss and accuracy:\n\n![](https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp)\n\nNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wanâ€™t to see the plots by metric name, so each plot will represent a metricâ€™s values.\n\n![](https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp)\n\nWe simply need to access theÂ `metric.name`Â .\n\n![](https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp)\n\nTo have unique coloring for each run, we can use the â€œGroup by colorâ€ functionality, and provide the run.hash. Aim assigns a unique hash to each run.\n\n> *Note: in this example the Aim UI automatically assigns individual colors for each run.*\n\n\n\nNote: in this example the Aim UI automatically assigns individual colors for each run.)\n\n\\\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the â€œadvanced search modeâ€ click on the pen button under the search button:\n\n![](https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp)\n\nthis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\n\n```\n((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n```\n\nThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\n\n```\nmetric.name in [\"accuracy\", \"loss\"]\n```\n\nFor more please see the â€œ[Search Runs](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs)â€ page on docs.\n\n# Hyperparameter Explorer\n\nTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the â€œAdd Panelâ€ button, and add the parameters and the metrics we need, e.g.:\n\n![](https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp)\n\nAim counterpart parameters explorer is on the page of â€œParamsâ€. Where by using the â€œ+ Run Paramsâ€ button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\n\n![](https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp)\n\n## Plot Operations\n\nThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more â€˜readableâ€™ and explicit. On wandb one needs to click on â€œEdit panelâ€ sign to see this view:\n\n![](https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp)\n\n\\\nOn Aim you can find the operations on the right sidebar of the Metrics page:\n\n![](https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp)\n\n# Conclusion\n\nAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid Iâ€™ve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more pleaseÂ [read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.","html":"<p>I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.</p>\n<p>I have been usingÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).</p>\n<p>There are lots of converters already available to migrate or use along with other experiment trackers:Â <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html\">Aim Converters</a>.</p>\n<p>Using Aimâ€™s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights &#x26; Biases (wandb) runs into the native format and explore them viaÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\">Aim UI</a>.</p>\n<p>In this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.</p>\n<h1>Example Project Setup</h1>\n<p>Letâ€™s take a look at a concrete example. We will be usingÂ <code>keras-tuner</code>Â to train aÂ <code>CNN</code>Â onÂ <code>Cifar10</code>Â dataset. Create a project and track the experiments using wandb. We will set the team name/entity to beÂ <code>sample-team</code>:</p>\n<pre><code>import wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n</code></pre>\n<p>Model creation, data loading and other parts can be done as:</p>\n<pre><code>import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n</code></pre>\n<p>This code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded letâ€™s convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.</p>\n<blockquote>\n<p><em>Note: During this project I noticed that the</em>Â <code>wandb.keras.WandbCallback</code>Â <em>was tracking the metrics on epoch end. For better experience we recommend using the</em>Â <code>aim.keras_tuner.AimCallback</code><em>: to track all the metrics on batch end.</em></p>\n</blockquote>\n<h1>Converting runs from Wandb to Aim</h1>\n<p>To be able to explore Weights &#x26; Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.</p>\n<p>Pick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:</p>\n<pre><code>$ aim init\n</code></pre>\n<p>To start converting wandb experiments from entity/team:Â <code>sample-team</code>Â and project:Â <code>my-awesome-project</code>Â run:</p>\n<pre><code>$ aim convert wandb --entity sample-team --project my-awesome-project\n</code></pre>\n<p>The converter will iterate over all the experiments in the projectÂ <code>my-awesome-project</code>Â and create a distinct Aim run for each experiment.</p>\n<p>For more please see theÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim\">â€œShow Weights and Biases logs in Aimâ€</a>Â section inÂ <a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim docs</a>.</p>\n<p>Now that we have our Aim repository initialized, we simply need to do:</p>\n<pre><code>$ aim up\n</code></pre>\n<p>to start the Aim UI.</p>\n<h1>Exploring the differences in Aim and wandb UIs</h1>\n<p>To see the configs, metadata and more on wandb we need to navigate to the â€œOverviewâ€ page:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp\" alt=\"\"></p>\n<p>Under the Config section, our logged configurations are shown. On the Summary page the metricsâ€™ latest results.</p>\n<p>The wandb run â€œOverviewâ€ page counterpart on Aim UI is the individual run page, which can be accessed from the â€œRunsâ€ page:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp\" alt=\"\"></p>\n<p>More information can be found when opening an individual runâ€™s overview page.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp\" alt=\"\"></p>\n<p>On this page we can see the params, even theÂ <code>wandb_run_id</code>Â and theÂ <code>wandb_run_name</code>Â which are extracted from wandb runs during the conversion process.</p>\n<p>More detailed information about the run can be found on each of the tabs, e.g. â€œRun Paramsâ€, â€œMetricsâ€, â€œSystemâ€ etc.</p>\n<h1>Filtering and Grouping the Metrics</h1>\n<p>One of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.</p>\n<p>To see the tracked metrics and compare them with other runs we need to open the â€œWorkspaceâ€ page on wandb:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp\" alt=\"\"></p>\n<p>To see the metrics on Aim you need to navigate to the â€œMetricsâ€ page. By default you will see an empty page with â€œno resultsâ€ message (which I hope they make this experience better soon), this is because we havenâ€™t selected any metrics to show. To do that we can use the â€œ+ Metricsâ€ button to open up the list of available metrics and select the desired metrics, e.g. â€œlossâ€ and â€œaccuracyâ€:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp\" alt=\"\"></p>\n<p>This is the view we will get after selecting to show the loss and accuracy:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp\" alt=\"\"></p>\n<p>Now as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wanâ€™t to see the plots by metric name, so each plot will represent a metricâ€™s values.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp\" alt=\"\"></p>\n<p>We simply need to access theÂ <code>metric.name</code>Â .</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp\" alt=\"\"></p>\n<p>To have unique coloring for each run, we can use the â€œGroup by colorâ€ functionality, and provide the run.hash. Aim assigns a unique hash to each run.</p>\n<blockquote>\n<p><em>Note: in this example the Aim UI automatically assigns individual colors for each run.</em></p>\n</blockquote>\n<p>Note: in this example the Aim UI automatically assigns individual colors for each run.)</p>\n<p><br>\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the â€œadvanced search modeâ€ click on the pen button under the search button:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp\" alt=\"\"></p>\n<p>this operation will automatically convert the selections into the form of a query. In our case the query should look like this:</p>\n<pre><code>((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n</code></pre>\n<p>The syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:</p>\n<pre><code>metric.name in [\"accuracy\", \"loss\"]\n</code></pre>\n<p>For more please see the â€œ<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs\">Search Runs</a>â€ page on docs.</p>\n<h1>Hyperparameter Explorer</h1>\n<p>To explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the â€œAdd Panelâ€ button, and add the parameters and the metrics we need, e.g.:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp\" alt=\"\"></p>\n<p>Aim counterpart parameters explorer is on the page of â€œParamsâ€. Where by using the â€œ+ Run Paramsâ€ button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp\" alt=\"\"></p>\n<h2>Plot Operations</h2>\n<p>The operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more â€˜readableâ€™ and explicit. On wandb one needs to click on â€œEdit panelâ€ sign to see this view:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp\" alt=\"\"></p>\n<p><br>\nOn Aim you can find the operations on the right sidebar of the Metrics page:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp\" alt=\"\"></p>\n<h1>Conclusion</h1>\n<p>As much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid Iâ€™ve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more pleaseÂ <a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">read the docs</a>.</p>\n<p>Aim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.</p>"},"_id":"posts/aim-tutorial-for-weights-and-biases-users.md","_raw":{"sourceFilePath":"posts/aim-tutorial-for-weights-and-biases-users.md","sourceFileName":"aim-tutorial-for-weights-and-biases-users.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-tutorial-for-weights-and-biases-users"},"type":"Post"},{"title":"Aim v2.2.0 â€” Hugging Face integration","date":"2021-03-24T13:20:24.937Z","author":"Gev Soghomonian","description":"Aim 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.","slug":"aim-v2-2-0-â€”-hugging-face-integration","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MqbllMOE307ZvLLuH0MhvA.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)Â 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\n\nThanks toÂ [siddk](https://github.com/siddk),Â [TommasoBendinelli](https://github.com/TommasoBendinelli)Â andÂ [Khazhak](https://github.com/Khazhak)Â for their contribution to this release.\n\n> **Note on the Aim versioning:**Â The previous two release posts:Â [Aim 1.3.5](https://aimstack.io/blog/new-releases/aim-1-3-5-%E2%80%94-activity-view-and-x-axis-alignment)Â andÂ [Aim 1.3.8](https://aimstack.io/blog/new-releases/aim-1-3-8-%E2%80%94-enhanced-context-table-and-advanced-group-coloring)Â had used the version number ofÂ [AimUI](https://aimstack.readthedocs.io/en/latest/ui/overview.html)Â as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.Â [Check out the Aim CHANGELOG](https://github.com/aimhubio/aim/blob/main/CHANGELOG.md).\n\nWe have also addedÂ [milestones](https://github.com/aimhubio/aim/milestones)Â for each version. As well as theÂ [Roadmap](https://github.com/aimhubio/aim#roadmap).\n\nCheck out the new features atÂ [play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\n## Hugging Face integration\n\nHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\n\n```\nfrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n```\n\nHere is how it works:Â `aim_callback`Â will automatically open an AimÂ `Session`Â and close it when the trainer is done. WhenÂ `trainer.train()`,Â `trainer.evaluate()`Â orÂ `trainer.predict()`Â is called,Â `aim_callback`Â will automatically log the hyper-params and respective metrics.\n\nFind a full exampleÂ [here](https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py).\n\n## Metric Visibility Control\n\nWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\n\nA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*yZAWw55lUWVCa35zptdCrA.gif \"Hide individual metrics as well as collectively\")\n\n## Column resize\n\nWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.Â Resize is available both on Explore and Dashboard tables.Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*KFUs6pmpqOCVhfdWMO6CVg.gif \"Drag column edges back-and-forth to resize\")\n\n## Hide columns with similar values\n\nMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\n\nThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*klN4dR6T8nSHLqyvqIUjOg.gif \"Leave only different columns with a button click, if needed customize afterwards\")\n\n## Logscale\n\nOne of the must-have features that we hadnâ€™t had a chance to work on. Finally itâ€™s available!!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cin-2u7a14bj9fB46WmZJw.gif \"log-scale on Aim\")\n\n## New methods for aggregation\n\nNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*xfp0OtVsz6s4vhIdQ1nLIw.gif)\n\n## Learn More\n\n[Aim is on the mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://slack.aimstack.io/), share your feedback.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for supportÂ ðŸ™Œ.","html":"<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.</p>\n<p>Thanks toÂ <a href=\"https://github.com/siddk\">siddk</a>,Â <a href=\"https://github.com/TommasoBendinelli\">TommasoBendinelli</a>Â andÂ <a href=\"https://github.com/Khazhak\">Khazhak</a>Â for their contribution to this release.</p>\n<blockquote>\n<p><strong>Note on the Aim versioning:</strong>Â The previous two release posts:Â <a href=\"https://aimstack.io/blog/new-releases/aim-1-3-5-%E2%80%94-activity-view-and-x-axis-alignment\">Aim 1.3.5</a>Â andÂ <a href=\"https://aimstack.io/blog/new-releases/aim-1-3-8-%E2%80%94-enhanced-context-table-and-advanced-group-coloring\">Aim 1.3.8</a>Â had used the version number ofÂ <a href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\">AimUI</a>Â as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.Â <a href=\"https://github.com/aimhubio/aim/blob/main/CHANGELOG.md\">Check out the Aim CHANGELOG</a>.</p>\n</blockquote>\n<p>We have also addedÂ <a href=\"https://github.com/aimhubio/aim/milestones\">milestones</a>Â for each version. As well as theÂ <a href=\"https://github.com/aimhubio/aim#roadmap\">Roadmap</a>.</p>\n<p>Check out the new features atÂ <a href=\"http://play.aimstack.io:43900/dashboard\">play.aimstack.io</a>.</p>\n<h2>Hugging Face integration</h2>\n<p>Hugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.</p>\n<pre><code>from aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n</code></pre>\n<p>Here is how it works:Â <code>aim_callback</code>Â will automatically open an AimÂ <code>Session</code>Â and close it when the trainer is done. WhenÂ <code>trainer.train()</code>,Â <code>trainer.evaluate()</code>Â orÂ <code>trainer.predict()</code>Â is called,Â <code>aim_callback</code>Â will automatically log the hyper-params and respective metrics.</p>\n<p>Find a full exampleÂ <a href=\"https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py\">here</a>.</p>\n<h2>Metric Visibility Control</h2>\n<p>When dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.</p>\n<p>A toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*yZAWw55lUWVCa35zptdCrA.gif\" alt=\"\" title=\"Hide individual metrics as well as collectively\"></p>\n<h2>Column resize</h2>\n<p>With lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.Â Resize is available both on Explore and Dashboard tables.Here is a quick demo:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*KFUs6pmpqOCVhfdWMO6CVg.gif\" alt=\"\" title=\"Drag column edges back-and-forth to resize\"></p>\n<h2>Hide columns with similar values</h2>\n<p>More often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.</p>\n<p>This feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*klN4dR6T8nSHLqyvqIUjOg.gif\" alt=\"\" title=\"Leave only different columns with a button click, if needed customize afterwards\"></p>\n<h2>Logscale</h2>\n<p>One of the must-have features that we hadnâ€™t had a chance to work on. Finally itâ€™s available!!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cin-2u7a14bj9fB46WmZJw.gif\" alt=\"\" title=\"log-scale on Aim\"></p>\n<h2>New methods for aggregation</h2>\n<p>Now you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*xfp0OtVsz6s4vhIdQ1nLIw.gif\" alt=\"\"></p>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on the mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>, share your feedback.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for supportÂ ðŸ™Œ.</p>"},"_id":"posts/aim-v2-2-0-â€”-hugging-face-integration.md","_raw":{"sourceFilePath":"posts/aim-v2-2-0-â€”-hugging-face-integration.md","sourceFileName":"aim-v2-2-0-â€”-hugging-face-integration.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-2-0-â€”-hugging-face-integration"},"type":"Post"},{"title":"Aim v2.3.0 â€” System Resource Usage and Reverse Grouping","date":"2021-04-20T13:39:39.516Z","author":"Gev Soghomonian","description":"Aim 2.3.0 allowing System Resource Usage optimization is out! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out...","slug":"aim-v2-3-0-â€”-system-resource-usage-and-reverse-grouping","image":"https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)Â 2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\n\nCheck out the updated Aim atÂ [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\n\nBelow are the highlight features of the update. Find the full list of changesÂ [here](https://github.com/aimhubio/aim/milestone/3?closed=1).\n\n## System Resource Usage\n\nNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session withÂ `system_tracking_interval==0`.\n\nOnce you run the training, you will see the following. Here is a demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*pOMxDdOpUDIOedrNIM53pQ.gif \"Aim automatic system resource tracking demo\")\n\nOf course you can alsoÂ [query all those new metrics](https://aimstack.io/wp-admin/post.php?post=706&action=edit)Â in the Explore page and compare, group, aggregate with the rest of the metrics!\n\n## Reverse grouping (â€œagainstâ€ the param)\n\nAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it aÂ Reverse Grouping. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif \"Aim Reverse grouping demo\")\n\nUse the toggle next to switch between grouping modes.\n\n## Line Chart Smoothing\n\nNow you can apply smoothing on metrics. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*MgNuAaH6gzm7u-H2QUh15g.gif \"Aim metric smoothing\")\n\nThere are two type of smoothing options available:Â `Exponential Moving Average`Â andÂ `Central Moving Average`. Further info ofÂ [how](https://en.wikipedia.org/wiki/Moving_average)Â itâ€™s calculated.\n\n## New aggregation modes\n\nWe have additionally added two more aggregation modes:Â `standard error`Â andÂ `standard deviation`Â . Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*LS17WlLey5aJfMyJbtgtBg.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for support.","html":"<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â 2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.</p>\n<p>Check out the updated Aim atÂ <a href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\">play.aimstack.io</a>.</p>\n<p>Below are the highlight features of the update. Find the full list of changesÂ <a href=\"https://github.com/aimhubio/aim/milestone/3?closed=1\">here</a>.</p>\n<h2>System Resource Usage</h2>\n<p>Now you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session withÂ <code>system_tracking_interval==0</code>.</p>\n<p>Once you run the training, you will see the following. Here is a demo:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pOMxDdOpUDIOedrNIM53pQ.gif\" alt=\"\" title=\"Aim automatic system resource tracking demo\"></p>\n<p>Of course you can alsoÂ <a href=\"https://aimstack.io/wp-admin/post.php?post=706&#x26;action=edit\">query all those new metrics</a>Â in the Explore page and compare, group, aggregate with the rest of the metrics!</p>\n<h2>Reverse grouping (â€œagainstâ€ the param)</h2>\n<p>As much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it aÂ Reverse Grouping. Here is how it works:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif\" alt=\"\" title=\"Aim Reverse grouping demo\"></p>\n<p>Use the toggle next to switch between grouping modes.</p>\n<h2>Line Chart Smoothing</h2>\n<p>Now you can apply smoothing on metrics. Here is how it works:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*MgNuAaH6gzm7u-H2QUh15g.gif\" alt=\"\" title=\"Aim metric smoothing\"></p>\n<p>There are two type of smoothing options available:Â <code>Exponential Moving Average</code>Â andÂ <code>Central Moving Average</code>. Further info ofÂ <a href=\"https://en.wikipedia.org/wiki/Moving_average\">how</a>Â itâ€™s calculated.</p>\n<h2>New aggregation modes</h2>\n<p>We have additionally added two more aggregation modes:Â <code>standard error</code>Â andÂ <code>standard deviation</code>Â . Here is how it works:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*LS17WlLey5aJfMyJbtgtBg.gif\" alt=\"\"></p>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support.</p>"},"_id":"posts/aim-v2-3-0-â€”-system-resource-usage-and-reverse-grouping.md","_raw":{"sourceFilePath":"posts/aim-v2-3-0-â€”-system-resource-usage-and-reverse-grouping.md","sourceFileName":"aim-v2-3-0-â€”-system-resource-usage-and-reverse-grouping.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-3-0-â€”-system-resource-usage-and-reverse-grouping"},"type":"Post"},{"title":"Aim v2.6.0â€Šâ€”â€ŠDocker requirement removed and Metric as x-axis","date":"2021-06-24T14:25:34.071Z","author":"Gev Soghomonian","description":"Before and after Aim v2.5.0","slug":"aim-v2-6-0â€Šâ€”â€Šdocker-requirement-removed-and-metric-as-x-axis","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8Nw162Czv6_lzfQeh2m9A.png","draft":false,"categories":["New Releases"],"body":{"raw":"## Metric as an alternative x-axis\n\nSometimes when comparing the metrics, itâ€™s not enough to just put them into perspective via subplots.\n\nFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared â€”Â `loss`Â and`bleu`. One of the main indicators of a performant model in NMT, is when higher values ofÂ `bleu`Â correlates with the lower values ofÂ `loss`.\n\nJust plotting them besides each other sometimes isnâ€™t enough to see how they correlate. In this case we useÂ `loss`Â as the x-axis to directly see the correlation between theÂ `loss`and the`bleu`Â metrics for two groups of runs (Â `max_k==3`andÂ `max_k == 1`).\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Y0wAL9nZ7IhgLOUr-cCRRQ.gif \"Demo for metric as alternative x-axis\")\n\nUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\n\n> **Note**: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for support.","html":"<h2>Metric as an alternative x-axis</h2>\n<p>Sometimes when comparing the metrics, itâ€™s not enough to just put them into perspective via subplots.</p>\n<p>For instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared â€”Â <code>loss</code>Â and<code>bleu</code>. One of the main indicators of a performant model in NMT, is when higher values ofÂ <code>bleu</code>Â correlates with the lower values ofÂ <code>loss</code>.</p>\n<p>Just plotting them besides each other sometimes isnâ€™t enough to see how they correlate. In this case we useÂ <code>loss</code>Â as the x-axis to directly see the correlation between theÂ <code>loss</code>and the<code>bleu</code>Â metrics for two groups of runs (Â <code>max_k==3</code>andÂ <code>max_k == 1</code>).</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Y0wAL9nZ7IhgLOUr-cCRRQ.gif\" alt=\"\" title=\"Demo for metric as alternative x-axis\"></p>\n<p>Using a different metric as the x-axis helps to see the correlation between two metrics of training runs better.</p>\n<blockquote>\n<p><strong>Note</strong>: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).</p>\n</blockquote>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for support.</p>"},"_id":"posts/aim-v2-6-0â€Šâ€”â€Šdocker-requirement-removed-and-metric-as-x-axis.md","_raw":{"sourceFilePath":"posts/aim-v2-6-0â€Šâ€”â€Šdocker-requirement-removed-and-metric-as-x-axis.md","sourceFileName":"aim-v2-6-0â€Šâ€”â€Šdocker-requirement-removed-and-metric-as-x-axis.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-6-0â€Šâ€”â€Šdocker-requirement-removed-and-metric-as-x-axis"},"type":"Post"},{"title":"Aim v2.7.1 â€” Table export and Explore view bookmarks","date":"2021-07-15T14:31:32.317Z","author":"Gev Soghomonian","description":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback! Check out the updated","slug":"aim-v2-7-1-â€”-table-export-and-explore-view-bookmarks","image":"https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif","draft":false,"categories":["New Releases"],"body":{"raw":"Aim v2.7.1Â is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\n\nCheck out the updated Aim demo atÂ [play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\nBelow are the two main highlights ofÂ Aim v2.7.1.\n\n## Aim Table CSV export\n\nNow you can export both tables fromÂ ExploreÂ andÂ DashboardÂ to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. withÂ [Pandas](https://pandas.pydata.org/)) for your MLOps needs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif \"Exporting CSV from the Explore table\")\n\n## Bookmark/Save the Explore State \\[experimental]\n\nThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights youâ€™d like to revisit.\n\nIn the futureÂ [releases](https://aimstack.io/blog/new-releases/aim%E2%80%99s-foundations-why-we%E2%80%99re-building-a-tensorboard-alternative), we will build on top of this feature to potentially add notes, fully manage and share them.Â *For now, this feature is in experimental mode*. Please feel free to share more feedback and how we can improve its experience.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*U5a36lDx7-xgSONKh1-D0A.gif \"Use bookmarks to save important explore states\")\n\n## Learn More\n\n[Aim is on a mission to democratize MLOps tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd donâ€™t forget to leaveÂ [Aim](https://github.com/aimhubio/aim)Â a star on GitHub for supportÂ .","html":"<p>Aim v2.7.1Â is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!</p>\n<p>Check out the updated Aim demo atÂ <a href=\"http://play.aimstack.io:43900/dashboard\">play.aimstack.io</a>.</p>\n<p>Below are the two main highlights ofÂ Aim v2.7.1.</p>\n<h2>Aim Table CSV export</h2>\n<p>Now you can export both tables fromÂ ExploreÂ andÂ DashboardÂ to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. withÂ <a href=\"https://pandas.pydata.org/\">Pandas</a>) for your MLOps needs.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif\" alt=\"\" title=\"Exporting CSV from the Explore table\"></p>\n<h2>Bookmark/Save the Explore State [experimental]</h2>\n<p>This new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights youâ€™d like to revisit.</p>\n<p>In the futureÂ <a href=\"https://aimstack.io/blog/new-releases/aim%E2%80%99s-foundations-why-we%E2%80%99re-building-a-tensorboard-alternative\">releases</a>, we will build on top of this feature to potentially add notes, fully manage and share them.Â <em>For now, this feature is in experimental mode</em>. Please feel free to share more feedback and how we can improve its experience.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*U5a36lDx7-xgSONKh1-D0A.gif\" alt=\"\" title=\"Use bookmarks to save important explore states\"></p>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize MLOps tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring.</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>And donâ€™t forget to leaveÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â a star on GitHub for supportÂ .</p>"},"_id":"posts/aim-v2-7-1-â€”-table-export-and-explore-view-bookmarks.md","_raw":{"sourceFilePath":"posts/aim-v2-7-1-â€”-table-export-and-explore-view-bookmarks.md","sourceFileName":"aim-v2-7-1-â€”-table-export-and-explore-view-bookmarks.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-7-1-â€”-table-export-and-explore-view-bookmarks"},"type":"Post"},{"title":"Aim v3.16 â€” Run messages in UI, TensorBoard real-time sync, integration with Hugging Face Datasets","date":"2023-02-15T06:56:43.141Z","author":"Gor Arakelyan","description":"Excited to announce Aim v3.16 is out! ðŸš€","slug":"aim-v3-16-â€”-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-AzW2_QT8nvsgLUQH9JeA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, excited to announce Aim v3.16 is out! ðŸš€ It is packed with new integrations and key enhancements.\n\n> *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Every question and every issue makes Aim better!*\n>\n> *Congratulations toÂ [timokau,](https://github.com/timokau)Â [dsblank](https://github.com/dsblank)Â andÂ [grigoryan-davit](https://github.com/grigoryan-davit)Â for their first contributions. ðŸ™Œ*\n\n# Key highlights\n\n## **Run messages tab in UI**\n\nStarting from version 3.15, Aim has enabled an ability toÂ [log training messages](https://aimstack.readthedocs.io/en/latest/using/logging.html)Â and evenÂ [send notifications](https://aimstack.readthedocs.io/en/latest/using/notifications.html)Â to Slack or Workplace.\n\nWith the version 3.16, youâ€™ll be able to view all yourÂ [run messages](https://aimstack.readthedocs.io/en/latest/using/logging.html)Â right in the UI. All the information you need is just a click away! ðŸ’«\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxqbmJJcqYaH1tLmcg59Xw.png)\n\n## **Real-time sync with TensorBoard logs**\n\n\n\nAim smoothly integrates with experiment tracking tools. If youâ€™re part of a team thatâ€™s already deeply integrated TensorBoard into your projects and pipelines, you will love this enhancement.\n\nWith just one simple line of code, you can integrate Aim with your existing TensorBoard projects. This means that all your logs will be automatically converted to Aim format in real-time. ðŸ”¥ðŸ”¥ðŸ”¥\n\n```\nfrom aim.ext.tensorboard_tracker import Run\n\naim_run = Run(\n    sync_tensorboard_log_dir='TB_LOG_DIR_TO_SYNC_RUNS'\n)\n\n```\n\n## **Support for Python 3.11**\n\nThe Python community has received fantastic news towards the end of 2022. The stable Python 3.11 has been officially released!\n\nWith Aim 3.16 release, you can install and use Aim in your python 3.11 projects. ðŸŽ‰\n\n```\npip3.11 install aim\n```\n\n## **Dropped support for Python 3.6**\n\nTime to say goodbye: Python 3.6 has reached the end-of-life. ðŸ’€\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpHtGUIfWc5RXtDk0TNKTQ.png)\n\nThis change allows us to take advantage of the latest advancements in the Python language and provide you with a more robust and reliable library.\n\nPlease note that if you have been using Aim in your Python 3.6 projects, you will need to upgrade to newer Python versions in order to continue using Aim.\n\n**We apologize for any inconvenience this may cause, but rest assured that the improved stability of Aim will make it worth the transition.**\n\n# New integrations\n\nAim 3.16 is packed with new integrations with favorite ML tools!\n\n## **Aim + Hugging Face Datasets = â¤**\n\nHappy to share now you can easily track and store dataset metadata in Aim run and explore it on the UI.\n\n```\nfrom datasets import load_dataset\n\nfrom aim import Run\nfrom aim.hf_dataset import HFDataset\n\n# Load the dataset\ndataset = load_dataset('rotten_tomatoes')\n\n# Store the dataset metadata\nrun = Run()\nrun['datasets_info'] = HFDataset(dataset)\n```\n\nSee the docsÂ [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\n\n## **Aim + Acme = â¤**\n\nAimâ€™s been added a built-in support for trackingÂ [Acme](https://dm-acme.readthedocs.io/en/latest/)Â trainings. It takes few simple steps to integrate Aim into your training script.\n\n1. Explicitly import theÂ `AimCallback`Â andÂ `AimWriter`Â for tracking training metadata:\n\n```\nfrom aim.sdk.acme import AimCallback, AimWriter\n```\n\n2. Initialize an Aim Run viaÂ `AimCallback`, and create a log factory:\n\n```\naim_run = AimCallback(repo=\".\", experiment_name=\"acme_test\")\n\ndef logger_factory(\n    name: str,\n    steps_key: Optional[str] = None,\n    task_id: Optional[int] = None,\n) -> loggers.Logger:\n    return AimWriter(aim_run, name, steps_key, task_id)\n```\n\n3. Pass the logger factory toÂ `logger_factory`Â upon initiating your training:\n\n```\nexperiment_config = experiments.ExperimentConfig(\n    builder=d4pg_builder,\n    environment_factory=make_environment,\n    network_factory=network_factory,\n    logger_factory=logger_factory,\n    seed=0,\n    max_num_actor_steps=5000)\n```\n\n\n\nSee the docsÂ [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-acme).\n\n## **Aim + Stable-Baselines3 = â¤**\n\nNow you can easily trackÂ [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)Â trainings with Aim. It takes two steps to integrate Aim into your training script.\n\n1. Explicitly import theÂ `AimCallback`Â for tracking training metadata\n\n   ```\n   from aim.sb3 import AimCallback\n   ```\n\n   2. Pass the callback toÂ `callback`Â upon initiating your training:\n\n   ```\n   model.learn(total_timesteps=10_000, callback=AimCallback(repo='.', experiment_name='sb3_test'))\n   ```\n\n   See the docsÂ [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-stable-baselines3).\n\n   # Learn more\n\n   [Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)Â ðŸ™Œ\n\n   Try outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n   Donâ€™t forget to leave us a star onÂ [GitHub](https://github.com/aimhubio/aim)Â if you think Aim is useful. â­ï¸","html":"<p>Hey community, excited to announce Aim v3.16 is out! ðŸš€ It is packed with new integrations and key enhancements.</p>\n<blockquote>\n<p><em>We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Every question and every issue makes Aim better!</em></p>\n<p><em>Congratulations toÂ <a href=\"https://github.com/timokau\">timokau,</a>Â <a href=\"https://github.com/dsblank\">dsblank</a>Â andÂ <a href=\"https://github.com/grigoryan-davit\">grigoryan-davit</a>Â for their first contributions. ðŸ™Œ</em></p>\n</blockquote>\n<h1>Key highlights</h1>\n<h2><strong>Run messages tab in UI</strong></h2>\n<p>Starting from version 3.15, Aim has enabled an ability toÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\">log training messages</a>Â and evenÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/notifications.html\">send notifications</a>Â to Slack or Workplace.</p>\n<p>With the version 3.16, youâ€™ll be able to view all yourÂ <a href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\">run messages</a>Â right in the UI. All the information you need is just a click away! ðŸ’«</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxqbmJJcqYaH1tLmcg59Xw.png\" alt=\"\"></p>\n<h2><strong>Real-time sync with TensorBoard logs</strong></h2>\n<p>Aim smoothly integrates with experiment tracking tools. If youâ€™re part of a team thatâ€™s already deeply integrated TensorBoard into your projects and pipelines, you will love this enhancement.</p>\n<p>With just one simple line of code, you can integrate Aim with your existing TensorBoard projects. This means that all your logs will be automatically converted to Aim format in real-time. ðŸ”¥ðŸ”¥ðŸ”¥</p>\n<pre><code>from aim.ext.tensorboard_tracker import Run\n\naim_run = Run(\n    sync_tensorboard_log_dir='TB_LOG_DIR_TO_SYNC_RUNS'\n)\n\n</code></pre>\n<h2><strong>Support for Python 3.11</strong></h2>\n<p>The Python community has received fantastic news towards the end of 2022. The stable Python 3.11 has been officially released!</p>\n<p>With Aim 3.16 release, you can install and use Aim in your python 3.11 projects. ðŸŽ‰</p>\n<pre><code>pip3.11 install aim\n</code></pre>\n<h2><strong>Dropped support for Python 3.6</strong></h2>\n<p>Time to say goodbye: Python 3.6 has reached the end-of-life. ðŸ’€</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpHtGUIfWc5RXtDk0TNKTQ.png\" alt=\"\"></p>\n<p>This change allows us to take advantage of the latest advancements in the Python language and provide you with a more robust and reliable library.</p>\n<p>Please note that if you have been using Aim in your Python 3.6 projects, you will need to upgrade to newer Python versions in order to continue using Aim.</p>\n<p><strong>We apologize for any inconvenience this may cause, but rest assured that the improved stability of Aim will make it worth the transition.</strong></p>\n<h1>New integrations</h1>\n<p>Aim 3.16 is packed with new integrations with favorite ML tools!</p>\n<h2><strong>Aim + Hugging Face Datasets = â¤</strong></h2>\n<p>Happy to share now you can easily track and store dataset metadata in Aim run and explore it on the UI.</p>\n<pre><code>from datasets import load_dataset\n\nfrom aim import Run\nfrom aim.hf_dataset import HFDataset\n\n# Load the dataset\ndataset = load_dataset('rotten_tomatoes')\n\n# Store the dataset metadata\nrun = Run()\nrun['datasets_info'] = HFDataset(dataset)\n</code></pre>\n<p>See the docsÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\">here</a>.</p>\n<h2><strong>Aim + Acme = â¤</strong></h2>\n<p>Aimâ€™s been added a built-in support for trackingÂ <a href=\"https://dm-acme.readthedocs.io/en/latest/\">Acme</a>Â trainings. It takes few simple steps to integrate Aim into your training script.</p>\n<ol>\n<li>Explicitly import theÂ <code>AimCallback</code>Â andÂ <code>AimWriter</code>Â for tracking training metadata:</li>\n</ol>\n<pre><code>from aim.sdk.acme import AimCallback, AimWriter\n</code></pre>\n<ol start=\"2\">\n<li>Initialize an Aim Run viaÂ <code>AimCallback</code>, and create a log factory:</li>\n</ol>\n<pre><code>aim_run = AimCallback(repo=\".\", experiment_name=\"acme_test\")\n\ndef logger_factory(\n    name: str,\n    steps_key: Optional[str] = None,\n    task_id: Optional[int] = None,\n) -> loggers.Logger:\n    return AimWriter(aim_run, name, steps_key, task_id)\n</code></pre>\n<ol start=\"3\">\n<li>Pass the logger factory toÂ <code>logger_factory</code>Â upon initiating your training:</li>\n</ol>\n<pre><code>experiment_config = experiments.ExperimentConfig(\n    builder=d4pg_builder,\n    environment_factory=make_environment,\n    network_factory=network_factory,\n    logger_factory=logger_factory,\n    seed=0,\n    max_num_actor_steps=5000)\n</code></pre>\n<p>See the docsÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-acme\">here</a>.</p>\n<h2><strong>Aim + Stable-Baselines3 = â¤</strong></h2>\n<p>Now you can easily trackÂ <a href=\"https://stable-baselines3.readthedocs.io/en/master/\">Stable-Baselines3</a>Â trainings with Aim. It takes two steps to integrate Aim into your training script.</p>\n<ol>\n<li>\n<p>Explicitly import theÂ <code>AimCallback</code>Â for tracking training metadata</p>\n<pre><code>from aim.sb3 import AimCallback\n</code></pre>\n<ol start=\"2\">\n<li>Pass the callback toÂ <code>callback</code>Â upon initiating your training:</li>\n</ol>\n<pre><code>model.learn(total_timesteps=10_000, callback=AimCallback(repo='.', experiment_name='sb3_test'))\n</code></pre>\n<p>See the docsÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-stable-baselines3\">here</a>.</p>\n<h1>Learn more</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a>Â ðŸ™Œ</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>Donâ€™t forget to leave us a star onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>Â if you think Aim is useful. â­ï¸</p>\n</li>\n</ol>"},"_id":"posts/aim-v3-16-â€”-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","_raw":{"sourceFilePath":"posts/aim-v3-16-â€”-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","sourceFileName":"aim-v3-16-â€”-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v3-16-â€”-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets"},"type":"Post"},{"title":"Aimâ€™s foundations & why weâ€™re building a Tensorboard alternative","date":"2021-10-22T14:48:48.452Z","author":"Gev Soghomonian","description":"The origins of Aim. In the fateful summer of 2020, our friend mahnerak â€“ a researcher at a non-profit lab was hitting the limits of Tensorboard. He wasnâ€™t","slug":"aimâ€™s-foundations-why-weâ€™re-building-a-tensorboard-alternative","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd5FbXnYnB3fbELmRLTPwA.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The origins of Aim\n\nIn the fateful summer of 2020, our friendÂ [mahnerak](https://twitter.com/mahnerak)Â â€“ a researcher atÂ [a non-profit lab](https://yerevann.com/)Â was hitting the limits of Tensorboard. He wasnâ€™t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. Thatâ€™s how we decided to build a Tensorboard alternative.\n\n[Gor](https://github.com/gorarakelyan)Â and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\n\nBy fall 2020,Â [Aim 2.0](https://aimstack.io/aim-v2-2-0-hugging-face-integration/)Â launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise evenÂ [r/MachineLearning loved it](https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/).\n\n By spring 2021, mahnerak co-authoerd a paper [WARP](https://aclanthology.org/2021.acl-long.381/)Â ([code](https://github.com/yerevann/warp)): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\n\n## A scale problem\n\nBut Aimâ€™s power users â€” who often do 5K+ runs â€”were hitting issues.\n\nAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests.Â *Live updates, image tracking, distribution tracking*â€¦ and Aim 2.0 was hitting the limits of Aim 1.0â€™s design.\n\nIn order to support the future, we had to make changes to the foundation now.\n\n## Launching Aim 3.0.0\n\nAn additionalÂ [317 pull requests](https://github.com/aimhubio/aim/milestone/13?closed=1)Â later, we are excited to launchÂ Aim v3.0.0Â !!!\n\nAs a result, the most important changes include:\n\n**A completely revamped UI**\n\n* Home page and run detail page\n* Runs, metrics and params explorers\n* Bookmarks and Tags\n\n**A completely revamped Aim Python SDK**\n\n* New and much more intuitive (but still quite vanilla) API to track your training runs\n* New and 10x faster embedded storage based onÂ [Rocksdb](http://rocksdb.org/). This will allow us to store virtually any type of AI metadata. On the contrary,Â [AimRecords](https://github.com/aimhubio/aimrecords)Â was designed for metrics and hyperparams only.\n\nEnjoy the changes!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*xbg148dzileheVdOALBMBg.gif)\n\n## Performance improvements\n\n* Average run query execution time on ~2000 runs: 0.784s.\n* Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\n* New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (*for comparison, v2 was performant with limitation for only 100 metrics*).\n\n## Comparisons to familiar tools\n\n### Tensorboard\n\n**Training run comparison**\n\n* The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\n* With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params.Â After all,Â TensorBoard doesnâ€™t have features to group, aggregate the metrics\n\n**Scalability**\n\n* Aim can handle 1000s of training runs both on the backend and on the UI.\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\n\nAim will have the beloved TB visualizations\n\n* Embedding projector.\n* Neural network visualization.\n\n### [](https://github.com/aimhubio/aim#mlflow)MLFlow\n\nMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\n\n**Run comparison**\n\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\n* MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\n\n**UI Scalability**\n\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\n\n### [](https://github.com/aimhubio/aim#weights-and-biases)Weights and Biases\n\n**Hosted vs self-hosted**\n\n* Weights and Biases is a hosted closed-source MLOps platform.\n* Aim is self-hosted, free and open-source experiment tracking tool.\n\n## Aim Roadmap[](https://github.com/aimhubio/aim#tensorboard)\n\nWith this version we are also publishing the AimÂ [roadmap](https://github.com/aimhubio/aim#roadmap)Â for the next 3 months.\n\nThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\n\nWe are alsoÂ [inviting community contributors](https://github.com/aimhubio/aim#community)Â to help us get there faster!\n\n## Why are we building Aim?\n\nWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\n\nExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\n\nHowever we see lots of improvements to be made. Especially around issues like:\n\n* ability to handle 1000s of large-scale experiments\n* actionable, beautiful and performant visualizations\n* extensibility â€” how easy are the apis for extension/democratization?\n\nWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\n\nOur missionâ€¦\n\nAimâ€™s mission is to democratize AI dev tools. We believe that the best AI tools need to be:\n\n* open-source, open-data-format, community-driven\n* have great UI/UX, CLI and other interfaces for automation\n* performant both on UI and data\n* extensible â€” enable ways to build around for so many use-cases\n\n## Thanks to\n\n[Ruben Karapetyan](https://twitter.com/roubkar)Â for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\n\n[Mahnerak](https://twitter.com/mahnerak)Â for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\n\nAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\n\nThe contributors who have been relentlessly iterating over the course of the summer.\n\nOn to the next generation of ML tools!!\n\n## Join Us!\n\nJoin theÂ [Aim community](https://slack.aimstack.io/), test Aim out, ask questions, help us build the future of AI tooling!\n\nIf you find Aim useful, drop by and star the repo â­","html":"<h2>The origins of Aim</h2>\n<p>In the fateful summer of 2020, our friendÂ <a href=\"https://twitter.com/mahnerak\">mahnerak</a>Â â€“ a researcher atÂ <a href=\"https://yerevann.com/\">a non-profit lab</a>Â was hitting the limits of Tensorboard. He wasnâ€™t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. Thatâ€™s how we decided to build a Tensorboard alternative.</p>\n<p><a href=\"https://github.com/gorarakelyan\">Gor</a>Â and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.</p>\n<p>By fall 2020,Â <a href=\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\">Aim 2.0</a>Â launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise evenÂ <a href=\"https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/\">r/MachineLearning loved it</a>.</p>\n<p>By spring 2021, mahnerak co-authoerd a paper <a href=\"https://aclanthology.org/2021.acl-long.381/\">WARP</a>Â (<a href=\"https://github.com/yerevann/warp\">code</a>): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.</p>\n<h2>A scale problem</h2>\n<p>But Aimâ€™s power users â€” who often do 5K+ runs â€”were hitting issues.</p>\n<p>After over 250 pull requests, 1.2K GitHub stars and 200 feature requests.Â <em>Live updates, image tracking, distribution tracking</em>â€¦ and Aim 2.0 was hitting the limits of Aim 1.0â€™s design.</p>\n<p>In order to support the future, we had to make changes to the foundation now.</p>\n<h2>Launching Aim 3.0.0</h2>\n<p>An additionalÂ <a href=\"https://github.com/aimhubio/aim/milestone/13?closed=1\">317 pull requests</a>Â later, we are excited to launchÂ Aim v3.0.0Â !!!</p>\n<p>As a result, the most important changes include:</p>\n<p><strong>A completely revamped UI</strong></p>\n<ul>\n<li>Home page and run detail page</li>\n<li>Runs, metrics and params explorers</li>\n<li>Bookmarks and Tags</li>\n</ul>\n<p><strong>A completely revamped Aim Python SDK</strong></p>\n<ul>\n<li>New and much more intuitive (but still quite vanilla) API to track your training runs</li>\n<li>New and 10x faster embedded storage based onÂ <a href=\"http://rocksdb.org/\">Rocksdb</a>. This will allow us to store virtually any type of AI metadata. On the contrary,Â <a href=\"https://github.com/aimhubio/aimrecords\">AimRecords</a>Â was designed for metrics and hyperparams only.</li>\n</ul>\n<p>Enjoy the changes!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*xbg148dzileheVdOALBMBg.gif\" alt=\"\"></p>\n<h2>Performance improvements</h2>\n<ul>\n<li>Average run query execution time on ~2000 runs: 0.784s.</li>\n<li>Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.</li>\n<li>New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (<em>for comparison, v2 was performant with limitation for only 100 metrics</em>).</li>\n</ul>\n<h2>Comparisons to familiar tools</h2>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<ul>\n<li>The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params.Â After all,Â TensorBoard doesnâ€™t have features to group, aggregate the metrics</li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim can handle 1000s of training runs both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p>Aim will have the beloved TB visualizations</p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3><a href=\"https://github.com/aimhubio/aim#mlflow\"></a>MLFlow</h3>\n<p>MLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.</li>\n<li>MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3><a href=\"https://github.com/aimhubio/aim#weights-and-biases\"></a>Weights and Biases</h3>\n<p><strong>Hosted vs self-hosted</strong></p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h2>Aim Roadmap<a href=\"https://github.com/aimhubio/aim#tensorboard\"></a></h2>\n<p>With this version we are also publishing the AimÂ <a href=\"https://github.com/aimhubio/aim#roadmap\">roadmap</a>Â for the next 3 months.</p>\n<p>This is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.</p>\n<p>We are alsoÂ <a href=\"https://github.com/aimhubio/aim#community\">inviting community contributors</a>Â to help us get there faster!</p>\n<h2>Why are we building Aim?</h2>\n<p>We have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.</p>\n<p>Existing open-source tools (TensorBoard, MLFlow) are super-inspiring for us.</p>\n<p>However we see lots of improvements to be made. Especially around issues like:</p>\n<ul>\n<li>ability to handle 1000s of large-scale experiments</li>\n<li>actionable, beautiful and performant visualizations</li>\n<li>extensibility â€” how easy are the apis for extension/democratization?</li>\n</ul>\n<p>With this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.</p>\n<p>Our missionâ€¦</p>\n<p>Aimâ€™s mission is to democratize AI dev tools. We believe that the best AI tools need to be:</p>\n<ul>\n<li>open-source, open-data-format, community-driven</li>\n<li>have great UI/UX, CLI and other interfaces for automation</li>\n<li>performant both on UI and data</li>\n<li>extensible â€” enable ways to build around for so many use-cases</li>\n</ul>\n<h2>Thanks to</h2>\n<p><a href=\"https://twitter.com/roubkar\">Ruben Karapetyan</a>Â for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.</p>\n<p><a href=\"https://twitter.com/mahnerak\">Mahnerak</a>Â for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.</p>\n<p>Aim users Mohammad Elgaar, Vopani for continuous feedback on our work.</p>\n<p>The contributors who have been relentlessly iterating over the course of the summer.</p>\n<p>On to the next generation of ML tools!!</p>\n<h2>Join Us!</h2>\n<p>Join theÂ <a href=\"https://slack.aimstack.io/\">Aim community</a>, test Aim out, ask questions, help us build the future of AI tooling!</p>\n<p>If you find Aim useful, drop by and star the repo â­</p>"},"_id":"posts/aimâ€™s-foundations-why-weâ€™re-building-a-tensorboard-alternative.md","_raw":{"sourceFilePath":"posts/aimâ€™s-foundations-why-weâ€™re-building-a-tensorboard-alternative.md","sourceFileName":"aimâ€™s-foundations-why-weâ€™re-building-a-tensorboard-alternative.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aimâ€™s-foundations-why-weâ€™re-building-a-tensorboard-alternative"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":" Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out [â€¦]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger usedâ€¦ What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\n\n[Aim](https://github.com/aimhubio/aim)Â is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n## What is XGBoost?\n\n[XGBoost](https://github.com/dmlc/xgboost)Â is an optimized gradient boosting library with highlyÂ Â *efficient*,Â Â *flexible,*Â  andÂ Â *portable*Â design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\n## How to use Aim with XGBoost?\n\nCheck out end-to-endÂ [Aim integration](https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/)Â examples with multiple frameworksÂ [here](https://github.com/aimhubio/aim/tree/main/examples). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\n\n```\n# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n```\n\nAs you can see on line 49,Â AimCallbackÂ is imported fromÂ `aim.xgboost`Â and passed toÂ `xgb.train`Â as one of the callbacks. Aim session can open and close by the AimCallback and theÂ metrics and hparamsstore by XGBoost. In addition to that, thesystem measuresÂ pass to Aim as well.\n\n## What it looks like?\n\nAfter you run the experiment and theÂ `aim up`Â command in theÂ `aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png \"Aim UI dashboard page\")\n\nTo explore the run, we should:\n\n* Choose theÂ `xgboost_test`experiment.\n* Select the metrics to explore.\n* Divide into charts by metrics.\n\nFor example, the gif below illustrates the steps above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif)\n\n\\\n So this is what the final result looks like.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png)\n\nAs easy as that, we can analyze the runs and the system usage.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nIf you find Aim useful, support us and starÂ [the project](https://github.com/aimhubio/aim)Â on GitHub. Also, join theÂ [Aim community](https://aimstack.slack.com/ssb/redirect)Â and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is Aim?</h2>\n<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.</p>\n<h2>What is XGBoost?</h2>\n<p><a href=\"https://github.com/dmlc/xgboost\">XGBoost</a>Â is an optimized gradient boosting library with highlyÂ Â <em>efficient</em>,Â Â <em>flexible,</em>Â  andÂ Â <em>portable</em>Â design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.</p>\n<h2>How to use Aim with XGBoost?</h2>\n<p>Check out end-to-endÂ <a href=\"https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/\">Aim integration</a>Â examples with multiple frameworksÂ <a href=\"https://github.com/aimhubio/aim/tree/main/examples\">here</a>. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.</p>\n<pre><code># You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n</code></pre>\n<p>As you can see on line 49,Â AimCallbackÂ is imported fromÂ <code>aim.xgboost</code>Â and passed toÂ <code>xgb.train</code>Â as one of the callbacks. Aim session can open and close by the AimCallback and theÂ metrics and hparamsstore by XGBoost. In addition to that, thesystem measuresÂ pass to Aim as well.</p>\n<h2>What it looks like?</h2>\n<p>After you run the experiment and theÂ <code>aim up</code>Â command in theÂ <code>aim_logs</code>directory, Aim UI will be running. When first opened, the dashboard page will come up.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png\" alt=\"\" title=\"Aim UI dashboard page\"></p>\n<p>To explore the run, we should:</p>\n<ul>\n<li>Choose theÂ <code>xgboost_test</code>experiment.</li>\n<li>Select the metrics to explore.</li>\n<li>Divide into charts by metrics.</li>\n</ul>\n<p>For example, the gif below illustrates the steps above.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif\" alt=\"\"></p>\n<p><br>\nSo this is what the final result looks like.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png\" alt=\"\"></p>\n<p>As easy as that, we can analyze the runs and the system usage.</p>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>If you find Aim useful, support us and starÂ <a href=\"https://github.com/aimhubio/aim\">the project</a>Â on GitHub. Also, join theÂ <a href=\"https://aimstack.slack.com/ssb/redirect\">Aim community</a>Â and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"Exploring MLflow experiments with a powerful UI","date":"2023-01-17T21:52:43.983Z","author":"Gor Arakelyan","description":"We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! ðŸŽ‰","slug":"exploring-mlflow-experiments-with-a-powerful-ui","image":"https://miro.medium.com/max/1400/1*YMlI66d-QPxI3fcQCoPSlw.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"\n\nWe are excited to announce the release ofÂ [aimlflow](https://github.com/aimhubio/aimlflow), an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! ðŸŽ‰\n\n[MLflow](https://github.com/mlflow/mlflow)Â is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments.Â [Aim](https://github.com/aimhubio/aim)Â is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\n\n![](https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif)\n\n**With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aimâ€™s powerful features, leading to deeper understanding and more effective decision-making.**\n\n![](https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp)\n\nIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Letâ€™s dive in and see how to make it happen.\n\n> Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\n>\n> View more on GitHub:Â <https://github.com/aimhubio/aim>\n\n# Project overview\n\nWe use a simple project that trains a CNN usingÂ [PyTorch](https://github.com/pytorch/pytorch)Â andÂ [Ray Tune](https://github.com/ray-project/ray/tree/master/python/ray/tune)Â on theÂ [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)Â dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\n\n* **PyTorch**Â for building and training the model\n* **Ray Tune**Â for hyper-parameters tuning\n* **MLflow**Â for experiment tracking\n\nFind the full project code on GitHub:Â <https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning>\n\n![](https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp)\n\nRun the trainings by downloading and executingÂ `tune.py`Â python file:\n\n```\npython tune.py\n```\n\nYou should see a similar output, meaning the trainings are successfully initiated:\n\n![](https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp)\n\n## Getting started with aimlflow\n\nAfter the hyper-parameter tuning is ran, letâ€™s see how aimlflow can help us to explore the tracked experiments via UI.\n\nTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in aÂ `.aim`Â repo located on the file system.\n\n**This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isnâ€™t it amazing? ðŸ¤©**\n\n## 1. Install aimlflow on your machine\n\nIt is super easy to install aimlflow, simply run the following command:\n\n```\npip3 install aim-mlflow\n```\n\n## 2. Sync MLflow logs with Aim\n\nPick any directory on your file system and initialize aÂ `.aim`Â repo:\n\n```\naim init\n```\n\nRun theÂ `aimlflow sync`Â command to sync MLflow experiments with the Aim repo:\n\n```\naimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n```\n\n## 3. Run Aim\n\nNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\n\n```\naim up --repo=AIM_REPO_PATH\n```\n\nYou will see the following message on the terminal output:\n\n![](https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp)\n\nCongratulations! Now you can explore the training logs with Aim. ðŸŽ‰\n\n# Quick tour of Aim for MLflow users\n\nIn this section, we will take a quick tour of Aimâ€™s features, including:\n\n* Exploring hyper-parameters tuning results\n* Comparing tracked metrics\n* As well as, taking a look at the other capabilities Aim provides\n\n## Exploring MLflow experiments\n\nNow then Aim is set up and running, we navigate to the project overview page atÂ `127.0.0.1:43800`, where the summary of the project is displayed:\n\n* The number of tracked training runs and experiments\n* Statistics on the amount of tracked metadata\n* A list of experiments and tags, with the ability to quickly explore selected items\n* A calendar and feed of contributions\n* A table of in-progress trainings\n\n![](https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp \"Project overview\")\n\n\\\nTo view the results of the trainings, letâ€™s navigate to the runs dashboard atÂ `127.0.0.1:43800/runs`. Here, you can see hyper-parameters and metrics results all of the trainings.\n\n![](https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp \" Runs dashboard\")\n\nWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\n\n![](https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp \"Run page\")\n\nOn this page, we can view the tracked hparams, including theÂ `mlflow_run_id`Â and theÂ `mlflow_run_name`Â which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\n\n## Comparing metrics\n\nComparing metrics across several runs is super easy with Aim:\n\n* Open the metrics page from the left sidebar\n* Select desired metrics by clicking onÂ `+ Metrics`Â button\n* PressingÂ `Search`Â button on the top right corner\n\nWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\n\n![](https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp)\n\nAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\n\n* **by color**Â â€” each group of metrics will be filled in with its unique color\n* **by stroke style**Â â€” each group will have a unique stroke style (solid, dashed, etc)\n* **by facet**Â â€” each group of metrics will be displayed in a separate subplot\n\nTo learn which set of trials performed the best, letâ€™s apply several groupings:\n\n* Group byÂ `run.hparams.l1`Â hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\n* Group byÂ `metric.name`Â to divide losses and accuracies into separate subplots (this grouping is applied by default)\n\n![](https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp)\n\n**Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.**\n\nFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\n\n```\nrun.metrics['training_iteration'].last > 1\n```\n\n\n\n> Read more about Aim query language capabilities in the docs:Â <https://aimstack.readthedocs.io/en/latest/using/search.html>\n\nThis will result in querying and displaying 9 matched runs:\n\n![](https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp)\n\nLetâ€™s aggregate the groups to see which one performed the best:\n\n![](https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp)\n\nFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. ðŸŽ‰\n\n> For more please see Aim official docs here:Â <https://aimstack.readthedocs.io/en/latest/>\n\n## Last, but not least: a closer look at Aimâ€™s key features\n\n* Use powerful pythonic search to select the runs you want to analyze:\n\n![](https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp)\n\nGroup metrics by hyperparameters to analyze hyperparametersâ€™ influence on run performance:\n\n![](https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp)\n\nSelect multiple metrics and analyze them side by side:\n\n![](https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp)\n\nAggregate metrics by std.dev, std.err, conf.interval:\n\n* ![](https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp)\n\n  Align x axis by any other metric:\n\n  * ![](https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp)\n\n  Scatter plots to learn correlations and trends:\n\n  ![](https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp)\n\n  High dimensional data visualization via parallel coordinate plot:\n\n  ![](https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp)\n\n  Explore media metadata, such as images and audio objects via Aim Explorers:\n\n  * ![](https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp \"Audio Explorer\")\n\n  ![](https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp \"Images Explorer\")\n\n  # Conclusion\n\n  In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\n\n  In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\n\n  We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more pleaseÂ [read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\n  # Learn more\n\n  If you have any questions joinÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. ðŸ™Œ\n\n  Show some love by dropping a â­ï¸ onÂ [GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"<p>We are excited to announce the release ofÂ <a href=\"https://github.com/aimhubio/aimlflow\">aimlflow</a>, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! ðŸŽ‰</p>\n<p><a href=\"https://github.com/mlflow/mlflow\">MLflow</a>Â is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments.Â <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif\" alt=\"\"></p>\n<p><strong>With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aimâ€™s powerful features, leading to deeper understanding and more effective decision-making.</strong></p>\n<p><img src=\"https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp\" alt=\"\"></p>\n<p>In this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Letâ€™s dive in and see how to make it happen.</p>\n<blockquote>\n<p>Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.</p>\n<p>View more on GitHub:Â <a href=\"https://github.com/aimhubio/aim\">https://github.com/aimhubio/aim</a></p>\n</blockquote>\n<h1>Project overview</h1>\n<p>We use a simple project that trains a CNN usingÂ <a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>Â andÂ <a href=\"https://github.com/ray-project/ray/tree/master/python/ray/tune\">Ray Tune</a>Â on theÂ <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a>Â dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:</p>\n<ul>\n<li><strong>PyTorch</strong>Â for building and training the model</li>\n<li><strong>Ray Tune</strong>Â for hyper-parameters tuning</li>\n<li><strong>MLflow</strong>Â for experiment tracking</li>\n</ul>\n<p>Find the full project code on GitHub:Â <a href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\">https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning</a></p>\n<p><img src=\"https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp\" alt=\"\"></p>\n<p>Run the trainings by downloading and executingÂ <code>tune.py</code>Â python file:</p>\n<pre><code>python tune.py\n</code></pre>\n<p>You should see a similar output, meaning the trainings are successfully initiated:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp\" alt=\"\"></p>\n<h2>Getting started with aimlflow</h2>\n<p>After the hyper-parameter tuning is ran, letâ€™s see how aimlflow can help us to explore the tracked experiments via UI.</p>\n<p>To be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in aÂ <code>.aim</code>Â repo located on the file system.</p>\n<p><strong>This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isnâ€™t it amazing? ðŸ¤©</strong></p>\n<h2>1. Install aimlflow on your machine</h2>\n<p>It is super easy to install aimlflow, simply run the following command:</p>\n<pre><code>pip3 install aim-mlflow\n</code></pre>\n<h2>2. Sync MLflow logs with Aim</h2>\n<p>Pick any directory on your file system and initialize aÂ <code>.aim</code>Â repo:</p>\n<pre><code>aim init\n</code></pre>\n<p>Run theÂ <code>aimlflow sync</code>Â command to sync MLflow experiments with the Aim repo:</p>\n<pre><code>aimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n</code></pre>\n<h2>3. Run Aim</h2>\n<p>Now that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:</p>\n<pre><code>aim up --repo=AIM_REPO_PATH\n</code></pre>\n<p>You will see the following message on the terminal output:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp\" alt=\"\"></p>\n<p>Congratulations! Now you can explore the training logs with Aim. ðŸŽ‰</p>\n<h1>Quick tour of Aim for MLflow users</h1>\n<p>In this section, we will take a quick tour of Aimâ€™s features, including:</p>\n<ul>\n<li>Exploring hyper-parameters tuning results</li>\n<li>Comparing tracked metrics</li>\n<li>As well as, taking a look at the other capabilities Aim provides</li>\n</ul>\n<h2>Exploring MLflow experiments</h2>\n<p>Now then Aim is set up and running, we navigate to the project overview page atÂ <code>127.0.0.1:43800</code>, where the summary of the project is displayed:</p>\n<ul>\n<li>The number of tracked training runs and experiments</li>\n<li>Statistics on the amount of tracked metadata</li>\n<li>A list of experiments and tags, with the ability to quickly explore selected items</li>\n<li>A calendar and feed of contributions</li>\n<li>A table of in-progress trainings</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp\" alt=\"\" title=\"Project overview\"></p>\n<p><br>\nTo view the results of the trainings, letâ€™s navigate to the runs dashboard atÂ <code>127.0.0.1:43800/runs</code>. Here, you can see hyper-parameters and metrics results all of the trainings.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp\" alt=\"\" title=\" Runs dashboard\"></p>\n<p>We can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp\" alt=\"\" title=\"Run page\"></p>\n<p>On this page, we can view the tracked hparams, including theÂ <code>mlflow_run_id</code>Â and theÂ <code>mlflow_run_name</code>Â which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.</p>\n<h2>Comparing metrics</h2>\n<p>Comparing metrics across several runs is super easy with Aim:</p>\n<ul>\n<li>Open the metrics page from the left sidebar</li>\n<li>Select desired metrics by clicking onÂ <code>+ Metrics</code>Â button</li>\n<li>PressingÂ <code>Search</code>Â button on the top right corner</li>\n</ul>\n<p>We will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp\" alt=\"\"></p>\n<p>Aim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:</p>\n<ul>\n<li><strong>by color</strong>Â â€” each group of metrics will be filled in with its unique color</li>\n<li><strong>by stroke style</strong>Â â€” each group will have a unique stroke style (solid, dashed, etc)</li>\n<li><strong>by facet</strong>Â â€” each group of metrics will be displayed in a separate subplot</li>\n</ul>\n<p>To learn which set of trials performed the best, letâ€™s apply several groupings:</p>\n<ul>\n<li>Group byÂ <code>run.hparams.l1</code>Â hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer</li>\n<li>Group byÂ <code>metric.name</code>Â to divide losses and accuracies into separate subplots (this grouping is applied by default)</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp\" alt=\"\"></p>\n<p><strong>Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.</strong></p>\n<p>For example, in order to display only runs that were trained for more than one iteration, we will run the following query:</p>\n<pre><code>run.metrics['training_iteration'].last > 1\n</code></pre>\n<blockquote>\n<p>Read more about Aim query language capabilities in the docs:Â <a href=\"https://aimstack.readthedocs.io/en/latest/using/search.html\">https://aimstack.readthedocs.io/en/latest/using/search.html</a></p>\n</blockquote>\n<p>This will result in querying and displaying 9 matched runs:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp\" alt=\"\"></p>\n<p>Letâ€™s aggregate the groups to see which one performed the best:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp\" alt=\"\"></p>\n<p>From the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. ðŸŽ‰</p>\n<blockquote>\n<p>For more please see Aim official docs here:Â <a href=\"https://aimstack.readthedocs.io/en/latest/\">https://aimstack.readthedocs.io/en/latest/</a></p>\n</blockquote>\n<h2>Last, but not least: a closer look at Aimâ€™s key features</h2>\n<ul>\n<li>Use powerful pythonic search to select the runs you want to analyze:</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp\" alt=\"\"></p>\n<p>Group metrics by hyperparameters to analyze hyperparametersâ€™ influence on run performance:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp\" alt=\"\"></p>\n<p>Select multiple metrics and analyze them side by side:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp\" alt=\"\"></p>\n<p>Aggregate metrics by std.dev, std.err, conf.interval:</p>\n<ul>\n<li>\n<p><img src=\"https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp\" alt=\"\"></p>\n<p>Align x axis by any other metric:</p>\n<ul>\n<li><img src=\"https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp\" alt=\"\"></li>\n</ul>\n<p>Scatter plots to learn correlations and trends:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp\" alt=\"\"></p>\n<p>High dimensional data visualization via parallel coordinate plot:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp\" alt=\"\"></p>\n<p>Explore media metadata, such as images and audio objects via Aim Explorers:</p>\n<ul>\n<li><img src=\"https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp\" alt=\"\" title=\"Audio Explorer\"></li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp\" alt=\"\" title=\"Images Explorer\"></p>\n<h1>Conclusion</h1>\n<p>In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!</p>\n<p>In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.</p>\n<p>We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more pleaseÂ <a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">read the docs</a>.</p>\n<h1>Learn more</h1>\n<p>If you have any questions joinÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. ðŸ™Œ</p>\n<p>Show some love by dropping a â­ï¸ onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you think Aim is useful.</p>\n</li>\n</ul>"},"_id":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","_raw":{"sourceFilePath":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileName":"exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-mlflow-experiments-with-a-powerful-ui"},"type":"Post"},{"title":"How to integrate aimlflow with your remote MLflow","date":"2023-01-30T06:25:58.163Z","author":"Hovhannes Tamoyan","description":"We are thrilled to unveil aimlflow, a tool that allows for a smooth integration of a robust experiment tracking UI with MLflow logs! ðŸš€","slug":"how-to-integrate-aimlflow-with-your-remote-mlflow","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSCbDO5InRlBXTxzD3NVwQ.png","draft":false,"categories":["Tutorials"],"body":{"raw":"With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aimâ€™s powerful features, leading to deeper understanding and more effective decision-making.\n\nWe have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:\n\nRunning Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here:Â <https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e>\n\nIn this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.\n\n# Project overview\n\nWe will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.\n\nWe will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.\n\nFind the full project code on GitHub:Â <https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning>\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png)\n\n# Server-side/Remote Configuration\n\nLetâ€™s create a separate directory for the demo and name itÂ `mlflow-demo-remote`. After which download and run theÂ `tune.py`Â python script from the Github repo to conduct the training sessions:\n\n```\n$ python tune.py\n```\n\nRay Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png)\n\nOnce started, mlflow will commence recording the results in theÂ `mlruns`Â directory. Our remote directory will have the following structure:\n\n```\nmlflow-demo-remote\nâ”œâ”€â”€ tune.py\nâ””â”€â”€ mlruns\n    â”œâ”€â”€ ...\n```\n\nLetâ€™s open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from theÂ `mlflow-demo-remote`Â directory:\n\n```\n$ mlflow ui --host 0.0.0.0\n```\n\nBy default, theÂ `--host`Â is set toÂ `127.0.0.1`, limiting access to the service to the local machine only. To expose it to external machines, set the host toÂ `0.0.0.0`.\n\nBy default, the system listens on portÂ `5000`.\n\nOne can setÂ `--backend-store-uri`Â param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path ofÂ `mlruns`Â directory.\n\nUpon navigating toÂ `http://127.0.0.1:5000`, you will be presented with a page that looks similar to this:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png)\n\n# Synchronising MLflow Runs with Aim\n\n\n\nAfter successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.\n\nFirst, letâ€™s move forward with the installation process of aimlflow. Itâ€™s incredibly easy to set up on your device, just execute the following command:\n\n```\n$ pip install aim-mlflow\n```\n\nAfter successfully installing aimlflow on your machine letâ€™s create a directory namedÂ `mlflow-demo-local`Â where theÂ `.aim`repository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:\n\n```\n$ aim init\n```\n\nThis will establish an Aim repository in the present directory and it will be namedÂ `.aim`.\n\nThis is how our local system directory will look like:\n\n```\nmlflow-demo-local\nâ””â”€â”€ .aim\n    â”œâ”€â”€ ...\n```\n\nIn order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into theÂ `.aim`Â repository.\n\nTo begin the process of converting MLflow experiments from the the hosted urlÂ `YOUR_REMOTE_IP:5000`Â into the Aim repositoryÂ `.aim`, execute the following command from our localÂ `mlflow-demo-local`Â directory:\n\n```\n$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n```\n\nThe converter will go through all experiments within the project and create a uniqueÂ `Aim`Â run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.\n\nThis means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? â˜ºï¸\n\nNow that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:\n\n```\n$ aim up\n```\n\nto open the user interface and explore our metrics and other information.\n\nFor further reading please referee toÂ [Aim documentation](https://aimstack.readthedocs.io/en/latest/)Â where you will learn more about the superpowers of Aim.\n\n# Conclusion\n\n\n\nTo sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.\n\nIn this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.\n\nThe added value of Aim makes installing aimlflow and enabling the additional capability well worth it.\n\n# Learn more\n\nIf you have any questions joinÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. ðŸ™Œ\n\nShow some love by dropping a â­ï¸ onÂ [GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"<p>With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aimâ€™s powerful features, leading to deeper understanding and more effective decision-making.</p>\n<p>We have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:</p>\n<p>Running Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here:Â <a href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\">https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e</a></p>\n<p>In this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.</p>\n<h1>Project overview</h1>\n<p>We will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.</p>\n<p>We will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.</p>\n<p>Find the full project code on GitHub:Â <a href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\">https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning</a></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png\" alt=\"\"></p>\n<h1>Server-side/Remote Configuration</h1>\n<p>Letâ€™s create a separate directory for the demo and name itÂ <code>mlflow-demo-remote</code>. After which download and run theÂ <code>tune.py</code>Â python script from the Github repo to conduct the training sessions:</p>\n<pre><code>$ python tune.py\n</code></pre>\n<p>Ray Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png\" alt=\"\"></p>\n<p>Once started, mlflow will commence recording the results in theÂ <code>mlruns</code>Â directory. Our remote directory will have the following structure:</p>\n<pre><code>mlflow-demo-remote\nâ”œâ”€â”€ tune.py\nâ””â”€â”€ mlruns\n    â”œâ”€â”€ ...\n</code></pre>\n<p>Letâ€™s open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from theÂ <code>mlflow-demo-remote</code>Â directory:</p>\n<pre><code>$ mlflow ui --host 0.0.0.0\n</code></pre>\n<p>By default, theÂ <code>--host</code>Â is set toÂ <code>127.0.0.1</code>, limiting access to the service to the local machine only. To expose it to external machines, set the host toÂ <code>0.0.0.0</code>.</p>\n<p>By default, the system listens on portÂ <code>5000</code>.</p>\n<p>One can setÂ <code>--backend-store-uri</code>Â param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path ofÂ <code>mlruns</code>Â directory.</p>\n<p>Upon navigating toÂ <code>http://127.0.0.1:5000</code>, you will be presented with a page that looks similar to this:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png\" alt=\"\"></p>\n<h1>Synchronising MLflow Runs with Aim</h1>\n<p>After successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.</p>\n<p>First, letâ€™s move forward with the installation process of aimlflow. Itâ€™s incredibly easy to set up on your device, just execute the following command:</p>\n<pre><code>$ pip install aim-mlflow\n</code></pre>\n<p>After successfully installing aimlflow on your machine letâ€™s create a directory namedÂ <code>mlflow-demo-local</code>Â where theÂ <code>.aim</code>repository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:</p>\n<pre><code>$ aim init\n</code></pre>\n<p>This will establish an Aim repository in the present directory and it will be namedÂ <code>.aim</code>.</p>\n<p>This is how our local system directory will look like:</p>\n<pre><code>mlflow-demo-local\nâ””â”€â”€ .aim\n    â”œâ”€â”€ ...\n</code></pre>\n<p>In order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into theÂ <code>.aim</code>Â repository.</p>\n<p>To begin the process of converting MLflow experiments from the the hosted urlÂ <code>YOUR_REMOTE_IP:5000</code>Â into the Aim repositoryÂ <code>.aim</code>, execute the following command from our localÂ <code>mlflow-demo-local</code>Â directory:</p>\n<pre><code>$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n</code></pre>\n<p>The converter will go through all experiments within the project and create a uniqueÂ <code>Aim</code>Â run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.</p>\n<p>This means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? â˜ºï¸</p>\n<p>Now that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:</p>\n<pre><code>$ aim up\n</code></pre>\n<p>to open the user interface and explore our metrics and other information.</p>\n<p>For further reading please referee toÂ <a href=\"https://aimstack.readthedocs.io/en/latest/\">Aim documentation</a>Â where you will learn more about the superpowers of Aim.</p>\n<h1>Conclusion</h1>\n<p>To sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.</p>\n<p>In this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.</p>\n<p>The added value of Aim makes installing aimlflow and enabling the additional capability well worth it.</p>\n<h1>Learn more</h1>\n<p>If you have any questions joinÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. ðŸ™Œ</p>\n<p>Show some love by dropping a â­ï¸ onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you think Aim is useful.</p>"},"_id":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","_raw":{"sourceFilePath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileName":"how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\n\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\n\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\n\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.Â One thing you do not need when tuning hyper-parameters is variance.\n\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\n\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results usingÂ [Aim](https://github.com/aimhubio/aim).\n\n## **How to fix the seed in PyTorch Lightning**\n\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\n\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we useÂ `pl.seed_everything(seed)`Â . See the docsÂ [here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\n\n> Note: in other libraries you would use something like:Â `np.random.seed()`Â orÂ `torch.manual_seed()`Â \n\n## **Implementation**\n\nFind the full code for this and other tutorialsÂ [here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\n\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n```\n\n## Analyzing the Training Runs\n\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard afterÂ [Aim 1.3.5 release](https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)) and have multiple charts of different metrics on the same screen.\n\nDo the following steps to see the different effects of the optimizers\n\n* Go to dashboard, explore by experiment\n* Add loss toÂ `SELECT`Â and divide into subplots by metric\n* Group by experiment to make all metrics of similar color\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\n\nHere is how it looks on Aim:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png)\n\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\n\nIf you apply the same settings to the learning rate, this is the result:\n\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\n\n* Remove both previous groupings\n* Group by color by learning rate\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\n\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\n\nOn top of this, if we also add grouping by style by optimizer:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png)\n\nNow, it is obvious that the the runs with SGD optimizer andÂ `lr=0.01`Â (green, broken lines) are the best choices for all the seeds we have tried.\n\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\n\n## Learn More\n\nIf you find Aim useful, support us andÂ [star the project](https://github.com/aimhubio/aim)Â on GitHub. Join theÂ [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect)Â and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is a random seed and how is it important?</h2>\n<p>The random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:</p>\n<ul>\n<li>Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.</li>\n<li>Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.</li>\n<li>Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).</li>\n</ul>\n<p>As you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.Â One thing you do not need when tuning hyper-parameters is variance.</p>\n<p>The purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.</p>\n<p>Later in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results usingÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>.</p>\n<h2><strong>How to fix the seed in PyTorch Lightning</strong></h2>\n<p>Fixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python<code>random.seed(seed)</code>, but it is not enough for PL.</p>\n<p>Pytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we useÂ <code>pl.seed_everything(seed)</code>Â . See the docsÂ <a href=\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\">here</a>.</p>\n<blockquote>\n<p>Note: in other libraries you would use something like:Â <code>np.random.seed()</code>Â orÂ <code>torch.manual_seed()</code>Â </p>\n</blockquote>\n<h2><strong>Implementation</strong></h2>\n<p>Find the full code for this and other tutorialsÂ <a href=\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\">here</a>.</p>\n<pre><code>import torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n</code></pre>\n<h2>Analyzing the Training Runs</h2>\n<p>After each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard afterÂ <a href=\"https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\">Aim 1.3.5 release</a>) and have multiple charts of different metrics on the same screen.</p>\n<p>Do the following steps to see the different effects of the optimizers</p>\n<ul>\n<li>Go to dashboard, explore by experiment</li>\n<li>Add loss toÂ <code>SELECT</code>Â and divide into subplots by metric</li>\n<li>Group by experiment to make all metrics of similar color</li>\n<li>Group by style by optimizer to see different optimizers on loss and accuracy and its effects</li>\n</ul>\n<p>Here is how it looks on Aim:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png\" alt=\"\"></p>\n<p>From the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.</p>\n<p>If you apply the same settings to the learning rate, this is the result:</p>\n<h2>For the next step to analyze how learning rate affects the experiments, do the following steps:</h2>\n<ul>\n<li>Remove both previous groupings</li>\n<li>Group by color by learning rate</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"></p>\n<p>As you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.</p>\n<p>We showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.</p>\n<p>On top of this, if we also add grouping by style by optimizer:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png\" alt=\"\"></p>\n<p>Now, it is obvious that the the runs with SGD optimizer andÂ <code>lr=0.01</code>Â (green, broken lines) are the best choices for all the seeds we have tried.</p>\n<p>Fixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.</p>\n<h2>Learn More</h2>\n<p>If you find Aim useful, support us andÂ <a href=\"https://github.com/aimhubio/aim\">star the project</a>Â on GitHub. Join theÂ <a href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\">Aim community</a>Â and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"},{"title":"Hugging the Chaos: Connecting Datasets to Trainings with Hugging Face and Aim","date":"2023-02-17T07:05:28.709Z","author":"Gor Arakelyan","description":"Combining Hugging Face and Aim to make machine learning experiments traceable, reproducible and easier to compare.","slug":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-luoNtZBpA1SGkTsHzCrkA.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"# The cost of neglecting experiments management\n\n\n\nWorking with large and frequently changing datasets is hard!\\\nYou can easily end up in a mess if you donâ€™t have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.\n\nIn this article, we will explore how you can combineÂ [Hugging Face Datasets](https://github.com/huggingface/datasets)Â andÂ [Aim](https://github.com/aimhubio/aim)Â to make machine learning experiments traceable, reproducible and easier to compare.\n\nLetâ€™s dive in and get started!\n\n# Hugging Face Datasets + Aim = â¤\n\n[Hugging Face Datasets](https://github.com/huggingface/datasets)Â is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, youâ€™ll never have to worry about manually loading and versioning your data again.\n\n[Aim](https://github.com/aimhubio/aim), on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png)\n\nHugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.\n\n# Project overview\n\nLetâ€™s go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.\n\nHereâ€™s what weâ€™ll be using:\n\n* [Hugging Face Datasets](https://github.com/huggingface/datasets)Â to load and manage the dataset.\n* [Hugging Face Hub](http://huggingface.co/)Â to host the dataset.\n* [PyTorch](https://github.com/pytorch/pytorch)Â to build and train the model.\n* [Aim](https://github.com/aimhubio/aim)Â to keep track of all the model and dataset metadata.\n\nOur dataset is going to be called â€œA-MNISTâ€ â€” a version of the â€œMNISTâ€ dataset with extra samples added. Weâ€™ll start with the original â€œMNISTâ€ dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.\n\n# Dataset preparation\n\n## Uploading the dataset to Hub\n\n\n\nLetâ€™s head over to Hugging Face Hub and create a new dataset repository calledÂ [â€œA-MNISTâ€](https://huggingface.co/datasets/gorar/A-MNIST)Â to store our dataset.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png)\n\nCurrently, the repository is empty, so letâ€™s upload the initial version of the dataset.\n\nWeâ€™ll use the original MNIST dataset, as the first version v1.0.0. To do this, weâ€™ll need to upload the dataset files along with the dataset loading scriptÂ `A-MNIST.py`. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:\n\n```\nclass AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n```\n\n*See the full script here:Â <https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py>*\n\nWeâ€™ll have the following setup, including all the necessary git configurations and the dataset card:\n\n```\n- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n```\n\nLetâ€™s commit the changes and push the dataset to the Hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png)\n\nAwesome! Now weâ€™ve got the first version of â€œA-MNISTâ€ hosted on Hugging Face Hub!\n\n## Augmenting the dataset\n\n\n\nNext up, letâ€™s add more images to the MNIST dataset using augmentation techniques.\n\nWe will rotate all the train images by 20 degree and append to the dataset:\n\n```\nrotated_images[i] = rotate(images[i], angle=20, reshape=False)\n```\n\nAs well as, update the â€œA-MNISTâ€ dataset version to 1.1.0:\n\n```\nBUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n```\n\nAfter preparing the new train dataset, letâ€™s commit the changes and push the upgraded version to the hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png)\n\nPerfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. ðŸŽ‰\n\n# Training setup\n\nLetâ€™s load the dataset using datasets python package. Itâ€™s really simple, just one line of code:\n\n```\ndataset = load_dataset(\"gorar/A-MNIST\")\n```\n\nOne of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:\n\n```\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n```\n\nWeâ€™re ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:\n\n```\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n```\n\n*See the full code here:Â <https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8>*\n\nWeâ€™ve got everything ready now. Letâ€™s kick off the training by running:\n\n```\npython train.py\n```\n\nHurray, the training is underway! ðŸƒ\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png)\n\nIt would be tough to monitor my training progress on terminal. This is where Aimâ€™s superpowers come into play.\n\n# Integrating Aim\n\n\n\n## Trainings tracking\n\n\n\nAim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:\n\n```\nimport aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n```\n\nFurthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.\n\nSince v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply importÂ `HFDataset`Â module from Aim and you can track all of your dataset metadata with just a single line of code!\n\n```\nfrom aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n```\n\nAim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the datasetâ€™s description, version, features, etc.\n\n# Experimentation\n\n## Conducting trainings\n\n\n\nWeâ€™ll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, weâ€™ll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.\n\nDatasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:\n\n```\n# Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n```\n\nWe will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.\n\n## Exploring training results via Aim\n\n\n\nNow that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aimâ€™s UI, we need to run theÂ `aim up`Â command. A following message would be printed on the terminal output meaning the UI is successfully running:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png)\n\nTo access the UI, letâ€™s navigate toÂ `127.0.0.1:43800`Â in the browser.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png)\n\nWe can see our trainings on the contributions map and in the activity feed. Letâ€™s take a closer look at an individual run details by navigating to the runâ€™s page on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png)\n\nAll of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.\n\nWe can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png)\n\nThe UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.\n\n## Comparing trainings via Aim\n\nLetâ€™s find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics ExplorerÂ `127.0.0.1:43800/metrics`.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png)\n\nAim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.\n\nLetâ€™s group byÂ `run.dataset.dataset.meta.version`Â to see how the models performed based on the dataset version they were trained on.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png)\n\n\\\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.\n\nNow, to improve visibility and better evaluate performance, letâ€™s go ahead and smooth out these lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png)\n\nIt appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, letâ€™s group metrics by the learning rate to learn its impact on models performance.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png)\n\nIt seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png)\n\nTo sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.\n\nWith just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! ðŸŽ‰\n\n# Reproducibility\n\nHave you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.\n\nAim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!\n\n# Conclusion\n\nIn this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.\n\nWe compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.\n\nSimplifying our daily work with efficient stack helps us focus on what really matters â€” getting the best results from our machine learning experiments.\n\n# Learn more\n\n\n\nCheck out the Aim + Datasets integration docsÂ [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\\\nDatasets repo:Â <https://github.com/huggingface/datasets>\\\nAim repo:Â <https://github.com/aimhubio/aim>\n\nIf you have questions, join theÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. Youâ€™re most welcome! ðŸ™Œ\n\nDrop a â­ï¸ onÂ [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<h1>The cost of neglecting experiments management</h1>\n<p>Working with large and frequently changing datasets is hard!<br>\nYou can easily end up in a mess if you donâ€™t have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.</p>\n<p>In this article, we will explore how you can combineÂ <a href=\"https://github.com/huggingface/datasets\">Hugging Face Datasets</a>Â andÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>Â to make machine learning experiments traceable, reproducible and easier to compare.</p>\n<p>Letâ€™s dive in and get started!</p>\n<h1>Hugging Face Datasets + Aim = â¤</h1>\n<p><a href=\"https://github.com/huggingface/datasets\">Hugging Face Datasets</a>Â is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, youâ€™ll never have to worry about manually loading and versioning your data again.</p>\n<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>, on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png\" alt=\"\"></p>\n<p>Hugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.</p>\n<h1>Project overview</h1>\n<p>Letâ€™s go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.</p>\n<p>Hereâ€™s what weâ€™ll be using:</p>\n<ul>\n<li><a href=\"https://github.com/huggingface/datasets\">Hugging Face Datasets</a>Â to load and manage the dataset.</li>\n<li><a href=\"http://huggingface.co/\">Hugging Face Hub</a>Â to host the dataset.</li>\n<li><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>Â to build and train the model.</li>\n<li><a href=\"https://github.com/aimhubio/aim\">Aim</a>Â to keep track of all the model and dataset metadata.</li>\n</ul>\n<p>Our dataset is going to be called â€œA-MNISTâ€ â€” a version of the â€œMNISTâ€ dataset with extra samples added. Weâ€™ll start with the original â€œMNISTâ€ dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.</p>\n<h1>Dataset preparation</h1>\n<h2>Uploading the dataset to Hub</h2>\n<p>Letâ€™s head over to Hugging Face Hub and create a new dataset repository calledÂ <a href=\"https://huggingface.co/datasets/gorar/A-MNIST\">â€œA-MNISTâ€</a>Â to store our dataset.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png\" alt=\"\"></p>\n<p>Currently, the repository is empty, so letâ€™s upload the initial version of the dataset.</p>\n<p>Weâ€™ll use the original MNIST dataset, as the first version v1.0.0. To do this, weâ€™ll need to upload the dataset files along with the dataset loading scriptÂ <code>A-MNIST.py</code>. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:</p>\n<pre><code>class AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n</code></pre>\n<p><em>See the full script here:Â <a href=\"https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\">https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py</a></em></p>\n<p>Weâ€™ll have the following setup, including all the necessary git configurations and the dataset card:</p>\n<pre><code>- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n</code></pre>\n<p>Letâ€™s commit the changes and push the dataset to the Hub.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png\" alt=\"\"></p>\n<p>Awesome! Now weâ€™ve got the first version of â€œA-MNISTâ€ hosted on Hugging Face Hub!</p>\n<h2>Augmenting the dataset</h2>\n<p>Next up, letâ€™s add more images to the MNIST dataset using augmentation techniques.</p>\n<p>We will rotate all the train images by 20 degree and append to the dataset:</p>\n<pre><code>rotated_images[i] = rotate(images[i], angle=20, reshape=False)\n</code></pre>\n<p>As well as, update the â€œA-MNISTâ€ dataset version to 1.1.0:</p>\n<pre><code>BUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n</code></pre>\n<p>After preparing the new train dataset, letâ€™s commit the changes and push the upgraded version to the hub.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png\" alt=\"\"></p>\n<p>Perfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. ðŸŽ‰</p>\n<h1>Training setup</h1>\n<p>Letâ€™s load the dataset using datasets python package. Itâ€™s really simple, just one line of code:</p>\n<pre><code>dataset = load_dataset(\"gorar/A-MNIST\")\n</code></pre>\n<p>One of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:</p>\n<pre><code>from datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n</code></pre>\n<p>Weâ€™re ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:</p>\n<pre><code>class ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n</code></pre>\n<p><em>See the full code here:Â <a href=\"https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\">https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8</a></em></p>\n<p>Weâ€™ve got everything ready now. Letâ€™s kick off the training by running:</p>\n<pre><code>python train.py\n</code></pre>\n<p>Hurray, the training is underway! ðŸƒ</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png\" alt=\"\"></p>\n<p>It would be tough to monitor my training progress on terminal. This is where Aimâ€™s superpowers come into play.</p>\n<h1>Integrating Aim</h1>\n<h2>Trainings tracking</h2>\n<p>Aim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:</p>\n<pre><code>import aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n</code></pre>\n<p>Furthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.</p>\n<p>Since v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply importÂ <code>HFDataset</code>Â module from Aim and you can track all of your dataset metadata with just a single line of code!</p>\n<pre><code>from aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n</code></pre>\n<p>Aim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the datasetâ€™s description, version, features, etc.</p>\n<h1>Experimentation</h1>\n<h2>Conducting trainings</h2>\n<p>Weâ€™ll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, weâ€™ll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.</p>\n<p>Datasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:</p>\n<pre><code># Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n</code></pre>\n<p>We will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.</p>\n<h2>Exploring training results via Aim</h2>\n<p>Now that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aimâ€™s UI, we need to run theÂ <code>aim up</code>Â command. A following message would be printed on the terminal output meaning the UI is successfully running:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png\" alt=\"\"></p>\n<p>To access the UI, letâ€™s navigate toÂ <code>127.0.0.1:43800</code>Â in the browser.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png\" alt=\"\"></p>\n<p>We can see our trainings on the contributions map and in the activity feed. Letâ€™s take a closer look at an individual run details by navigating to the runâ€™s page on the UI.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png\" alt=\"\"></p>\n<p>All of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.</p>\n<p>We can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png\" alt=\"\"></p>\n<p>The UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.</p>\n<h2>Comparing trainings via Aim</h2>\n<p>Letâ€™s find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics ExplorerÂ <code>127.0.0.1:43800/metrics</code>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png\" alt=\"\"></p>\n<p>Aim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.</p>\n<p>Letâ€™s group byÂ <code>run.dataset.dataset.meta.version</code>Â to see how the models performed based on the dataset version they were trained on.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png\" alt=\"\"></p>\n<p><br>\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.</p>\n<p>Now, to improve visibility and better evaluate performance, letâ€™s go ahead and smooth out these lines.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png\" alt=\"\"></p>\n<p>It appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, letâ€™s group metrics by the learning rate to learn its impact on models performance.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png\" alt=\"\"></p>\n<p>It seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png\" alt=\"\"></p>\n<p>To sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.</p>\n<p>With just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! ðŸŽ‰</p>\n<h1>Reproducibility</h1>\n<p>Have you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.</p>\n<p>Aim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!</p>\n<h1>Conclusion</h1>\n<p>In this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.</p>\n<p>We compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.</p>\n<p>Simplifying our daily work with efficient stack helps us focus on what really matters â€” getting the best results from our machine learning experiments.</p>\n<h1>Learn more</h1>\n<p>Check out the Aim + Datasets integration docsÂ <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\">here</a>.<br>\nDatasets repo:Â <a href=\"https://github.com/huggingface/datasets\">https://github.com/huggingface/datasets</a><br>\nAim repo:Â <a href=\"https://github.com/aimhubio/aim\">https://github.com/aimhubio/aim</a></p>\n<p>If you have questions, join theÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. Youâ€™re most welcome! ðŸ™Œ</p>\n<p>Drop a â­ï¸ onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","_raw":{"sourceFilePath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileName":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim"},"type":"Post"},{"title":"LangChain + Aim: Building and Debugging AI Systems Made EASY!","date":"2023-04-06T17:51:14.014Z","author":"Gor Arakelyan","description":"As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial. Use Aim to easily trace complex AI systems built with LangChain.","slug":"langchain-aim-building-and-debugging-ai-systems-made-easy","image":"/images/dynamic/langchain_header.jpg","draft":false,"categories":["Integrations"],"body":{"raw":"\n\n# The Rise of Complex AI Systems\n\nWith the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.\n\nBut the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.\n\n**As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.** Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.\n\nIn this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:\n\n* track all inputs and outputs of chains,\n* visualize and explore individual chains,\n* compare several chains side-by-side.\n\n# LangChain: Building AI Systems with LLMs\n\nLangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.\n\nIt provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!\n\n# **Aim:** Upgraded Debugging Experience for AI Systems\n\nMonitoring and debugging AI systems requires more than just scanning output logs on a terminal.\n\n**Introducing Aim!**\n\nAim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.\n\nImportantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.\n\nIn that way, Aim makes debugging, monitoring, comparing different executions a breeze.\n\n**Experience the ultimate control with Aim!**\n\nCheck out Aim on GitHub: **[github.com/aimhubio/aim](http://github.com/aimhubio/aim)**\n\n![](/images/dynamic/explorer.jpg)\n\n# Aim + LangChain = ðŸš€\n\nWith the release of LangChain **[v0.0.127](https://github.com/hwchase17/langchain/releases/tag/v0.0.127)**, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! **All you need to do is configure the Aim callback and run your executions as usual.**\n\nAim does the rest for you by tracking tools and LLMsâ€™ inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.\n\n![](/images/dynamic/langchain_aim.jpg)\n\nLet's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.\n\n# Hands-On Example: Building a Multi-Task AI Agent\n\n## Setting up the agent and the Aim callback\n\nLetâ€™s build an agent equipped with two tools:\n\n* the SerpApi tool to access Google search results,\n* the LLM-math tool to perform required mathematical operations.\n\nIn this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:\n\n```python\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n```\n\nNow that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.\n\n```python\nfrom langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n```\n\n> **Aim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.**\n\nFind the full script and more examples in the official LangChain docs: [](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html)**<https://python.langchain.com/en/latest/ecosystem/aim_tracking.html>**\n\n## **Executing the agent and running Aim**\n\nBefore executing the agent, ensure that Aim is installed by executing the following command:\n\n```shell\npip install aim\n```\n\nNow, let's run multiple executions and launch the Aim UI to visualize and explore the results:\n\n1. execute the script by running `python example.py`,\n2. then, start the UI with `aim up` command.\n\nWith the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.\n\n## Exploring executions via Aim\n\n### Home page\n\nOn the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.\n\n![](/images/dynamic/home.jpg)\n\n\n\n### Deep dive into a single execution\n\nWhen navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:\n\n* CLI command and arguments,\n* Environment variables,\n* Packages,\n* Git information,\n* System resource usage,\n* and other relevant information about an individual execution.\n\n![](/images/dynamic/individual_exec.jpg)\n\nAim automatically captures terminal outputs during execution. Access these logs in the â€œLogsâ€ tab to easily keep track of the progress of your AI system and identify issues.\n\n![](/images/dynamic/logs.jpg)\n\nIn the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.\n\n![](/images/dynamic/text_tab.jpg)\n\nWith Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.\n\n![](/images/dynamic/explorer.jpg)\n\nFor instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.\n\n**With Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.**\n\n# Wrapping Up\n\nIn conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!\n\n# Learn more\n\nCheck out the Aim + LangChain integration docsÂ [here](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html).\n\nLangChain repo:Â [](https://github.com/hwchase17/langchain)<https://github.com/hwchase17/langchain>\n\nAim repo:Â [](https://github.com/aimhubio/aim)<https://github.com/aimhubio/aim>\n\nIf you have questions, join theÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. Youâ€™re most welcome! ðŸ™Œ\n\nDrop a â­ï¸ onÂ [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<h1>The Rise of Complex AI Systems</h1>\n<p>With the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.</p>\n<p>But the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.</p>\n<p><strong>As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.</strong> Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.</p>\n<p>In this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:</p>\n<ul>\n<li>track all inputs and outputs of chains,</li>\n<li>visualize and explore individual chains,</li>\n<li>compare several chains side-by-side.</li>\n</ul>\n<h1>LangChain: Building AI Systems with LLMs</h1>\n<p>LangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.</p>\n<p>It provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!</p>\n<h1><strong>Aim:</strong> Upgraded Debugging Experience for AI Systems</h1>\n<p>Monitoring and debugging AI systems requires more than just scanning output logs on a terminal.</p>\n<p><strong>Introducing Aim!</strong></p>\n<p>Aim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.</p>\n<p>Importantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.</p>\n<p>In that way, Aim makes debugging, monitoring, comparing different executions a breeze.</p>\n<p><strong>Experience the ultimate control with Aim!</strong></p>\n<p>Check out Aim on GitHub: <strong><a href=\"http://github.com/aimhubio/aim\">github.com/aimhubio/aim</a></strong></p>\n<p><img src=\"/images/dynamic/explorer.jpg\" alt=\"\"></p>\n<h1>Aim + LangChain = ðŸš€</h1>\n<p>With the release of LangChain <strong><a href=\"https://github.com/hwchase17/langchain/releases/tag/v0.0.127\">v0.0.127</a></strong>, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! <strong>All you need to do is configure the Aim callback and run your executions as usual.</strong></p>\n<p>Aim does the rest for you by tracking tools and LLMsâ€™ inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.</p>\n<p><img src=\"/images/dynamic/langchain_aim.jpg\" alt=\"\"></p>\n<p>Let's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.</p>\n<h1>Hands-On Example: Building a Multi-Task AI Agent</h1>\n<h2>Setting up the agent and the Aim callback</h2>\n<p>Letâ€™s build an agent equipped with two tools:</p>\n<ul>\n<li>the SerpApi tool to access Google search results,</li>\n<li>the LLM-math tool to perform required mathematical operations.</li>\n</ul>\n<p>In this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:</p>\n<pre><code class=\"language-python\">tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n</code></pre>\n<p>Now that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.</p>\n<pre><code class=\"language-python\">from langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n</code></pre>\n<blockquote>\n<p><strong>Aim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.</strong></p>\n</blockquote>\n<p>Find the full script and more examples in the official LangChain docs: <a href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"></a><strong><a href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\">https://python.langchain.com/en/latest/ecosystem/aim_tracking.html</a></strong></p>\n<h2><strong>Executing the agent and running Aim</strong></h2>\n<p>Before executing the agent, ensure that Aim is installed by executing the following command:</p>\n<pre><code class=\"language-shell\">pip install aim\n</code></pre>\n<p>Now, let's run multiple executions and launch the Aim UI to visualize and explore the results:</p>\n<ol>\n<li>execute the script by running <code>python example.py</code>,</li>\n<li>then, start the UI with <code>aim up</code> command.</li>\n</ol>\n<p>With the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.</p>\n<h2>Exploring executions via Aim</h2>\n<h3>Home page</h3>\n<p>On the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.</p>\n<p><img src=\"/images/dynamic/home.jpg\" alt=\"\"></p>\n<h3>Deep dive into a single execution</h3>\n<p>When navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:</p>\n<ul>\n<li>CLI command and arguments,</li>\n<li>Environment variables,</li>\n<li>Packages,</li>\n<li>Git information,</li>\n<li>System resource usage,</li>\n<li>and other relevant information about an individual execution.</li>\n</ul>\n<p><img src=\"/images/dynamic/individual_exec.jpg\" alt=\"\"></p>\n<p>Aim automatically captures terminal outputs during execution. Access these logs in the â€œLogsâ€ tab to easily keep track of the progress of your AI system and identify issues.</p>\n<p><img src=\"/images/dynamic/logs.jpg\" alt=\"\"></p>\n<p>In the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.</p>\n<p><img src=\"/images/dynamic/text_tab.jpg\" alt=\"\"></p>\n<p>With Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.</p>\n<p><img src=\"/images/dynamic/explorer.jpg\" alt=\"\"></p>\n<p>For instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.</p>\n<p><strong>With Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.</strong></p>\n<h1>Wrapping Up</h1>\n<p>In conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!</p>\n<h1>Learn more</h1>\n<p>Check out the Aim + LangChain integration docsÂ <a href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\">here</a>.</p>\n<p>LangChain repo:Â <a href=\"https://github.com/hwchase17/langchain\"></a><a href=\"https://github.com/hwchase17/langchain\">https://github.com/hwchase17/langchain</a></p>\n<p>Aim repo:Â <a href=\"https://github.com/aimhubio/aim\"></a><a href=\"https://github.com/aimhubio/aim\">https://github.com/aimhubio/aim</a></p>\n<p>If you have questions, join theÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. Youâ€™re most welcome! ðŸ™Œ</p>\n<p>Drop a â­ï¸ onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","_raw":{"sourceFilePath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileName":"langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy"},"type":"Post"},{"title":"Launching Aim on Hugging Face Spaces","date":"2023-04-19T13:41:34.213Z","author":"Gor Arakelyan","description":"Deploy Aim on Hugging Face Spaces within seconds using the Docker template!","slug":"launching-aim-on-hugging-face-spaces","image":"/images/dynamic/hf_space_header.png","draft":false,"categories":["Integrations"],"body":{"raw":"We are excited to announce the launch of Aim on Hugging Face Spaces! ðŸš€\n\nWith just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.\n\n![](/images/dynamic/hf_space_me.png)\n\nAim is an open-source, self-hosted AI Metadata tracking tool.\\\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata â€” perfect for automations and Jupyter Notebook analysis.\n\nIn this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Letâ€™s dive in and get started!\n\nLearn more about Aim on the GitHub repository: **[github.com/aimhubio/aim](https://github.com/aimhubio/aim)**\n\n## Deploy Aim on Hugging Face Spaces within seconds using the Docker template\n\nTo get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the â€œCreate new Spaceâ€ button, or open the page directly by the following link: [](https://huggingface.co/new-space?template=aimstack/aim)**<https://huggingface.co/new-space?template=aimstack/aim>**\n\n![](/images/dynamic/hf_space_deploy.png)\n\nSet up your Aim Space in no time:\n\n1. Choose a name for your Space.\n2. Adjust Space hardware and the visibility mode.\n3. Submit your Space!\n\nAfter submitting the Space, you'll be able to monitor its progress through the building status:\n\n![](/images/dynamic/hf_space_building.png)\n\nOnce it transitions to â€œRunningâ€, your space is ready to go!\n\n![](/images/dynamic/hf_space_running.png)\n\n**Ta-da! ðŸŽ‰ You're all set to start using Aim on Hugging Face.**\n\nBy pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:\n\n1. Run the following bash command to compress `.aim` directory:\n\n   ```shell\n   tar -czvf aim_repo.tar.gz .aim\n   ```\n2. Commit and push files to your Space.\n\nThat's it! Now open the App section of your Space, and Aim will display your training logs.\n\nUpdating Spaces is incredibly convenient â€“ you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. ðŸ”¥\n\n## See Aim in Action with Existing Demos on the Hub\n\nLetâ€™s explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.\n\n* Neural machine translation task: [](https://huggingface.co/spaces/aimstack/nmt)<https://huggingface.co/spaces/aimstack/nmt>\n* Simple handwritten digits recognition task: [](https://huggingface.co/spaces/aimstack/digit-recognition)<https://huggingface.co/spaces/aimstack/digit-recognition>\n* Image generation task with lightweight GAN implementation: [](https://huggingface.co/spaces/aimstack/image-generation)<https://huggingface.co/spaces/aimstack/image-generation>\n\n![](/images/dynamic/hf_space_demos.png)\n\nWhen navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.\n\n![Aim homepage](/images/dynamic/hf_space_overview.png \"Aim homepage\")\n\nOpen the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.\n\n![Individual run page](/images/dynamic/hf_space_individual_run.png \"Individual run page\")\n\nTake your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.\n\nMetrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.\n\n![Metrics Explorer](/images/dynamic/hf_space_me.png \"Metrics Explorer\")\n\nExplorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.\n\n![Pythonic search](/images/dynamic/hf_space_pythonic_search.png \"Pythonic search\")\n\nIn addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.\n\n![Images Explorer](/images/dynamic/hf_space_media.png \"Images Explorer\")\n\nUse Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!\n\n## One more thingâ€¦ ðŸ‘€\n\nHaving Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.\n\nTo embed your Space, construct the following link based on Space owner and Space name: **`https://owner-space-name.hf.space`**. This link can be used to embed your Space in any website or notebook using the following HTML code:\n\n```html\n%%html\n<iframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n>\n</iframe>\n```\n\n## Next steps\n\nWe are going to continuously iterate over Aim Space onboarding and usability, including:\n\n* the ability to read logs directly from Hugging Face Hub model repos,\n* automatic conversion of TensorBoard logs to Aim format,\n* Aim HF Space-specific onboarding steps.\n\nMuch more coming soon... stay tuned for the updates!\n\n## Learn more\n\nCheck out Aim Space documentation [here](https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html)\n\nAim repo on GitHub: [github.com/aimhubio/aim](http://github.com/aimhubio/aim)\n\nIf you have questions, join theÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. Youâ€™re most welcome! ðŸ™Œ\n\nDrop a â­ï¸ onÂ [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<p>We are excited to announce the launch of Aim on Hugging Face Spaces! ðŸš€</p>\n<p>With just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.</p>\n<p><img src=\"/images/dynamic/hf_space_me.png\" alt=\"\"></p>\n<p>Aim is an open-source, self-hosted AI Metadata tracking tool.<br>\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata â€” perfect for automations and Jupyter Notebook analysis.</p>\n<p>In this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Letâ€™s dive in and get started!</p>\n<p>Learn more about Aim on the GitHub repository: <strong><a href=\"https://github.com/aimhubio/aim\">github.com/aimhubio/aim</a></strong></p>\n<h2>Deploy Aim on Hugging Face Spaces within seconds using the Docker template</h2>\n<p>To get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the â€œCreate new Spaceâ€ button, or open the page directly by the following link: <a href=\"https://huggingface.co/new-space?template=aimstack/aim\"></a><strong><a href=\"https://huggingface.co/new-space?template=aimstack/aim\">https://huggingface.co/new-space?template=aimstack/aim</a></strong></p>\n<p><img src=\"/images/dynamic/hf_space_deploy.png\" alt=\"\"></p>\n<p>Set up your Aim Space in no time:</p>\n<ol>\n<li>Choose a name for your Space.</li>\n<li>Adjust Space hardware and the visibility mode.</li>\n<li>Submit your Space!</li>\n</ol>\n<p>After submitting the Space, you'll be able to monitor its progress through the building status:</p>\n<p><img src=\"/images/dynamic/hf_space_building.png\" alt=\"\"></p>\n<p>Once it transitions to â€œRunningâ€, your space is ready to go!</p>\n<p><img src=\"/images/dynamic/hf_space_running.png\" alt=\"\"></p>\n<p><strong>Ta-da! ðŸŽ‰ You're all set to start using Aim on Hugging Face.</strong></p>\n<p>By pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:</p>\n<ol>\n<li>\n<p>Run the following bash command to compress <code>.aim</code> directory:</p>\n<pre><code class=\"language-shell\">tar -czvf aim_repo.tar.gz .aim\n</code></pre>\n</li>\n<li>\n<p>Commit and push files to your Space.</p>\n</li>\n</ol>\n<p>That's it! Now open the App section of your Space, and Aim will display your training logs.</p>\n<p>Updating Spaces is incredibly convenient â€“ you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. ðŸ”¥</p>\n<h2>See Aim in Action with Existing Demos on the Hub</h2>\n<p>Letâ€™s explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.</p>\n<ul>\n<li>Neural machine translation task: <a href=\"https://huggingface.co/spaces/aimstack/nmt\"></a><a href=\"https://huggingface.co/spaces/aimstack/nmt\">https://huggingface.co/spaces/aimstack/nmt</a></li>\n<li>Simple handwritten digits recognition task: <a href=\"https://huggingface.co/spaces/aimstack/digit-recognition\"></a><a href=\"https://huggingface.co/spaces/aimstack/digit-recognition\">https://huggingface.co/spaces/aimstack/digit-recognition</a></li>\n<li>Image generation task with lightweight GAN implementation: <a href=\"https://huggingface.co/spaces/aimstack/image-generation\"></a><a href=\"https://huggingface.co/spaces/aimstack/image-generation\">https://huggingface.co/spaces/aimstack/image-generation</a></li>\n</ul>\n<p><img src=\"/images/dynamic/hf_space_demos.png\" alt=\"\"></p>\n<p>When navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.</p>\n<p><img src=\"/images/dynamic/hf_space_overview.png\" alt=\"Aim homepage\" title=\"Aim homepage\"></p>\n<p>Open the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.</p>\n<p><img src=\"/images/dynamic/hf_space_individual_run.png\" alt=\"Individual run page\" title=\"Individual run page\"></p>\n<p>Take your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.</p>\n<p>Metrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.</p>\n<p><img src=\"/images/dynamic/hf_space_me.png\" alt=\"Metrics Explorer\" title=\"Metrics Explorer\"></p>\n<p>Explorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.</p>\n<p><img src=\"/images/dynamic/hf_space_pythonic_search.png\" alt=\"Pythonic search\" title=\"Pythonic search\"></p>\n<p>In addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.</p>\n<p><img src=\"/images/dynamic/hf_space_media.png\" alt=\"Images Explorer\" title=\"Images Explorer\"></p>\n<p>Use Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!</p>\n<h2>One more thingâ€¦ ðŸ‘€</h2>\n<p>Having Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.</p>\n<p>To embed your Space, construct the following link based on Space owner and Space name: <strong><code>https://owner-space-name.hf.space</code></strong>. This link can be used to embed your Space in any website or notebook using the following HTML code:</p>\n<pre><code class=\"language-html\">%%html\n&#x3C;iframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n>\n&#x3C;/iframe>\n</code></pre>\n<h2>Next steps</h2>\n<p>We are going to continuously iterate over Aim Space onboarding and usability, including:</p>\n<ul>\n<li>the ability to read logs directly from Hugging Face Hub model repos,</li>\n<li>automatic conversion of TensorBoard logs to Aim format,</li>\n<li>Aim HF Space-specific onboarding steps.</li>\n</ul>\n<p>Much more coming soon... stay tuned for the updates!</p>\n<h2>Learn more</h2>\n<p>Check out Aim Space documentation <a href=\"https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html\">here</a></p>\n<p>Aim repo on GitHub: <a href=\"http://github.com/aimhubio/aim\">github.com/aimhubio/aim</a></p>\n<p>If you have questions, join theÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. Youâ€™re most welcome! ðŸ™Œ</p>\n<p>Drop a â­ï¸ onÂ <a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/launching-aim-on-hugging-face-spaces.md","_raw":{"sourceFilePath":"posts/launching-aim-on-hugging-face-spaces.md","sourceFileName":"launching-aim-on-hugging-face-spaces.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/launching-aim-on-hugging-face-spaces"},"type":"Post"},{"title":"Reproducible forecasting with Prophet and Aim","date":"2023-03-14T07:19:42.802Z","author":"Davit Grigoryan","description":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. ","slug":"reproducible-forecasting-with-prophet-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaIDQvLbqy0KyVG22D0epw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.\n\n# What is Aim?\n\n[Aim](https://aimstack.io/)Â is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n# What is Prophet?\n\n[Prophet](https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/)Â is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.\n\nA key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.\n\n# Tracking Prophet experiments with Aim\n\nProphet isnâ€™t trained like neural networks, so you canâ€™t track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost weâ€™re going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check outÂ [the repo](https://github.com/aimhubio/aim/tree/main/examples).\n\nIn the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.\n\n```\nfrom aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n```\n\nAdditionally, if youâ€™re working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Hereâ€™s a simple code snippet doing just that:\n\n```\n# Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n```\n\nNow, the extra feature(s) will be tracked as a hyperparameter calledÂ **extra_regressors**.\n\nTake a look at a simple, end-to-end exampleÂ [here](https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py).\n\n# Viewing experiment logs\n\nAfter running the experiment, we can view the logs by executingÂ `aim up`Â from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png)\n\nAdditionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif)\n\nThese features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.\n\n# Using Aimâ€™s Pythonic search to filter metrics and hyperparameters\n\n\n\nGiven the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if weâ€™re working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aimâ€™sÂ *pythonic search*Â functionality can be super useful. Say we only want to see the models with MAE â‰¤ 0.25. We can go to the metrics section and search exactly like we would in Python, sayÂ `((metric.name == \"mae\") and (metric.last <= 0.25))`Â (theÂ **last**Â part is meant for neural networks, where you might want to see the metric at the last epoch). Hereâ€™s a visual demonstration of this feature:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png)\n\nAs you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.\n\n# Conclusion\n\n\n\nTo sum up, Aimâ€™s integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with itsÂ *pythonic search*Â functionality. All these features can make forecasting with Prophet a breeze!\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring ðŸ™Œ\n\nTry outÂ [Aim](https://github.com/aimhubio/aim), join theÂ [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs.","html":"<p>You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.</p>\n<h1>What is Aim?</h1>\n<p><a href=\"https://aimstack.io/\">Aim</a>Â is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.</p>\n<h1>What is Prophet?</h1>\n<p><a href=\"https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/\">Prophet</a>Â is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.</p>\n<p>A key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.</p>\n<h1>Tracking Prophet experiments with Aim</h1>\n<p>Prophet isnâ€™t trained like neural networks, so you canâ€™t track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost weâ€™re going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check outÂ <a href=\"https://github.com/aimhubio/aim/tree/main/examples\">the repo</a>.</p>\n<p>In the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.</p>\n<pre><code>from aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n</code></pre>\n<p>Additionally, if youâ€™re working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Hereâ€™s a simple code snippet doing just that:</p>\n<pre><code># Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n</code></pre>\n<p>Now, the extra feature(s) will be tracked as a hyperparameter calledÂ <strong>extra_regressors</strong>.</p>\n<p>Take a look at a simple, end-to-end exampleÂ <a href=\"https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py\">here</a>.</p>\n<h1>Viewing experiment logs</h1>\n<p>After running the experiment, we can view the logs by executingÂ <code>aim up</code>Â from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png\" alt=\"\"></p>\n<p>Additionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif\" alt=\"\"></p>\n<p>These features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.</p>\n<h1>Using Aimâ€™s Pythonic search to filter metrics and hyperparameters</h1>\n<p>Given the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if weâ€™re working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aimâ€™sÂ <em>pythonic search</em>Â functionality can be super useful. Say we only want to see the models with MAE â‰¤ 0.25. We can go to the metrics section and search exactly like we would in Python, sayÂ <code>((metric.name == \"mae\") and (metric.last &#x3C;= 0.25))</code>Â (theÂ <strong>last</strong>Â part is meant for neural networks, where you might want to see the metric at the last epoch). Hereâ€™s a visual demonstration of this feature:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png\" alt=\"\"></p>\n<p>As you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.</p>\n<h1>Conclusion</h1>\n<p>To sum up, Aimâ€™s integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with itsÂ <em>pythonic search</em>Â functionality. All these features can make forecasting with Prophet a breeze!</p>\n<h1>Learn More</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. Itâ€™s humbling and inspiring ðŸ™Œ</p>\n<p>Try outÂ <a href=\"https://github.com/aimhubio/aim\">Aim</a>, join theÂ <a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs.</p>"},"_id":"posts/reproducible-forecasting-with-prophet-and-aim.md","_raw":{"sourceFilePath":"posts/reproducible-forecasting-with-prophet-and-aim.md","sourceFileName":"reproducible-forecasting-with-prophet-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/reproducible-forecasting-with-prophet-and-aim"},"type":"Post"}]},"__N_SSG":true}