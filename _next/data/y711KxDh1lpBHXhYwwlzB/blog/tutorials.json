{"pageProps":{"posts":[{"title":"3D Spleen Segmentation with MONAI and Aim","date":"2022-06-27T20:55:12.121Z","author":"Gev Soghomonian","description":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from http://medicaldecathlon.com/. This is a","slug":"3d-spleen-segmentation-with-monai-and-aim","image":"/images/dynamic/image-7-.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from¬†[](http://medicaldecathlon.com/)<http://medicaldecathlon.com/>.\n\nThis is a longer form of the¬†[3D spleen segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb)¬†on the MONAI tutorials repo.\n\nFor a complete in-depth overview of the code and the methodology please follow¬†[this tutorial](https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb)¬†directly or our very own¬†[Aim from Zero to Hero tutorial](https://aimstack.io/blog/tutorials/aim-from-zero-to-hero).\n\n## ***Tracking the basics***\n\nAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\n\n### ***Transformations Matter***\n\nThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the`Single Run Page`.\n\n![](/images/dynamic/image-10-.png)\n\nYou can further search/filter through the runs based on these transformations using Aims‚Äô very own pythonic search language¬†`AimQL`. An Advanced usage looks like this\n\n![](/images/dynamic/image-11-.png)\n\n### *Lets talk about models and optimizers*\n\nSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic¬†`dict`-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from`AimQL`\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n```\n\n![](/images/dynamic/image-12-.png)\n\n## What about Losses?\n\nMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\n\nFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image-5-.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the¬†[complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html)¬†or simply play around in our¬†[interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&chart=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&select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n## *A picture is worth a thousand words*\n\nIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at¬†`Aim`. You can¬†track images¬†as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\n\n```\naim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n```\n\nAll the grouping/filtering/aggregation functional presented above are also available for Images.\n\n![](/images/dynamic/image-6-.jpeg)\n\n![](/images/dynamic/image-7-.jpeg)\n\n## *A figure is worth a thousand pictures*\n\nWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\n\n```\naim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n```\n\nWithin¬†*`Aim`*¬†you can access all the tracked Figures from within the¬†`Single Run Page`\n\n``\n\n![](/images/dynamic/image-2-.gif)\n\nAnother interesting thing that one can¬†[track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\n## ***The curtain falls***\n\nIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with¬†*`1-2`*¬†lines of code. You can also play around with the completed MONAI integration results with our¬†[interactive demo](http://play.aimstack.io:10005/).\n\nIf you have any bug reports please follow up on our¬†[github repository](https://github.com/aimhubio/aim/issues/new/choose). Feel free to join our growing¬†[Slack community](https://slack.aimstack.io/)¬†and ask questions directly to the developers and seasoned users.\n\nIf you find Aim useful,¬†[please stop by](https://github.com/aimhubio/aim)¬†and drop us a star.","html":"<p>In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from¬†<a href=\"http://medicaldecathlon.com/\"></a><a href=\"http://medicaldecathlon.com/\">http://medicaldecathlon.com/</a>.</p>\n<p>This is a longer form of the¬†<a href=\"https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb\">3D spleen segmentation tutorial</a>¬†on the MONAI tutorials repo.</p>\n<p>For a complete in-depth overview of the code and the methodology please follow¬†<a href=\"https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb\">this tutorial</a>¬†directly or our very own¬†<a href=\"https://aimstack.io/blog/tutorials/aim-from-zero-to-hero\">Aim from Zero to Hero tutorial</a>.</p>\n<h2><em><strong>Tracking the basics</strong></em></h2>\n<p>As we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.</p>\n<h3><em><strong>Transformations Matter</strong></em></h3>\n<p>The way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the<code>Single Run Page</code>.</p>\n<p><img src=\"/images/dynamic/image-10-.png\" alt=\"\"></p>\n<p>You can further search/filter through the runs based on these transformations using Aims‚Äô very own pythonic search language¬†<code>AimQL</code>. An Advanced usage looks like this</p>\n<p><img src=\"/images/dynamic/image-11-.png\" alt=\"\"></p>\n<h3><em>Lets talk about models and optimizers</em></h3>\n<p>Similar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic¬†<code>dict</code>-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from<code>AimQL</code></p>\n<pre><code>aim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n</code></pre>\n<p><img src=\"/images/dynamic/image-12-.png\" alt=\"\"></p>\n<h2>What about Losses?</h2>\n<p>Majority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).</p>\n<p>Furthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion</p>\n<pre><code>aim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n</code></pre>\n<p>After grouping, we end up with a visualization akin to this.</p>\n<p><img src=\"/images/dynamic/image-5-.jpeg\" alt=\"\"></p>\n<p>Aggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\">complete documentation</a>¬†or simply play around in our¬†<a href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&#x26;chart=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&#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\">interactive Demo</a>.</p>\n<h2><em>A picture is worth a thousand words</em></h2>\n<p>In the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at¬†<code>Aim</code>. You can¬†track images¬†as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.</p>\n<pre><code>aim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n</code></pre>\n<p>All the grouping/filtering/aggregation functional presented above are also available for Images.</p>\n<p><img src=\"/images/dynamic/image-6-.jpeg\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-7-.jpeg\" alt=\"\"></p>\n<h2><em>A figure is worth a thousand pictures</em></h2>\n<p>We decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always</p>\n<pre><code>aim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n</code></pre>\n<p>Within¬†<em><code>Aim</code></em>¬†you can access all the tracked Figures from within the¬†<code>Single Run Page</code></p>\n<p>``</p>\n<p><img src=\"/images/dynamic/image-2-.gif\" alt=\"\"></p>\n<p>Another interesting thing that one can¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\">track is distributions</a>. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.</p>\n<pre><code>from aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n</code></pre>\n<p>Distributions can be accessed from the Single Run Page as well.</p>\n<p><img src=\"/images/dynamic/image-6-.png\" alt=\"\"></p>\n<h2><em><strong>The curtain falls</strong></em></h2>\n<p>In this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with¬†<em><code>1-2</code></em>¬†lines of code. You can also play around with the completed MONAI integration results with our¬†<a href=\"http://play.aimstack.io:10005/\">interactive demo</a>.</p>\n<p>If you have any bug reports please follow up on our¬†<a href=\"https://github.com/aimhubio/aim/issues/new/choose\">github repository</a>. Feel free to join our growing¬†<a href=\"https://slack.aimstack.io/\">Slack community</a>¬†and ask questions directly to the developers and seasoned users.</p>\n<p>If you find Aim useful,¬†<a href=\"https://github.com/aimhubio/aim\">please stop by</a>¬†and drop us a star.</p>"},"_id":"posts/3d-spleen-segmentation-with-monai-and-aim.md","_raw":{"sourceFilePath":"posts/3d-spleen-segmentation-with-monai-and-aim.md","sourceFileName":"3d-spleen-segmentation-with-monai-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/3d-spleen-segmentation-with-monai-and-aim"},"type":"Post"},{"title":"Aim and MLflow ‚Äî Choosing Experiment Tracker for Zero-Shot Cross-Lingual Transfer","date":"2023-02-14T06:41:14.310Z","author":"Hovhannes Tamoyan","description":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user...","slug":"aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v64PbdBn6kBvsH3t5bkv8w.png","draft":false,"categories":["Tutorials"],"body":{"raw":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.\n\nThe question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.\n\nIn this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.\n\n# Task Setup\n\nThe task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.\n\nParticularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.\n\nZero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.\n\nWe will explore two techniques in our experimentation:\n\n* Fine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.\n* Feature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.\n\nIn both techniques, we will undertake training utilizing the¬†`en`¬†subset of the¬†`XNLI`¬†dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from the¬†`XNLI`¬†dataset, including English(`en`), German(`de`), French(`fr`), Spanish(`es`), Chinese(`zh`), and Arabic(`ar`).\n\n# The Datasets\n\nWe will utilize the¬†`XNLI`¬†(cross-lingual¬†`NLI`) dataset, which is a selection of a few thousand examples from the¬†`MNLI`¬†(multi¬†`NLI`) dataset, translated into 14 different languages, including some with limited resources.\n\nThe template of the¬†`NLI`¬†task is as follows. Given a pair of sentences, a¬†`premise`¬†and a¬†`hypothesis`¬†need to determine whether a¬†`hypothesis`¬†is true (entailment), false (contradiction), or undetermined (neutral) given a¬†`premise`.\n\nLet‚Äôs take a look at a few samples to get hang of it. Say the given hypothesis is¬†`‚ÄúIssues in Data Synthesis.‚Äù`¬†and the premise is¬†`‚ÄúProblems in data synthesis.‚Äù`. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.\n\nAnother example this time of a neutral pair of sentences is the following: the hypothesis is¬†`‚ÄúShe was so happy she couldn't stop smiling.‚Äù`¬†and the premise is¬†`‚ÄúShe smiled back.‚Äù`. The first sentence doesn‚Äôt imply the second one, however, it doesn‚Äôt contradict it as well. Thus they are neutral.\n\nAn instance of contradiction, the hypothesis is¬†`‚ÄúThe analysis proves that there is no link between PM and bronchitis.‚Äù`¬†and the premise is¬†`‚ÄúThis analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.‚Äù`. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.\n\nFor more examples please explore the HuggingFace Datasets page powered by Streamlit:¬†<https://huggingface.co/datasets/viewer/>.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png)\n\n# The Models\n\n\n\nIn our experiments, we will utilize the following set of pre-trained multilingual language models:\n\n![](/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png)\n\nWe will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.\n\n# Setting up Training Environment\n\nBefore beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:\n\n\n\n```\naim-and-mlflow-usecase\n‚îú‚îÄ‚îÄ logs\n‚îÇ   ‚îú‚îÄ‚îÄ aim_callback\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ aimlflow\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ checkpoints\n‚îÇ   ‚îî‚îÄ‚îÄ mlruns\n‚îî‚îÄ‚îÄ main.py\n```\n\nLet‚Äôs start off by creating the main directory, we named it¬†`aim-and-mlflow-usecase`, you can simply name anything you want. After which we need to download the¬†`main.py`¬†from the following source:¬†<https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer>. The code explanation and sample usage can be found in the¬†`README.md`¬†file of the directory. We will be using this script to run our experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png)\n\nThe¬†`logs`¬†directory as the name suggests stores the logs. In the¬†`checkpoints`¬†folder, all the model states will be saved. The¬†`mlruns`¬†is the repository for MLflow experiments. The¬†`aim_callback`¬†will store the repository of Aim runs tracked using Aim‚Äôs built-in callback for Hugging Face Transformers, meanwhile, the¬†`aimlflow`¬†will store the runs converted from MLflow using the aimlflow tool.\n\n> *It is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.*\n\nIn order to start the training process, we will be using the following command. But first, let‚Äôs navigate to the directory where our script is located (`aim-and-mlflow-usecase`¬†in our case).\n\n```\npython main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nWhere¬†`{PATH_TO}`¬†is the absolute path of the¬†`aim-and-mlflow-usecase`¬†directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:\n\n```\npython main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nGo grab some snacks, trainings take a while üç´ ‚ò∫Ô∏è.\n\n# Using aimlflow\n\nMeanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use the¬†`aimlflow`¬†to simply convert and view our experiments live on Aim. Let‚Äôs first show how this can be done after which tackle the question.\n\nInstal¬†`aimlflow`¬†on your machine via¬†`pip`, if it is not already installed:\n\n```\n$ pip install aimlflow\n```\n\n```\n$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n```\n\nThis command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.\n\nMore on how the¬†`aimlflow`¬†can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:\n\n* **[Exploring MLflow experiments with a powerful UI](https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e)**\n* **[How to integrate aimlflow with your remote MLflow](https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf)**\n\nThis is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aim‚Äôs built-in callback,¬†`aim.hugging_face.AimCallback`, which is specifically designed for t`ransformers.Trainer`¬†functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.\n\n# Unlocking the Power of Data Analysis\n\n\n\nOnce the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, let‚Äôs launch both tools‚Äô UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:\n\n```\n$ mlflow ui\n```\n\n```\n$ aim up\n```\n\n\n\n> *Note that for the best possible experience, we will be using the*¬†`aim_callback/.aim`¬†*repository in this demonstration, as it has deeper integration with the*¬†`Trainer`*.*\n\nThe UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visiting¬†`http://127.0.0.1:5000`¬†for MLflow and¬†`http://127.0.0.1:43800`¬†for Aim.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png \"The user interface of MLflow on first look\")\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png \"The user interface of Aim on first look\")\n\nIn order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit the¬†`Compare Runs`¬†page:\n\nBy selecting all the experiments, navigate to the comparison page by clicking the¬†`Compare`¬†button.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png)\n\nThe¬†`Run details`¬†section presents the run metadata, including the start and end time and duration of the run. The¬†`Parameters`¬†section displays the hyperparameters used for the run, such as the optimizer and architecture. The¬†`Metrics`¬†section showcases the latest values for each metric.\n\nHaving access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.\n\nTo organize the parameters into meaningful groups for our experiment, simply go to Aim‚Äôs¬†`Metrics Explorer`¬†page and follow a few straightforward steps:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif)\n\n\n\n\n\n# Gaining insights\n\n\n\nLet‚Äôs examine the charts more closely and uncover valuable insights from our experiments.\n\nA quick examination of the charts reveals the following observations:\n\n* The fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.\n* The graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.\n\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png)\n\n\n\n* Even though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only by¬†`train_batch_size`. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png)\n\n\n\n* The charts unmistakably show that the¬†`bert-base-multilingual-cased`¬†model achieved the best accuracy results, with the highest score observed for the¬†`en`¬†subset, as the model was trained on that subset. Subsequently,¬†`es`,¬†`fr`,¬†`de`,¬†`zh`, and¬†`ar`¬†followed. Unsurprisingly the scores for the¬†`zh`¬†and¬†`ar`¬†datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png)\n\n\n\n* Let us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.\n* ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png)\n\n\n\n  One can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. The¬†`Parameter Explorer`¬†will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how the¬†`Parameter Explorer`¬†works, let‚Äôs pick the following parameters:¬†`train_batch_size`,¬†`learning_rate`,`_name_or_path`,¬†`loss`, and the accuracies of¬†`sub_dataset`s. The following chart will be observed after clicking the¬†`Search`¬†button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses the¬†`bert-base-multilingual-cased`¬†model, with 5¬∑10‚Åª‚Åµ¬†`learning_rate`¬†and the¬†`batch_size`¬†is 8.\n\n  Taking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.\n\n  # Conclusion\n\n  This blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aim‚Äôs powerful user interface to obtain valuable insights from the experiments.\n\n  We also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.","html":"<p>The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.</p>\n<p>The question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.</p>\n<p>In this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.</p>\n<h1>Task Setup</h1>\n<p>The task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.</p>\n<p>Particularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.</p>\n<p>Zero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.</p>\n<p>We will explore two techniques in our experimentation:</p>\n<ul>\n<li>Fine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.</li>\n<li>Feature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.</li>\n</ul>\n<p>In both techniques, we will undertake training utilizing the¬†<code>en</code>¬†subset of the¬†<code>XNLI</code>¬†dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from the¬†<code>XNLI</code>¬†dataset, including English(<code>en</code>), German(<code>de</code>), French(<code>fr</code>), Spanish(<code>es</code>), Chinese(<code>zh</code>), and Arabic(<code>ar</code>).</p>\n<h1>The Datasets</h1>\n<p>We will utilize the¬†<code>XNLI</code>¬†(cross-lingual¬†<code>NLI</code>) dataset, which is a selection of a few thousand examples from the¬†<code>MNLI</code>¬†(multi¬†<code>NLI</code>) dataset, translated into 14 different languages, including some with limited resources.</p>\n<p>The template of the¬†<code>NLI</code>¬†task is as follows. Given a pair of sentences, a¬†<code>premise</code>¬†and a¬†<code>hypothesis</code>¬†need to determine whether a¬†<code>hypothesis</code>¬†is true (entailment), false (contradiction), or undetermined (neutral) given a¬†<code>premise</code>.</p>\n<p>Let‚Äôs take a look at a few samples to get hang of it. Say the given hypothesis is¬†<code>‚ÄúIssues in Data Synthesis.‚Äù</code>¬†and the premise is¬†<code>‚ÄúProblems in data synthesis.‚Äù</code>. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.</p>\n<p>Another example this time of a neutral pair of sentences is the following: the hypothesis is¬†<code>‚ÄúShe was so happy she couldn't stop smiling.‚Äù</code>¬†and the premise is¬†<code>‚ÄúShe smiled back.‚Äù</code>. The first sentence doesn‚Äôt imply the second one, however, it doesn‚Äôt contradict it as well. Thus they are neutral.</p>\n<p>An instance of contradiction, the hypothesis is¬†<code>‚ÄúThe analysis proves that there is no link between PM and bronchitis.‚Äù</code>¬†and the premise is¬†<code>‚ÄúThis analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.‚Äù</code>. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.</p>\n<p>For more examples please explore the HuggingFace Datasets page powered by Streamlit:¬†<a href=\"https://huggingface.co/datasets/viewer/\">https://huggingface.co/datasets/viewer/</a>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png\" alt=\"\"></p>\n<h1>The Models</h1>\n<p>In our experiments, we will utilize the following set of pre-trained multilingual language models:</p>\n<p><img src=\"/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png\" alt=\"\"></p>\n<p>We will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.</p>\n<h1>Setting up Training Environment</h1>\n<p>Before beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:</p>\n<pre><code>aim-and-mlflow-usecase\n‚îú‚îÄ‚îÄ logs\n‚îÇ   ‚îú‚îÄ‚îÄ aim_callback\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ aimlflow\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ checkpoints\n‚îÇ   ‚îî‚îÄ‚îÄ mlruns\n‚îî‚îÄ‚îÄ main.py\n</code></pre>\n<p>Let‚Äôs start off by creating the main directory, we named it¬†<code>aim-and-mlflow-usecase</code>, you can simply name anything you want. After which we need to download the¬†<code>main.py</code>¬†from the following source:¬†<a href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\">https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer</a>. The code explanation and sample usage can be found in the¬†<code>README.md</code>¬†file of the directory. We will be using this script to run our experiments.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png\" alt=\"\"></p>\n<p>The¬†<code>logs</code>¬†directory as the name suggests stores the logs. In the¬†<code>checkpoints</code>¬†folder, all the model states will be saved. The¬†<code>mlruns</code>¬†is the repository for MLflow experiments. The¬†<code>aim_callback</code>¬†will store the repository of Aim runs tracked using Aim‚Äôs built-in callback for Hugging Face Transformers, meanwhile, the¬†<code>aimlflow</code>¬†will store the runs converted from MLflow using the aimlflow tool.</p>\n<blockquote>\n<p><em>It is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.</em></p>\n</blockquote>\n<p>In order to start the training process, we will be using the following command. But first, let‚Äôs navigate to the directory where our script is located (<code>aim-and-mlflow-usecase</code>¬†in our case).</p>\n<pre><code>python main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n</code></pre>\n<p>Where¬†<code>{PATH_TO}</code>¬†is the absolute path of the¬†<code>aim-and-mlflow-usecase</code>¬†directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:</p>\n<pre><code>python main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n</code></pre>\n<p>Go grab some snacks, trainings take a while üç´ ‚ò∫Ô∏è.</p>\n<h1>Using aimlflow</h1>\n<p>Meanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use the¬†<code>aimlflow</code>¬†to simply convert and view our experiments live on Aim. Let‚Äôs first show how this can be done after which tackle the question.</p>\n<p>Instal¬†<code>aimlflow</code>¬†on your machine via¬†<code>pip</code>, if it is not already installed:</p>\n<pre><code>$ pip install aimlflow\n</code></pre>\n<pre><code>$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n</code></pre>\n<p>This command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.</p>\n<p>More on how the¬†<code>aimlflow</code>¬†can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:</p>\n<ul>\n<li><strong><a href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\">Exploring MLflow experiments with a powerful UI</a></strong></li>\n<li><strong><a href=\"https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf\">How to integrate aimlflow with your remote MLflow</a></strong></li>\n</ul>\n<p>This is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aim‚Äôs built-in callback,¬†<code>aim.hugging_face.AimCallback</code>, which is specifically designed for t<code>ransformers.Trainer</code>¬†functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.</p>\n<h1>Unlocking the Power of Data Analysis</h1>\n<p>Once the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, let‚Äôs launch both tools‚Äô UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:</p>\n<pre><code>$ mlflow ui\n</code></pre>\n<pre><code>$ aim up\n</code></pre>\n<blockquote>\n<p><em>Note that for the best possible experience, we will be using the</em>¬†<code>aim_callback/.aim</code>¬†<em>repository in this demonstration, as it has deeper integration with the</em>¬†<code>Trainer</code><em>.</em></p>\n</blockquote>\n<p>The UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visiting¬†<code>http://127.0.0.1:5000</code>¬†for MLflow and¬†<code>http://127.0.0.1:43800</code>¬†for Aim.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png\" alt=\"\" title=\"The user interface of MLflow on first look\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png\" alt=\"\" title=\"The user interface of Aim on first look\"></p>\n<p>In order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit the¬†<code>Compare Runs</code>¬†page:</p>\n<p>By selecting all the experiments, navigate to the comparison page by clicking the¬†<code>Compare</code>¬†button.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png\" alt=\"\"></p>\n<p>The¬†<code>Run details</code>¬†section presents the run metadata, including the start and end time and duration of the run. The¬†<code>Parameters</code>¬†section displays the hyperparameters used for the run, such as the optimizer and architecture. The¬†<code>Metrics</code>¬†section showcases the latest values for each metric.</p>\n<p>Having access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.</p>\n<p>To organize the parameters into meaningful groups for our experiment, simply go to Aim‚Äôs¬†<code>Metrics Explorer</code>¬†page and follow a few straightforward steps:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif\" alt=\"\"></p>\n<h1>Gaining insights</h1>\n<p>Let‚Äôs examine the charts more closely and uncover valuable insights from our experiments.</p>\n<p>A quick examination of the charts reveals the following observations:</p>\n<ul>\n<li>The fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.</li>\n<li>The graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png\" alt=\"\"></p>\n<ul>\n<li>Even though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only by¬†<code>train_batch_size</code>. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png\" alt=\"\"></p>\n<ul>\n<li>The charts unmistakably show that the¬†<code>bert-base-multilingual-cased</code>¬†model achieved the best accuracy results, with the highest score observed for the¬†<code>en</code>¬†subset, as the model was trained on that subset. Subsequently,¬†<code>es</code>,¬†<code>fr</code>,¬†<code>de</code>,¬†<code>zh</code>, and¬†<code>ar</code>¬†followed. Unsurprisingly the scores for the¬†<code>zh</code>¬†and¬†<code>ar</code>¬†datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png\" alt=\"\"></p>\n<ul>\n<li>\n<p>Let us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.</p>\n</li>\n<li>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png\" alt=\"\"></p>\n<p>One can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. The¬†<code>Parameter Explorer</code>¬†will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how the¬†<code>Parameter Explorer</code>¬†works, let‚Äôs pick the following parameters:¬†<code>train_batch_size</code>,¬†<code>learning_rate</code>,<code>_name_or_path</code>,¬†<code>loss</code>, and the accuracies of¬†<code>sub_dataset</code>s. The following chart will be observed after clicking the¬†<code>Search</code>¬†button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses the¬†<code>bert-base-multilingual-cased</code>¬†model, with 5¬∑10‚Åª‚Åµ¬†<code>learning_rate</code>¬†and the¬†<code>batch_size</code>¬†is 8.</p>\n<p>Taking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.</p>\n<h1>Conclusion</h1>\n<p>This blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aim‚Äôs powerful user interface to obtain valuable insights from the experiments.</p>\n<p>We also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.</p>\n</li>\n</ul>"},"_id":"posts/aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","_raw":{"sourceFilePath":"posts/aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileName":"aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets ‚Äî train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets ‚Äî train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that on¬†[Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\n\n\nUse the¬†[aim.track](https://github.com/aimhubio/aim#track)¬†context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n```\nimport aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n```\n\nOnce the training is ran, execute¬†`aim up`¬†in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n> **\\*Note:**¬†The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLet‚Äôs go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.**¬†Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.**¬†Add the¬†`bleu`¬†metric to the Select input ‚Äî query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.**¬†Search by¬†`context.subset`¬†to show both¬†`test`¬†and¬†`val`¬†`loss`¬†and¬†`bleu`¬†metrics. Divide into subplots further by¬†`context.subset`¬†too so Aim UI shows¬†`test`¬†and¬†`val`¬†metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot it‚Äôs easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\nHere is a full summary video on how to do it on the UI.\n\n![](https://youtu.be/jPNZ7JVkA-c)\n\n# Learn More\n\nIf you find Aim useful, support us and¬†[star the project](https://github.com/aimhubio/aim)¬†on GitHub. Join the¬†[Aim community](https://slack.aimstack.io/)¬†and share more about your use-cases and how we can improve Aim to suit them.","html":"<p>Researchers divide datasets into three subsets ‚Äî train, validation and test so they can test their model performance at different levels.</p>\n<p>The model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.</p>\n<p>The validation and test sets are used to test the model on additional unseen data to verify how well it generalise.</p>\n<p>Models are usually ran on validation subset after each epoch.<br>\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.</p>\n<p>There is a need to collect and effectively compare all these metrics.</p>\n<p>Here is how to do that on¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a></p>\n<h1>Using context to track for different subsets?</h1>\n<p>Use the¬†<a href=\"https://github.com/aimhubio/aim#track\">aim.track</a>¬†context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.</p>\n<pre><code>import aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n</code></pre>\n<p>Once the training is ran, execute¬†<code>aim up</code>¬†in your terminal and start the Aim UI.</p>\n<h1>Using subplots to compare test, val loss and bleu metrics</h1>\n<blockquote>\n<p><strong>*Note:</strong>¬†The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*</p>\n</blockquote>\n<p>Let‚Äôs go step-by-step on how to break down lots of experiments using subplots.</p>\n<p><strong>Step 1.</strong>¬†Explore the runs, the context table, play with the query language.</p>\n<p><img src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"></p>\n<p><strong>Step 2.</strong>¬†Add the¬†<code>bleu</code>¬†metric to the Select input ‚Äî query both metrics at the same time. Divide into subplots by metric.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"></p>\n<p><strong>Step 3.</strong>¬†Search by¬†<code>context.subset</code>¬†to show both¬†<code>test</code>¬†and¬†<code>val</code>¬†<code>loss</code>¬†and¬†<code>bleu</code>¬†metrics. Divide into subplots further by¬†<code>context.subset</code>¬†too so Aim UI shows¬†<code>test</code>¬†and¬†<code>val</code>¬†metrics on different subplots for better comparison.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"></p>\n<p>Not it‚Äôs easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.</p>\n<h1>Summary</h1>\n<p>Here is a full summary video on how to do it on the UI.</p>\n<p><img src=\"https://youtu.be/jPNZ7JVkA-c\" alt=\"\"></p>\n<h1>Learn More</h1>\n<p>If you find Aim useful, support us and¬†<a href=\"https://github.com/aimhubio/aim\">star the project</a>¬†on GitHub. Join the¬†<a href=\"https://slack.aimstack.io/\">Aim community</a>¬†and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim from Zero to Hero","date":"2022-03-16T16:39:52.780Z","author":"Gev Soghomonian","description":"In this blog post, we show how to use Aim‚Äôs basic to highly advanced functionality in order to track your machine learning experiments with various","slug":"aim-from-zero-to-hero","image":"/images/dynamic/image.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"In this blog post, we show how to use Aim‚Äôs basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\n\n## *Starting from basics: how Aim tracks machine learning experiments*\n\nMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with¬†`matplotlib`. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\n\nTo track the losses, simply create an experiment run and add a tracking common like this\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n```\n\nYou are going to end up with a visualization of this kind.\n\n![](/images/dynamic/image-2-.png)\n\nA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the¬†[complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html)¬†or simply play around in our¬†[interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&chart=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&select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n- - -\n\n## *Rising beyond watching losses*\n\nAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic¬†dict¬†or dictionary-like object, i.e.\n\n```\naim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n```\n\nIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can¬†[group with respect to the parameters/hyperparameters](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter)¬†that you saved with aim.\n\nViewing the¬†[standalone parameters](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page)¬†is fast for each separate Single Run\n\n![](/images/dynamic/image.png)\n\n[Filtering/grouping your runs using the selected trackable](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter)¬†can be accessed and looked through in our¬†[interactive tutorial](http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM&chart=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&select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n![](/images/dynamic/image-1-.png)\n\nFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language¬†[AimQL](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2), by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\n\n![](/images/dynamic/image-4-.png)\n\n## ***What if numbers are simply not enough?***\n\nVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\n\nAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\n\n```\naim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n```\n\nYou can find the complete guide to tracking in the¬†[official documentation](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#).\n\nAll the grouping/filtering/aggregation functional presented above is also available for Images.\n\n![](/images/dynamic/image-2-.jpeg)\n\n![](/images/dynamic/image-3-.jpeg)\n\n![](/images/dynamic/image-5-.png)\n\nThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\n\n![](/images/dynamic/image.gif)\n\nThe grouping is rather flexible with a¬†[multitude of options](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer).\n\nAnother interesting thing that one can¬†[track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\nYou can play around with the image and single run explorers and see all the trackables across our numerous Demos ([FS2](http://play.aimstack.io:10004/),¬†[Spleen Segmentation](http://play.aimstack.io:10005/),¬†[Lightweight GAN](http://play.aimstack.io:10002/),¬†[Machine Translation](http://play.aimstack.io:10001/)).\n\n***One Run to rule them all***\n\nThere are times when a researcher would need to focus upon only a¬†[single run of the experiment](https://aimstack.io/blog/new-releases/aim-3-7-%E2%80%94-revamped-run-single-page-and-aim-docker-image), where he can iterate through a complete list of all the things he tracked. Aim has a dedicated¬†[Single Run Page](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page)¬†for this very purpose.\n\nYou can view all the trackables in the following Tabs:\n\n* Parameters/ Hyperparametrs ‚Äì Everything tracked regarding the experiment\n* Metrics ‚Äì All the Tracked Metrics/Losses etc.\n* System ‚Äì All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\n* Distributions ‚Äì All the distributions tracked (i.e. Flowing gradients and weights)\n* Images ‚Äì All the Image objects saved\n* Audios ‚Äì All the Audio objects saved\n* Texts ‚Äì All the raw text tracked during experimentation\n* Figures ‚Äì All the¬†`matplotlin`/`Plotly`¬†etc. Figures\n* Settings ‚Äì Settings that runs share\n\nLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\n\n![](/images/dynamic/image-7-.png)\n\n![](/images/dynamic/image-8-.png)\n\n![](/images/dynamic/image-9-.png)\n\nYou can look aroud the individual runs in one of our interactive Demos ([FS2](http://play.aimstack.io:10004/),¬†[Spleen Segmentation](http://play.aimstack.io:10005/),¬†[Lightweight GAN](http://play.aimstack.io:10002/),¬†[Machine Translation](http://play.aimstack.io:10001/)).\n\n- - -\n\n## ***No More localhost. Track experiments remotely***\n\nAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\n\n```\naim init\n# Tringgering aim server\naim server --repo <REPO_PATH> --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo <REPO_PATH> --host 0.0.0.0 --port some_open_port\n```\n\nIntegrating this newly created¬†[remote tracking server](https://aimstack.io/blog/new-releases/aim-3-4-%E2%80%93-remote-tracking-alpha-sorting-deleting-runs)¬†within your experimentation is even easier.\n\n```\n# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n```\n\nAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\n\n![](/images/dynamic/image-4-.jpeg)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling¬† and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"<p>In this blog post, we show how to use Aim‚Äôs basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.</p>\n<h2><em>Starting from basics: how Aim tracks machine learning experiments</em></h2>\n<p>Most of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with¬†<code>matplotlib</code>. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)</p>\n<p>To track the losses, simply create an experiment run and add a tracking common like this</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n</code></pre>\n<p>You are going to end up with a visualization of this kind.</p>\n<p><img src=\"/images/dynamic/image-2-.png\" alt=\"\"></p>\n<p>A natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n</code></pre>\n<p>After grouping, we end up with a visualization akin to this.</p>\n<p><img src=\"/images/dynamic/image.jpeg\" alt=\"\"></p>\n<p>Aggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\">complete documentation</a>¬†or simply play around in our¬†<a href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT&#x26;chart=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&#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\">interactive Demo</a>.</p>\n<hr>\n<h2><em>Rising beyond watching losses</em></h2>\n<p>At one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic¬†dict¬†or dictionary-like object, i.e.</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n</code></pre>\n<p>In the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\">group with respect to the parameters/hyperparameters</a>¬†that you saved with aim.</p>\n<p>Viewing the¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\">standalone parameters</a>¬†is fast for each separate Single Run</p>\n<p><img src=\"/images/dynamic/image.png\" alt=\"\"></p>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\">Filtering/grouping your runs using the selected trackable</a>¬†can be accessed and looked through in our¬†<a href=\"http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM&#x26;chart=7DTSXL8yGkKjwNDtEcYjw4opymsQFTsrFJVani1UKjeQnySYAJU8THRU8HtWL3VdRyvJz4M6JQ4i5aQhRdAY2eMcNhsWdc3GBGfJ557AxjDiYoBKS4mAFvEyFP7iiBKcQcNZmTYCtF2Kc5v5sYaxoxb1nrG2LHfngen5ec8K96qQFnmgBbmhx6avZ2KFV5SAoY4iJ6Yu9Y5Bs6VENDzHWKCGVTsRsoBaY94TLJmzpxM6CfCdL5W4jEV1rnVmruGM7YwobY4vPBq49DyvMdYkcfQnKudBEUYLcrEavGiiXgXBdNbQeri7D2ErVTDKVzRPf6qiQdYBXfFceHnRaYNRvAuge3mY4ixbYz24rKKtVnaWCkEX9nTUdfYzPzbgDdgs5m8k4QPcQqs4AiHju2RDtrinvYcj9F1GPdepQvrkvqSfxZS1hMSuS68Yxn769UzjCPD7WwjEpobY6C32U5J4DazxZkzdVce3maPQj4osX3BFmP9j9xTR98JPuU3ApP5pUYZqbBpaY9QkFELgLoVR1eixABK8bXyYnu3mDomMUHCmZqi34ttmX2hKVRV2i84zFQ82fq6SAMpHADdSGagp9u2H5CGCWRrBvQ3RmbXzBJ2R6qn41CERpCv2rmBXT9qs71fGQ73a51K7Bft3mBUi83rXdt1wRMHMMi9LSnEeiba4JdaapTrBbZV4tS3m5p9wi9vxeWDtVSTtZjJRqHN96x3FtsAG1xSpcgiWGutBAxw7yS5Ng9Pcd5ZfUCtVhgQdeTqJGZ4ThmJaNNFv2AMo2Vxiu1BdUAmTB3bQ35WJNauQhybmJX61rE83F3s7Ahd47ASLu7e6LEP2JL9XkiCWNG7rLGPv18oHKWkj2RtKcqUbstP8afoRdFhfibnResF7Jnb2Cqa4yET3iPboGmj6APutdy7YEy6itShcuRRx3c5h7ZXMfxU1atMDxK2bEBksu7UYrCzszGDFjuj9PU58TX2Y4m3izdEm9hRALYw4Tp49SdxdE6XMqjRu3XDpwSjFAGZXPZ4i58CL9gLydZmsJVjyuA43i8F6UGz34KhcbGw61hXCKuHmkzTftKyK7MHz7ZvYwa1hthDDdXTayD8BcZsT4VNWYPeHXUiVbYufWapjRDTVkUFVYNFKa7oDVY3uP373sKCZuBL2kVxwbRKniwjLYMndmAWmzF179PaqjtdkvDFqwW8xWDBDJSV2uMxet7Eu5K5qbRUAqZJiHad1yyWpVcmCJgiMzrfLLtMm9jXAEy1P7w5XnhZ1vJbPx5wzzb7p6MNS1EwTPugu1X8GvXeuPuBq2jsd3LuGcbHC3nhPfCJeX8PVxNyEpEKdCMojYThfGnE5UrztsPqoCet9Py1jzEoayNnyD3NVec5RsGFvmRrusHy8GprvAfDrFTmF5ryhM2GXpSELvwHMMcKqXuaUrT4rbEM7Yirh2de7fnncziZjWSv3YV1FjCsSPvwEAnL7jjnm1sn6vig7JCpqD4bJRchSkMQmENcxLfthcd2ZXMga2uHhPAK5vxDLwrdX7J8Npxgur7jbYvAUmvynhWXC5bGCG9uY46ej8Rxwa8iu5LNP2BqpJ1YDWMgbHxoTQnbtokEZbTkwfrRDoCe5wvoGJuC2StnXEfGMjbqFHL7q5kKWeQv4C16cgCv2YE3hEsvc9Q2LLY5HXA2NcGxHSZHWLi4f13nsGzVX6YEiVcYKyDwBcQcv4ZmiV9G2QxUfwhUK3w7ZpATFKq3mjaGZJNTRiBp1RKieBZ2Yd7S9arWjg5cHKeW9STtc5ZeuQjzGuZwwhiwygfUW4PWiTbtJpMjkVuxQLA6zAXGxzmkgrH9weaP4pQHjdTj2gj282RYa6H9p48dtscE4c8fqhkkG71WRrE3ZKpC5hrg8fhbaCyrBFFcwCZAJubz4JGBik7w8vxEbuCJ5MMj7NSChvXHTqnvwaALhqFiVWh2qqwr1z8cXJJ9Z6pZmhvdArAXPPjAr15gVddxFb5yBKeREqb6rM4pf8zSY6fhDnX52TvBP6Jv3cnHbtxsrDmPgVCumRsgenWsDmqfGLKHoJNwmZ9viuzqwa7ZpWpT5wj8xX1BL8ZDBUiEj1yQgxKKj52s5gT8kjHXa2CfKW2VjDEV4qi4Ww67YFAcS5y1qLZGhmyUDsau9yX5AWwo9CbBgukTGxQFSGAiqT61AEkUBQSukH3U2GJkxdtFjRLsCaQRysmna44x9Uxs4TELbh9PSjmdQXC5KegQU6MhqKRq3aWmwTJ8Vr2tuyAaWVLPaBG8Ptme2F9Ru8r3D7ksPDn5hXRhZMHLFRAGkqCoVnTwv1zfGB9b6mJgFCVHbZerUshFPaWNj1quoY8673eNLiDB3c7mJrci6wBxkdXrcTvdKzdhMCMiUthoRjy2dpXu5QuU1puDfMw8W8SsGJfwVxeTYQixkEo1e7URCECqHc31eM4S3fhY7MxmxrSdXpNegTSQwcvEPoWu17PWHFo4k5EeFaWnAsHb3FxYEYeBHWhPB2i8uj3oR4tAr77ZSWxPxu8dZTywFpK8y5wVxnn7kU3kzrXjZA2M7x532AA4kVaHhYwq4UAzSdYqNxYPJgJVjj31Uq7mEVjQdVCvcMQsWKmMsj8Mf7dpRhry28iEUnTMnMfVDoMHcX9KeGU8kuQ7SCu6QJfXJnva3G5P54AACpDCoAKTH4B5omFSU5KqERKTk5gnEYXjPeB3vUPQyRchdDK5CBeAnxNp3JwfiMCeDuZ4WY3Se2BuxLpW2R54rsMjrRW33yAWhH86y8P77UTfnsQUR2ZCyhpyzhUHDwYeqN3XoegykHwmTkug1APed7n5BUBMRiGEDySh1V8uYSutFLuT3fp3KwExSGh46ihqsxTz3GuKmD7i63s6ZgEnM17tYd4mGGTpnn1bdjAqPyUs59V6c9Fsv94QCzfhx5PgRUazG81TuQ6zd169ZS73TFzAi4eeJP9FXTu41yRkYzEfHAWtov4WvmtcTEiogi8ryPETwD2ELh7gYpRRpY3nBDx9CNuGx4rrpzWhycLXn5qK9Epsd9xDBqD3K8DDDQSZo6pNxd6cvBwo274bH2L4fRrofmcGyHzNT51tXxqDSiokaHTDN6q39A1UwmjATiQK5ZmXoi7mJSZ4PGyAUXZFxGq4GRYao7HKHMWcEArbhQkJgJm2aJo4SH6X5GQNGxiskrJAebNem4a3mLvGP8LwPwTjQfT9QZa47hgm6JTVQnXbQ2BqYfXxqjpomizqCwiYpEQrcKs6vkeCtKdh8Mu824BeomKjV3HouBa4ecopqV8zKudHNSpUcmUPahxQpqe7KKaX6gWnDk7h6UNCwZvrQ28qNFky8s5bniXHxxhk7jiEvqzUS7A4jndMjCAaLTbr6LrUNdNaUybDzi7ZpXomY8o6bJxDjoAgo6WNVaKjQHAMgcRcUptazCqey4jNaAi9gv8jAXAHr8h7iRRqsjuriwp8zfZnkDFvSbbiahYK4FhhYRdimp6eWjuXTbtDsrdS2tcCfCEpKwEf3Q1Vqs5CZrMWwqsGF6nGYGPyNmS8GzaK5ZYQ3u4PF7nVgRkeBwyFpEWXG8fEUm6LXgYSejbEaxBCq6nnheLEWT69FoE7zcbdmRsMrtuKLxmRxUm13W7DkAsktXJHMu2EaeRCkmtqJT8okNWzAkc9aNm9rWaYevhDitrsiZhhGdwQqmLjWMtrHAUtEBh6W7XbGgRSmV9XxXb3fc6RaQTdEwqT76RT8GHNLgTnS5Rs2Ek9oWZkVuk7VQbBHo8JP11z49my3DNL1yu9ec5XpwGyd6SUaQW2GdJ4rprexUDAPzEUUzeFJB1ucNDnrswMCHEtVeqp6cjCTZQ3sLzJCv8JTDRYs6UgnRsnSkVMTtSLa4GkZmcTAZqSitxmWEbPtGgUkAqfugMRHqsAqCMBrkCkJ8fh6f7ffFYMyAGv3y6eY3DAQwSqUqr3k6oHVhMSN28A6rn91r146vNSsSa9NTTCdLGhuS2uxEJuKHhhoVvbzq1DQtup6spCYDFJ8Upxdhdypq7SXXaS99KtH8bW5p8i6DcpAQ4YBzJ6F5E1tSZQQhzb1abByA51fnQRqfZqhXZjWh9ky6A1hmDLhjLfrL9jthecYGbWq6bbnac4DrmrHggWN5KfUPJVX3ix4brpppdDrrZQZ5HUo3e6ps67hyTEd8n86ofcWwFqq44aPabCVD8yRFZysUyccKTpVaCgthAysW9zCefN6wFPsurVZA6bVbzdivXHB9CtEUjwexM439i7faF4VmZFhXjENh7TdGCghePGnKNnTkhM3FjPUAV2Udfq19jFvY21cYbTNE7KCKXbb7QD8nr9KXMKv71k1XsrL4QDz7owCmu7JXYvQpwzxZxh2Ac4RWtoP1qG2f9EcAVpiyNv6WPM8gxQgMfYK2EwnZAiHQQbM3qT7YL8QTNKAXkvuvew4uSbNPgQtcGYYvAnDz6LBvBVabV6aAEzU7RvfNJQMEczxjy2gkAbY8rEfqgFFXDBWPFs5WS4wjAqrhTMcDqyQQhMX3wd8K8G9qqoXTUBsW4vg74ZAUMmiSTdJ4wJZhSVunr9dYtJntPGZvDNzoLxgJfxyzPLuF5v8Xscs41LPifQWa3uCCLvqWPfeYW1FDELfsSskFGYmcs8r11t1Yu5GGRsbxxrpi5PFTr5dgdAUGqb7eUvX87H4PfAhDT6vWcAeemifpbQjBGGPY3idrDBn1tb9MqxsGXNrRuBMhBd1q3SyVARZ1Jcu4stfmfRLSaXYr7FMvZGyjUAtErby22J87JAKUZY4D3LQAA1bagYNQDuwmx525W14CCWG996KDMzG8gm1Ruvn1Y72HCJ8t9zqwuuPTkoMFPb13Mn4M8mR4xnCwvoh21MKvjHizhTmGoUQ86UVYg3gjqFuLxxhyZTDvw517qdzmGqcPdd1EhTRJFHc1yfq5YxaQ2291XgFXHZfaghmRFzn2JhoD8gwVtR72pA9bgPYprJLQZvDzAz5EPmthHbcQNPM9B3V3Y6Ds6Wj9kcJesqAUohi4wmf46vJrN1CxqkKq8rVW6E2kWUDYm8WKUYUvYKWrkQMgjcccQwsaGcNNKYUaumrQeA1BcCpCkm9Z9JMdWCHJhd1Z7GHZotRFyHigJ7YR4fMWpoVQ6eFfqZFW11UUgpRGUYJqHBb1z9mybkmgCFKjC6XKB5qocLZkVhd589cHgLXz6797eTmZtzcRFeFz45GpECtzSGLXQ95BUE1h1k5xYrmNAYeKxLojL6ugcu4hYuTiEihGfzRyLhNoY5Jh63Akdnkx6mLjoSaSo3UpjjTcNYsCCDjFWRJgPnzxVdA11WGfYomVy74vS3asBw7ropcGvjAsPF754tsdbpTgqNs5bacd8s3HUvxYRo7jVWQAhb62qsXqwRMNUWakvAQioXmPbLAwP4Vq8v3wiCH1D4gFKWs7AVnaBGf1WCA1adnhbYT12tGGnMhYqsPZTtUAqrR55TM2VDaSiDW3e5S17kzb3weNtjRVc93FAeLF7vm65kNxnDYchB8RcVEhpSn1y4bWscb8jCAoU7DcatK2ksaYjvEE6XxhC21yjcSzLH9283WGh2rFa8u5WBSneqZmYFH97V2Sv3XWdoeUkGb8ouqFKTCtK4BurMLhVFhosjYaMyyuJW9Gz1oGQyhzH5Ff56qYriu8Q4t69v2qyin5B8dgAfZzAM9qme4uhDNCLUH2v4GV5zYnh36YLau8iSDB6sN1XeuzCA9rDTq25FVbGVHvRGDRr8RkQusVcCnRHPx3YhmrSJymp91fiBxDRif7TDaaukad8hNodnkMVJpDN2gYrrJVYraoJcjJ5b6KfZzSJfP8x9ChVMhq8DcrzezNeXpDtJBCnAX99ayKvSXEEstQMJfAYHYip1d9MUqgCnJeA8B8byDt9HgY7gdq24s4MCP1nC5pmtDWeSp2VHCXUCp5bf4zJJ1RGUMcJDfxvu1SzT8ApwGLLFNz7BWPgHkibU4mPXAzHhBj8Ms9BgBZf8StsVXLDewaqbgDjWkoTNZEeJVgNbJsG3uzXbuCDxbCLahkJSPRkvtLgh5KkK15jR3epzrQzHfMDivm6aPbj1A6vJwBLsqjHqkapnXZVfcZYWzSRQ2ZYGQjw6Ph4isyFA4p8mkJafUqtZjvBUWcgDRyjQNA4PLE7B6syMY3g9HwJbkfsh6hqZ1TZhFfr1UnA3cx2Auk2vEVkPH5wjquk6zS3HjAQTDutBkGWrLcEr32QMNZyMGP11njWoj3EeWnR1Gw8v1THCZaFqq9Cd7q9XMRqXGtSLoXatPPD4GJwfeHnBKyQTt8xt6QeytBRgYNhS16GZ3DyWP78zvLdvDcdfYkrQ19yoNwh1Ne6BTQajTr3ci1p6UDxFkab4LCQkJq9Li49XxwsWzmCLpz9p4GnXbzq3y2XF1Zr9kHpv9Nq3D2JnjLGRUu41wiDwGqoqFKKyy2D3hRWYS7MzsPgCEKCGv34iipoq8yNVc7twSmUvyQtH6q&#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\">interactive tutorial</a>.</p>\n<p><img src=\"/images/dynamic/image-1-.png\" alt=\"\"></p>\n<p>For a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2\">AimQL</a>, by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.</p>\n<p><img src=\"/images/dynamic/image-4-.png\" alt=\"\"></p>\n<h2><em><strong>What if numbers are simply not enough?</strong></em></h2>\n<p>Various machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.</p>\n<p>Aim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:</p>\n<pre><code>aim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n</code></pre>\n<p>You can find the complete guide to tracking in the¬†<a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#\">official documentation</a>.</p>\n<p>All the grouping/filtering/aggregation functional presented above is also available for Images.</p>\n<p><img src=\"/images/dynamic/image-2-.jpeg\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-3-.jpeg\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-5-.png\" alt=\"\"></p>\n<p>The Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.</p>\n<p><img src=\"/images/dynamic/image.gif\" alt=\"\"></p>\n<p>The grouping is rather flexible with a¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer\">multitude of options</a>.</p>\n<p>Another interesting thing that one can¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\">track is distributions</a>. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.</p>\n<pre><code>from aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n</code></pre>\n<p>Distributions can be accessed from the Single Run Page as well.</p>\n<p><img src=\"/images/dynamic/image-6-.png\" alt=\"\"></p>\n<p>You can play around with the image and single run explorers and see all the trackables across our numerous Demos (<a href=\"http://play.aimstack.io:10004/\">FS2</a>,¬†<a href=\"http://play.aimstack.io:10005/\">Spleen Segmentation</a>,¬†<a href=\"http://play.aimstack.io:10002/\">Lightweight GAN</a>,¬†<a href=\"http://play.aimstack.io:10001/\">Machine Translation</a>).</p>\n<p><em><strong>One Run to rule them all</strong></em></p>\n<p>There are times when a researcher would need to focus upon only a¬†<a href=\"https://aimstack.io/blog/new-releases/aim-3-7-%E2%80%94-revamped-run-single-page-and-aim-docker-image\">single run of the experiment</a>, where he can iterate through a complete list of all the things he tracked. Aim has a dedicated¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\">Single Run Page</a>¬†for this very purpose.</p>\n<p>You can view all the trackables in the following Tabs:</p>\n<ul>\n<li>Parameters/ Hyperparametrs ‚Äì Everything tracked regarding the experiment</li>\n<li>Metrics ‚Äì All the Tracked Metrics/Losses etc.</li>\n<li>System ‚Äì All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)</li>\n<li>Distributions ‚Äì All the distributions tracked (i.e. Flowing gradients and weights)</li>\n<li>Images ‚Äì All the Image objects saved</li>\n<li>Audios ‚Äì All the Audio objects saved</li>\n<li>Texts ‚Äì All the raw text tracked during experimentation</li>\n<li>Figures ‚Äì All the¬†<code>matplotlin</code>/<code>Plotly</code>¬†etc. Figures</li>\n<li>Settings ‚Äì Settings that runs share</li>\n</ul>\n<p>Looking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form</p>\n<p><img src=\"/images/dynamic/image-7-.png\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-8-.png\" alt=\"\"></p>\n<p><img src=\"/images/dynamic/image-9-.png\" alt=\"\"></p>\n<p>You can look aroud the individual runs in one of our interactive Demos (<a href=\"http://play.aimstack.io:10004/\">FS2</a>,¬†<a href=\"http://play.aimstack.io:10005/\">Spleen Segmentation</a>,¬†<a href=\"http://play.aimstack.io:10002/\">Lightweight GAN</a>,¬†<a href=\"http://play.aimstack.io:10001/\">Machine Translation</a>).</p>\n<hr>\n<h2><em><strong>No More localhost. Track experiments remotely</strong></em></h2>\n<p>Aim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands</p>\n<pre><code>aim init\n# Tringgering aim server\naim server --repo &#x3C;REPO_PATH> --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo &#x3C;REPO_PATH> --host 0.0.0.0 --port some_open_port\n</code></pre>\n<p>Integrating this newly created¬†<a href=\"https://aimstack.io/blog/new-releases/aim-3-4-%E2%80%93-remote-tracking-alpha-sorting-deleting-runs\">remote tracking server</a>¬†within your experimentation is even easier.</p>\n<pre><code># This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n</code></pre>\n<p>After this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.</p>\n<p><img src=\"/images/dynamic/image-4-.jpeg\" alt=\"\"></p>\n<h2>Learn more</h2>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling¬† and inspiring.</p>\n<p>Try out¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>, join the¬†<a href=\"https://community.aimstack.io/\">Aim community,</a> share your feedback, open issues for new features, bugs.</p>\n<p>And don‚Äôt forget to leave¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>¬†a star on GitHub for support.</p>"},"_id":"posts/aim-from-zero-to-hero.md","_raw":{"sourceFilePath":"posts/aim-from-zero-to-hero.md","sourceFileName":"aim-from-zero-to-hero.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-from-zero-to-hero"},"type":"Post"},{"title":"Aim tutorial for Weights and Biases users","date":"2022-09-07T21:13:32.862Z","author":"Hovhannes Tamoyan","description":"Using Aim‚Äôs Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights & Biases (wandb) runs into the native format and explore them via Aim UI. In this blog/post we will go over the steps required to migrate wandb logs to Aim. ","slug":"aim-tutorial-for-weights-and-biases-users","image":"https://miro.medium.com/max/1400/1*PBF_k6VevuUrquadJ86vfA.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\n\nI have been using¬†[Aim](https://github.com/aimhubio/aim)¬†for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\n\nThere are lots of converters already available to migrate or use along with other experiment trackers:¬†[Aim Converters](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html).\n\nUsing Aim‚Äôs Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights & Biases (wandb) runs into the native format and explore them via¬†[Aim UI](https://aimstack.readthedocs.io/en/latest/ui/overview.html).\n\nIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\n\n# Example Project Setup\n\nLet‚Äôs take a look at a concrete example. We will be using¬†`keras-tuner`¬†to train a¬†`CNN`¬†on¬†`Cifar10`¬†dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be¬†`sample-team`:\n\n```\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n```\n\nModel creation, data loading and other parts can be done as:\n\n```\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n```\n\nThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let‚Äôs convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\n\n> *Note: During this project I noticed that the*¬†`wandb.keras.WandbCallback`¬†*was tracking the metrics on epoch end. For better experience we recommend using the*¬†`aim.keras_tuner.AimCallback`*: to track all the metrics on batch end.*\n\n# Converting runs from Wandb to Aim\n\nTo be able to explore Weights & Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\n\nPick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\n\n```\n$ aim init\n```\n\nTo start converting wandb experiments from entity/team:¬†`sample-team`¬†and project:¬†`my-awesome-project`¬†run:\n\n```\n$ aim convert wandb --entity sample-team --project my-awesome-project\n```\n\nThe converter will iterate over all the experiments in the project¬†`my-awesome-project`¬†and create a distinct Aim run for each experiment.\n\nFor more please see the¬†[‚ÄúShow Weights and Biases logs in Aim‚Äù](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim)¬†section in¬†[Aim docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nNow that we have our Aim repository initialized, we simply need to do:\n\n```\n$ aim up\n```\n\nto start the Aim UI.\n\n# Exploring the differences in Aim and wandb UIs\n\nTo see the configs, metadata and more on wandb we need to navigate to the ‚ÄúOverview‚Äù page:\n\n![](https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp)\n\nUnder the Config section, our logged configurations are shown. On the Summary page the metrics‚Äô latest results.\n\nThe wandb run ‚ÄúOverview‚Äù page counterpart on Aim UI is the individual run page, which can be accessed from the ‚ÄúRuns‚Äù page:\n\n![](https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp)\n\nMore information can be found when opening an individual run‚Äôs overview page.\n\n![](https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp)\n\nOn this page we can see the params, even the¬†`wandb_run_id`¬†and the¬†`wandb_run_name`¬†which are extracted from wandb runs during the conversion process.\n\nMore detailed information about the run can be found on each of the tabs, e.g. ‚ÄúRun Params‚Äù, ‚ÄúMetrics‚Äù, ‚ÄúSystem‚Äù etc.\n\n# Filtering and Grouping the Metrics\n\nOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\n\nTo see the tracked metrics and compare them with other runs we need to open the ‚ÄúWorkspace‚Äù page on wandb:\n\n![](https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp)\n\nTo see the metrics on Aim you need to navigate to the ‚ÄúMetrics‚Äù page. By default you will see an empty page with ‚Äúno results‚Äù message (which I hope they make this experience better soon), this is because we haven‚Äôt selected any metrics to show. To do that we can use the ‚Äú+ Metrics‚Äù button to open up the list of available metrics and select the desired metrics, e.g. ‚Äúloss‚Äù and ‚Äúaccuracy‚Äù:\n\n![](https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp)\n\nThis is the view we will get after selecting to show the loss and accuracy:\n\n![](https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp)\n\nNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan‚Äôt to see the plots by metric name, so each plot will represent a metric‚Äôs values.\n\n![](https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp)\n\nWe simply need to access the¬†`metric.name`¬†.\n\n![](https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp)\n\nTo have unique coloring for each run, we can use the ‚ÄúGroup by color‚Äù functionality, and provide the run.hash. Aim assigns a unique hash to each run.\n\n> *Note: in this example the Aim UI automatically assigns individual colors for each run.*\n\n\n\nNote: in this example the Aim UI automatically assigns individual colors for each run.)\n\n\\\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the ‚Äúadvanced search mode‚Äù click on the pen button under the search button:\n\n![](https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp)\n\nthis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\n\n```\n((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n```\n\nThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\n\n```\nmetric.name in [\"accuracy\", \"loss\"]\n```\n\nFor more please see the ‚Äú[Search Runs](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs)‚Äù page on docs.\n\n# Hyperparameter Explorer\n\nTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the ‚ÄúAdd Panel‚Äù button, and add the parameters and the metrics we need, e.g.:\n\n![](https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp)\n\nAim counterpart parameters explorer is on the page of ‚ÄúParams‚Äù. Where by using the ‚Äú+ Run Params‚Äù button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\n\n![](https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp)\n\n## Plot Operations\n\nThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‚Äòreadable‚Äô and explicit. On wandb one needs to click on ‚ÄúEdit panel‚Äù sign to see this view:\n\n![](https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp)\n\n\\\nOn Aim you can find the operations on the right sidebar of the Metrics page:\n\n![](https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp)\n\n# Conclusion\n\nAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I‚Äôve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please¬†[read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.","html":"<p>I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.</p>\n<p>I have been using¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>¬†for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).</p>\n<p>There are lots of converters already available to migrate or use along with other experiment trackers:¬†<a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html\">Aim Converters</a>.</p>\n<p>Using Aim‚Äôs Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights &#x26; Biases (wandb) runs into the native format and explore them via¬†<a href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\">Aim UI</a>.</p>\n<p>In this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.</p>\n<h1>Example Project Setup</h1>\n<p>Let‚Äôs take a look at a concrete example. We will be using¬†<code>keras-tuner</code>¬†to train a¬†<code>CNN</code>¬†on¬†<code>Cifar10</code>¬†dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be¬†<code>sample-team</code>:</p>\n<pre><code>import wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n</code></pre>\n<p>Model creation, data loading and other parts can be done as:</p>\n<pre><code>import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n</code></pre>\n<p>This code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let‚Äôs convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.</p>\n<blockquote>\n<p><em>Note: During this project I noticed that the</em>¬†<code>wandb.keras.WandbCallback</code>¬†<em>was tracking the metrics on epoch end. For better experience we recommend using the</em>¬†<code>aim.keras_tuner.AimCallback</code><em>: to track all the metrics on batch end.</em></p>\n</blockquote>\n<h1>Converting runs from Wandb to Aim</h1>\n<p>To be able to explore Weights &#x26; Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.</p>\n<p>Pick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:</p>\n<pre><code>$ aim init\n</code></pre>\n<p>To start converting wandb experiments from entity/team:¬†<code>sample-team</code>¬†and project:¬†<code>my-awesome-project</code>¬†run:</p>\n<pre><code>$ aim convert wandb --entity sample-team --project my-awesome-project\n</code></pre>\n<p>The converter will iterate over all the experiments in the project¬†<code>my-awesome-project</code>¬†and create a distinct Aim run for each experiment.</p>\n<p>For more please see the¬†<a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim\">‚ÄúShow Weights and Biases logs in Aim‚Äù</a>¬†section in¬†<a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim docs</a>.</p>\n<p>Now that we have our Aim repository initialized, we simply need to do:</p>\n<pre><code>$ aim up\n</code></pre>\n<p>to start the Aim UI.</p>\n<h1>Exploring the differences in Aim and wandb UIs</h1>\n<p>To see the configs, metadata and more on wandb we need to navigate to the ‚ÄúOverview‚Äù page:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp\" alt=\"\"></p>\n<p>Under the Config section, our logged configurations are shown. On the Summary page the metrics‚Äô latest results.</p>\n<p>The wandb run ‚ÄúOverview‚Äù page counterpart on Aim UI is the individual run page, which can be accessed from the ‚ÄúRuns‚Äù page:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp\" alt=\"\"></p>\n<p>More information can be found when opening an individual run‚Äôs overview page.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp\" alt=\"\"></p>\n<p>On this page we can see the params, even the¬†<code>wandb_run_id</code>¬†and the¬†<code>wandb_run_name</code>¬†which are extracted from wandb runs during the conversion process.</p>\n<p>More detailed information about the run can be found on each of the tabs, e.g. ‚ÄúRun Params‚Äù, ‚ÄúMetrics‚Äù, ‚ÄúSystem‚Äù etc.</p>\n<h1>Filtering and Grouping the Metrics</h1>\n<p>One of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.</p>\n<p>To see the tracked metrics and compare them with other runs we need to open the ‚ÄúWorkspace‚Äù page on wandb:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp\" alt=\"\"></p>\n<p>To see the metrics on Aim you need to navigate to the ‚ÄúMetrics‚Äù page. By default you will see an empty page with ‚Äúno results‚Äù message (which I hope they make this experience better soon), this is because we haven‚Äôt selected any metrics to show. To do that we can use the ‚Äú+ Metrics‚Äù button to open up the list of available metrics and select the desired metrics, e.g. ‚Äúloss‚Äù and ‚Äúaccuracy‚Äù:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp\" alt=\"\"></p>\n<p>This is the view we will get after selecting to show the loss and accuracy:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp\" alt=\"\"></p>\n<p>Now as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan‚Äôt to see the plots by metric name, so each plot will represent a metric‚Äôs values.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp\" alt=\"\"></p>\n<p>We simply need to access the¬†<code>metric.name</code>¬†.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp\" alt=\"\"></p>\n<p>To have unique coloring for each run, we can use the ‚ÄúGroup by color‚Äù functionality, and provide the run.hash. Aim assigns a unique hash to each run.</p>\n<blockquote>\n<p><em>Note: in this example the Aim UI automatically assigns individual colors for each run.</em></p>\n</blockquote>\n<p>Note: in this example the Aim UI automatically assigns individual colors for each run.)</p>\n<p><br>\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the ‚Äúadvanced search mode‚Äù click on the pen button under the search button:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp\" alt=\"\"></p>\n<p>this operation will automatically convert the selections into the form of a query. In our case the query should look like this:</p>\n<pre><code>((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n</code></pre>\n<p>The syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:</p>\n<pre><code>metric.name in [\"accuracy\", \"loss\"]\n</code></pre>\n<p>For more please see the ‚Äú<a href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs\">Search Runs</a>‚Äù page on docs.</p>\n<h1>Hyperparameter Explorer</h1>\n<p>To explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the ‚ÄúAdd Panel‚Äù button, and add the parameters and the metrics we need, e.g.:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp\" alt=\"\"></p>\n<p>Aim counterpart parameters explorer is on the page of ‚ÄúParams‚Äù. Where by using the ‚Äú+ Run Params‚Äù button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp\" alt=\"\"></p>\n<h2>Plot Operations</h2>\n<p>The operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‚Äòreadable‚Äô and explicit. On wandb one needs to click on ‚ÄúEdit panel‚Äù sign to see this view:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp\" alt=\"\"></p>\n<p><br>\nOn Aim you can find the operations on the right sidebar of the Metrics page:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp\" alt=\"\"></p>\n<h1>Conclusion</h1>\n<p>As much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I‚Äôve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please¬†<a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">read the docs</a>.</p>\n<p>Aim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.</p>"},"_id":"posts/aim-tutorial-for-weights-and-biases-users.md","_raw":{"sourceFilePath":"posts/aim-tutorial-for-weights-and-biases-users.md","sourceFileName":"aim-tutorial-for-weights-and-biases-users.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-tutorial-for-weights-and-biases-users"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":" Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out [‚Ä¶]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger used‚Ä¶ What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\n\n[Aim](https://github.com/aimhubio/aim)¬†is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n## What is XGBoost?\n\n[XGBoost](https://github.com/dmlc/xgboost)¬†is an optimized gradient boosting library with highly¬†¬†*efficient*,¬†¬†*flexible,*¬† and¬†¬†*portable*¬†design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\n## How to use Aim with XGBoost?\n\nCheck out end-to-end¬†[Aim integration](https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/)¬†examples with multiple frameworks¬†[here](https://github.com/aimhubio/aim/tree/main/examples). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\n\n```\n# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n```\n\nAs you can see on line 49,¬†AimCallback¬†is imported from¬†`aim.xgboost`¬†and passed to¬†`xgb.train`¬†as one of the callbacks. Aim session can open and close by the AimCallback and the¬†metrics and hparamsstore by XGBoost. In addition to that, thesystem measures¬†pass to Aim as well.\n\n## What it looks like?\n\nAfter you run the experiment and the¬†`aim up`¬†command in the¬†`aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png \"Aim UI dashboard page\")\n\nTo explore the run, we should:\n\n* Choose the¬†`xgboost_test`experiment.\n* Select the metrics to explore.\n* Divide into charts by metrics.\n\nFor example, the gif below illustrates the steps above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif)\n\n\\\n So this is what the final result looks like.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png)\n\nAs easy as that, we can analyze the runs and the system usage.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nIf you find Aim useful, support us and star¬†[the project](https://github.com/aimhubio/aim)¬†on GitHub. Also, join the¬†[Aim community](https://aimstack.slack.com/ssb/redirect)¬†and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is Aim?</h2>\n<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>¬†is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.</p>\n<h2>What is XGBoost?</h2>\n<p><a href=\"https://github.com/dmlc/xgboost\">XGBoost</a>¬†is an optimized gradient boosting library with highly¬†¬†<em>efficient</em>,¬†¬†<em>flexible,</em>¬† and¬†¬†<em>portable</em>¬†design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.</p>\n<h2>How to use Aim with XGBoost?</h2>\n<p>Check out end-to-end¬†<a href=\"https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/\">Aim integration</a>¬†examples with multiple frameworks¬†<a href=\"https://github.com/aimhubio/aim/tree/main/examples\">here</a>. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.</p>\n<pre><code># You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n</code></pre>\n<p>As you can see on line 49,¬†AimCallback¬†is imported from¬†<code>aim.xgboost</code>¬†and passed to¬†<code>xgb.train</code>¬†as one of the callbacks. Aim session can open and close by the AimCallback and the¬†metrics and hparamsstore by XGBoost. In addition to that, thesystem measures¬†pass to Aim as well.</p>\n<h2>What it looks like?</h2>\n<p>After you run the experiment and the¬†<code>aim up</code>¬†command in the¬†<code>aim_logs</code>directory, Aim UI will be running. When first opened, the dashboard page will come up.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png\" alt=\"\" title=\"Aim UI dashboard page\"></p>\n<p>To explore the run, we should:</p>\n<ul>\n<li>Choose the¬†<code>xgboost_test</code>experiment.</li>\n<li>Select the metrics to explore.</li>\n<li>Divide into charts by metrics.</li>\n</ul>\n<p>For example, the gif below illustrates the steps above.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif\" alt=\"\"></p>\n<p><br>\nSo this is what the final result looks like.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png\" alt=\"\"></p>\n<p>As easy as that, we can analyze the runs and the system usage.</p>\n<h2>Learn More</h2>\n<p><a href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>If you find Aim useful, support us and star¬†<a href=\"https://github.com/aimhubio/aim\">the project</a>¬†on GitHub. Also, join the¬†<a href=\"https://aimstack.slack.com/ssb/redirect\">Aim community</a>¬†and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"Exploring MLflow experiments with a powerful UI","date":"2023-01-17T21:52:43.983Z","author":"Gor Arakelyan","description":"We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! üéâ","slug":"exploring-mlflow-experiments-with-a-powerful-ui","image":"https://miro.medium.com/max/1400/1*YMlI66d-QPxI3fcQCoPSlw.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"\n\nWe are excited to announce the release of¬†[aimlflow](https://github.com/aimhubio/aimlflow), an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! üéâ\n\n[MLflow](https://github.com/mlflow/mlflow)¬†is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments.¬†[Aim](https://github.com/aimhubio/aim)¬†is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\n\n![](https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif)\n\n**With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.**\n\n![](https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp)\n\nIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let‚Äôs dive in and see how to make it happen.\n\n> Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\n>\n> View more on GitHub:¬†<https://github.com/aimhubio/aim>\n\n# Project overview\n\nWe use a simple project that trains a CNN using¬†[PyTorch](https://github.com/pytorch/pytorch)¬†and¬†[Ray Tune](https://github.com/ray-project/ray/tree/master/python/ray/tune)¬†on the¬†[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)¬†dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\n\n* **PyTorch**¬†for building and training the model\n* **Ray Tune**¬†for hyper-parameters tuning\n* **MLflow**¬†for experiment tracking\n\nFind the full project code on GitHub:¬†<https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning>\n\n![](https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp)\n\nRun the trainings by downloading and executing¬†`tune.py`¬†python file:\n\n```\npython tune.py\n```\n\nYou should see a similar output, meaning the trainings are successfully initiated:\n\n![](https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp)\n\n## Getting started with aimlflow\n\nAfter the hyper-parameter tuning is ran, let‚Äôs see how aimlflow can help us to explore the tracked experiments via UI.\n\nTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a¬†`.aim`¬†repo located on the file system.\n\n**This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn‚Äôt it amazing? ü§©**\n\n## 1. Install aimlflow on your machine\n\nIt is super easy to install aimlflow, simply run the following command:\n\n```\npip3 install aim-mlflow\n```\n\n## 2. Sync MLflow logs with Aim\n\nPick any directory on your file system and initialize a¬†`.aim`¬†repo:\n\n```\naim init\n```\n\nRun the¬†`aimlflow sync`¬†command to sync MLflow experiments with the Aim repo:\n\n```\naimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n```\n\n## 3. Run Aim\n\nNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\n\n```\naim up --repo=AIM_REPO_PATH\n```\n\nYou will see the following message on the terminal output:\n\n![](https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp)\n\nCongratulations! Now you can explore the training logs with Aim. üéâ\n\n# Quick tour of Aim for MLflow users\n\nIn this section, we will take a quick tour of Aim‚Äôs features, including:\n\n* Exploring hyper-parameters tuning results\n* Comparing tracked metrics\n* As well as, taking a look at the other capabilities Aim provides\n\n## Exploring MLflow experiments\n\nNow then Aim is set up and running, we navigate to the project overview page at¬†`127.0.0.1:43800`, where the summary of the project is displayed:\n\n* The number of tracked training runs and experiments\n* Statistics on the amount of tracked metadata\n* A list of experiments and tags, with the ability to quickly explore selected items\n* A calendar and feed of contributions\n* A table of in-progress trainings\n\n![](https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp \"Project overview\")\n\n\\\nTo view the results of the trainings, let‚Äôs navigate to the runs dashboard at¬†`127.0.0.1:43800/runs`. Here, you can see hyper-parameters and metrics results all of the trainings.\n\n![](https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp \" Runs dashboard\")\n\nWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\n\n![](https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp \"Run page\")\n\nOn this page, we can view the tracked hparams, including the¬†`mlflow_run_id`¬†and the¬†`mlflow_run_name`¬†which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\n\n## Comparing metrics\n\nComparing metrics across several runs is super easy with Aim:\n\n* Open the metrics page from the left sidebar\n* Select desired metrics by clicking on¬†`+ Metrics`¬†button\n* Pressing¬†`Search`¬†button on the top right corner\n\nWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\n\n![](https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp)\n\nAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\n\n* **by color**¬†‚Äî each group of metrics will be filled in with its unique color\n* **by stroke style**¬†‚Äî each group will have a unique stroke style (solid, dashed, etc)\n* **by facet**¬†‚Äî each group of metrics will be displayed in a separate subplot\n\nTo learn which set of trials performed the best, let‚Äôs apply several groupings:\n\n* Group by¬†`run.hparams.l1`¬†hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\n* Group by¬†`metric.name`¬†to divide losses and accuracies into separate subplots (this grouping is applied by default)\n\n![](https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp)\n\n**Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.**\n\nFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\n\n```\nrun.metrics['training_iteration'].last > 1\n```\n\n\n\n> Read more about Aim query language capabilities in the docs:¬†<https://aimstack.readthedocs.io/en/latest/using/search.html>\n\nThis will result in querying and displaying 9 matched runs:\n\n![](https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp)\n\nLet‚Äôs aggregate the groups to see which one performed the best:\n\n![](https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp)\n\nFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. üéâ\n\n> For more please see Aim official docs here:¬†<https://aimstack.readthedocs.io/en/latest/>\n\n## Last, but not least: a closer look at Aim‚Äôs key features\n\n* Use powerful pythonic search to select the runs you want to analyze:\n\n![](https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp)\n\nGroup metrics by hyperparameters to analyze hyperparameters‚Äô influence on run performance:\n\n![](https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp)\n\nSelect multiple metrics and analyze them side by side:\n\n![](https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp)\n\nAggregate metrics by std.dev, std.err, conf.interval:\n\n* ![](https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp)\n\n  Align x axis by any other metric:\n\n  * ![](https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp)\n\n  Scatter plots to learn correlations and trends:\n\n  ![](https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp)\n\n  High dimensional data visualization via parallel coordinate plot:\n\n  ![](https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp)\n\n  Explore media metadata, such as images and audio objects via Aim Explorers:\n\n  * ![](https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp \"Audio Explorer\")\n\n  ![](https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp \"Images Explorer\")\n\n  # Conclusion\n\n  In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\n\n  In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\n\n  We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please¬†[read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\n  # Learn more\n\n  If you have any questions join¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. üôå\n\n  Show some love by dropping a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"<p>We are excited to announce the release of¬†<a href=\"https://github.com/aimhubio/aimlflow\">aimlflow</a>, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! üéâ</p>\n<p><a href=\"https://github.com/mlflow/mlflow\">MLflow</a>¬†is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments.¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>¬†is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif\" alt=\"\"></p>\n<p><strong>With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.</strong></p>\n<p><img src=\"https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp\" alt=\"\"></p>\n<p>In this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let‚Äôs dive in and see how to make it happen.</p>\n<blockquote>\n<p>Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.</p>\n<p>View more on GitHub:¬†<a href=\"https://github.com/aimhubio/aim\">https://github.com/aimhubio/aim</a></p>\n</blockquote>\n<h1>Project overview</h1>\n<p>We use a simple project that trains a CNN using¬†<a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>¬†and¬†<a href=\"https://github.com/ray-project/ray/tree/master/python/ray/tune\">Ray Tune</a>¬†on the¬†<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a>¬†dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:</p>\n<ul>\n<li><strong>PyTorch</strong>¬†for building and training the model</li>\n<li><strong>Ray Tune</strong>¬†for hyper-parameters tuning</li>\n<li><strong>MLflow</strong>¬†for experiment tracking</li>\n</ul>\n<p>Find the full project code on GitHub:¬†<a href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\">https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning</a></p>\n<p><img src=\"https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp\" alt=\"\"></p>\n<p>Run the trainings by downloading and executing¬†<code>tune.py</code>¬†python file:</p>\n<pre><code>python tune.py\n</code></pre>\n<p>You should see a similar output, meaning the trainings are successfully initiated:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp\" alt=\"\"></p>\n<h2>Getting started with aimlflow</h2>\n<p>After the hyper-parameter tuning is ran, let‚Äôs see how aimlflow can help us to explore the tracked experiments via UI.</p>\n<p>To be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a¬†<code>.aim</code>¬†repo located on the file system.</p>\n<p><strong>This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn‚Äôt it amazing? ü§©</strong></p>\n<h2>1. Install aimlflow on your machine</h2>\n<p>It is super easy to install aimlflow, simply run the following command:</p>\n<pre><code>pip3 install aim-mlflow\n</code></pre>\n<h2>2. Sync MLflow logs with Aim</h2>\n<p>Pick any directory on your file system and initialize a¬†<code>.aim</code>¬†repo:</p>\n<pre><code>aim init\n</code></pre>\n<p>Run the¬†<code>aimlflow sync</code>¬†command to sync MLflow experiments with the Aim repo:</p>\n<pre><code>aimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n</code></pre>\n<h2>3. Run Aim</h2>\n<p>Now that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:</p>\n<pre><code>aim up --repo=AIM_REPO_PATH\n</code></pre>\n<p>You will see the following message on the terminal output:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp\" alt=\"\"></p>\n<p>Congratulations! Now you can explore the training logs with Aim. üéâ</p>\n<h1>Quick tour of Aim for MLflow users</h1>\n<p>In this section, we will take a quick tour of Aim‚Äôs features, including:</p>\n<ul>\n<li>Exploring hyper-parameters tuning results</li>\n<li>Comparing tracked metrics</li>\n<li>As well as, taking a look at the other capabilities Aim provides</li>\n</ul>\n<h2>Exploring MLflow experiments</h2>\n<p>Now then Aim is set up and running, we navigate to the project overview page at¬†<code>127.0.0.1:43800</code>, where the summary of the project is displayed:</p>\n<ul>\n<li>The number of tracked training runs and experiments</li>\n<li>Statistics on the amount of tracked metadata</li>\n<li>A list of experiments and tags, with the ability to quickly explore selected items</li>\n<li>A calendar and feed of contributions</li>\n<li>A table of in-progress trainings</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp\" alt=\"\" title=\"Project overview\"></p>\n<p><br>\nTo view the results of the trainings, let‚Äôs navigate to the runs dashboard at¬†<code>127.0.0.1:43800/runs</code>. Here, you can see hyper-parameters and metrics results all of the trainings.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp\" alt=\"\" title=\" Runs dashboard\"></p>\n<p>We can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp\" alt=\"\" title=\"Run page\"></p>\n<p>On this page, we can view the tracked hparams, including the¬†<code>mlflow_run_id</code>¬†and the¬†<code>mlflow_run_name</code>¬†which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.</p>\n<h2>Comparing metrics</h2>\n<p>Comparing metrics across several runs is super easy with Aim:</p>\n<ul>\n<li>Open the metrics page from the left sidebar</li>\n<li>Select desired metrics by clicking on¬†<code>+ Metrics</code>¬†button</li>\n<li>Pressing¬†<code>Search</code>¬†button on the top right corner</li>\n</ul>\n<p>We will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp\" alt=\"\"></p>\n<p>Aim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:</p>\n<ul>\n<li><strong>by color</strong>¬†‚Äî each group of metrics will be filled in with its unique color</li>\n<li><strong>by stroke style</strong>¬†‚Äî each group will have a unique stroke style (solid, dashed, etc)</li>\n<li><strong>by facet</strong>¬†‚Äî each group of metrics will be displayed in a separate subplot</li>\n</ul>\n<p>To learn which set of trials performed the best, let‚Äôs apply several groupings:</p>\n<ul>\n<li>Group by¬†<code>run.hparams.l1</code>¬†hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer</li>\n<li>Group by¬†<code>metric.name</code>¬†to divide losses and accuracies into separate subplots (this grouping is applied by default)</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp\" alt=\"\"></p>\n<p><strong>Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.</strong></p>\n<p>For example, in order to display only runs that were trained for more than one iteration, we will run the following query:</p>\n<pre><code>run.metrics['training_iteration'].last > 1\n</code></pre>\n<blockquote>\n<p>Read more about Aim query language capabilities in the docs:¬†<a href=\"https://aimstack.readthedocs.io/en/latest/using/search.html\">https://aimstack.readthedocs.io/en/latest/using/search.html</a></p>\n</blockquote>\n<p>This will result in querying and displaying 9 matched runs:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp\" alt=\"\"></p>\n<p>Let‚Äôs aggregate the groups to see which one performed the best:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp\" alt=\"\"></p>\n<p>From the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. üéâ</p>\n<blockquote>\n<p>For more please see Aim official docs here:¬†<a href=\"https://aimstack.readthedocs.io/en/latest/\">https://aimstack.readthedocs.io/en/latest/</a></p>\n</blockquote>\n<h2>Last, but not least: a closer look at Aim‚Äôs key features</h2>\n<ul>\n<li>Use powerful pythonic search to select the runs you want to analyze:</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp\" alt=\"\"></p>\n<p>Group metrics by hyperparameters to analyze hyperparameters‚Äô influence on run performance:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp\" alt=\"\"></p>\n<p>Select multiple metrics and analyze them side by side:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp\" alt=\"\"></p>\n<p>Aggregate metrics by std.dev, std.err, conf.interval:</p>\n<ul>\n<li>\n<p><img src=\"https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp\" alt=\"\"></p>\n<p>Align x axis by any other metric:</p>\n<ul>\n<li><img src=\"https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp\" alt=\"\"></li>\n</ul>\n<p>Scatter plots to learn correlations and trends:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp\" alt=\"\"></p>\n<p>High dimensional data visualization via parallel coordinate plot:</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp\" alt=\"\"></p>\n<p>Explore media metadata, such as images and audio objects via Aim Explorers:</p>\n<ul>\n<li><img src=\"https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp\" alt=\"\" title=\"Audio Explorer\"></li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp\" alt=\"\" title=\"Images Explorer\"></p>\n<h1>Conclusion</h1>\n<p>In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!</p>\n<p>In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.</p>\n<p>We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please¬†<a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">read the docs</a>.</p>\n<h1>Learn more</h1>\n<p>If you have any questions join¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. üôå</p>\n<p>Show some love by dropping a ‚≠êÔ∏è on¬†<a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you think Aim is useful.</p>\n</li>\n</ul>"},"_id":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","_raw":{"sourceFilePath":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileName":"exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-mlflow-experiments-with-a-powerful-ui"},"type":"Post"},{"title":"How to integrate aimlflow with your remote MLflow","date":"2023-01-30T06:25:58.163Z","author":"Hovhannes Tamoyan","description":"We are thrilled to unveil aimlflow, a tool that allows for a smooth integration of a robust experiment tracking UI with MLflow logs! üöÄ","slug":"how-to-integrate-aimlflow-with-your-remote-mlflow","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSCbDO5InRlBXTxzD3NVwQ.png","draft":false,"categories":["Tutorials"],"body":{"raw":"With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.\n\nWe have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:\n\nRunning Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here:¬†<https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e>\n\nIn this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.\n\n# Project overview\n\nWe will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.\n\nWe will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.\n\nFind the full project code on GitHub:¬†<https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning>\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png)\n\n# Server-side/Remote Configuration\n\nLet‚Äôs create a separate directory for the demo and name it¬†`mlflow-demo-remote`. After which download and run the¬†`tune.py`¬†python script from the Github repo to conduct the training sessions:\n\n```\n$ python tune.py\n```\n\nRay Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png)\n\nOnce started, mlflow will commence recording the results in the¬†`mlruns`¬†directory. Our remote directory will have the following structure:\n\n```\nmlflow-demo-remote\n‚îú‚îÄ‚îÄ tune.py\n‚îî‚îÄ‚îÄ mlruns\n    ‚îú‚îÄ‚îÄ ...\n```\n\nLet‚Äôs open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from the¬†`mlflow-demo-remote`¬†directory:\n\n```\n$ mlflow ui --host 0.0.0.0\n```\n\nBy default, the¬†`--host`¬†is set to¬†`127.0.0.1`, limiting access to the service to the local machine only. To expose it to external machines, set the host to¬†`0.0.0.0`.\n\nBy default, the system listens on port¬†`5000`.\n\nOne can set¬†`--backend-store-uri`¬†param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path of¬†`mlruns`¬†directory.\n\nUpon navigating to¬†`http://127.0.0.1:5000`, you will be presented with a page that looks similar to this:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png)\n\n# Synchronising MLflow Runs with Aim\n\n\n\nAfter successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.\n\nFirst, let‚Äôs move forward with the installation process of aimlflow. It‚Äôs incredibly easy to set up on your device, just execute the following command:\n\n```\n$ pip install aim-mlflow\n```\n\nAfter successfully installing aimlflow on your machine let‚Äôs create a directory named¬†`mlflow-demo-local`¬†where the¬†`.aim`repository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:\n\n```\n$ aim init\n```\n\nThis will establish an Aim repository in the present directory and it will be named¬†`.aim`.\n\nThis is how our local system directory will look like:\n\n```\nmlflow-demo-local\n‚îî‚îÄ‚îÄ .aim\n    ‚îú‚îÄ‚îÄ ...\n```\n\nIn order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into the¬†`.aim`¬†repository.\n\nTo begin the process of converting MLflow experiments from the the hosted url¬†`YOUR_REMOTE_IP:5000`¬†into the Aim repository¬†`.aim`, execute the following command from our local¬†`mlflow-demo-local`¬†directory:\n\n```\n$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n```\n\nThe converter will go through all experiments within the project and create a unique¬†`Aim`¬†run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.\n\nThis means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? ‚ò∫Ô∏è\n\nNow that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:\n\n```\n$ aim up\n```\n\nto open the user interface and explore our metrics and other information.\n\nFor further reading please referee to¬†[Aim documentation](https://aimstack.readthedocs.io/en/latest/)¬†where you will learn more about the superpowers of Aim.\n\n# Conclusion\n\n\n\nTo sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.\n\nIn this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.\n\nThe added value of Aim makes installing aimlflow and enabling the additional capability well worth it.\n\n# Learn more\n\nIf you have any questions join¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. üôå\n\nShow some love by dropping a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"<p>With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.</p>\n<p>We have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:</p>\n<p>Running Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here:¬†<a href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\">https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e</a></p>\n<p>In this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.</p>\n<h1>Project overview</h1>\n<p>We will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.</p>\n<p>We will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.</p>\n<p>Find the full project code on GitHub:¬†<a href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\">https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning</a></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png\" alt=\"\"></p>\n<h1>Server-side/Remote Configuration</h1>\n<p>Let‚Äôs create a separate directory for the demo and name it¬†<code>mlflow-demo-remote</code>. After which download and run the¬†<code>tune.py</code>¬†python script from the Github repo to conduct the training sessions:</p>\n<pre><code>$ python tune.py\n</code></pre>\n<p>Ray Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png\" alt=\"\"></p>\n<p>Once started, mlflow will commence recording the results in the¬†<code>mlruns</code>¬†directory. Our remote directory will have the following structure:</p>\n<pre><code>mlflow-demo-remote\n‚îú‚îÄ‚îÄ tune.py\n‚îî‚îÄ‚îÄ mlruns\n    ‚îú‚îÄ‚îÄ ...\n</code></pre>\n<p>Let‚Äôs open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from the¬†<code>mlflow-demo-remote</code>¬†directory:</p>\n<pre><code>$ mlflow ui --host 0.0.0.0\n</code></pre>\n<p>By default, the¬†<code>--host</code>¬†is set to¬†<code>127.0.0.1</code>, limiting access to the service to the local machine only. To expose it to external machines, set the host to¬†<code>0.0.0.0</code>.</p>\n<p>By default, the system listens on port¬†<code>5000</code>.</p>\n<p>One can set¬†<code>--backend-store-uri</code>¬†param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path of¬†<code>mlruns</code>¬†directory.</p>\n<p>Upon navigating to¬†<code>http://127.0.0.1:5000</code>, you will be presented with a page that looks similar to this:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png\" alt=\"\"></p>\n<h1>Synchronising MLflow Runs with Aim</h1>\n<p>After successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.</p>\n<p>First, let‚Äôs move forward with the installation process of aimlflow. It‚Äôs incredibly easy to set up on your device, just execute the following command:</p>\n<pre><code>$ pip install aim-mlflow\n</code></pre>\n<p>After successfully installing aimlflow on your machine let‚Äôs create a directory named¬†<code>mlflow-demo-local</code>¬†where the¬†<code>.aim</code>repository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:</p>\n<pre><code>$ aim init\n</code></pre>\n<p>This will establish an Aim repository in the present directory and it will be named¬†<code>.aim</code>.</p>\n<p>This is how our local system directory will look like:</p>\n<pre><code>mlflow-demo-local\n‚îî‚îÄ‚îÄ .aim\n    ‚îú‚îÄ‚îÄ ...\n</code></pre>\n<p>In order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into the¬†<code>.aim</code>¬†repository.</p>\n<p>To begin the process of converting MLflow experiments from the the hosted url¬†<code>YOUR_REMOTE_IP:5000</code>¬†into the Aim repository¬†<code>.aim</code>, execute the following command from our local¬†<code>mlflow-demo-local</code>¬†directory:</p>\n<pre><code>$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n</code></pre>\n<p>The converter will go through all experiments within the project and create a unique¬†<code>Aim</code>¬†run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.</p>\n<p>This means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? ‚ò∫Ô∏è</p>\n<p>Now that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:</p>\n<pre><code>$ aim up\n</code></pre>\n<p>to open the user interface and explore our metrics and other information.</p>\n<p>For further reading please referee to¬†<a href=\"https://aimstack.readthedocs.io/en/latest/\">Aim documentation</a>¬†where you will learn more about the superpowers of Aim.</p>\n<h1>Conclusion</h1>\n<p>To sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.</p>\n<p>In this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.</p>\n<p>The added value of Aim makes installing aimlflow and enabling the additional capability well worth it.</p>\n<h1>Learn more</h1>\n<p>If you have any questions join¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. üôå</p>\n<p>Show some love by dropping a ‚≠êÔ∏è on¬†<a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you think Aim is useful.</p>"},"_id":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","_raw":{"sourceFilePath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileName":"how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\n\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\n\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\n\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.¬†One thing you do not need when tuning hyper-parameters is variance.\n\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\n\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using¬†[Aim](https://github.com/aimhubio/aim).\n\n## **How to fix the seed in PyTorch Lightning**\n\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\n\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use¬†`pl.seed_everything(seed)`¬†. See the docs¬†[here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\n\n> Note: in other libraries you would use something like:¬†`np.random.seed()`¬†or¬†`torch.manual_seed()`¬†\n\n## **Implementation**\n\nFind the full code for this and other tutorials¬†[here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\n\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n```\n\n## Analyzing the Training Runs\n\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after¬†[Aim 1.3.5 release](https://aimstack.io/blog/new-releases/aim-1-3-5-%E2%80%94-activity-view-and-x-axis-alignment)) and have multiple charts of different metrics on the same screen.\n\nDo the following steps to see the different effects of the optimizers\n\n* Go to dashboard, explore by experiment\n* Add loss to¬†`SELECT`¬†and divide into subplots by metric\n* Group by experiment to make all metrics of similar color\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\n\nHere is how it looks on Aim:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png)\n\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\n\nIf you apply the same settings to the learning rate, this is the result:\n\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\n\n* Remove both previous groupings\n* Group by color by learning rate\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\n\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\n\nOn top of this, if we also add grouping by style by optimizer:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png)\n\nNow, it is obvious that the the runs with SGD optimizer and¬†`lr=0.01`¬†(green, broken lines) are the best choices for all the seeds we have tried.\n\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\n\n## Learn More\n\nIf you find Aim useful, support us and¬†[star the project](https://github.com/aimhubio/aim)¬†on GitHub. Join the¬†[Aim community](https://community.aimstack.io/)¬†and share more about your use-cases and how we can improve Aim to suit them.","html":"<h2>What is a random seed and how is it important?</h2>\n<p>The random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:</p>\n<ul>\n<li>Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.</li>\n<li>Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.</li>\n<li>Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).</li>\n</ul>\n<p>As you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.¬†One thing you do not need when tuning hyper-parameters is variance.</p>\n<p>The purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.</p>\n<p>Later in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>.</p>\n<h2><strong>How to fix the seed in PyTorch Lightning</strong></h2>\n<p>Fixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python<code>random.seed(seed)</code>, but it is not enough for PL.</p>\n<p>Pytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use¬†<code>pl.seed_everything(seed)</code>¬†. See the docs¬†<a href=\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\">here</a>.</p>\n<blockquote>\n<p>Note: in other libraries you would use something like:¬†<code>np.random.seed()</code>¬†or¬†<code>torch.manual_seed()</code>¬†</p>\n</blockquote>\n<h2><strong>Implementation</strong></h2>\n<p>Find the full code for this and other tutorials¬†<a href=\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\">here</a>.</p>\n<pre><code>import torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n</code></pre>\n<h2>Analyzing the Training Runs</h2>\n<p>After each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after¬†<a href=\"https://aimstack.io/blog/new-releases/aim-1-3-5-%E2%80%94-activity-view-and-x-axis-alignment\">Aim 1.3.5 release</a>) and have multiple charts of different metrics on the same screen.</p>\n<p>Do the following steps to see the different effects of the optimizers</p>\n<ul>\n<li>Go to dashboard, explore by experiment</li>\n<li>Add loss to¬†<code>SELECT</code>¬†and divide into subplots by metric</li>\n<li>Group by experiment to make all metrics of similar color</li>\n<li>Group by style by optimizer to see different optimizers on loss and accuracy and its effects</li>\n</ul>\n<p>Here is how it looks on Aim:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png\" alt=\"\"></p>\n<p>From the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.</p>\n<p>If you apply the same settings to the learning rate, this is the result:</p>\n<h2>For the next step to analyze how learning rate affects the experiments, do the following steps:</h2>\n<ul>\n<li>Remove both previous groupings</li>\n<li>Group by color by learning rate</li>\n</ul>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"></p>\n<p>As you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.</p>\n<p>We showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.</p>\n<p>On top of this, if we also add grouping by style by optimizer:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png\" alt=\"\"></p>\n<p>Now, it is obvious that the the runs with SGD optimizer and¬†<code>lr=0.01</code>¬†(green, broken lines) are the best choices for all the seeds we have tried.</p>\n<p>Fixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.</p>\n<h2>Learn More</h2>\n<p>If you find Aim useful, support us and¬†<a href=\"https://github.com/aimhubio/aim\">star the project</a>¬†on GitHub. Join the¬†<a href=\"https://community.aimstack.io/\">Aim community</a>¬†and share more about your use-cases and how we can improve Aim to suit them.</p>"},"_id":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"},{"title":"How YerevaNN researchers use Aim to compare 100s of models","date":"2022-05-31T16:29:36.844Z","author":"Gev Soghomonian","description":"YerevaNN is a non-profit computer science and mathematics research lab based in Yerevan, Armenia.","slug":"how-yerevann-researchers-use-aim-to-compare-100s-of-models","image":"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*lnmBXL_nhBGxOvjNO56-Rw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"*Thanks to¬†[Knarik Mheryan](https://medium.com/u/309b97b32187?source=post_page-----1a34ced2ced0--------------------------------)¬†and¬†[Hrant Khachatrian](https://medium.com/u/83126d4b3c1b?source=post_page-----1a34ced2ced0--------------------------------)for co-writing this blogpost.*\n\n[YerevaNN](https://yerevann.com/) is a non-profit computer science and mathematics research lab based in Yerevan, Armenia. The lab‚Äôs main research interests are focused around machine learning algorithms and machine learning for biomedical data. YerevaNN also spends significant effort on supervision of student projects in order to support the up and coming Armenian scholars.\n\n[YerevaNN¬†](https://yerevann.com/)collaborates with University of Southern California ‚Äî focused on low-resource learning problems, ENSAE / ParisTech on theoretical statistics problems, biologists and chemists from around the world on ML applications in various biomedical problems. YerevaNN has publications at ACL, CVPR, WMT, Annals of Statistics and Nature Scientific Data among others.\n\nWe traditionally onboard new teammates with a task to reproduce a well-known paper. This allows us to both introduce them to our internal systems, codebase, resources as well as it‚Äôs a great leeway into the problems they are going to focus on as part of their research at YerevaNN. This is a story about how Knarik Mheryan has gotten started at YerevaNN and how¬†[Aim](https://github.com/aimhubio/aim/)¬†has helped her in the process.\n\nThe Challenges\n\n## Model selection for cross-lingual transfer\n\nDeep learning has enabled high-accuracy solutions to a wide variety of NLP tasks.. As these models are trained primarily on English corpora, they aren‚Äôt readily applicable or available for other human languages (due to the lack of labeled data).\n\nOne approach to solve this problem is to pre-train a model on an unlabeled multilingual dataset in an unsupervised fashion, then fine-tune it only on English labeled data for the given NLP task. It turns out the model‚Äôs performance on the target languages is strong, even though the model is fine-tuned only on English data.\n\nModel selection is a crucial step in this approach. When fine-tuning on English data with different hyperparams and seeds we are left with a set of models. The challenge is to select the model with the highest performance (e.g. F1 score) in the target language.\n\n**In 2021, Yang Chen and Alan Ritter, have published a¬†[paper in EMNLP](https://arxiv.org/abs/2010.06127)¬†that tries to address this problem. Knarik Mheryan‚Äôs first task at YerevaNN has been to reproduce this work as part of the¬†[ML Reproducibility Challenge 2021](https://paperswithcode.com/rc2021).**\n\n## Reproducing the study\n\nA common tactic has been to select models that perform well on the English data. This paper shows that the accuracy of models on English data does not always correlate well with the target language performance. To reproduce this, we have fine-tuned 240 models on a Named Entity Recognition task.\n\nThe dimensionality of these experiments is high. So analyzing several 100 models is not a trivial task and it can take hours when comparing them with traditional tools. At YerevaNN we have been using Aim as it allows us to compare 1000s of high-dimensional (all metrics, hyperparams and other metadata) experiments in a few clicks.\n\n# The Solution\n\n## Using Params Explorer to compare models\n\nParams explorer allows us to directly compare the models with one view and with a few clicks. We have tracked the F1 scores for all models, languages. Using Aim‚Äôs¬†[context](https://aimstack.readthedocs.io/en/latest/understanding/concepts.html?highlight=context#sequence-context)¬†makes it easy to add additional metadata to the metrics (such as lang=‚Äùfr‚Äù) so there is no need to bake that data into the name.\n\nWe have selected two hyperparameters of interest and F1 scores of 5 languages of interest. This one view shows all the models (each curve is a model) at once and we can compare them relative to selected hyperparams and metrics. (see Figure 1)\n\nMore importantly, the plot below shows that the best performing model on English data (F1 lang=‚Äùen‚Äù) certainly doesn‚Äôt perform well on German, Spanish, Dutch and Chinese.¬†**The red bold curve represents the best model on the English dataset.**\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMiTQD1jlsycSLCgPV0hfw.png)\n\nFigure 1: Aim‚Äôs Params plot for hundreds of fine-tuned models. Each curve corresponds to a single model with some hyperparameters and random seeds. Each vertical axis shows the model‚Äôs hyperparameters or the F1 scores for some language (German, Spanish, Dutch, Chinese, English).\n\n\\\nIt took us only a few clicks to see this key insight.\n\n**And this makes a couple of hours of work into something that‚Äôs less than a minute.**\n\n# Qualitative metrics comparison with \n\n# [Metrics Explorer](<https://aimstack.io/blog/new-releases/aim-3-1-%E2%80%94-images-tracker-and-images-explorer>)\n\n## Catastrophic Forgetting on the best Eng-based model\n\nIn our pursuit of understanding why models behave that way, we have tracked F1 scores across 60 epochs for the model that performs best on the English dataset. The Figure 2 below shows the behavior of F1 scores for 5 different languages (German, Spanish, Dutch, Chinese and English).\n\nThe red line on Figure 2 represents the evaluation on English data. There is a clear difference in performance between the languages. Effectively Aim shows that the model overfits on English and what the ‚ÄòCatastrophic forgetting‚Äô looks like.\n\nNotably, the Chinese (orange) gets worse faster than the other languages. Possibly because there are major linguistic differences between Chinese and the rest of the languages.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTYunp3onfPgzAorAUed9A.png)\n\nFigure 2: Visualization of the Catastrophic forgetting phenomenon. Evaluations on 5 languages development datasets for 60 epochs after fine-tuning the model on the English training data. The languages are English (red), Dutch (blue), German (green), Spanish (violet), and Chinese (orange).\n\n## Proposed model selection algorithm analysis\n\nWe reproduce the paper in two phases.\n\n1. First, we have created candidate models by fine-tuning them on English labeled data and checked their performance on the proposed 5 target languages.\n2. Second, we are going to build the algorithm for model selection ‚Äî proposed in the paper and verify if it works better than the conventional methods.\n\n**Analyzing the Learning Rates**\n\nAim‚Äôs Metric Explorer has allowed us to analyze the fine-tuning process through all the runs. In Figure 3, each line is a run with a specific target language, hyperparameter and random seed. Here we group runs by learning rate giving them the same color.\n\nWith Aim we frequently group runs into different charts. In this case we have grouped the metrics into charts by language . This allows us to visualize the fine-tuning with different learning rates per language.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab3o77BJ0R8hbEezh7NrcA.png)\n\nFigure 3: Visualization of the fine-tuning process with different learning rates and random seeds for Spanish, German, Chinese, Dutch and English languages.\n\nWith one more click, we aggregate runs of the same group (grouped by learning rate through all seeds). The aggregation helps us to see the trends of the F1 metrics collected across epochs.\n\nFrom Figure 4 it is clear that all models with very small learning rates (red) barely learn anything in 20 epochs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiam-hSc66ISC3deKgMoUg.png)\n\nFigure 4: Visualization of the fine-tuning process aggregated by learning rates through different random seeds. Lines through highlighted intervals are medians from runs with different seeds. Each chart plots the fine-tuning process for a particular language (Spanish, German, Chinese, Dutch and English)\n\nIn this paper the authors suggest an alternative strategy for model selection named Learned Model Selection (LMS). LMS is a neural network based model that learns to score the compatibility between a fine-tuned model and a target language. According to the paper, this method consistently selects better models than English development data across the 4 target languages.\n\nFor this final step we have also calculated the LMS scores for each model and have used Aim Params Explorer to compare the new and old methods for their efficiency. By just selecting the params and metrics we have produced the Figure 5 on Aim. Each highlighted curve in Figure 5 corresponds to one of the model candidates. With the old method (with the English axis) the best candidate F1 on the English axis is 0.699. On the other hand, if we select by the new algorithm (with LMS score axis), the F1 score would be 0.71, which is indeed higher than with the English candidate model.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7Jpwdraw_XlSji_GTk2yA.png)\n\nFigure 5: Visualization of the final process of model selection. The most left axis is the model number (like id) from 0 to 59. ‚ÄòF1 lang=‚ÄùEnglish‚Äù‚Äò is the F1 score on the English development data. ‚ÄòF1 lang=‚ÄùGerman‚Äù‚Äò is the F1 score on the German test data. ‚ÄòLMS score‚Äô is the ranking score provided by LMS, the algorithm proposed in the paper, the higher the better.\n\nWith Aim our experiments analysis at YerevaNN has become order of magnitude faster due to intuitive, beautiful and fast UI and the Aim explorers. The team at YerevaNN uses Aim on a daily basis to analyze their experiments.\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nThis article was originally published on¬†[Aim Blog](https://aimstack.io/blog). Find more in depth guides and details of the newest releases there.","html":"<p><em>Thanks to¬†<a href=\"https://medium.com/u/309b97b32187?source=post_page-----1a34ced2ced0--------------------------------\">Knarik Mheryan</a>¬†and¬†<a href=\"https://medium.com/u/83126d4b3c1b?source=post_page-----1a34ced2ced0--------------------------------\">Hrant Khachatrian</a>for co-writing this blogpost.</em></p>\n<p><a href=\"https://yerevann.com/\">YerevaNN</a> is a non-profit computer science and mathematics research lab based in Yerevan, Armenia. The lab‚Äôs main research interests are focused around machine learning algorithms and machine learning for biomedical data. YerevaNN also spends significant effort on supervision of student projects in order to support the up and coming Armenian scholars.</p>\n<p><a href=\"https://yerevann.com/\">YerevaNN¬†</a>collaborates with University of Southern California ‚Äî focused on low-resource learning problems, ENSAE / ParisTech on theoretical statistics problems, biologists and chemists from around the world on ML applications in various biomedical problems. YerevaNN has publications at ACL, CVPR, WMT, Annals of Statistics and Nature Scientific Data among others.</p>\n<p>We traditionally onboard new teammates with a task to reproduce a well-known paper. This allows us to both introduce them to our internal systems, codebase, resources as well as it‚Äôs a great leeway into the problems they are going to focus on as part of their research at YerevaNN. This is a story about how Knarik Mheryan has gotten started at YerevaNN and how¬†<a href=\"https://github.com/aimhubio/aim/\">Aim</a>¬†has helped her in the process.</p>\n<p>The Challenges</p>\n<h2>Model selection for cross-lingual transfer</h2>\n<p>Deep learning has enabled high-accuracy solutions to a wide variety of NLP tasks.. As these models are trained primarily on English corpora, they aren‚Äôt readily applicable or available for other human languages (due to the lack of labeled data).</p>\n<p>One approach to solve this problem is to pre-train a model on an unlabeled multilingual dataset in an unsupervised fashion, then fine-tune it only on English labeled data for the given NLP task. It turns out the model‚Äôs performance on the target languages is strong, even though the model is fine-tuned only on English data.</p>\n<p>Model selection is a crucial step in this approach. When fine-tuning on English data with different hyperparams and seeds we are left with a set of models. The challenge is to select the model with the highest performance (e.g. F1 score) in the target language.</p>\n<p><strong>In 2021, Yang Chen and Alan Ritter, have published a¬†<a href=\"https://arxiv.org/abs/2010.06127\">paper in EMNLP</a>¬†that tries to address this problem. Knarik Mheryan‚Äôs first task at YerevaNN has been to reproduce this work as part of the¬†<a href=\"https://paperswithcode.com/rc2021\">ML Reproducibility Challenge 2021</a>.</strong></p>\n<h2>Reproducing the study</h2>\n<p>A common tactic has been to select models that perform well on the English data. This paper shows that the accuracy of models on English data does not always correlate well with the target language performance. To reproduce this, we have fine-tuned 240 models on a Named Entity Recognition task.</p>\n<p>The dimensionality of these experiments is high. So analyzing several 100 models is not a trivial task and it can take hours when comparing them with traditional tools. At YerevaNN we have been using Aim as it allows us to compare 1000s of high-dimensional (all metrics, hyperparams and other metadata) experiments in a few clicks.</p>\n<h1>The Solution</h1>\n<h2>Using Params Explorer to compare models</h2>\n<p>Params explorer allows us to directly compare the models with one view and with a few clicks. We have tracked the F1 scores for all models, languages. Using Aim‚Äôs¬†<a href=\"https://aimstack.readthedocs.io/en/latest/understanding/concepts.html?highlight=context#sequence-context\">context</a>¬†makes it easy to add additional metadata to the metrics (such as lang=‚Äùfr‚Äù) so there is no need to bake that data into the name.</p>\n<p>We have selected two hyperparameters of interest and F1 scores of 5 languages of interest. This one view shows all the models (each curve is a model) at once and we can compare them relative to selected hyperparams and metrics. (see Figure 1)</p>\n<p>More importantly, the plot below shows that the best performing model on English data (F1 lang=‚Äùen‚Äù) certainly doesn‚Äôt perform well on German, Spanish, Dutch and Chinese.¬†<strong>The red bold curve represents the best model on the English dataset.</strong></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMiTQD1jlsycSLCgPV0hfw.png\" alt=\"\"></p>\n<p>Figure 1: Aim‚Äôs Params plot for hundreds of fine-tuned models. Each curve corresponds to a single model with some hyperparameters and random seeds. Each vertical axis shows the model‚Äôs hyperparameters or the F1 scores for some language (German, Spanish, Dutch, Chinese, English).</p>\n<p><br>\nIt took us only a few clicks to see this key insight.</p>\n<p><strong>And this makes a couple of hours of work into something that‚Äôs less than a minute.</strong></p>\n<h1>Qualitative metrics comparison with</h1>\n<h1><a href=\"https://aimstack.io/blog/new-releases/aim-3-1-%E2%80%94-images-tracker-and-images-explorer\">Metrics Explorer</a></h1>\n<h2>Catastrophic Forgetting on the best Eng-based model</h2>\n<p>In our pursuit of understanding why models behave that way, we have tracked F1 scores across 60 epochs for the model that performs best on the English dataset. The Figure 2 below shows the behavior of F1 scores for 5 different languages (German, Spanish, Dutch, Chinese and English).</p>\n<p>The red line on Figure 2 represents the evaluation on English data. There is a clear difference in performance between the languages. Effectively Aim shows that the model overfits on English and what the ‚ÄòCatastrophic forgetting‚Äô looks like.</p>\n<p>Notably, the Chinese (orange) gets worse faster than the other languages. Possibly because there are major linguistic differences between Chinese and the rest of the languages.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTYunp3onfPgzAorAUed9A.png\" alt=\"\"></p>\n<p>Figure 2: Visualization of the Catastrophic forgetting phenomenon. Evaluations on 5 languages development datasets for 60 epochs after fine-tuning the model on the English training data. The languages are English (red), Dutch (blue), German (green), Spanish (violet), and Chinese (orange).</p>\n<h2>Proposed model selection algorithm analysis</h2>\n<p>We reproduce the paper in two phases.</p>\n<ol>\n<li>First, we have created candidate models by fine-tuning them on English labeled data and checked their performance on the proposed 5 target languages.</li>\n<li>Second, we are going to build the algorithm for model selection ‚Äî proposed in the paper and verify if it works better than the conventional methods.</li>\n</ol>\n<p><strong>Analyzing the Learning Rates</strong></p>\n<p>Aim‚Äôs Metric Explorer has allowed us to analyze the fine-tuning process through all the runs. In Figure 3, each line is a run with a specific target language, hyperparameter and random seed. Here we group runs by learning rate giving them the same color.</p>\n<p>With Aim we frequently group runs into different charts. In this case we have grouped the metrics into charts by language . This allows us to visualize the fine-tuning with different learning rates per language.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab3o77BJ0R8hbEezh7NrcA.png\" alt=\"\"></p>\n<p>Figure 3: Visualization of the fine-tuning process with different learning rates and random seeds for Spanish, German, Chinese, Dutch and English languages.</p>\n<p>With one more click, we aggregate runs of the same group (grouped by learning rate through all seeds). The aggregation helps us to see the trends of the F1 metrics collected across epochs.</p>\n<p>From Figure 4 it is clear that all models with very small learning rates (red) barely learn anything in 20 epochs.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiam-hSc66ISC3deKgMoUg.png\" alt=\"\"></p>\n<p>Figure 4: Visualization of the fine-tuning process aggregated by learning rates through different random seeds. Lines through highlighted intervals are medians from runs with different seeds. Each chart plots the fine-tuning process for a particular language (Spanish, German, Chinese, Dutch and English)</p>\n<p>In this paper the authors suggest an alternative strategy for model selection named Learned Model Selection (LMS). LMS is a neural network based model that learns to score the compatibility between a fine-tuned model and a target language. According to the paper, this method consistently selects better models than English development data across the 4 target languages.</p>\n<p>For this final step we have also calculated the LMS scores for each model and have used Aim Params Explorer to compare the new and old methods for their efficiency. By just selecting the params and metrics we have produced the Figure 5 on Aim. Each highlighted curve in Figure 5 corresponds to one of the model candidates. With the old method (with the English axis) the best candidate F1 on the English axis is 0.699. On the other hand, if we select by the new algorithm (with LMS score axis), the F1 score would be 0.71, which is indeed higher than with the English candidate model.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7Jpwdraw_XlSji_GTk2yA.png\" alt=\"\"></p>\n<p>Figure 5: Visualization of the final process of model selection. The most left axis is the model number (like id) from 0 to 59. ‚ÄòF1 lang=‚ÄùEnglish‚Äù‚Äò is the F1 score on the English development data. ‚ÄòF1 lang=‚ÄùGerman‚Äù‚Äò is the F1 score on the German test data. ‚ÄòLMS score‚Äô is the ranking score provided by LMS, the algorithm proposed in the paper, the higher the better.</p>\n<p>With Aim our experiments analysis at YerevaNN has become order of magnitude faster due to intuitive, beautiful and fast UI and the Aim explorers. The team at YerevaNN uses Aim on a daily basis to analyze their experiments.</p>\n<h1>Learn More</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå</p>\n<p>Try out¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>, join the¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features, bugs.</p>\n<p>This article was originally published on¬†<a href=\"https://aimstack.io/blog\">Aim Blog</a>. Find more in depth guides and details of the newest releases there.</p>"},"_id":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","_raw":{"sourceFilePath":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","sourceFileName":"how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models"},"type":"Post"},{"title":"Hugging the Chaos: Connecting Datasets to Trainings with Hugging Face and Aim","date":"2023-02-17T07:05:28.709Z","author":"Gor Arakelyan","description":"Combining Hugging Face and Aim to make machine learning experiments traceable, reproducible and easier to compare.","slug":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-luoNtZBpA1SGkTsHzCrkA.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"# The cost of neglecting experiments management\n\n\n\nWorking with large and frequently changing datasets is hard!\\\nYou can easily end up in a mess if you don‚Äôt have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.\n\nIn this article, we will explore how you can combine¬†[Hugging Face Datasets](https://github.com/huggingface/datasets)¬†and¬†[Aim](https://github.com/aimhubio/aim)¬†to make machine learning experiments traceable, reproducible and easier to compare.\n\nLet‚Äôs dive in and get started!\n\n# Hugging Face Datasets + Aim = ‚ù§\n\n[Hugging Face Datasets](https://github.com/huggingface/datasets)¬†is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, you‚Äôll never have to worry about manually loading and versioning your data again.\n\n[Aim](https://github.com/aimhubio/aim), on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png)\n\nHugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.\n\n# Project overview\n\nLet‚Äôs go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.\n\nHere‚Äôs what we‚Äôll be using:\n\n* [Hugging Face Datasets](https://github.com/huggingface/datasets)¬†to load and manage the dataset.\n* [Hugging Face Hub](http://huggingface.co/)¬†to host the dataset.\n* [PyTorch](https://github.com/pytorch/pytorch)¬†to build and train the model.\n* [Aim](https://github.com/aimhubio/aim)¬†to keep track of all the model and dataset metadata.\n\nOur dataset is going to be called ‚ÄúA-MNIST‚Äù ‚Äî a version of the ‚ÄúMNIST‚Äù dataset with extra samples added. We‚Äôll start with the original ‚ÄúMNIST‚Äù dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.\n\n# Dataset preparation\n\n## Uploading the dataset to Hub\n\n\n\nLet‚Äôs head over to Hugging Face Hub and create a new dataset repository called¬†[‚ÄúA-MNIST‚Äù](https://huggingface.co/datasets/gorar/A-MNIST)¬†to store our dataset.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png)\n\nCurrently, the repository is empty, so let‚Äôs upload the initial version of the dataset.\n\nWe‚Äôll use the original MNIST dataset, as the first version v1.0.0. To do this, we‚Äôll need to upload the dataset files along with the dataset loading script¬†`A-MNIST.py`. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:\n\n```\nclass AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n```\n\n*See the full script here:¬†<https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py>*\n\nWe‚Äôll have the following setup, including all the necessary git configurations and the dataset card:\n\n```\n- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n```\n\nLet‚Äôs commit the changes and push the dataset to the Hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png)\n\nAwesome! Now we‚Äôve got the first version of ‚ÄúA-MNIST‚Äù hosted on Hugging Face Hub!\n\n## Augmenting the dataset\n\n\n\nNext up, let‚Äôs add more images to the MNIST dataset using augmentation techniques.\n\nWe will rotate all the train images by 20 degree and append to the dataset:\n\n```\nrotated_images[i] = rotate(images[i], angle=20, reshape=False)\n```\n\nAs well as, update the ‚ÄúA-MNIST‚Äù dataset version to 1.1.0:\n\n```\nBUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n```\n\nAfter preparing the new train dataset, let‚Äôs commit the changes and push the upgraded version to the hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png)\n\nPerfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. üéâ\n\n# Training setup\n\nLet‚Äôs load the dataset using datasets python package. It‚Äôs really simple, just one line of code:\n\n```\ndataset = load_dataset(\"gorar/A-MNIST\")\n```\n\nOne of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:\n\n```\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n```\n\nWe‚Äôre ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:\n\n```\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n```\n\n*See the full code here:¬†<https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8>*\n\nWe‚Äôve got everything ready now. Let‚Äôs kick off the training by running:\n\n```\npython train.py\n```\n\nHurray, the training is underway! üèÉ\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png)\n\nIt would be tough to monitor my training progress on terminal. This is where Aim‚Äôs superpowers come into play.\n\n# Integrating Aim\n\n\n\n## Trainings tracking\n\n\n\nAim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:\n\n```\nimport aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n```\n\nFurthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.\n\nSince v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply import¬†`HFDataset`¬†module from Aim and you can track all of your dataset metadata with just a single line of code!\n\n```\nfrom aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n```\n\nAim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the dataset‚Äôs description, version, features, etc.\n\n# Experimentation\n\n## Conducting trainings\n\n\n\nWe‚Äôll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, we‚Äôll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.\n\nDatasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:\n\n```\n# Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n```\n\nWe will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.\n\n## Exploring training results via Aim\n\n\n\nNow that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aim‚Äôs UI, we need to run the¬†`aim up`¬†command. A following message would be printed on the terminal output meaning the UI is successfully running:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png)\n\nTo access the UI, let‚Äôs navigate to¬†`127.0.0.1:43800`¬†in the browser.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png)\n\nWe can see our trainings on the contributions map and in the activity feed. Let‚Äôs take a closer look at an individual run details by navigating to the run‚Äôs page on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png)\n\nAll of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.\n\nWe can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png)\n\nThe UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.\n\n## Comparing trainings via Aim\n\nLet‚Äôs find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics Explorer¬†`127.0.0.1:43800/metrics`.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png)\n\nAim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.\n\nLet‚Äôs group by¬†`run.dataset.dataset.meta.version`¬†to see how the models performed based on the dataset version they were trained on.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png)\n\n\\\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.\n\nNow, to improve visibility and better evaluate performance, let‚Äôs go ahead and smooth out these lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png)\n\nIt appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, let‚Äôs group metrics by the learning rate to learn its impact on models performance.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png)\n\nIt seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png)\n\nTo sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.\n\nWith just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! üéâ\n\n# Reproducibility\n\nHave you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.\n\nAim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!\n\n# Conclusion\n\nIn this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.\n\nWe compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.\n\nSimplifying our daily work with efficient stack helps us focus on what really matters ‚Äî getting the best results from our machine learning experiments.\n\n# Learn more\n\n\n\nCheck out the Aim + Datasets integration docs¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\\\nDatasets repo:¬†<https://github.com/huggingface/datasets>\\\nAim repo:¬†<https://github.com/aimhubio/aim>\n\nIf you have questions, join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\n\nDrop a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"<h1>The cost of neglecting experiments management</h1>\n<p>Working with large and frequently changing datasets is hard!<br>\nYou can easily end up in a mess if you don‚Äôt have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.</p>\n<p>In this article, we will explore how you can combine¬†<a href=\"https://github.com/huggingface/datasets\">Hugging Face Datasets</a>¬†and¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>¬†to make machine learning experiments traceable, reproducible and easier to compare.</p>\n<p>Let‚Äôs dive in and get started!</p>\n<h1>Hugging Face Datasets + Aim = ‚ù§</h1>\n<p><a href=\"https://github.com/huggingface/datasets\">Hugging Face Datasets</a>¬†is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, you‚Äôll never have to worry about manually loading and versioning your data again.</p>\n<p><a href=\"https://github.com/aimhubio/aim\">Aim</a>, on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png\" alt=\"\"></p>\n<p>Hugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.</p>\n<h1>Project overview</h1>\n<p>Let‚Äôs go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.</p>\n<p>Here‚Äôs what we‚Äôll be using:</p>\n<ul>\n<li><a href=\"https://github.com/huggingface/datasets\">Hugging Face Datasets</a>¬†to load and manage the dataset.</li>\n<li><a href=\"http://huggingface.co/\">Hugging Face Hub</a>¬†to host the dataset.</li>\n<li><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>¬†to build and train the model.</li>\n<li><a href=\"https://github.com/aimhubio/aim\">Aim</a>¬†to keep track of all the model and dataset metadata.</li>\n</ul>\n<p>Our dataset is going to be called ‚ÄúA-MNIST‚Äù ‚Äî a version of the ‚ÄúMNIST‚Äù dataset with extra samples added. We‚Äôll start with the original ‚ÄúMNIST‚Äù dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.</p>\n<h1>Dataset preparation</h1>\n<h2>Uploading the dataset to Hub</h2>\n<p>Let‚Äôs head over to Hugging Face Hub and create a new dataset repository called¬†<a href=\"https://huggingface.co/datasets/gorar/A-MNIST\">‚ÄúA-MNIST‚Äù</a>¬†to store our dataset.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png\" alt=\"\"></p>\n<p>Currently, the repository is empty, so let‚Äôs upload the initial version of the dataset.</p>\n<p>We‚Äôll use the original MNIST dataset, as the first version v1.0.0. To do this, we‚Äôll need to upload the dataset files along with the dataset loading script¬†<code>A-MNIST.py</code>. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:</p>\n<pre><code>class AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n</code></pre>\n<p><em>See the full script here:¬†<a href=\"https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\">https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py</a></em></p>\n<p>We‚Äôll have the following setup, including all the necessary git configurations and the dataset card:</p>\n<pre><code>- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n</code></pre>\n<p>Let‚Äôs commit the changes and push the dataset to the Hub.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png\" alt=\"\"></p>\n<p>Awesome! Now we‚Äôve got the first version of ‚ÄúA-MNIST‚Äù hosted on Hugging Face Hub!</p>\n<h2>Augmenting the dataset</h2>\n<p>Next up, let‚Äôs add more images to the MNIST dataset using augmentation techniques.</p>\n<p>We will rotate all the train images by 20 degree and append to the dataset:</p>\n<pre><code>rotated_images[i] = rotate(images[i], angle=20, reshape=False)\n</code></pre>\n<p>As well as, update the ‚ÄúA-MNIST‚Äù dataset version to 1.1.0:</p>\n<pre><code>BUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n</code></pre>\n<p>After preparing the new train dataset, let‚Äôs commit the changes and push the upgraded version to the hub.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png\" alt=\"\"></p>\n<p>Perfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. üéâ</p>\n<h1>Training setup</h1>\n<p>Let‚Äôs load the dataset using datasets python package. It‚Äôs really simple, just one line of code:</p>\n<pre><code>dataset = load_dataset(\"gorar/A-MNIST\")\n</code></pre>\n<p>One of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:</p>\n<pre><code>from datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n</code></pre>\n<p>We‚Äôre ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:</p>\n<pre><code>class ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n</code></pre>\n<p><em>See the full code here:¬†<a href=\"https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\">https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8</a></em></p>\n<p>We‚Äôve got everything ready now. Let‚Äôs kick off the training by running:</p>\n<pre><code>python train.py\n</code></pre>\n<p>Hurray, the training is underway! üèÉ</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png\" alt=\"\"></p>\n<p>It would be tough to monitor my training progress on terminal. This is where Aim‚Äôs superpowers come into play.</p>\n<h1>Integrating Aim</h1>\n<h2>Trainings tracking</h2>\n<p>Aim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:</p>\n<pre><code>import aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n</code></pre>\n<p>Furthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.</p>\n<p>Since v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply import¬†<code>HFDataset</code>¬†module from Aim and you can track all of your dataset metadata with just a single line of code!</p>\n<pre><code>from aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n</code></pre>\n<p>Aim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the dataset‚Äôs description, version, features, etc.</p>\n<h1>Experimentation</h1>\n<h2>Conducting trainings</h2>\n<p>We‚Äôll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, we‚Äôll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.</p>\n<p>Datasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:</p>\n<pre><code># Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n</code></pre>\n<p>We will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.</p>\n<h2>Exploring training results via Aim</h2>\n<p>Now that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aim‚Äôs UI, we need to run the¬†<code>aim up</code>¬†command. A following message would be printed on the terminal output meaning the UI is successfully running:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png\" alt=\"\"></p>\n<p>To access the UI, let‚Äôs navigate to¬†<code>127.0.0.1:43800</code>¬†in the browser.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png\" alt=\"\"></p>\n<p>We can see our trainings on the contributions map and in the activity feed. Let‚Äôs take a closer look at an individual run details by navigating to the run‚Äôs page on the UI.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png\" alt=\"\"></p>\n<p>All of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.</p>\n<p>We can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png\" alt=\"\"></p>\n<p>The UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.</p>\n<h2>Comparing trainings via Aim</h2>\n<p>Let‚Äôs find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics Explorer¬†<code>127.0.0.1:43800/metrics</code>.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png\" alt=\"\"></p>\n<p>Aim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.</p>\n<p>Let‚Äôs group by¬†<code>run.dataset.dataset.meta.version</code>¬†to see how the models performed based on the dataset version they were trained on.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png\" alt=\"\"></p>\n<p><br>\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.</p>\n<p>Now, to improve visibility and better evaluate performance, let‚Äôs go ahead and smooth out these lines.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png\" alt=\"\"></p>\n<p>It appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, let‚Äôs group metrics by the learning rate to learn its impact on models performance.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png\" alt=\"\"></p>\n<p>It seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png\" alt=\"\"></p>\n<p>To sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.</p>\n<p>With just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! üéâ</p>\n<h1>Reproducibility</h1>\n<p>Have you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.</p>\n<p>Aim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!</p>\n<h1>Conclusion</h1>\n<p>In this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.</p>\n<p>We compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.</p>\n<p>Simplifying our daily work with efficient stack helps us focus on what really matters ‚Äî getting the best results from our machine learning experiments.</p>\n<h1>Learn more</h1>\n<p>Check out the Aim + Datasets integration docs¬†<a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\">here</a>.<br>\nDatasets repo:¬†<a href=\"https://github.com/huggingface/datasets\">https://github.com/huggingface/datasets</a><br>\nAim repo:¬†<a href=\"https://github.com/aimhubio/aim\">https://github.com/aimhubio/aim</a></p>\n<p>If you have questions, join the¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå</p>\n<p>Drop a ‚≠êÔ∏è on¬†<a href=\"https://github.com/aimhubio/aim\">GitHub</a>, if you find Aim useful.</p>"},"_id":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","_raw":{"sourceFilePath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileName":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim"},"type":"Post"},{"title":"Reproducible forecasting with Prophet and Aim","date":"2023-03-14T07:19:42.802Z","author":"Davit Grigoryan","description":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. ","slug":"reproducible-forecasting-with-prophet-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaIDQvLbqy0KyVG22D0epw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.\n\n# What is Aim?\n\n[Aim](https://aimstack.io/)¬†is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n# What is Prophet?\n\n[Prophet](https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/)¬†is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.\n\nA key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.\n\n# Tracking Prophet experiments with Aim\n\nProphet isn‚Äôt trained like neural networks, so you can‚Äôt track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost we‚Äôre going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check out¬†[the repo](https://github.com/aimhubio/aim/tree/main/examples).\n\nIn the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.\n\n```\nfrom aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n```\n\nAdditionally, if you‚Äôre working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Here‚Äôs a simple code snippet doing just that:\n\n```\n# Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n```\n\nNow, the extra feature(s) will be tracked as a hyperparameter called¬†**extra_regressors**.\n\nTake a look at a simple, end-to-end example¬†[here](https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py).\n\n# Viewing experiment logs\n\nAfter running the experiment, we can view the logs by executing¬†`aim up`¬†from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png)\n\nAdditionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif)\n\nThese features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.\n\n# Using Aim‚Äôs Pythonic search to filter metrics and hyperparameters\n\n\n\nGiven the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if we‚Äôre working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aim‚Äôs¬†*pythonic search*¬†functionality can be super useful. Say we only want to see the models with MAE ‚â§ 0.25. We can go to the metrics section and search exactly like we would in Python, say¬†`((metric.name == \"mae\") and (metric.last <= 0.25))`¬†(the¬†**last**¬†part is meant for neural networks, where you might want to see the metric at the last epoch). Here‚Äôs a visual demonstration of this feature:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png)\n\nAs you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.\n\n# Conclusion\n\n\n\nTo sum up, Aim‚Äôs integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with its¬†*pythonic search*¬†functionality. All these features can make forecasting with Prophet a breeze!\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs.","html":"<p>You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.</p>\n<h1>What is Aim?</h1>\n<p><a href=\"https://aimstack.io/\">Aim</a>¬†is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.</p>\n<h1>What is Prophet?</h1>\n<p><a href=\"https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/\">Prophet</a>¬†is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.</p>\n<p>A key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.</p>\n<h1>Tracking Prophet experiments with Aim</h1>\n<p>Prophet isn‚Äôt trained like neural networks, so you can‚Äôt track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost we‚Äôre going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check out¬†<a href=\"https://github.com/aimhubio/aim/tree/main/examples\">the repo</a>.</p>\n<p>In the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.</p>\n<pre><code>from aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n</code></pre>\n<p>Additionally, if you‚Äôre working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Here‚Äôs a simple code snippet doing just that:</p>\n<pre><code># Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n</code></pre>\n<p>Now, the extra feature(s) will be tracked as a hyperparameter called¬†<strong>extra_regressors</strong>.</p>\n<p>Take a look at a simple, end-to-end example¬†<a href=\"https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py\">here</a>.</p>\n<h1>Viewing experiment logs</h1>\n<p>After running the experiment, we can view the logs by executing¬†<code>aim up</code>¬†from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png\" alt=\"\"></p>\n<p>Additionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif\" alt=\"\"></p>\n<p>These features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.</p>\n<h1>Using Aim‚Äôs Pythonic search to filter metrics and hyperparameters</h1>\n<p>Given the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if we‚Äôre working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aim‚Äôs¬†<em>pythonic search</em>¬†functionality can be super useful. Say we only want to see the models with MAE ‚â§ 0.25. We can go to the metrics section and search exactly like we would in Python, say¬†<code>((metric.name == \"mae\") and (metric.last &#x3C;= 0.25))</code>¬†(the¬†<strong>last</strong>¬†part is meant for neural networks, where you might want to see the metric at the last epoch). Here‚Äôs a visual demonstration of this feature:</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png\" alt=\"\"></p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png\" alt=\"\"></p>\n<p>As you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.</p>\n<h1>Conclusion</h1>\n<p>To sum up, Aim‚Äôs integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with its¬†<em>pythonic search</em>¬†functionality. All these features can make forecasting with Prophet a breeze!</p>\n<h1>Learn More</h1>\n<p><a href=\"https://aimstack.readthedocs.io/en/latest/overview.html\">Aim is on a mission to democratize AI dev tools.</a></p>\n<p>We have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå</p>\n<p>Try out¬†<a href=\"https://github.com/aimhubio/aim\">Aim</a>, join the¬†<a href=\"https://community.aimstack.io/\">Aim community</a>, share your feedback, open issues for new features and bugs.</p>"},"_id":"posts/reproducible-forecasting-with-prophet-and-aim.md","_raw":{"sourceFilePath":"posts/reproducible-forecasting-with-prophet-and-aim.md","sourceFileName":"reproducible-forecasting-with-prophet-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/reproducible-forecasting-with-prophet-and-aim"},"type":"Post"}]},"__N_SSG":true}