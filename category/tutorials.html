<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Access your category realted articles</title><meta name="robots" content="index,follow"/><meta name="description" content="Access your category realted articles"/><meta property="og:title" content="Access your category realted articles"/><meta property="og:description" content="Access your category realted articles"/><meta property="og:url" content="https://aimstack.io/"/><meta property="og:image" content="https://v4.aimstack.io/banner.png"/><meta property="og:image:alt" content="banner"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1224"/><meta property="og:image:height" content="724"/><meta property="og:site_name" content="Aimstack"/><meta name="next-head-count" content="14"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><style id="stitches">--sxs{--sxs:0 t-fsRyUS}@media{:root,.t-fsRyUS{--fonts-OpenSans:'Open Sans', sans-serif;--fonts-Lora:'Lora', serif;--fonts-Inconsolata:'Inconsolata', monospace;--fonts-base:var(--fonts-OpenSans);--colors-blue:#1093F2;--colors-blueHover:#0F7AC8;--colors-lightBlue:rgba(16, 147, 242, 0.1);--colors-lightBlueHover:rgba(16, 147, 242, 0.2);--colors-darkBlue:#0C1031;--colors-darkBlue500:rgba(12,16,49,0.5);--colors-bigStone:#191D3C;--colors-bigStoneHover:#313551;--colors-green:#14C89D;--colors-black:#000000;--colors-black003:rgba(0,0,0,0.03);--colors-black700:rgba(0,0,0,0.7);--colors-black800:rgba(0,0,0,0.8);--colors-white:#ffffff;--colors-white100:rgba(255,255,255,0.1);--colors-white500:rgba(255,255,255,0.5);--colors-white700:rgba(255,255,255,0.7);--colors-red:#CC231A;--colors-lightGrey:#F4F7F9;--colors-lightGreyHover:#ECEEF0;--colors-grey:#CFD3D6;--colors-darkGrey:#737379;--colors-darkGreyHover:#393940;--colors-primary:var(--colors-blue);--colors-primaryHover:var(--colors-blueHover);--colors-primaryLight:var(--colors-lightBlue);--colors-primaryLightHover:var(--colors-lightBlueHover);--colors-secondary:var(--colors-green);--colors-textColor:var(--colors-black);--colors-bgColor:var(--colors-darkBlue);--colors-danger:var(--colors-red);--fontSizes-1:14px;--fontSizes-2:16px;--fontSizes-3:18px;--fontSizes-4:20px;--fontSizes-5:22px;--fontSizes-6:24px;--fontSizes-7:32px;--fontSizes-8:44px;--fontSizes-9:64px;--fontSizes-10:68px;--fontSizes-baseSize:var(--fontSizes-2);--space-1:4px;--space-2:8px;--space-3:12px;--space-4:16px;--space-5:20px;--space-6:24px;--space-7:28px;--space-8:32px;--space-9:36px;--space-10:40px;--space-11:44px;--space-12:48px;--fontWeights-1:400;--fontWeights-2:500;--fontWeights-3:600;--fontWeights-4:700;--fontWeights-5:800;--shadows-1:0px 60px 66px -65px rgba(11, 47, 97, 0.2);--shadows-2:0px 96px 66px -65px rgba(11, 47, 97, 0.2);--shadows-3:1px 1px 10px 3px  rgb(0, 0, 0, 10%);--shadows-4:0px 10px 24px -10px rgba(11, 47, 97, 0.4);--radii-1:6px;--radii-2:8px;--transitions-main:0.2s ease-out}}--sxs{--sxs:1 bptdzO k-iKtTFk}@media{html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,main,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section{display:block}*[hidden]{display:none}*{box-sizing:border-box}ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:none}table{border-spacing:0}strong{font-weight:var(--fontWeights-5)}body{background:var(--colors-white);color:var(--colors-textColor);font-family:var(--fonts-base);font-size:var(--fontSizes-baseSize);line-height:1.35;overscroll-behavior:none}a{text-decoration:none;color:inherit}a.link{color:var(--colors-primary);font-weight:var(--fontWeights-4);transition:var(--transitions-main)}a.link:hover{color:var(--colors-primaryHover)}.text-center{text-align:center}@keyframes k-iKtTFk{0%{opacity:0}10%{opacity:1}90%{opacity:1}100%{opacity:0}}}--sxs{--sxs:2 c-iSkjJi c-guYHoi c-fnKlzS c-hCkBfr c-ePqJJt c-kFnRKY c-BqIMD c-iCFsHS c-kPczbf c-lizetl c-fOPBY c-jCnBs c-iKzMen c-PJLV c-doWOai c-dhzjXW c-cZmHrB c-kSnMQa c-kXnqgD c-fgXcmv c-gPAtaN c-dMnFVR c-kHrbHn c-hJzboU c-dRMHzO c-FsNLu c-jgJYki c-bFmrFE c-MNZuo c-cChYXC c-jcfAOr c-jYGtqD c-fKjLcl c-fXjjoT c-gGXXkN c-hnRRWM c-hQOWqi c-jiSXep c-fRHVpw c-bzrsgE c-bMqEGV c-cBxMqI c-cgVSTP c-cPiarr c-jsGLMb c-fVgYPG c-fxrEBZ c-hTKfPd c-kBUzHM c-hePpyc c-bJTNKo c-kqsoHo c-eCJxrf c-hMVjTK c-cQwwIn c-fagtgx c-gsaXpY c-eOYVvF c-fWgRbD c-gswrlc c-jVwFOG c-eYYUfS c-depOeu c-dOghFk c-ckoDwU c-covwPd c-cSRvhF c-bcvtmY c-cnJPJj c-bjrmKQ c-hdKdh c-eVsMjz}@media{.c-iSkjJi{position:relative}.c-iSkjJi .bg-top{z-index:2}.c-iSkjJi .bg-bottom{z-index:1;position:absolute;bottom:300px;left:0;right:0;object-fit:contain;width:100%;height:60%}.c-guYHoi{position:relative;z-index:3;display:flex;flex-direction:column;min-height:100vh}.c-fnKlzS{height:72px;position:fixed;top:0px;left:0px;right:0px;z-index:99;transition:var(--transitions-main)}.c-fnKlzS.dark{background-color:var(--colors-darkBlue)}.c-fnKlzS.dark a{color:var(--colors-white)}.c-fnKlzS.dark.fixed a{color:var(--colors-textColor)}.c-fnKlzS.fixed{box-shadow:var(--shadows-3);background-color:var(--colors-white)}.c-hCkBfr{margin-left:auto;margin-right:auto;padding-left:var(--space-6);padding-right:var(--space-6);width:100%;max-width:1300px}.c-ePqJJt{display:flex;align-items:center;height:100%}.c-kFnRKY{margin-right:50px}.c-kFnRKY .logo{max-width:158px;width:100%;display:block}@media (max-width: 1024px){.c-kFnRKY{position:relative;z-index:11}}.c-BqIMD{display:flex;justify-content:center}.c-BqIMD .nav-list{display:flex}.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-6)}@media (max-width: 1440px){.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-4)}}.c-BqIMD .nav-list li a{-webkit-backface-visibility:hidden;backface-visibility:hidden;display:inline-flex}.c-BqIMD .nav-list li a .text{transition:var(--transitions-main)}.c-BqIMD .nav-list li a:hover .text{opacity:.6}@media (max-width: 1024px){.c-BqIMD{position:fixed;top:72px;right:0;bottom:0;left:0;overflow-y:auto;z-index:10;height:0;transition:height 0.5s;background-color:var(--colors-white);flex-direction:column;justify-content:space-between}}@media (max-width: 1024px){.open .c-BqIMD{height:calc(100% - 72px)}}@media (max-width: 1024px){.c-BqIMD .nav-inner{width:100%;flex-direction:column;align-items:inherit;justify-content:space-between;padding-top:16px}}@media (max-width: 1024px){.c-BqIMD .nav-list{flex-direction:column;align-items:flex-start}}@media (max-width: 1024px){.c-BqIMD .nav-list > li{font-size:var(--fontSizes-2);font-weight:var(--fontWeights-3);width:100%}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a{display:block;padding:var(--space-3) var(--space-5)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a:active{background-color:var(--colors-primaryLight)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li:not(:last-child){margin-right:0}}.c-iCFsHS{display:inline-block;height:18px;line-height:18px;padding:0 4px;border-radius:2px;background-color:var(--colors-primary);color:var(--colors-white);font-weight:700;font-size:10px;margin-left:6px}.c-kPczbf{margin-left:auto}.c-kPczbf span span{display:flex;align-items:center}.c-kPczbf.desktop-btn span span{justify-content:flex-end}.c-lizetl{display:none}@media (max-width: 1024px){.c-lizetl{display:flex;justify-content:center;padding-top:var(--space-6);padding-bottom:var(--space-6)}}@media (max-width: 1024px){.c-lizetl > li{margin-right:var(--space-6)}}.c-fOPBY{display:none}@media (max-width: 1024px){.c-fOPBY{position:relative;z-index:11;display:inline-block;-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer}}.c-jCnBs{flex:1;padding-top:72px}.c-iKzMen{text-align:center;padding-top:80px;padding-bottom:80px}.c-doWOai{padding-top:80px;padding-bottom:80px;color:var(--colors-white);background-color:var(--colors-darkBlue);position:relative}@media (max-width: 1024px){.c-doWOai{padding-top:44px;padding-bottom:44px}}.c-dhzjXW{display:flex}.c-cZmHrB{flex:1}.c-kSnMQa{display:flex;flex-wrap:wrap;margin-left:-60px;margin-top:60px}@media (max-width: 743px){.c-kSnMQa{margin-left:0;margin-top:var(--space-8)}}.c-kXnqgD{width:calc((100% / 2) - 60px);margin-left:60px;margin-bottom:60px}@media (max-width: 743px){.c-kXnqgD{width:100%;margin-left:0;margin-bottom:var(--space-10)}}.c-fgXcmv{max-width:650px}.c-fgXcmv img{width:100%;height:auto}@media (max-width: 1024px){.c-fgXcmv{margin:0 auto}}.c-gPAtaN{text-align:center;padding-top:100px}.c-dMnFVR{display:flex;flex-wrap:wrap;margin-left:-40px;margin-top:100px}@media (max-width: 1024px){.c-dMnFVR{justify-content:center}}@media (max-width: 743px){.c-dMnFVR{margin-left:0}}.c-kHrbHn{width:calc((100% / 4) - 40px);margin-left:40px;margin-bottom:60px}@media (max-width: 1024px){.c-kHrbHn{width:calc((100% / 2) - 40px);margin-bottom:40px}}@media (max-width: 743px){.c-kHrbHn{width:100%;margin-left:0}}.c-kHrbHn .card-content{margin-top:var(--space-6)}@media (max-width: 743px){.c-kHrbHn .card-content{text-align:left;margin-top:0}}.c-hJzboU{-webkit-mask-size:100%;mask-size:100%;-webkit-mask-repeat:no-repeat}.c-hJzboU img{object-fit:contain;height:100%;width:100%}@media (max-width: 743px){.c-hJzboU{width:100px;min-width:100px;margin-right:var(--space-6)}}.c-dRMHzO{background-color:var(--colors-darkBlue)}.c-FsNLu{display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;color:var(--colors-white);padding-top:27px;padding-bottom:27px;border-bottom:1px solid var(--colors-white100)}.c-jgJYki{margin-right:var(--space-9)}.c-jgJYki img{display:block}@media (max-width: 743px){.c-jgJYki img{margin:0 auto}}@media (max-width: 1440px){.c-jgJYki{margin-right:var(--space-6)}}@media (max-width: 743px){.c-jgJYki{width:100%;margin-right:0;margin-bottom:var(--space-1);text-align:center}}.c-bFmrFE{display:flex;margin-right:auto}.c-bFmrFE li:not(:last-child){margin-right:var(--space-9)}@media (max-width: 1440px){.c-bFmrFE li:not(:last-child){margin-right:var(--space-4)}}.c-bFmrFE li{transition:var(--transitions-main)}.c-bFmrFE li:hover{opacity:.6}@media (max-width: 1024px){.c-bFmrFE{order:3;width:100%;margin-top:var(--space-7)}}@media (max-width: 1024px){.c-bFmrFE li:last-child{margin-right:0}}@media (max-width: 743px){.c-bFmrFE{order:2;width:100%;margin-bottom:var(--space-2);flex-direction:column;text-align:center}}@media (max-width: 743px){.c-bFmrFE li{margin-bottom:var(--space-6)}}@media (max-width: 743px){.c-bFmrFE li:not(:last-child){margin-right:0}}.c-MNZuo{display:flex}.c-MNZuo li:not(:last-child){margin-right:var(--space-6)}.c-MNZuo li a{transition:var(--transitions-main)}.c-MNZuo li a:hover{opacity:.6}@media (max-width: 743px){.c-MNZuo{margin:0 auto;order:3}}.c-cChYXC{padding-top:var(--space-4);padding-bottom:var(--space-4);color:var(--colors-white500);text-align:center}.c-jcfAOr{display:flex;flex-wrap:wrap;margin-left:-40px}@media (max-width: 1024px){.c-jcfAOr{margin-left:-54px}}@media (max-width: 575px){.c-jcfAOr{margin-left:0}}.c-jYGtqD{width:calc((100% / 3) - 40px);margin-left:40px;margin-bottom:40px}@media (max-width: 1024px){.c-jYGtqD{width:calc((100% / 2) - 54px);margin-left:54px;margin-bottom:54px}}@media (max-width: 575px){.c-jYGtqD{width:100%;margin-left:0;margin-bottom:48px}}.c-fKjLcl{position:relative;height:0;padding-bottom:65%}.c-fXjjoT{padding-top:50px;position:relative}.c-gGXXkN{margin-bottom:var(--space-2);display:flex;align-items:center;color:var(--colors-grey);transition:var(--transitions-main);position:absolute;top:var(--space-6)}.c-gGXXkN .icon{margin-right:var(--space-1);fill:var(--colors-grey);transition:var(--transitions-main)}.c-gGXXkN:hover{color:var(--colors-darkGray)}.c-gGXXkN:hover .icon{fill:var(--colors-darkGray)}.c-hnRRWM{display:flex;align-items:center;justify-content:flex-end}.c-hnRRWM .icon{margin-right:var(--space-1);fill:var(--colors-grey)}.c-hQOWqi{margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-jiSXep{display:flex;align-items:center;justify-content:center}.c-jiSXep li{height:44px;width:44px;font-size:var(--fontSizes-1);font-weight:var(--fontWeights-4);text-align:center;margin-right:var(--space-1)}.c-jiSXep li a{display:block;line-height:44px;transition:var(--transitions-main)}.c-jiSXep li a.active{background-color:var(--colors-primary);color:var(--colors-white)}.c-jiSXep li a:hover:not(.active){color:var(--colors-primary)}.c-jiSXep li a:hover .icon{fill:var(--colors-primary)}.c-fRHVpw{padding:80px 0 64px;text-align:center}@media (max-width: 1024px){.c-fRHVpw{padding:44px 0}}@media (max-width: 743px){.c-fRHVpw{padding:44px 0 24px}}.c-bzrsgE{max-width:948px;width:100%;margin-left:auto;margin-right:auto;position:relative}.c-bzrsgE .github-btn{position:absolute;bottom:0;left:50%;translate:-50% 0}.c-bMqEGV{-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer;border-radius:var(--radii-1);display:inline-block;transition:var(--transitions-main)}.c-cBxMqI{height:150px;display:flex;background-color:var(--colors-white500);box-shadow:var(--shadows-1)}.c-cgVSTP{display:flex;align-items:center;overflow:visible !important}.c-cgVSTP img{width:95px !important}.c-cPiarr{padding:64px 0 150px}@media (max-width: 1024px){.c-cPiarr{padding:44px 0 80px}}@media (max-width: 743px){.c-cPiarr{padding:24px 0 60px}}.c-jsGLMb{position:relative}.c-jsGLMb .yt-lite{border-radius:var(--radii-2)}.c-jsGLMb .yt-lite > .lty-playbtn{width:68px;height:48px;border:none;border-radius:25%}.c-jsGLMb iframe{position:absolute;width:100%;height:100%;top:0;left:0;border-radius:var(--radii-2);box-shadow:var(--shadows-2)}.c-fVgYPG{position:relative}.c-fVgYPG .float-text{float:left;width:45%}@media (max-width: 1024px){.c-fVgYPG .float-text{width:100%}}.c-fxrEBZ{overflow:hidden}.c-hTKfPd{width:47%;padding-top:80px;padding-bottom:var(--space-9);float:left}@media (max-width: 1024px){.c-hTKfPd{width:100%;float:none;text-align:center}}.c-kBUzHM{float:right;margin-left:80px}@media (max-width: 1024px){.c-kBUzHM{float:none;margin-left:0}}.c-hePpyc{padding-top:var(--space-5)}.c-bJTNKo{position:relative}.c-bJTNKo.light .hljs{background-color:var(--colors-white);color:var(--colors-textColor)}.c-bJTNKo button{position:absolute;right:12px;top:6px;background:none;border:none;cursor:pointer;height:42px;width:42px;transition:var(--transitions-main)}.c-bJTNKo button:hover{opacity:.6}.c-bJTNKo .copied{display:block;opacity:0;position:absolute;right:0;top:-20px;animation:k-iKtTFk 2.5s ease 0s alternate;color:var(--colors-secondary);font-size:var(--fontSizes-1);font-weight:var(--fontWeights-2);transition:var(--transitions-main);z-index:10}.c-bJTNKo .hljs{display:block;overflow-x:auto;color:#abb2bf;background:#282c34;margin-bottom:20px;border-radius:var(--radii-1);padding:var(--space-4)}.c-bJTNKo .hljs-comment,.c-bJTNKo .hljs-quote{color:#5c6370;font-style:italic}.c-bJTNKo .hljs-doctag,.c-bJTNKo .hljs-keyword,.c-bJTNKo .hljs-formula{color:#c678dd}.c-bJTNKo .hljs-section,.c-bJTNKo .hljs-name,.c-bJTNKo .hljs-selector-tag,.c-bJTNKo .hljs-deletion,.c-bJTNKo .hljs-subst{color:#e06c75}.c-bJTNKo .hljs-literal{color:#56b6c2}.c-bJTNKo .hljs-string,.c-bJTNKo .hljs-regexp,.c-bJTNKo .hljs-addition,.c-bJTNKo .hljs-attribute,.c-bJTNKo .hljs-meta-string{color:#98c379}.c-bJTNKo .hljs-built_in,.c-bJTNKo .hljs-class .hljs-title{color:#e6c07b}.c-bJTNKo .hljs-attr,.c-bJTNKo .hljs-variable,.c-bJTNKo .hljs-template-variable,.c-bJTNKo .hljs-type,.c-bJTNKo .hljs-selector-class,.c-bJTNKo .hljs-selector-attr,.c-bJTNKo .hljs-selector-pseudo,.c-bJTNKo .hljs-number{color:#d19a66}.c-bJTNKo .hljs-symbol,.c-bJTNKo .hljs-bullet,.c-bJTNKo .hljs-link,.c-bJTNKo .hljs-meta,.c-bJTNKo .hljs-selector-id,.c-bJTNKo .hljs-title{color:#61aeee}.c-bJTNKo .hljs-emphasis{font-style:italic}.c-bJTNKo .hljs-strong{font-weight:bold}.c-bJTNKo .hljs-link{text-decoration:underline}.c-kqsoHo{padding-top:150px;padding-bottom:150px}.c-kqsoHo .title{text-align:center;margin-bottom:100px}@media (max-width: 1024px){.c-kqsoHo{padding-top:80px;padding-bottom:80px}}@media (max-width: 1024px){.c-kqsoHo .title{margin-bottom:64px}}@media (max-width: 743px){.c-kqsoHo{padding-top:60px;padding-bottom:60px}}@media (max-width: 743px){.c-kqsoHo .title{margin-bottom:44px}}.c-eCJxrf{display:flex;align-items:center;justify-content:space-between}.c-eCJxrf .title-mobile{display:none}@media (max-width: 1024px){.c-eCJxrf{flex-wrap:wrap;align-items:flex-start}}@media (max-width: 1024px){.c-eCJxrf .title-mobile{display:block;width:100%}}@media (max-width: 743px){.c-eCJxrf{flex-direction:column;margin-bottom:60px}}.c-hMVjTK ul li{margin-bottom:var(--space-4);position:relative;padding-left:16px}.c-hMVjTK ul li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:var(--colors-secondary)}@media (max-width: 1024px){.c-hMVjTK ul{padding-top:var(--space-4)}}@media (max-width: 1024px){.c-hMVjTK{max-width:280px;width:100%}}@media (max-width: 1024px){.c-hMVjTK .title-desktop{display:none}}@media (max-width: 743px){.c-hMVjTK{max-width:100%;order:2}}.c-cQwwIn{display:flex;align-items:center;color:var(--colors-primary);font-family:var(--fonts-OpenSans600);text-decoration:none}.c-cQwwIn .icon{fill:var(--colors-primary);margin-left:var(--space-4)}.c-fagtgx{max-width:610px;width:100%;position:relative}.c-fagtgx img{width:100%;height:auto}@media (max-width: 1024px){.c-fagtgx{max-width:396px;margin-left:0}}@media (max-width: 743px){.c-fagtgx{max-width:100%;order:1}}.c-gsaXpY{padding:60px 0 100px;background-color:var(--colors-darkBlue);color:var(--colors-white)}@media (max-width: 1024px){.c-gsaXpY{padding:60px 0 26px}}.c-eOYVvF{display:flex;flex-wrap:wrap;margin-left:-24px}@media (max-width: 1024px){.c-eOYVvF{margin-left:-54px}}@media (max-width: 575px){.c-eOYVvF{margin-left:0}}.c-fWgRbD{width:calc((100% / 4) - 24px);margin-left:24px;margin-bottom:24px}.c-fWgRbD a{background-color:var(--colors-bigStone);display:block;height:100%;border-radius:var(--radii-1);transition:var(--transitions-main)}.c-fWgRbD a .inner{padding:var(--space-6)}.c-fWgRbD a:hover{background-color:var(--colors-bigStoneHover)}.c-fWgRbD img{display:block;border-radius:var(--radii-1) var(--radii-1) 0 0}@media (max-width: 1024px){.c-fWgRbD{width:calc((100% / 2) - 54px);margin-left:54px;margin-bottom:54px}}@media (max-width: 575px){.c-fWgRbD{width:100%;margin-left:0;margin-bottom:24px}}.c-gswrlc{padding:100px 0}.c-gswrlc .input{max-width:400px}@media (max-width: 1024px){.c-gswrlc{padding:80px 0;text-align:center}}@media (max-width: 1024px){.c-gswrlc .input{margin:0 auto}}@media (max-width: 743px){.c-gswrlc{padding:60px 0}}.c-jVwFOG{display:flex;align-items:center;justify-content:space-between}.c-eYYUfS{flex:1;margin-right:80px}@media (max-width: 1024px){.c-eYYUfS{margin-right:0}}.c-depOeu{position:relative;display:block}.c-depOeu .error-message{position:absolute;top:100%;transform:translateY(2px);color:var(--colors-danger);font-size:20px;padding-left: 12px}.c-depOeu input{border:none;border-radius:var(--radii-1);width:100%;transition:var(--transitions-main)}.c-dOghFk{flex:1}@media (max-width: 1024px){.c-dOghFk{display:none}}.c-ckoDwU{display:flex;flex:1;box-shadow:var(--shadows-4);border-radius:var(--radii-2);padding:var(--space-6);background-color:var(--colors-white)}.c-covwPd{margin-bottom:var(--space-6);padding-left:var(--space-5);list-style-type:disc;font-size:var(--fontSizes-2)}.c-covwPd li{color:var(--colors-secondary)}.c-covwPd li:not(:last-child){margin-bottom:var(--space-3)}.c-covwPd li span{color:var(--colors-black700)}.c-cSRvhF{position:relative}.c-cSRvhF img{position:static !important}.c-bcvtmY{margin:var(--space-10) auto;line-height:1.7}.c-bcvtmY h1,.c-bcvtmY h2,.c-bcvtmY h3,.c-bcvtmY h4,.c-bcvtmY h5,.c-bcvtmY h6{margin-top:var(--space-8);margin-bottom:var(--space-5)}.c-bcvtmY.blog-inner p,.c-bcvtmY.blog-inner ol,.c-bcvtmY.blog-inner ul,.c-bcvtmY.blog-inner a{font-family:var(--fonts-Lora)}.c-bcvtmY h1{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-3);border-bottom:1px solid var(--colors-grey)}@media (max-width: 743px){.c-bcvtmY h1{font-size:var(--fontSizes-7)}}.c-bcvtmY h2{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-bcvtmY h2{font-size:var(--fontSizes-6)}}.c-bcvtmY h3{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-bcvtmY h3{font-size:var(--fontSizes-4)}}.c-bcvtmY p{margin-bottom:var(--space-5);font-size:var(--fontSizes-4);color:var(--colors-black800)}.c-bcvtmY ol,.c-bcvtmY ul{margin-bottom:var(--space-5);padding-left:var(--space-10);font-size:var(--fontSizes-3)}.c-bcvtmY ol li >ul,.c-bcvtmY ul li >ul{list-style-type:circle}.c-bcvtmY ol li:not(:last-child),.c-bcvtmY ul li:not(:last-child){margin-bottom:var(--space-2)}.c-bcvtmY ul{list-style-type:disc}.c-bcvtmY ul :has(input){list-style-type:none}.c-bcvtmY a{text-decoration:underline;word-break:break-all}.c-bcvtmY a:hover{color:var(--colors-primary)}.c-bcvtmY img{max-width:100%;height:auto}.c-bcvtmY strong{font-weight:var(--fontWeights-4)}.c-bcvtmY blockquote{padding-left:var(--space-9);border-left:2px solid var(--colors-grey);font-size:var(--fontSizes-2);font-style:italic}.c-bcvtmY em,.c-bcvtmY q{font-style:italic}.c-bcvtmY pre{font-size:var(--fontSizes-2);font-family:var(--fonts-Inconsolata);line-height:1.7;overflow:auto;white-space:pre;word-break:normal;margin:0 0 var(--space-5);padding:var(--space-10);color:var(--colors-darkGreyHover);background-color:var(--colors-lightGrey);border:none;border-radius:2px}.c-bcvtmY code{white-space:pre-wrap}.c-bcvtmY table{margin-bottom:var(--space-4)}.c-bcvtmY table,.c-bcvtmY th,.c-bcvtmY td{border:1px solid var(--colors-grey)}.c-bcvtmY th,.c-bcvtmY td{padding:6px 13px}.c-bcvtmY iframe{max-width:100%}.c-cnJPJj{display:flex;justify-content:flex-end;margin-bottom:var(--space-10)}.c-cnJPJj button{height:40px;width:40px;background-color:var(--colors-lightGrey) !important;border-radius:var(--radii-1);margin-left:var(--space-3);transition:var(--transitions-main)}.c-cnJPJj button:hover[aria-label=twitter]{background-color:#43b1ff !important}.c-cnJPJj button:hover[aria-label=linkedin]{background-color:#0A66C2 !important}.c-cnJPJj button:hover[aria-label=facebook]{background-color:#6e8dd0 !important}.c-cnJPJj button:hover[aria-label=reddit]{background-color:#fb411c !important}.c-cnJPJj button:hover .icon{fill:var(--colors-white)}.c-bjrmKQ{padding:50px 0;border-top:2px solid var(--colors-lightGrey);border-bottom:2px solid var(--colors-lightGrey);margin-bottom:50px}.c-bjrmKQ a{transition:var(--transitions-main)}.c-bjrmKQ a .text,.c-bjrmKQ a .chevron-text{transition:var(--transitions-main)}.c-bjrmKQ a:hover .text{color:var(--colors-primary)}.c-hdKdh{flex:1;padding-right:var(--space-3);border-right:2px solid var(--colors-lightGrey)}.c-hdKdh a:hover .chevron-text{transform:translateX(15px)}@media (max-width: 743px){.c-hdKdh a:hover .chevron-text{transform:translateX(0)}}.c-eVsMjz{flex:1;text-align:right;padding-left:var(--space-3)}.c-eVsMjz a:hover .chevron-text{transform:translateX(-15px)}@media (max-width: 743px){.c-eVsMjz a:hover .chevron-text{transform:translateX(0)}}}--sxs{--sxs:3 c-PJLV-htjRrP-size-10 c-PJLV-dnvIlH-size-4 c-PJLV-ohsET-size-8 c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-PJLV-ypqAk-size-3 c-dhzjXW-iTKOFX-direction-column c-PJLV-cBLpOj-size-2 c-PJLV-EDrvg-size-1 c-dhzjXW-bICGYT-justify-center c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true c-PJLV-cllNQK-lineClamp-true c-bMqEGV-kwxnul-size-1 c-bMqEGV-dxfqfJ-variant-primary c-PJLV-pmxQl-size-9 c-depOeu-RBfUm-size-1 c-dhzjXW-irEjuD-align-stretch c-dhzjXW-dOBaZS-gap-12 c-dhzjXW-knmidH-justify-between c-bMqEGV-jDfpYu-variant-secondary c-dhzjXW-eKWVTQ-gap-5 c-dhzjXW-kdofoX-gap-2 c-PJLV-jszWhv-size-7 c-dhzjXW-bZmKkd-justify-end}@media{.c-PJLV-htjRrP-size-10{font-size:var(--fontSizes-10);font-weight:var(--fontWeights-5);line-height:1.25}@media (max-width: 1024px){.c-PJLV-htjRrP-size-10{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-htjRrP-size-10{font-size:var(--fontSizes-7)}}.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-4)}@media (max-width: 1024px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-3)}}@media (max-width: 743px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-2)}}.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-5);line-height:1.5}@media (max-width: 743px){.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-7)}}.c-dhzjXW-ejCoEP-direction-row{flex-direction:row}.c-dhzjXW-jroWjL-align-center{align-items:center}.c-dhzjXW-awKDG-justify-start{justify-content:flex-start}.c-dhzjXW-kVNAnR-wrap-noWrap{flex-wrap:nowrap}.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-3)}@media (max-width: 743px){.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-2)}}.c-dhzjXW-iTKOFX-direction-column{flex-direction:column}.c-PJLV-cBLpOj-size-2{font-size:var(--fontSizes-2)}.c-PJLV-EDrvg-size-1{font-size:var(--fontSizes-1)}.c-dhzjXW-bICGYT-justify-center{justify-content:center}.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-4)}}.c-PJLV-iMgaFX-truncate-true{max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.c-PJLV-cllNQK-lineClamp-true{display:-webkit-box;-webkit-line-clamp:var(---lineClamp);-webkit-box-orient:vertical;overflow:hidden}.c-bMqEGV-kwxnul-size-1{height:53px;line-height:53px;font-size:var(--fontSizes-3);padding-left:var(--space-6);padding-right:var(--space-6)}@media (max-width: 1024px){.c-bMqEGV-kwxnul-size-1{height:50px;line-height:50px}}.c-bMqEGV-dxfqfJ-variant-primary{background-color:var(--colors-primary);color:var(--colors-white)}.c-bMqEGV-dxfqfJ-variant-primary:hover{background-color:var(--colors-primaryHover)}.c-PJLV-pmxQl-size-9{font-size:var(--fontSizes-9);font-weight:var(--fontWeights-5);line-height:1.1}@media (max-width: 1024px){.c-PJLV-pmxQl-size-9{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-pmxQl-size-9{font-size:var(--fontSizes-7)}}.c-depOeu-RBfUm-size-1 input{padding:17px 24px;font-size:var(--fontSizes-2);background-color:var(--colors-lightGrey)}.c-depOeu-RBfUm-size-1 input:hover{background-color:var(--colors-lightGreyHover)}.c-dhzjXW-irEjuD-align-stretch{align-items:stretch}.c-dhzjXW-dOBaZS-gap-12{gap:var(--space-12)}.c-dhzjXW-knmidH-justify-between{justify-content:space-between}.c-bMqEGV-jDfpYu-variant-secondary{background-color:var(--colors-primaryLight);color:var(--colors-primary)}.c-bMqEGV-jDfpYu-variant-secondary:hover{background-color:var(--colors-primaryLightHover)}.c-dhzjXW-eKWVTQ-gap-5{gap:var(--space-5)}.c-dhzjXW-kdofoX-gap-2{gap:var(--space-2)}.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-2)}@media (max-width: 743px){.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-6)}}.c-dhzjXW-bZmKkd-justify-end{justify-content:flex-end}}--sxs{--sxs:4 c-dhzjXW-lelvPF-direction-columnReverse c-dhzjXW-cLKPGv-direction-row c-dhzjXW-ewYpoe-align-start c-dhzjXW-dAvekr-direction-column}@media{@media (max-width: 1024px){.c-dhzjXW-lelvPF-direction-columnReverse{flex-direction:column-reverse}}@media (max-width: 743px){.c-dhzjXW-cLKPGv-direction-row{flex-direction:row}}@media (max-width: 743px){.c-dhzjXW-ewYpoe-align-start{align-items:flex-start}}@media (max-width: 743px){.c-dhzjXW-dAvekr-direction-column{flex-direction:column}}}--sxs{--sxs:6 c-hCkBfr-ilkBNdM-css c-kPczbf-igSvvfr-css c-kPczbf-idOghFk-css c-fOPBY-ieBuAdh-css c-PJLV-ietOTGQ-css c-hCkBfr-ibrRWTZ-css c-hCkBfr-ikYMMba-css c-PJLV-igHOLBX-css c-PJLV-ijskIFF-css c-PJLV-ijpcBqa-css c-hJzboU-ieOwGK-css c-PJLV-idgasAY-css c-PJLV-ieFlRcC-css c-PJLV-iciGHnC-css c-hJzboU-icdsEyN-css c-hJzboU-iYSetQ-css c-hJzboU-iebhThe-css c-hJzboU-ifZMaZh-css c-hJzboU-ieVlAUk-css c-dhzjXW-iguWOGj-css c-PJLV-ieRZfPH-css c-PJLV-iUazGY-css c-PJLV-ikMWyde-css c-PJLV-iedNKou-css c-bMqEGV-idKitzl-css c-PJLV-ijfuMJQ-css c-PJLV-ieBSoMY-css c-PJLV-iiMrHxl-css c-cQwwIn-ihZnFyC-css c-PJLV-igjdJOs-css c-PJLV-ieNyba-css c-PJLV-ibZBPXN-css c-PJLV-iejLhMq-css c-bMqEGV-igyJvzA-css c-dhzjXW-ielgvHS-css c-dhzjXW-ieHGUxU-css c-dhzjXW-iejLhMq-css c-PJLV-idHAijf-css c-PJLV-ieXfYvc-css c-PJLV-ijoSYcG-css c-bMqEGV-icBLpOj-css c-dhzjXW-icNlJTv-css c-PJLV-ihjojsL-css c-dhzjXW-igyJvzA-css c-PJLV-ikaqJbg-css c-PJLV-idnFJmc-css}@media{.c-hCkBfr-ilkBNdM-css{height:100%}.c-kPczbf-igSvvfr-css{display:none}@media (max-width: 1024px){.c-kPczbf-igSvvfr-css{display:block;padding:var(--space-5)}}.c-kPczbf-idOghFk-css{flex:1}@media (max-width: 1024px){.c-kPczbf-idOghFk-css{display:none}}.c-fOPBY-ieBuAdh-css{margin-left:auto;padding:var(--space-3)}.c-PJLV-ietOTGQ-css{margin-bottom:var(--space-6)}.c-hCkBfr-ibrRWTZ-css{max-width:848px}.c-hCkBfr-ikYMMba-css{position:relative;z-index:2}@media (max-width: 1024px){.c-PJLV-igHOLBX-css{text-align:center}}.c-PJLV-ijskIFF-css{font-weight:var(--fontWeights-3);padding:var(--space-3) 0 var(--space-2)}.c-PJLV-ijpcBqa-css{color:var(--colors-white700)}.c-hJzboU-ieOwGK-css{-webkit-mask-image:url(/images/static/about-us/shapes/1.svg);mask-image:url(/images/static/about-us/shapes/1.svg)}.c-PJLV-idgasAY-css{font-weight:var(--fontWeights-4)}.c-PJLV-ieFlRcC-css{padding-top:var(--space-2);padding-bottom:var(--space-2);font-weight:var(--fontWeights-3)}.c-PJLV-iciGHnC-css{color:var(--colors-black700)}.c-hJzboU-icdsEyN-css{-webkit-mask-image:url(/images/static/about-us/shapes/2.svg);mask-image:url(/images/static/about-us/shapes/2.svg)}.c-hJzboU-iYSetQ-css{-webkit-mask-image:url(/images/static/about-us/shapes/3.svg);mask-image:url(/images/static/about-us/shapes/3.svg)}.c-hJzboU-iebhThe-css{-webkit-mask-image:url(/images/static/about-us/shapes/5.svg);mask-image:url(/images/static/about-us/shapes/5.svg)}.c-hJzboU-ifZMaZh-css{-webkit-mask-image:url(/images/static/about-us/shapes/6.svg);mask-image:url(/images/static/about-us/shapes/6.svg)}.c-hJzboU-ieVlAUk-css{-webkit-mask-image:url(/images/static/about-us/shapes/7.svg);mask-image:url(/images/static/about-us/shapes/7.svg)}.c-dhzjXW-iguWOGj-css{height:100vh}.c-PJLV-ieRZfPH-css{text-align:center;margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-PJLV-iUazGY-css{display:flex;align-items:center}.c-PJLV-ikMWyde-css{margin-top:var(--space-6);margin-bottom:var(--space-6);---lineClamp:3;font-family:var(--fonts-Lora)}.c-PJLV-iedNKou-css{margin-bottom:48px}.c-bMqEGV-idKitzl-css{margin-bottom:var(--space-12)}.c-PJLV-ijfuMJQ-css{opacity:0;transition:var(--transitions-main)}.c-PJLV-ieBSoMY-css span{background-color:var(--colors-secondary);color:var(--colors-white)}.c-PJLV-iiMrHxl-css{margin-bottom:var(--space-6)}.c-PJLV-iiMrHxl-css strong{font-weight:var(--fontWeights-5)}.c-cQwwIn-ihZnFyC-css{margin-top:var(--space-6);font-weight:var(--fontWeights-3)}.c-PJLV-igjdJOs-css{text-align:center}.c-PJLV-ieNyba-css{margin:0 auto 60px;text-align:center;max-width:560px}@media (max-width: 1024px){.c-PJLV-ieNyba-css{margin-bottom:44px}}.c-PJLV-ibZBPXN-css{margin-bottom:var(--space-2);font-weight:var(--fontWeights-4)}.c-PJLV-iejLhMq-css{margin-bottom:var(--space-4)}.c-bMqEGV-igyJvzA-css{margin-top:var(--space-6)}.c-dhzjXW-ielgvHS-css{padding-top:80px;padding-bottom:80px}.c-dhzjXW-ieHGUxU-css{padding-bottom:104px}.c-dhzjXW-iejLhMq-css{margin-bottom:var(--space-4)}.c-PJLV-idHAijf-css{font-weight:var(--fontWeights-4);margin-left:var(--space-4)}.c-PJLV-ieXfYvc-css{margin-bottom:var(--space-5);color:var(--colors-black700)}.c-PJLV-ijoSYcG-css{margin-bottom:var(--space-6);font-weight:var(--fontWeights-3)}.c-bMqEGV-icBLpOj-css{font-size:var(--fontSizes-2)}.c-dhzjXW-icNlJTv-css{margin-top:var(--space-10)}.c-PJLV-ihjojsL-css{font-weight:var(--fontWeights-3)}.c-dhzjXW-igyJvzA-css{margin-top:var(--space-6)}.c-PJLV-ikaqJbg-css{margin-top:var(--space-6);margin-bottom:var(--space-6);font-weight:var(--fontWeights-4)}.c-PJLV-idnFJmc-css{margin-top:var(--space-3);---lineClamp:2}}</style><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d52c472f627fec66.js" defer=""></script><script src="/_next/static/chunks/framework-ffee79c6390da51e.js" defer=""></script><script src="/_next/static/chunks/main-12fb79ef298a4077.js" defer=""></script><script src="/_next/static/chunks/pages/_app-fffd1f44f8a805e0.js" defer=""></script><script src="/_next/static/chunks/962-0a950ce516e89aa5.js" defer=""></script><script src="/_next/static/chunks/pages/category/%5Bslug%5D-3ec86f2d38f151b7.js" defer=""></script><script src="/_next/static/gQ9GDp-9lnline6LHZU5G/_buildManifest.js" defer=""></script><script src="/_next/static/gQ9GDp-9lnline6LHZU5G/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Inconsolata&family=Lora&family=Open+Sans:wght@400;500;600;700;800&display=swap">@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8aRk.woff) format('woff')}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuyJF.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjr0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsgH1y4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsg-1y4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgshZ1y4k.woff) format('woff')}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WRL2kXWdycuJDETf.woff) format('woff');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WRP2kXWdycuJDETf.woff) format('woff');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WR32kXWdycuJDA.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJMkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJFkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJOkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJPkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJBkqt8ndeYxZ0.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><main class="c-iSkjJi"><div class="c-guYHoi"><header class="c-fnKlzS"><div class="c-hCkBfr c-hCkBfr-ilkBNdM-css"><div class="c-ePqJJt"><div class="c-kFnRKY"><a class="logo" href="/"><img alt="AimStack" srcSet="/_next/static/chunks/images/images/static/main/logo_256_75.svg 1x, /_next/static/chunks/images/images/static/main/logo_384_75.svg 2x" src="/_next/static/chunks/images/images/static/main/logo_384_75.svg" width="156" height="37" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a></div><nav class="c-BqIMD"><div class="nav-inner"><ul class="nav-list"><li><a target="_self" class="" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" class="" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" class="" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" class="" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" class="" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" class="" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" class="" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" class="" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><div class="c-kPczbf c-kPczbf-igSvvfr-css"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div></div><ul class="c-lizetl"><li><a href="https://aimstack.slack.com/ssb/redirect#/shared-invite/email" rel="noopener noreferrer" target="_blank" aria-label="slack"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M380.907 539.056c-57.742 0-104.574 46.837-104.574 104.585v261.609c0 57.748 46.832 104.585 104.574 104.585s104.574-46.837 104.574-104.585v-261.609c-0.042-57.748-46.874-104.585-104.574-104.585z"></path><path d="M15.111 643.63c0 57.799 46.874 104.687 104.659 104.687s104.659-46.888 104.659-104.687v-104.685h-104.577c-0.042 0-0.042 0-0.082 0-57.785 0-104.659 46.886-104.659 104.685z"></path><path d="M381.003 14.167c-0.042 0-0.083 0-0.125 0-57.783 0-104.656 46.886-104.656 104.684s46.874 104.685 104.656 104.685h104.574v-104.685c0-0.041 0-0.124 0-0.207-0.042-57.715-46.791-104.477-104.449-104.477z"></path><path d="M118.88 485.95h262.080c57.784 0 104.657-46.892 104.657-104.697s-46.874-104.697-104.657-104.697h-262.080c-57.784 0-104.658 46.892-104.658 104.697s46.874 104.697 104.658 104.697z"></path><path d="M904.118 276.5c-57.702 0-104.454 46.767-104.454 104.49v104.905h104.573c57.788 0 104.658-46.892 104.658-104.698s-46.871-104.697-104.658-104.697c-0.040 0-0.080 0-0.119 0z"></path><path d="M538.444 118.795v262.365c0 57.741 46.834 104.573 104.577 104.573 57.737 0 104.573-46.832 104.573-104.573v-262.365c0-57.741-46.837-104.573-104.573-104.573-57.742 0-104.577 46.832-104.577 104.573z"></path><path d="M747.594 905.028c0-57.748-46.837-104.585-104.573-104.585h-104.577v104.67c0.042 57.702 46.834 104.499 104.577 104.499 57.737 0 104.573-46.837 104.573-104.585z"></path><path d="M905.068 538.944h-262.076c-57.788 0-104.659 46.886-104.659 104.685s46.871 104.687 104.659 104.687h262.076c57.788 0 104.658-46.888 104.658-104.687s-46.871-104.685-104.658-104.685z"></path></svg></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="twitter"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M322.14 927.976c386.322 0 597.681-320.14 597.681-597.678 0-9-0.199-18.2-0.604-27.2 41.116-29.734 76.601-66.565 104.782-108.76-38.292 17.037-78.95 28.164-120.579 33 43.833-26.275 76.654-67.553 92.381-116.18-41.24 24.439-86.339 41.679-133.363 50.98-31.685-33.666-73.577-55.957-119.199-63.427s-92.44 0.299-133.206 22.103c-40.766 21.805-73.211 56.432-92.324 98.527s-23.826 89.314-13.411 134.357c-83.5-4.19-165.188-25.881-239.767-63.667s-140.386-90.823-193.153-155.673c-26.819 46.239-35.025 100.955-22.952 153.027s43.521 97.594 87.952 127.313c-33.356-1.059-65.981-10.040-95.18-26.2v2.6c-0.030 48.525 16.745 95.562 47.475 133.116 30.729 37.552 73.515 63.309 121.085 72.886-30.899 8.451-63.328 9.685-94.78 3.6 13.424 41.731 39.54 78.228 74.706 104.405 35.166 26.171 77.626 40.712 121.454 41.591-74.408 58.449-166.322 90.155-260.94 90.004-16.78-0.027-33.543-1.056-50.2-3.083 96.122 61.666 207.937 94.424 322.14 94.359z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="linkedIn"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M948.201 0h-872.601c-41.8 0-75.6 33-75.6 73.8v876.199c0 40.801 33.8 74.001 75.6 74.001h872.601c41.796 0 75.799-33.2 75.799-73.802v-876.398c0-40.8-34.002-73.8-75.799-73.8zM303.8 872.602h-152v-488.802h152v488.802zM227.8 317.2c-48.8 0-88.2-39.4-88.2-88s39.4-88 88.2-88c48.6 0 88 39.4 88 88 0 48.4-39.4 88-88 88zM872.602 872.602h-151.802v-237.602c0-56.599-1.001-129.6-79.002-129.6-78.998 0-90.998 61.8-90.998 125.6v241.601h-151.6v-488.802h145.6v66.8h2c20.2-38.4 69.802-79 143.598-79 153.805 0 182.204 101.2 182.204 232.799v268.203z"></path></svg></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="fb"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M1024 512c0-282.77-229.228-512-512-512-282.77 0-512 229.23-512 512 0 255.551 187.23 467.371 432 505.782v-357.78h-130v-148.002h130v-112.8c0-128.32 76.44-199.2 193.391-199.2 56.001 0 114.608 10 114.608 10v126h-64.558c-63.602 0-83.445 39.47-83.445 80v96h142l-22.699 148.002h-119.302v357.78c244.77-38.411 432.003-250.231 432.003-505.782z"></path></svg></a></li></ul></nav><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div><button type="button" aria-label="menu" class="c-fOPBY c-fOPBY-ieBuAdh-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:black" d="M0 146.286c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.289 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM0 512.008c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.29 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM60.952 816.748c-33.663 0-60.952 27.29-60.952 60.956 0 33.661 27.289 60.951 60.952 60.951h902.097c33.661 0 60.951-27.29 60.951-60.951 0-33.667-27.29-60.956-60.951-60.956h-902.097z"></path></svg></button></div></div></header><div class="c-jCnBs"><div class="c-hCkBfr"><h1 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-ieRZfPH-css title">Category: <!-- -->Tutorials</h1><ul class="c-jcfAOr"><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/exploring-mlflow-experiments-with-a-powerful-ui"><img alt="Exploring MLflow experiments with a powerful UI" sizes="100vw" srcSet="/_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_640_75.webp 640w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_750_75.webp 750w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_828_75.webp 828w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_1080_75.webp 1080w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_1200_75.webp 1200w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_1920_75.webp 1920w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_2048_75.webp 2048w, /_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_3840_75.webp 3840w" src="/_next/static/chunks/images/max/1400/1*YMlI66d-QPxI3fcQCoPSlw_3840_75.webp" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/exploring-mlflow-experiments-with-a-powerful-ui">Exploring MLflow experiments with a powerful UI</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/exploring-mlflow-experiments-with-a-powerful-ui">We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/aim-tutorial-for-weights-and-biases-users"><img alt="Aim tutorial for Weights and Biases users" sizes="100vw" srcSet="/_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_640_75.webp 640w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_750_75.webp 750w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_828_75.webp 828w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_1080_75.webp 1080w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_1200_75.webp 1200w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_1920_75.webp 1920w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_2048_75.webp 2048w, /_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_3840_75.webp 3840w" src="/_next/static/chunks/images/max/1400/1*PBF_k6VevuUrquadJ86vfA_3840_75.webp" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/aim-tutorial-for-weights-and-biases-users">Aim tutorial for Weights and Biases users</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/aim-tutorial-for-weights-and-biases-users">Using Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights &amp; Biases (wandb) runs into the native format and explore them via Aim UI. In this blog/post we will go over the steps required to migrate wandb logs to Aim. </a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/3d-spleen-segmentation-with-monai-and-aim"><img alt="3D Spleen Segmentation with MONAI and Aim" sizes="100vw" srcSet="/_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_640_75.webp 640w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_750_75.webp 750w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_828_75.webp 828w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_1080_75.webp 1080w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_1200_75.webp 1200w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_1920_75.webp 1920w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_2048_75.webp 2048w, /_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_3840_75.webp 3840w" src="/_next/static/chunks/images/wp-content/uploads/2022/06/Medium_monai-1_3840_75.webp" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/3d-spleen-segmentation-with-monai-and-aim">3D Spleen Segmentation with MONAI and Aim</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/3d-spleen-segmentation-with-monai-and-aim">In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from http://medicaldecathlon.com/. This is a</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/aim-from-zero-to-hero"><img alt="Aim from Zero to Hero" sizes="100vw" srcSet="/_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_640_75.gif 640w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_750_75.gif 750w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_828_75.gif 828w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_1080_75.gif 1080w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_1200_75.gif 1200w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_1920_75.gif 1920w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_2048_75.gif 2048w, /_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_3840_75.gif 3840w" src="/_next/static/chunks/images/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D_3840_75.gif" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/aim-from-zero-to-hero">Aim from Zero to Hero</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/aim-from-zero-to-hero">In this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"><img alt="An end-to-end example of Aim logger used with XGBoost library" sizes="100vw" srcSet="/_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_640_75.webp 640w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_750_75.webp 750w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_828_75.webp 828w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_1080_75.webp 1080w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_1200_75.webp 1200w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_1920_75.webp 1920w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_2048_75.webp 2048w, /_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_3840_75.webp 3840w" src="/_next/static/chunks/images/wp-content/uploads/2022/02/xgboost_3840_75.webp" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/an-end-to-end-example-of-aim-logger-used-with-xgboost-library">An end-to-end example of Aim logger used with XGBoost library</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"> Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out […]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger used… What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. </a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"><img alt="How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim" sizes="100vw" srcSet="/_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_640_75.webp 640w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_750_75.webp 750w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_828_75.webp 828w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_1080_75.webp 1080w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_1200_75.webp 1200w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_1920_75.webp 1920w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_2048_75.webp 2048w, /_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_3840_75.webp 3840w" src="/_next/static/chunks/images/wp-content/uploads/2021/03/pytorch_3840_75.webp" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim">How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim">What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-jYGtqD"><div class="c-PJLV"><div class="c-fKjLcl"><a href="/blog/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"><img alt="Aim basics: using context and subplots to compare validation and test metrics" sizes="100vw" srcSet="/_next/static/chunks/images/images/dynamic/1_640_75.gif 640w, /_next/static/chunks/images/images/dynamic/1_750_75.gif 750w, /_next/static/chunks/images/images/dynamic/1_828_75.gif 828w, /_next/static/chunks/images/images/dynamic/1_1080_75.gif 1080w, /_next/static/chunks/images/images/dynamic/1_1200_75.gif 1200w, /_next/static/chunks/images/images/dynamic/1_1920_75.gif 1920w, /_next/static/chunks/images/images/dynamic/1_2048_75.gif 2048w, /_next/static/chunks/images/images/dynamic/1_3840_75.gif 3840w" src="/_next/static/chunks/images/images/dynamic/1_3840_75.gif" decoding="async" data-nimg="fill" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;object-position:top;color:transparent"/></a></div><div class="c-fXjjoT"><div class="c-gGXXkN"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a href="/blog/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics">Aim basics: using context and subplots to compare validation and test metrics</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics">Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model</a></p><div class="c-hnRRWM"></div></div></div></li></ul></div></div><footer class="c-dRMHzO"><div class="c-hCkBfr"><div class="c-FsNLu"><div class="c-jgJYki"><a class="logo" href="/"><picture><source height="26" width="109" media="(max-width: 1199px)" srcSet="/images/static/main/aim-logo-resp.svg"/><img height="26" width="26" src="/images/static/main/aim-logo.svg" alt="Aimstack"/></picture></a></div><ul class="c-bFmrFE"><li><a target="_self" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><ul class="c-MNZuo"><li><a href="https://aimstack.slack.com/ssb/redirect#/shared-invite/email" rel="noopener noreferrer" target="_blank" aria-label="slack"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M380.907 539.056c-57.742 0-104.574 46.837-104.574 104.585v261.609c0 57.748 46.832 104.585 104.574 104.585s104.574-46.837 104.574-104.585v-261.609c-0.042-57.748-46.874-104.585-104.574-104.585z"></path><path style="fill:white" d="M15.111 643.63c0 57.799 46.874 104.687 104.659 104.687s104.659-46.888 104.659-104.687v-104.685h-104.577c-0.042 0-0.042 0-0.082 0-57.785 0-104.659 46.886-104.659 104.685z"></path><path style="fill:white" d="M381.003 14.167c-0.042 0-0.083 0-0.125 0-57.783 0-104.656 46.886-104.656 104.684s46.874 104.685 104.656 104.685h104.574v-104.685c0-0.041 0-0.124 0-0.207-0.042-57.715-46.791-104.477-104.449-104.477z"></path><path style="fill:white" d="M118.88 485.95h262.080c57.784 0 104.657-46.892 104.657-104.697s-46.874-104.697-104.657-104.697h-262.080c-57.784 0-104.658 46.892-104.658 104.697s46.874 104.697 104.658 104.697z"></path><path style="fill:white" d="M904.118 276.5c-57.702 0-104.454 46.767-104.454 104.49v104.905h104.573c57.788 0 104.658-46.892 104.658-104.698s-46.871-104.697-104.658-104.697c-0.040 0-0.080 0-0.119 0z"></path><path style="fill:white" d="M538.444 118.795v262.365c0 57.741 46.834 104.573 104.577 104.573 57.737 0 104.573-46.832 104.573-104.573v-262.365c0-57.741-46.837-104.573-104.573-104.573-57.742 0-104.577 46.832-104.577 104.573z"></path><path style="fill:white" d="M747.594 905.028c0-57.748-46.837-104.585-104.573-104.585h-104.577v104.67c0.042 57.702 46.834 104.499 104.577 104.499 57.737 0 104.573-46.837 104.573-104.585z"></path><path style="fill:white" d="M905.068 538.944h-262.076c-57.788 0-104.659 46.886-104.659 104.685s46.871 104.687 104.659 104.687h262.076c57.788 0 104.658-46.888 104.658-104.687s-46.871-104.685-104.658-104.685z"></path></svg></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="twitter"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M322.14 927.976c386.322 0 597.681-320.14 597.681-597.678 0-9-0.199-18.2-0.604-27.2 41.116-29.734 76.601-66.565 104.782-108.76-38.292 17.037-78.95 28.164-120.579 33 43.833-26.275 76.654-67.553 92.381-116.18-41.24 24.439-86.339 41.679-133.363 50.98-31.685-33.666-73.577-55.957-119.199-63.427s-92.44 0.299-133.206 22.103c-40.766 21.805-73.211 56.432-92.324 98.527s-23.826 89.314-13.411 134.357c-83.5-4.19-165.188-25.881-239.767-63.667s-140.386-90.823-193.153-155.673c-26.819 46.239-35.025 100.955-22.952 153.027s43.521 97.594 87.952 127.313c-33.356-1.059-65.981-10.040-95.18-26.2v2.6c-0.030 48.525 16.745 95.562 47.475 133.116 30.729 37.552 73.515 63.309 121.085 72.886-30.899 8.451-63.328 9.685-94.78 3.6 13.424 41.731 39.54 78.228 74.706 104.405 35.166 26.171 77.626 40.712 121.454 41.591-74.408 58.449-166.322 90.155-260.94 90.004-16.78-0.027-33.543-1.056-50.2-3.083 96.122 61.666 207.937 94.424 322.14 94.359z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="linkedIn"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M948.201 0h-872.601c-41.8 0-75.6 33-75.6 73.8v876.199c0 40.801 33.8 74.001 75.6 74.001h872.601c41.796 0 75.799-33.2 75.799-73.802v-876.398c0-40.8-34.002-73.8-75.799-73.8zM303.8 872.602h-152v-488.802h152v488.802zM227.8 317.2c-48.8 0-88.2-39.4-88.2-88s39.4-88 88.2-88c48.6 0 88 39.4 88 88 0 48.4-39.4 88-88 88zM872.602 872.602h-151.802v-237.602c0-56.599-1.001-129.6-79.002-129.6-78.998 0-90.998 61.8-90.998 125.6v241.601h-151.6v-488.802h145.6v66.8h2c20.2-38.4 69.802-79 143.598-79 153.805 0 182.204 101.2 182.204 232.799v268.203z"></path></svg></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="fb"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M1024 512c0-282.77-229.228-512-512-512-282.77 0-512 229.23-512 512 0 255.551 187.23 467.371 432 505.782v-357.78h-130v-148.002h130v-112.8c0-128.32 76.44-199.2 193.391-199.2 56.001 0 114.608 10 114.608 10v126h-64.558c-63.602 0-83.445 39.47-83.445 80v96h142l-22.699 148.002h-119.302v357.78c244.77-38.411 432.003-250.231 432.003-505.782z"></path></svg></a></li></ul></div><div class="c-cChYXC"><p class="c-PJLV c-PJLV-EDrvg-size-1">Copyright © <!-- -->2023<!-- --> Aimstack</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"3D Spleen Segmentation with MONAI and Aim","date":"2022-06-27T20:55:12.121Z","author":"Gev Soghomonian","description":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from http://medicaldecathlon.com/. This is a","slug":"3d-spleen-segmentation-with-monai-and-aim","image":"https://aimstack.io/wp-content/uploads/2022/06/Medium_monai-1.png","draft":false,"categories":["Tutorials"],"body":{"raw":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from [](http://medicaldecathlon.com/)\u003chttp://medicaldecathlon.com/\u003e.\n\nThis is a longer form of the [3D spleen segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb) on the MONAI tutorials repo.\n\nFor a complete in-depth overview of the code and the methodology please follow [this tutorial](https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb) directly or our very own [Aim from Zero to Hero tutorial](https://aimstack.io/aim-from-zero-to-hero-track-machine-learning-experiments/).\n\n## ***Tracking the basics***\n\nAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\n\n### ***Transformations Matter***\n\nThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the` Single Run Page`.\n\n![](https://aimstack.io/wp-content/uploads/2022/06/transformations-matter-1.png)\n\nYou can further search/filter through the runs based on these transformations using Aims’ very own pythonic search language `AimQL`. An Advanced usage looks like this\n\n![](https://aimstack.io/wp-content/uploads/2022/06/tm2.png)\n\n### *Lets talk about models and optimizers*\n\nSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic `dict`-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from` AimQL`\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n```\n\n![](https://aimstack.io/wp-content/uploads/2022/06/optimizers.png)\n\n## What about Losses?\n\nMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\n\nFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](https://aimstack.io/wp-content/uploads/2022/06/what-about-losses.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the [complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html) or simply play around in our [interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n## *A picture is worth a thousand words*\n\nIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at `Aim`. You can [track images](https://aimstack.io/aim-monai-tutorial/(%3Chttps://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#%3E)) as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\n\n```\naim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n```\n\nAll the grouping/filtering/aggregation functional presented above are also available for Images.\n\n![](https://aimstack.io/wp-content/uploads/2022/06/A-picture-is-worth-a-thousand-words-1.jpeg)\n\n![](https://aimstack.io/wp-content/uploads/2022/06/A-picture-is-worth-a-thousand-words-2.jpeg)\n\n## *A figure is worth a thousand pictures*\n\nWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\n\n```\naim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n\n```\n\nWithin *`Aim`* you can access all the tracked Figures from within the `Single Run Page`\n\n``\n\n![](https://aimstack.io/wp-content/uploads/2022/06/A-figure-is-worth-a-thousand-pictures.gif)\n\nAnother interesting thing that one can [track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](https://aimstack.io/wp-content/uploads/2022/06/A-figure-is-worth-a-thousand-pictures-2.png)\n\n## ***The curtain falls***\n\nIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with *`1-2`* lines of code. You can also play around with the completed MONAI integration results with our [interactive demo](http://play.aimstack.io:10005/).\n\nIf you have any bug reports please follow up on our [github repository](https://github.com/aimhubio/aim/issues/new/choose). Feel free to join our growing [Slack community](https://slack.aimstack.io/) and ask questions directly to the developers and seasoned users.\n\nIf you find Aim useful, [please stop by](https://github.com/aimhubio/aim) and drop us a star.","html":"\u003cp\u003eIn this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from \u003ca href=\"http://medicaldecathlon.com/\"\u003e\u003c/a\u003e\u003ca href=\"http://medicaldecathlon.com/\"\u003ehttp://medicaldecathlon.com/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is a longer form of the \u003ca href=\"https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb\"\u003e3D spleen segmentation tutorial\u003c/a\u003e on the MONAI tutorials repo.\u003c/p\u003e\n\u003cp\u003eFor a complete in-depth overview of the code and the methodology please follow \u003ca href=\"https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb\"\u003ethis tutorial\u003c/a\u003e directly or our very own \u003ca href=\"https://aimstack.io/aim-from-zero-to-hero-track-machine-learning-experiments/\"\u003eAim from Zero to Hero tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eTracking the basics\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\u003c/p\u003e\n\u003ch3\u003e\u003cem\u003e\u003cstrong\u003eTransformations Matter\u003c/strong\u003e\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the\u003ccode\u003e Single Run Page\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/transformations-matter-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can further search/filter through the runs based on these transformations using Aims’ very own pythonic search language \u003ccode\u003eAimQL\u003c/code\u003e. An Advanced usage looks like this\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/tm2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cem\u003eLets talk about models and optimizers\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic \u003ccode\u003edict\u003c/code\u003e-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from\u003ccode\u003e AimQL\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/optimizers.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eWhat about Losses?\u003c/h2\u003e\n\u003cp\u003eMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\u003c/p\u003e\n\u003cp\u003eFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter grouping, we end up with a visualization akin to this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/what-about-losses.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003ecomplete documentation\u003c/a\u003e or simply play around in our \u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive Demo\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eA picture is worth a thousand words\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at \u003ccode\u003eAim\u003c/code\u003e. You can \u003ca href=\"https://aimstack.io/aim-monai-tutorial/(%3Chttps://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#%3E)\"\u003etrack images\u003c/a\u003e as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll the grouping/filtering/aggregation functional presented above are also available for Images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/A-picture-is-worth-a-thousand-words-1.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/A-picture-is-worth-a-thousand-words-2.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eA figure is worth a thousand pictures\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithin \u003cem\u003e\u003ccode\u003eAim\u003c/code\u003e\u003c/em\u003e you can access all the tracked Figures from within the \u003ccode\u003eSingle Run Page\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e``\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/A-figure-is-worth-a-thousand-pictures.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAnother interesting thing that one can \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\"\u003etrack is distributions\u003c/a\u003e. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDistributions can be accessed from the Single Run Page as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/06/A-figure-is-worth-a-thousand-pictures-2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eThe curtain falls\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with \u003cem\u003e\u003ccode\u003e1-2\u003c/code\u003e\u003c/em\u003e lines of code. You can also play around with the completed MONAI integration results with our \u003ca href=\"http://play.aimstack.io:10005/\"\u003einteractive demo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you have any bug reports please follow up on our \u003ca href=\"https://github.com/aimhubio/aim/issues/new/choose\"\u003egithub repository\u003c/a\u003e. Feel free to join our growing \u003ca href=\"https://slack.aimstack.io/\"\u003eSlack community\u003c/a\u003e and ask questions directly to the developers and seasoned users.\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, \u003ca href=\"https://github.com/aimhubio/aim\"\u003eplease stop by\u003c/a\u003e and drop us a star.\u003c/p\u003e"},"_id":"posts/3d-spleen-segmentation-with-monai-and-aim.md","_raw":{"sourceFilePath":"posts/3d-spleen-segmentation-with-monai-and-aim.md","sourceFileName":"3d-spleen-segmentation-with-monai-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/3d-spleen-segmentation-with-monai-and-aim"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that on [Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\nUse the [aim.track](https://github.com/aimhubio/aim#track) context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n![](\u003cscript src=\"https://gist.github.com/SGevorg/e08524b3538d3f71d14bf1857a7bc6e9.js\"\u003e\u003c/script\u003e \"Here is how it looks like on the code\")\n\nOnce the training is ran, execute `aim up` in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n\u003e **\\*Note:** The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLet’s go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.** Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.** Add the `bleu` metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.** Search by `context.subset` to show both `test` and `val` `loss` and `bleu` metrics. Divide into subplots further by `context.subset` too so Aim UI shows `test` and `val` metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\nHere is a full summary video on how to do it on the UI.\n\n![](https://www.youtube.com/watch?v=DGI8S7SUfEk)\n\n# Learn More\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://slack.aimstack.io/) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003cp\u003eResearchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\u003c/p\u003e\n\u003cp\u003eThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\u003c/p\u003e\n\u003cp\u003eThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\u003c/p\u003e\n\u003cp\u003eModels are usually ran on validation subset after each epoch.\u003cbr\u003e\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\u003c/p\u003e\n\u003cp\u003eThere is a need to collect and effectively compare all these metrics.\u003c/p\u003e\n\u003cp\u003eHere is how to do that on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003eUsing context to track for different subsets?\u003c/h1\u003e\n\u003cp\u003eUse the \u003ca href=\"https://github.com/aimhubio/aim#track\"\u003eaim.track\u003c/a\u003e context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\u003c/p\u003e\n\u003cp\u003e![]( \"Here is how it looks like on the code\")\u003c/p\u003e\n\u003cp\u003eOnce the training is ran, execute \u003ccode\u003eaim up\u003c/code\u003e in your terminal and start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eUsing subplots to compare test, val loss and bleu metrics\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e*Note:\u003c/strong\u003e The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet’s go step-by-step on how to break down lots of experiments using subplots.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1.\u003c/strong\u003e Explore the runs, the context table, play with the query language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2.\u003c/strong\u003e Add the \u003ccode\u003ebleu\u003c/code\u003e metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3.\u003c/strong\u003e Search by \u003ccode\u003econtext.subset\u003c/code\u003e to show both \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003ebleu\u003c/code\u003e metrics. Divide into subplots further by \u003ccode\u003econtext.subset\u003c/code\u003e too so Aim UI shows \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e metrics on different subplots for better comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"\u003e\u003c/p\u003e\n\u003cp\u003eNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\u003c/p\u003e\n\u003ch1\u003eSummary\u003c/h1\u003e\n\u003cp\u003eHere is a full summary video on how to do it on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://www.youtube.com/watch?v=DGI8S7SUfEk\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim from Zero to Hero","date":"2022-03-16T16:39:52.780Z","author":"Gev Soghomonian","description":"In this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various","slug":"aim-from-zero-to-hero","image":"https://aimstack.io/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"In this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\n\n## *Starting from basics: how Aim tracks machine learning experiments*\n\nMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with `matplotlib`. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\n\nTo track the losses, simply create an experiment run and add a tracking common like this\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n```\n\nYou are going to end up with a visualization of this kind.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/2D093F44-828E-4680-9AFB-3E3E8B051B9D.png)\n\nA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/4BF09F25-287D-4B07-8E54-23009F0162FD.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the [complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html) or simply play around in our [interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n- - -\n\n## *Rising beyond watching losses*\n\nAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic dict or dictionary-like object, i.e.\n\n```\naim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n```\n\nIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can [group with respect to the parameters/hyperparameters](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter) that you saved with aim.\n\nViewing the [standalone parameters](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page) is fast for each separate Single Run\n\n![](https://aimstack.io/wp-content/uploads/2022/03/367C296A-85E4-4A9B-90A7-5624B9B60C3A-2048x1141.png)\n\n[Filtering/grouping your runs using the selected trackable](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter) can be accessed and looked through in our [interactive tutorial](http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n![](https://aimstack.io/wp-content/uploads/2022/03/D2F6F0A9-BB62-4A03-A320-B7CCF2F71D8C-2048x815.png)\n\nFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language [AimQL](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2), by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/558D4917-A376-4C73-8EF5-8E57012B5A3E-2048x815.png)\n\n## ***What if numbers are simply not enough?***\n\n\n\nVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\n\nAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\n\n```\naim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n```\n\nYou can find the complete guide to tracking in the [official documentation](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#).\n\nAll the grouping/filtering/aggregation functional presented above is also available for Images.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/7EA29E1B-163D-4A1A-9BC5-6C56D723B6DC.jpeg)\n\n![](https://aimstack.io/wp-content/uploads/2022/03/83E489E6-8559-49E1-910D-0CBCA641ED43.jpeg)\n\n![](https://aimstack.io/wp-content/uploads/2022/03/04A01ADC-5CE3-47E6-994D-0D1310290710-2048x914.png)\n\nThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D-1024x714.gif)\n\nThe grouping is rather flexible with a [multitude of options](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer).\n\nAnother interesting thing that one can [track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/2C00E1A8-9E54-4714-B53E-456D6E41F845.png)\n\nYou can play around with the image and single run explorers and see all the trackables across our numerous Demos ([FS2](http://play.aimstack.io:10004/), [Spleen Segmentation](http://play.aimstack.io:10005/), [Lightweight GAN](http://play.aimstack.io:10002/), [Machine Translation](http://play.aimstack.io:10001/)).\n\n***One Run to rule them all***\n\nThere are times when a researcher would need to focus upon only a [single run of the experiment](https://aimstack.io/aim-3-7-revamped-run-single-page-and-aim-docker-image/), where he can iterate through a complete list of all the things he tracked. Aim has a dedicated [Single Run Page](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page) for this very purpose.\n\nYou can view all the trackables in the following Tabs:\n\n* Parameters/ Hyperparametrs – Everything tracked regarding the experiment\n* Metrics – All the Tracked Metrics/Losses etc.\n* System – All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\n* Distributions – All the distributions tracked (i.e. Flowing gradients and weights)\n* Images – All the Image objects saved\n* Audios – All the Audio objects saved\n* Texts – All the raw text tracked during experimentation\n* Figures – All the `matplotlin`/`Plotly` etc. Figures\n* Settings – Settings that runs share\n\nLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\n\n![](https://aimstack.io/wp-content/uploads/2022/03/DC6064D0-EE74-41FE-AA77-C24E60C07ED9-2048x1139.png)\n\n![](https://aimstack.io/wp-content/uploads/2022/03/img_0317-2048x1119.png)\n\n![](https://aimstack.io/wp-content/uploads/2022/03/EF4D6601-2D1A-45A4-840B-3A37B686E9C3-2048x1176.png)\n\nYou can look aroud the individual runs in one of our interactive Demos ([FS2](http://play.aimstack.io:10004/), [Spleen Segmentation](http://play.aimstack.io:10005/), [Lightweight GAN](http://play.aimstack.io:10002/), [Machine Translation](http://play.aimstack.io:10001/)).\n\n- - -\n\n## ***No More localhost. Track experiments remotely***\n\nAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\n\n```\naim init\n# Tringgering aim server\naim server --repo \u003cREPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo \u003cREPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n\n```\n\nIntegrating this newly created [remote tracking server](https://aimstack.io/aim3-4-remote-tracking-alpha-sorting-deleting-runs/) within your experimentation is even easier.\n\n```\n# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n```\n\nAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\n\n![](https://aimstack.io/wp-content/uploads/2022/03/8B2177A1-241E-4D9B-ADF7-4AEBA4D28A35.jpeg)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling  and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003eIn this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eStarting from basics: how Aim tracks machine learning experiments\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with \u003ccode\u003ematplotlib\u003c/code\u003e. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\u003c/p\u003e\n\u003cp\u003eTo track the losses, simply create an experiment run and add a tracking common like this\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou are going to end up with a visualization of this kind.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/2D093F44-828E-4680-9AFB-3E3E8B051B9D.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter grouping, we end up with a visualization akin to this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/4BF09F25-287D-4B07-8E54-23009F0162FD.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003ecomplete documentation\u003c/a\u003e or simply play around in our \u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive Demo\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\u003cem\u003eRising beyond watching losses\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic dict or dictionary-like object, i.e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\"\u003egroup with respect to the parameters/hyperparameters\u003c/a\u003e that you saved with aim.\u003c/p\u003e\n\u003cp\u003eViewing the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\"\u003estandalone parameters\u003c/a\u003e is fast for each separate Single Run\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/367C296A-85E4-4A9B-90A7-5624B9B60C3A-2048x1141.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\"\u003eFiltering/grouping your runs using the selected trackable\u003c/a\u003e can be accessed and looked through in our \u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/D2F6F0A9-BB62-4A03-A320-B7CCF2F71D8C-2048x815.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2\"\u003eAimQL\u003c/a\u003e, by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/558D4917-A376-4C73-8EF5-8E57012B5A3E-2048x815.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eWhat if numbers are simply not enough?\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\u003c/p\u003e\n\u003cp\u003eAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can find the complete guide to tracking in the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#\"\u003eofficial documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAll the grouping/filtering/aggregation functional presented above is also available for Images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/7EA29E1B-163D-4A1A-9BC5-6C56D723B6DC.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/83E489E6-8559-49E1-910D-0CBCA641ED43.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/04A01ADC-5CE3-47E6-994D-0D1310290710-2048x914.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/2099C2D5-318C-4925-BFB2-DA05AC422F3D-1024x714.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe grouping is rather flexible with a \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer\"\u003emultitude of options\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAnother interesting thing that one can \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\"\u003etrack is distributions\u003c/a\u003e. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDistributions can be accessed from the Single Run Page as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/2C00E1A8-9E54-4714-B53E-456D6E41F845.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can play around with the image and single run explorers and see all the trackables across our numerous Demos (\u003ca href=\"http://play.aimstack.io:10004/\"\u003eFS2\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10005/\"\u003eSpleen Segmentation\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10002/\"\u003eLightweight GAN\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10001/\"\u003eMachine Translation\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eOne Run to rule them all\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThere are times when a researcher would need to focus upon only a \u003ca href=\"https://aimstack.io/aim-3-7-revamped-run-single-page-and-aim-docker-image/\"\u003esingle run of the experiment\u003c/a\u003e, where he can iterate through a complete list of all the things he tracked. Aim has a dedicated \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\"\u003eSingle Run Page\u003c/a\u003e for this very purpose.\u003c/p\u003e\n\u003cp\u003eYou can view all the trackables in the following Tabs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eParameters/ Hyperparametrs – Everything tracked regarding the experiment\u003c/li\u003e\n\u003cli\u003eMetrics – All the Tracked Metrics/Losses etc.\u003c/li\u003e\n\u003cli\u003eSystem – All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\u003c/li\u003e\n\u003cli\u003eDistributions – All the distributions tracked (i.e. Flowing gradients and weights)\u003c/li\u003e\n\u003cli\u003eImages – All the Image objects saved\u003c/li\u003e\n\u003cli\u003eAudios – All the Audio objects saved\u003c/li\u003e\n\u003cli\u003eTexts – All the raw text tracked during experimentation\u003c/li\u003e\n\u003cli\u003eFigures – All the \u003ccode\u003ematplotlin\u003c/code\u003e/\u003ccode\u003ePlotly\u003c/code\u003e etc. Figures\u003c/li\u003e\n\u003cli\u003eSettings – Settings that runs share\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/DC6064D0-EE74-41FE-AA77-C24E60C07ED9-2048x1139.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/img_0317-2048x1119.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/EF4D6601-2D1A-45A4-840B-3A37B686E9C3-2048x1176.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can look aroud the individual runs in one of our interactive Demos (\u003ca href=\"http://play.aimstack.io:10004/\"\u003eFS2\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10005/\"\u003eSpleen Segmentation\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10002/\"\u003eLightweight GAN\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10001/\"\u003eMachine Translation\u003c/a\u003e).\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eNo More localhost. Track experiments remotely\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim init\n# Tringgering aim server\naim server --repo \u0026#x3C;REPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo \u0026#x3C;REPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIntegrating this newly created \u003ca href=\"https://aimstack.io/aim3-4-remote-tracking-alpha-sorting-deleting-runs/\"\u003eremote tracking server\u003c/a\u003e within your experimentation is even easier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/03/8B2177A1-241E-4D9B-ADF7-4AEBA4D28A35.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling  and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-from-zero-to-hero.md","_raw":{"sourceFilePath":"posts/aim-from-zero-to-hero.md","sourceFileName":"aim-from-zero-to-hero.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-from-zero-to-hero"},"type":"Post"},{"title":"Aim tutorial for Weights and Biases users","date":"2022-09-07T21:13:32.862Z","author":"Hovhannes Tamoyan","description":"Using Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026 Biases (wandb) runs into the native format and explore them via Aim UI. In this blog/post we will go over the steps required to migrate wandb logs to Aim. ","slug":"aim-tutorial-for-weights-and-biases-users","image":"https://miro.medium.com/max/1400/1*PBF_k6VevuUrquadJ86vfA.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\n\nI have been using [Aim](https://github.com/aimhubio/aim) for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\n\nThere are lots of converters already available to migrate or use along with other experiment trackers: [Aim Converters](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html).\n\nUsing Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026 Biases (wandb) runs into the native format and explore them via [Aim UI](https://aimstack.readthedocs.io/en/latest/ui/overview.html).\n\nIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\n\n# Example Project Setup\n\nLet’s take a look at a concrete example. We will be using `keras-tuner` to train a `CNN` on `Cifar10` dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be `sample-team`:\n\n```\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n\n```\n\nModel creation, data loading and other parts can be done as:\n\n```\n\n```\n\nThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let’s convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\n\n\u003e *Note: During this project I noticed that the* `wandb.keras.WandbCallback` *was tracking the metrics on epoch end. For better experience we recommend using the* `aim.keras_tuner.AimCallback`*: to track all the metrics on batch end.*\n\n# Converting runs from Wandb to Aim\n\nTo be able to explore Weights \u0026 Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\n\nPick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\n\n```\n$ aim init\n```\n\nTo start converting wandb experiments from entity/team: `sample-team` and project: `my-awesome-project` run:\n\n```\n$ aim convert wandb --entity sample-team --project my-awesome-project\n```\n\nThe converter will iterate over all the experiments in the project `my-awesome-project` and create a distinct Aim run for each experiment.\n\nFor more please see the [“Show Weights and Biases logs in Aim”](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim) section in [Aim docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nNow that we have our Aim repository initialized, we simply need to do:\n\n```\n$ aim up\n```\n\nto start the Aim UI.\n\n# Exploring the differences in Aim and wandb UIs\n\nTo see the configs, metadata and more on wandb we need to navigate to the “Overview” page:\n\n![](https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp)\n\nUnder the Config section, our logged configurations are shown. On the Summary page the metrics’ latest results.\n\nThe wandb run “Overview” page counterpart on Aim UI is the individual run page, which can be accessed from the “Runs” page:\n\n![](https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp)\n\nMore information can be found when opening an individual run’s overview page.\n\n![](https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp)\n\nOn this page we can see the params, even the `wandb_run_id` and the `wandb_run_name` which are extracted from wandb runs during the conversion process.\n\nMore detailed information about the run can be found on each of the tabs, e.g. “Run Params”, “Metrics”, “System” etc.\n\n# Filtering and Grouping the Metrics\n\nOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\n\nTo see the tracked metrics and compare them with other runs we need to open the “Workspace” page on wandb:\n\n![](https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp)\n\nTo see the metrics on Aim you need to navigate to the “Metrics” page. By default you will see an empty page with “no results” message (which I hope they make this experience better soon), this is because we haven’t selected any metrics to show. To do that we can use the “+ Metrics” button to open up the list of available metrics and select the desired metrics, e.g. “loss” and “accuracy”:\n\n![](https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp)\n\nThis is the view we will get after selecting to show the loss and accuracy:\n\n![](https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp)\n\nNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan’t to see the plots by metric name, so each plot will represent a metric’s values.\n\n![](https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp)\n\nWe simply need to access the `metric.name` .\n\n![](https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp)\n\nTo have unique coloring for each run, we can use the “Group by color” functionality, and provide the run.hash. Aim assigns a unique hash to each run.\n\n\u003e *Note: in this example the Aim UI automatically assigns individual colors for each run.*\n\n![](To have unique coloring for each run, we can use the “Group by color” functionality, and provide the run.hash. Aim assigns a unique hash to each run.\n\nNote: in this example the Aim UI automatically assigns individual colors for each run.)\n\n\\\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the “advanced search mode” click on the pen button under the search button:\n\n![](https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp)\n\nthis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\n\n```\n((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n```\n\nThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\n\n```\nmetric.name in [\"accuracy\", \"loss\"]\n```\n\nFor more please see the “[Search Runs](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs)” page on docs.\n\n\n\n# Hyperparameter Explorer\n\nTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the “Add Panel” button, and add the parameters and the metrics we need, e.g.:\n\n![](https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp)\n\nAim counterpart parameters explorer is on the page of “Params”. Where by using the “+ Run Params” button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\n\n![](https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp)\n\n## Plot Operations\n\nThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‘readable’ and explicit. On wandb one needs to click on “Edit panel” sign to see this view:\n\n![](https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp)\n\n\\\nOn Aim you can find the operations on the right sidebar of the Metrics page:\n\n![](https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp)\n\n# Conclusion\n\n\n\nAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I’ve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please [read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.","html":"\u003cp\u003eI am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\u003c/p\u003e\n\u003cp\u003eI have been using \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\u003c/p\u003e\n\u003cp\u003eThere are lots of converters already available to migrate or use along with other experiment trackers: \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html\"\u003eAim Converters\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUsing Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026#x26; Biases (wandb) runs into the native format and explore them via \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\"\u003eAim UI\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\u003c/p\u003e\n\u003ch1\u003eExample Project Setup\u003c/h1\u003e\n\u003cp\u003eLet’s take a look at a concrete example. We will be using \u003ccode\u003ekeras-tuner\u003c/code\u003e to train a \u003ccode\u003eCNN\u003c/code\u003e on \u003ccode\u003eCifar10\u003c/code\u003e dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be \u003ccode\u003esample-team\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eModel creation, data loading and other parts can be done as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let’s convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote: During this project I noticed that the\u003c/em\u003e \u003ccode\u003ewandb.keras.WandbCallback\u003c/code\u003e \u003cem\u003ewas tracking the metrics on epoch end. For better experience we recommend using the\u003c/em\u003e \u003ccode\u003eaim.keras_tuner.AimCallback\u003c/code\u003e\u003cem\u003e: to track all the metrics on batch end.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eConverting runs from Wandb to Aim\u003c/h1\u003e\n\u003cp\u003eTo be able to explore Weights \u0026#x26; Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\u003c/p\u003e\n\u003cp\u003ePick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo start converting wandb experiments from entity/team: \u003ccode\u003esample-team\u003c/code\u003e and project: \u003ccode\u003emy-awesome-project\u003c/code\u003e run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim convert wandb --entity sample-team --project my-awesome-project\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe converter will iterate over all the experiments in the project \u003ccode\u003emy-awesome-project\u003c/code\u003e and create a distinct Aim run for each experiment.\u003c/p\u003e\n\u003cp\u003eFor more please see the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim\"\u003e“Show Weights and Biases logs in Aim”\u003c/a\u003e section in \u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNow that we have our Aim repository initialized, we simply need to do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eExploring the differences in Aim and wandb UIs\u003c/h1\u003e\n\u003cp\u003eTo see the configs, metadata and more on wandb we need to navigate to the “Overview” page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eUnder the Config section, our logged configurations are shown. On the Summary page the metrics’ latest results.\u003c/p\u003e\n\u003cp\u003eThe wandb run “Overview” page counterpart on Aim UI is the individual run page, which can be accessed from the “Runs” page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eMore information can be found when opening an individual run’s overview page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOn this page we can see the params, even the \u003ccode\u003ewandb_run_id\u003c/code\u003e and the \u003ccode\u003ewandb_run_name\u003c/code\u003e which are extracted from wandb runs during the conversion process.\u003c/p\u003e\n\u003cp\u003eMore detailed information about the run can be found on each of the tabs, e.g. “Run Params”, “Metrics”, “System” etc.\u003c/p\u003e\n\u003ch1\u003eFiltering and Grouping the Metrics\u003c/h1\u003e\n\u003cp\u003eOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\u003c/p\u003e\n\u003cp\u003eTo see the tracked metrics and compare them with other runs we need to open the “Workspace” page on wandb:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo see the metrics on Aim you need to navigate to the “Metrics” page. By default you will see an empty page with “no results” message (which I hope they make this experience better soon), this is because we haven’t selected any metrics to show. To do that we can use the “+ Metrics” button to open up the list of available metrics and select the desired metrics, e.g. “loss” and “accuracy”:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the view we will get after selecting to show the loss and accuracy:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan’t to see the plots by metric name, so each plot will represent a metric’s values.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe simply need to access the \u003ccode\u003emetric.name\u003c/code\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo have unique coloring for each run, we can use the “Group by color” functionality, and provide the run.hash. Aim assigns a unique hash to each run.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote: in this example the Aim UI automatically assigns individual colors for each run.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e![](To have unique coloring for each run, we can use the “Group by color” functionality, and provide the run.hash. Aim assigns a unique hash to each run.\u003c/p\u003e\n\u003cp\u003eNote: in this example the Aim UI automatically assigns individual colors for each run.)\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the “advanced search mode” click on the pen button under the search button:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003ethis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emetric.name in [\"accuracy\", \"loss\"]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more please see the “\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs\"\u003eSearch Runs\u003c/a\u003e” page on docs.\u003c/p\u003e\n\u003ch1\u003eHyperparameter Explorer\u003c/h1\u003e\n\u003cp\u003eTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the “Add Panel” button, and add the parameters and the metrics we need, e.g.:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim counterpart parameters explorer is on the page of “Params”. Where by using the “+ Run Params” button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePlot Operations\u003c/h2\u003e\n\u003cp\u003eThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‘readable’ and explicit. On wandb one needs to click on “Edit panel” sign to see this view:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nOn Aim you can find the operations on the right sidebar of the Metrics page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I’ve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please \u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eread the docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.\u003c/p\u003e"},"_id":"posts/aim-tutorial-for-weights-and-biases-users.md","_raw":{"sourceFilePath":"posts/aim-tutorial-for-weights-and-biases-users.md","sourceFileName":"aim-tutorial-for-weights-and-biases-users.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-tutorial-for-weights-and-biases-users"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":" Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out […]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger used… What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\n\n[Aim](https://github.com/aimhubio/aim) is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n## What is XGBoost?\n\n[XGBoost](https://github.com/dmlc/xgboost) is an optimized gradient boosting library with highly  *efficient*,  *flexible,*  and  *portable* design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\n## How to use Aim with XGBoost?\n\nCheck out end-to-end [Aim integration](https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/) examples with multiple frameworks [here](https://github.com/aimhubio/aim/tree/main/examples). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\n\n```\n# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n```\n\nAs you can see on line 49, AimCallback is imported from `aim.xgboost` and passed to `xgb.train` as one of the callbacks. Aim session can open and close by the AimCallback and the metrics and hparamsstore by XGBoost. In addition to that, thesystem measures pass to Aim as well.\n\n## What it looks like?\n\nAfter you run the experiment and the `aim up` command in the `aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\n\n![](https://aimstack.io/wp-content/uploads/2021/05/dashboard.png \"Aim UI dashboard page\")\n\nTo explore the run, we should:\n\n* Choose the `xgboost_test`experiment.\n* Select the metrics to explore.\n* Divide into charts by metrics.\n\nFor example, the gif below illustrates the steps above.\n\n![](https://aimstack.io/wp-content/uploads/2021/05/1.gif)\n\n\\\n So this is what the final result looks like.\n\n![](https://aimstack.io/wp-content/uploads/2021/05/2.png)\n\nIn short, we can easily analyze the runs and the system usage.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nIf you find Aim useful, support us and star [the project](https://github.com/aimhubio/aim) on GitHub. Also, join the [Aim community](https://aimstack.slack.com/ssb/redirect) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is Aim?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\u003c/p\u003e\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dmlc/xgboost\"\u003eXGBoost\u003c/a\u003e is an optimized gradient boosting library with highly  \u003cem\u003eefficient\u003c/em\u003e,  \u003cem\u003eflexible,\u003c/em\u003e  and  \u003cem\u003eportable\u003c/em\u003e design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\u003c/p\u003e\n\u003ch2\u003eHow to use Aim with XGBoost?\u003c/h2\u003e\n\u003cp\u003eCheck out end-to-end \u003ca href=\"https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/\"\u003eAim integration\u003c/a\u003e examples with multiple frameworks \u003ca href=\"https://github.com/aimhubio/aim/tree/main/examples\"\u003ehere\u003c/a\u003e. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs you can see on line 49, AimCallback is imported from \u003ccode\u003eaim.xgboost\u003c/code\u003e and passed to \u003ccode\u003exgb.train\u003c/code\u003e as one of the callbacks. Aim session can open and close by the AimCallback and the metrics and hparamsstore by XGBoost. In addition to that, thesystem measures pass to Aim as well.\u003c/p\u003e\n\u003ch2\u003eWhat it looks like?\u003c/h2\u003e\n\u003cp\u003eAfter you run the experiment and the \u003ccode\u003eaim up\u003c/code\u003e command in the \u003ccode\u003eaim_logs\u003c/code\u003edirectory, Aim UI will be running. When first opened, the dashboard page will come up.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/05/dashboard.png\" alt=\"\" title=\"Aim UI dashboard page\"\u003e\u003c/p\u003e\n\u003cp\u003eTo explore the run, we should:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoose the \u003ccode\u003exgboost_test\u003c/code\u003eexperiment.\u003c/li\u003e\n\u003cli\u003eSelect the metrics to explore.\u003c/li\u003e\n\u003cli\u003eDivide into charts by metrics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, the gif below illustrates the steps above.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/05/1.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nSo this is what the final result looks like.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/05/2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn short, we can easily analyze the runs and the system usage.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, support us and star \u003ca href=\"https://github.com/aimhubio/aim\"\u003ethe project\u003c/a\u003e on GitHub. Also, join the \u003ca href=\"https://aimstack.slack.com/ssb/redirect\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"Exploring MLflow experiments with a powerful UI","date":"2023-01-17T21:52:43.983Z","author":"Gor Arakelyan","description":"We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉","slug":"exploring-mlflow-experiments-with-a-powerful-ui","image":"https://miro.medium.com/max/1400/1*YMlI66d-QPxI3fcQCoPSlw.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"\n\nWe are excited to announce the release of [aimlflow](https://github.com/aimhubio/aimlflow), an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉\n\n[MLflow](https://github.com/mlflow/mlflow) is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments. [Aim](https://github.com/aimhubio/aim) is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\n\n![](https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif)\n\n**With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim’s powerful features, leading to deeper understanding and more effective decision-making.**\n\n![](https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp)\n\nIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let’s dive in and see how to make it happen.\n\n\u003e Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\n\u003e\n\u003e View more on GitHub: \u003chttps://github.com/aimhubio/aim\u003e\n\n# Project overview\n\nWe use a simple project that trains a CNN using [PyTorch](https://github.com/pytorch/pytorch) and [Ray Tune](https://github.com/ray-project/ray/tree/master/python/ray/tune) on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\n\n* **PyTorch** for building and training the model\n* **Ray Tune** for hyper-parameters tuning\n* **MLflow** for experiment tracking\n\nFind the full project code on GitHub: \u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003e\n\n![](https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp)\n\nRun the trainings by downloading and executing `tune.py` python file:\n\n```\npython tune.py\n```\n\nYou should see a similar output, meaning the trainings are successfully initiated:\n\n![](https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp)\n\n## Getting started with aimlflow\n\nAfter the hyper-parameter tuning is ran, let’s see how aimlflow can help us to explore the tracked experiments via UI.\n\nTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a `.aim` repo located on the file system.\n\n**This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn’t it amazing? 🤩**\n\n## 1. Install aimlflow on your machine\n\nIt is super easy to install aimlflow, simply run the following command:\n\n```\npip3 install aim-mlflow\n```\n\n## 2. Sync MLflow logs with Aim\n\nPick any directory on your file system and initialize a `.aim` repo:\n\n```\naim init\n```\n\nRun the `aimlflow sync` command to sync MLflow experiments with the Aim repo:\n\n```\naimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n```\n\n## 3. Run Aim\n\nNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\n\n```\naim up --repo=AIM_REPO_PATH\n```\n\nYou will see the following message on the terminal output:\n\n![](https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp)\n\nCongratulations! Now you can explore the training logs with Aim. 🎉\n\n# Quick tour of Aim for MLflow users\n\nIn this section, we will take a quick tour of Aim’s features, including:\n\n* Exploring hyper-parameters tuning results\n* Comparing tracked metrics\n* As well as, taking a look at the other capabilities Aim provides\n\n## Exploring MLflow experiments\n\nNow then Aim is set up and running, we navigate to the project overview page at `127.0.0.1:43800`, where the summary of the project is displayed:\n\n* The number of tracked training runs and experiments\n* Statistics on the amount of tracked metadata\n* A list of experiments and tags, with the ability to quickly explore selected items\n* A calendar and feed of contributions\n* A table of in-progress trainings\n\n![](https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp \"Project overview\")\n\n\\\nTo view the results of the trainings, let’s navigate to the runs dashboard at `127.0.0.1:43800/runs`. Here, you can see hyper-parameters and metrics results all of the trainings.\n\n![](https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp \" Runs dashboard\")\n\nWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\n\n![](https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp \"Run page\")\n\nOn this page, we can view the tracked hparams, including the `mlflow_run_id` and the `mlflow_run_name` which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\n\n## Comparing metrics\n\nComparing metrics across several runs is super easy with Aim:\n\n* Open the metrics page from the left sidebar\n* Select desired metrics by clicking on `+ Metrics` button\n* Pressing `Search` button on the top right corner\n\nWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\n\n![](https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp)\n\nAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\n\n* **by color** — each group of metrics will be filled in with its unique color\n* **by stroke style** — each group will have a unique stroke style (solid, dashed, etc)\n* **by facet** — each group of metrics will be displayed in a separate subplot\n\nTo learn which set of trials performed the best, let’s apply several groupings:\n\n* Group by `run.hparams.l1` hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\n* Group by `metric.name` to divide losses and accuracies into separate subplots (this grouping is applied by default)\n\n![](https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp)\n\n**Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.**\n\nFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\n\n```\nrun.metrics['training_iteration'].last \u003e 1\n```\n\n\n\n\u003e Read more about Aim query language capabilities in the docs: \u003chttps://aimstack.readthedocs.io/en/latest/using/search.html\u003e\n\nThis will result in querying and displaying 9 matched runs:\n\n![](https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp)\n\nLet’s aggregate the groups to see which one performed the best:\n\n![](https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp)\n\nFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. 🎉\n\n\u003e For more please see Aim official docs here: \u003chttps://aimstack.readthedocs.io/en/latest/\u003e\n\n## Last, but not least: a closer look at Aim’s key features\n\n* Use powerful pythonic search to select the runs you want to analyze:\n\n![](https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp)\n\nGroup metrics by hyperparameters to analyze hyperparameters’ influence on run performance:\n\n![](https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp)\n\nSelect multiple metrics and analyze them side by side:\n\n![](https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp)\n\nAggregate metrics by std.dev, std.err, conf.interval:\n\n* ![](https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp)\n\n  Align x axis by any other metric:\n\n  * ![](https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp)\n\n  Scatter plots to learn correlations and trends:\n\n  ![](https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp)\n\n  High dimensional data visualization via parallel coordinate plot:\n\n  ![](https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp)\n\n  Explore media metadata, such as images and audio objects via Aim Explorers:\n\n  * ![](https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp \"Audio Explorer\")\n\n  ![](https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp \"Images Explorer\")\n\n  # Conclusion\n\n  In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\n\n  In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\n\n  We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please [read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\n  # Learn more\n\n  If you have any questions join [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. 🙌\n\n  Show some love by dropping a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"\u003cp\u003eWe are excited to announce the release of \u003ca href=\"https://github.com/aimhubio/aimlflow\"\u003eaimlflow\u003c/a\u003e, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mlflow/mlflow\"\u003eMLflow\u003c/a\u003e is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments. \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim’s powerful features, leading to deeper understanding and more effective decision-making.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let’s dive in and see how to make it happen.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\u003c/p\u003e\n\u003cp\u003eView more on GitHub: \u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eWe use a simple project that trains a CNN using \u003ca href=\"https://github.com/pytorch/pytorch\"\u003ePyTorch\u003c/a\u003e and \u003ca href=\"https://github.com/ray-project/ray/tree/master/python/ray/tune\"\u003eRay Tune\u003c/a\u003e on the \u003ca href=\"https://www.cs.toronto.edu/~kriz/cifar.html\"\u003eCIFAR-10\u003c/a\u003e dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePyTorch\u003c/strong\u003e for building and training the model\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRay Tune\u003c/strong\u003e for hyper-parameters tuning\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMLflow\u003c/strong\u003e for experiment tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFind the full project code on GitHub: \u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eRun the trainings by downloading and executing \u003ccode\u003etune.py\u003c/code\u003e python file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython tune.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should see a similar output, meaning the trainings are successfully initiated:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eGetting started with aimlflow\u003c/h2\u003e\n\u003cp\u003eAfter the hyper-parameter tuning is ran, let’s see how aimlflow can help us to explore the tracked experiments via UI.\u003c/p\u003e\n\u003cp\u003eTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a \u003ccode\u003e.aim\u003c/code\u003e repo located on the file system.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn’t it amazing? 🤩\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e1. Install aimlflow on your machine\u003c/h2\u003e\n\u003cp\u003eIt is super easy to install aimlflow, simply run the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install aim-mlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e2. Sync MLflow logs with Aim\u003c/h2\u003e\n\u003cp\u003ePick any directory on your file system and initialize a \u003ccode\u003e.aim\u003c/code\u003e repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the \u003ccode\u003eaimlflow sync\u003c/code\u003e command to sync MLflow experiments with the Aim repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e3. Run Aim\u003c/h2\u003e\n\u003cp\u003eNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up --repo=AIM_REPO_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will see the following message on the terminal output:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eCongratulations! Now you can explore the training logs with Aim. 🎉\u003c/p\u003e\n\u003ch1\u003eQuick tour of Aim for MLflow users\u003c/h1\u003e\n\u003cp\u003eIn this section, we will take a quick tour of Aim’s features, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExploring hyper-parameters tuning results\u003c/li\u003e\n\u003cli\u003eComparing tracked metrics\u003c/li\u003e\n\u003cli\u003eAs well as, taking a look at the other capabilities Aim provides\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExploring MLflow experiments\u003c/h2\u003e\n\u003cp\u003eNow then Aim is set up and running, we navigate to the project overview page at \u003ccode\u003e127.0.0.1:43800\u003c/code\u003e, where the summary of the project is displayed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe number of tracked training runs and experiments\u003c/li\u003e\n\u003cli\u003eStatistics on the amount of tracked metadata\u003c/li\u003e\n\u003cli\u003eA list of experiments and tags, with the ability to quickly explore selected items\u003c/li\u003e\n\u003cli\u003eA calendar and feed of contributions\u003c/li\u003e\n\u003cli\u003eA table of in-progress trainings\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp\" alt=\"\" title=\"Project overview\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nTo view the results of the trainings, let’s navigate to the runs dashboard at \u003ccode\u003e127.0.0.1:43800/runs\u003c/code\u003e. Here, you can see hyper-parameters and metrics results all of the trainings.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp\" alt=\"\" title=\" Runs dashboard\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp\" alt=\"\" title=\"Run page\"\u003e\u003c/p\u003e\n\u003cp\u003eOn this page, we can view the tracked hparams, including the \u003ccode\u003emlflow_run_id\u003c/code\u003e and the \u003ccode\u003emlflow_run_name\u003c/code\u003e which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\u003c/p\u003e\n\u003ch2\u003eComparing metrics\u003c/h2\u003e\n\u003cp\u003eComparing metrics across several runs is super easy with Aim:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen the metrics page from the left sidebar\u003c/li\u003e\n\u003cli\u003eSelect desired metrics by clicking on \u003ccode\u003e+ Metrics\u003c/code\u003e button\u003c/li\u003e\n\u003cli\u003ePressing \u003ccode\u003eSearch\u003c/code\u003e button on the top right corner\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eby color\u003c/strong\u003e — each group of metrics will be filled in with its unique color\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eby stroke style\u003c/strong\u003e — each group will have a unique stroke style (solid, dashed, etc)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eby facet\u003c/strong\u003e — each group of metrics will be displayed in a separate subplot\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo learn which set of trials performed the best, let’s apply several groupings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGroup by \u003ccode\u003erun.hparams.l1\u003c/code\u003e hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\u003c/li\u003e\n\u003cli\u003eGroup by \u003ccode\u003emetric.name\u003c/code\u003e to divide losses and accuracies into separate subplots (this grouping is applied by default)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.metrics['training_iteration'].last \u003e 1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRead more about Aim query language capabilities in the docs: \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/search.html\"\u003ehttps://aimstack.readthedocs.io/en/latest/using/search.html\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis will result in querying and displaying 9 matched runs:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eLet’s aggregate the groups to see which one performed the best:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. 🎉\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFor more please see Aim official docs here: \u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003ehttps://aimstack.readthedocs.io/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eLast, but not least: a closer look at Aim’s key features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUse powerful pythonic search to select the runs you want to analyze:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eGroup metrics by hyperparameters to analyze hyperparameters’ influence on run performance:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSelect multiple metrics and analyze them side by side:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregate metrics by std.dev, std.err, conf.interval:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAlign x axis by any other metric:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp\" alt=\"\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eScatter plots to learn correlations and trends:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHigh dimensional data visualization via parallel coordinate plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eExplore media metadata, such as images and audio objects via Aim Explorers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp\" alt=\"\" title=\"Audio Explorer\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp\" alt=\"\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eIn conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\u003c/p\u003e\n\u003cp\u003eIn this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\u003c/p\u003e\n\u003cp\u003eWe covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please \u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eread the docs\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. 🙌\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you think Aim is useful.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","_raw":{"sourceFilePath":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileName":"exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-mlflow-experiments-with-a-powerful-ui"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://aimstack.io/wp-content/uploads/2021/03/pytorch.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\n\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\n\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\n\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance. One thing you do not need when tuning hyper-parameters is variance.\n\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\n\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using [Aim](https://github.com/aimhubio/aim).\n\n## **How to fix the seed in PyTorch Lightning**\n\n\n\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\n\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use `pl.seed_everything(seed)` . See the docs [here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\n\n\u003e Note: in other libraries you would use something like: `np.random.seed()` or `torch.manual_seed()` \n\n## **Implementation**\n\n\n\nFind the full code for this and other tutorials [here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\n\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n```\n\n\n\n## Analyzing the Training Runs\n\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after [Aim 1.3.5 release](https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)) and have multiple charts of different metrics on the same screen.\n\nDo the following steps to see the different effects of the optimizers\n\n* Go to dashboard, explore by experiment\n* Add loss to `SELECT` and divide into subplots by metric\n* Group by experiment to make all metrics of similar color\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\n\nHere is how it looks on Aim:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/10.gif)\n\n![](https://aimstack.io/wp-content/uploads/2022/02/11.png)\n\n\n\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\n\nIf you apply the same settings to the learning rate, this is the result:\n\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\n\n\n\n* Remove both previous groupings\n* Group by color by learning rate\n\n![](https://aimstack.io/wp-content/uploads/2022/02/12.gif)\n\n![](https://aimstack.io/wp-content/uploads/2022/02/13.png)\n\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\n\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\n\nOn top of this, if we also add grouping by style by optimizer:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/24.gif)\n\n![](https://aimstack.io/wp-content/uploads/2022/02/25.png)\n\nNow, it is obvious that the the runs with SGD optimizer and `lr=0.01` (green, broken lines) are the best choices for all the seeds we have tried.\n\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\n\n## Learn More\n\n\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is a random seed and how is it important?\u003c/h2\u003e\n\u003cp\u003eThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\u003c/li\u003e\n\u003cli\u003eDropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\u003c/li\u003e\n\u003cli\u003eAugmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance. One thing you do not need when tuning hyper-parameters is variance.\u003c/p\u003e\n\u003cp\u003eThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\u003c/p\u003e\n\u003cp\u003eLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eHow to fix the seed in PyTorch Lightning\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python\u003ccode\u003erandom.seed(seed)\u003c/code\u003e, but it is not enough for PL.\u003c/p\u003e\n\u003cp\u003ePytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use \u003ccode\u003epl.seed_everything(seed)\u003c/code\u003e . See the docs \u003ca href=\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: in other libraries you would use something like: \u003ccode\u003enp.random.seed()\u003c/code\u003e or \u003ccode\u003etorch.manual_seed()\u003c/code\u003e \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFind the full code for this and other tutorials \u003ca href=\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAnalyzing the Training Runs\u003c/h2\u003e\n\u003cp\u003eAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after \u003ca href=\"https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\"\u003eAim 1.3.5 release\u003c/a\u003e) and have multiple charts of different metrics on the same screen.\u003c/p\u003e\n\u003cp\u003eDo the following steps to see the different effects of the optimizers\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGo to dashboard, explore by experiment\u003c/li\u003e\n\u003cli\u003eAdd loss to \u003ccode\u003eSELECT\u003c/code\u003e and divide into subplots by metric\u003c/li\u003e\n\u003cli\u003eGroup by experiment to make all metrics of similar color\u003c/li\u003e\n\u003cli\u003eGroup by style by optimizer to see different optimizers on loss and accuracy and its effects\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere is how it looks on Aim:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/10.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/11.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\u003c/p\u003e\n\u003cp\u003eIf you apply the same settings to the learning rate, this is the result:\u003c/p\u003e\n\u003ch2\u003eFor the next step to analyze how learning rate affects the experiments, do the following steps:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemove both previous groupings\u003c/li\u003e\n\u003cli\u003eGroup by color by learning rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/12.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/13.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\u003c/p\u003e\n\u003cp\u003eWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\u003c/p\u003e\n\u003cp\u003eOn top of this, if we also add grouping by style by optimizer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/24.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/25.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, it is obvious that the the runs with SGD optimizer and \u003ccode\u003elr=0.01\u003c/code\u003e (green, broken lines) are the best choices for all the seeds we have tried.\u003c/p\u003e\n\u003cp\u003eFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"}]},"__N_SSG":true},"page":"/category/[slug]","query":{"slug":"tutorials"},"buildId":"gQ9GDp-9lnline6LHZU5G","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>