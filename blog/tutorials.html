<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Tutorials | AimStack</title><meta name="robots" content="index,follow"/><meta name="description" content="Access your category related articles"/><meta property="og:title" content="Tutorials | AimStack"/><meta property="og:description" content="Access your category related articles"/><meta property="og:url" content="https://aimstack.io/blog/tutorials"/><meta property="og:type" content="website"/><meta property="og:image" content="https://aimstack.io/banner.png"/><meta property="og:image:alt" content="Tutorials | AimStack"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1224"/><meta property="og:image:height" content="724"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="AimStack"/><link rel="canonical" href="https://aimstack.io/blog/tutorials"/><link rel="preload" href="/_next/static/media/bg.eb38a857.svg" as="image" fetchpriority="high"/><meta name="next-head-count" content="18"/><style id="stitches">--sxs{--sxs:0 t-dSaWrp}@media{:root,.t-dSaWrp{--fonts-base:inherit;--colors-blue:#1093F2;--colors-blueHover:#0F7AC8;--colors-lightBlue:rgba(16, 147, 242, 0.1);--colors-lightBlueHover:rgba(16, 147, 242, 0.2);--colors-darkBlue:#0C1031;--colors-darkBlue500:rgba(12,16,49,0.5);--colors-bigStone:#191D3C;--colors-bigStoneHover:#313551;--colors-green:#14C89D;--colors-black:#000000;--colors-black003:rgba(0,0,0,0.03);--colors-black700:rgba(0,0,0,0.7);--colors-black800:rgba(0,0,0,0.8);--colors-white:#ffffff;--colors-white100:rgba(255,255,255,0.1);--colors-white500:rgba(255,255,255,0.5);--colors-white700:rgba(255,255,255,0.7);--colors-red:#CC231A;--colors-lightGrey:#F4F7F9;--colors-lightGreyHover:#ECEEF0;--colors-grey:#CFD3D6;--colors-darkGrey:#737379;--colors-darkGreyHover:#393940;--colors-primary:var(--colors-blue);--colors-primaryHover:var(--colors-blueHover);--colors-primaryLight:var(--colors-lightBlue);--colors-primaryLightHover:var(--colors-lightBlueHover);--colors-secondary:var(--colors-green);--colors-textColor:var(--colors-black);--colors-bgColor:var(--colors-darkBlue);--colors-danger:var(--colors-red);--fontSizes-1:14px;--fontSizes-2:16px;--fontSizes-3:18px;--fontSizes-4:20px;--fontSizes-5:22px;--fontSizes-6:24px;--fontSizes-7:32px;--fontSizes-8:44px;--fontSizes-9:56px;--fontSizes-10:64px;--fontSizes-11:68px;--fontSizes-baseSize:var(--fontSizes-2);--space-1:4px;--space-2:8px;--space-3:12px;--space-4:16px;--space-5:20px;--space-6:24px;--space-7:28px;--space-8:32px;--space-9:36px;--space-10:40px;--space-11:44px;--space-12:48px;--space-13:52px;--space-14:56px;--fontWeights-1:400;--fontWeights-2:500;--fontWeights-3:600;--fontWeights-4:700;--fontWeights-5:800;--fontWeights-6:900;--shadows-1:0px 60px 66px -65px rgba(11, 47, 97, 0.2);--shadows-2:0px 96px 66px -65px rgba(11, 47, 97, 0.2);--shadows-3:1px 1px 10px 3px  rgb(0, 0, 0, 10%);--shadows-4:0px 10px 24px -10px rgba(11, 47, 97, 0.4);--shadows-5:0px 10px 32px -10px rgba(11, 47, 97, 0.2);--radii-1:6px;--radii-2:8px;--transitions-main:0.2s ease-out}}--sxs{--sxs:1 iPqvQa k-iKtTFk}@media{html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,main,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section{display:block}*[hidden]{display:none}*{box-sizing:border-box}ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:none}table{border-spacing:0}strong{font-weight:var(--fontWeights-5)}body{background:var(--colors-white);color:var(--colors-textColor);font-family:var(--fonts-base);font-size:var(--fontSizes-baseSize);line-height:1.35;overscroll-behavior:none}img{user-drag:none;-webkit-user-drag:none}a{text-decoration:none;color:inherit}a.link{color:var(--colors-primary);font-weight:var(--fontWeights-4);transition:var(--transitions-main)}a.link:hover{color:var(--colors-primaryHover)}.text-center{text-align:center}@keyframes k-iKtTFk{0%{opacity:0}10%{opacity:1}90%{opacity:1}100%{opacity:0}}}--sxs{--sxs:2 c-iSkjJi c-guYHoi c-ckOUFN c-hCkBfr c-ePqJJt c-jafYUQ c-BqIMD c-iCFsHS c-kPczbf c-bMqEGV c-fSprKn c-lizetl c-fOPBY c-cZmHrB c-iKzMen c-dhzjXW c-PJLV c-hpFSsv c-kSnMQa c-kXnqgD c-fgXcmv c-gPAtaN c-dMnFVR c-kHrbHn c-hJzboU c-ghrOTq c-kBJMsw c-fcTZTi c-gvTmKt c-FsNLu c-jgJYki c-bFmrFE c-MNZuo c-cChYXC c-hPoOJn c-kbuvzy c-cXVAvf c-dsvWej c-cmpvrW c-crdKmK c-jpbidf c-jFppbe c-hnRRWM c-hQOWqi c-jiSXep}@media{.c-iSkjJi{position:relative}.c-iSkjJi .bg-top{z-index:2}.c-iSkjJi .bg-bottom{z-index:1;position:absolute;bottom:300px;left:0;right:0;object-fit:contain;width:100%;height:60%}.c-guYHoi{position:relative;z-index:3;display:flex;flex-direction:column;min-height:100vh}.c-ckOUFN{height:72px;position:fixed;top:0px;left:0px;right:0px;z-index:99;transition:var(--transitions-main);background-color:transparent}.c-ckOUFN.dark{background-color:var(--colors-darkBlue)}.c-ckOUFN.dark a{color:var(--colors-white)}.c-ckOUFN.dark.fixed a{color:var(--colors-textColor)}.c-ckOUFN.fixed{box-shadow:var(--shadows-3);background-color:var(--colors-white)}.c-hCkBfr{margin-left:auto;margin-right:auto;padding-left:var(--space-6);padding-right:var(--space-6);width:100%;max-width:1300px}.c-ePqJJt{display:flex;align-items:center;height:100%}.c-jafYUQ{margin-right:50px}.c-jafYUQ .logo{max-width:158px;width:100%;display:block}.c-jafYUQ .logo-image{display:block}@media (max-width: 1024px){.c-jafYUQ{position:relative;z-index:11}}.c-BqIMD{display:flex;justify-content:center}.c-BqIMD .nav-list{display:flex}.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-6)}@media (max-width: 1440px){.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-4)}}.c-BqIMD .nav-list li a{-webkit-backface-visibility:hidden;backface-visibility:hidden;display:inline-flex}.c-BqIMD .nav-list li a .text{transition:var(--transitions-main)}.c-BqIMD .nav-list li a:hover .text{opacity:.6}@media (max-width: 1024px){.c-BqIMD{position:fixed;top:72px;right:0;bottom:0;left:0;overflow-y:auto;z-index:10;height:0;transition:height 0.5s;background-color:var(--colors-white);flex-direction:column;justify-content:space-between}}@media (max-width: 1024px){.open .c-BqIMD{height:calc(100% - 72px)}}@media (max-width: 1024px){.c-BqIMD .nav-inner{width:100%;flex-direction:column;align-items:inherit;justify-content:space-between;padding-top:16px}}@media (max-width: 1024px){.c-BqIMD .nav-list{flex-direction:column;align-items:flex-start}}@media (max-width: 1024px){.c-BqIMD .nav-list > li{font-size:var(--fontSizes-2);font-weight:var(--fontWeights-3);width:100%}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a{display:block;padding:var(--space-3) var(--space-5)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a:active{background-color:var(--colors-primaryLight)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li:not(:last-child){margin-right:0}}.c-iCFsHS{display:inline-block;height:18px;line-height:18px;padding:0 4px;border-radius:2px;background-color:var(--colors-primary);color:var(--colors-white);font-weight:700;font-size:10px;margin-left:6px}.c-kPczbf{margin-left:auto}.c-kPczbf span span{display:flex;align-items:center}.c-kPczbf.desktop-btn span span{justify-content:flex-end}.c-bMqEGV{-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer;border-radius:var(--radii-1);display:inline-block;transition:var(--transitions-main)}.c-fSprKn{display:flex;gap:10px;align-items:center;height:100%;margin-right:2px;margin-left:2px}.c-lizetl{display:none}@media (max-width: 1024px){.c-lizetl{display:flex;justify-content:center;padding-top:var(--space-6);padding-bottom:var(--space-6)}}@media (max-width: 1024px){.c-lizetl > li{margin-right:var(--space-6)}}.c-fOPBY{display:none}@media (max-width: 1024px){.c-fOPBY{position:relative;z-index:11;display:inline-block;-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer}}.c-cZmHrB{flex:1}.c-iKzMen{text-align:center;padding-top:80px;padding-bottom:80px}.c-dhzjXW{display:flex}.c-hpFSsv{padding-top:80px;padding-bottom:80px;color:var(--colors-white);background-color:var(--colors-darkBlue);position:relative}.c-hpFSsv .bg-top{object-fit:cover;object-position:top}@media (max-width: 1024px){.c-hpFSsv{padding-top:44px;padding-bottom:44px}}.c-kSnMQa{display:flex;flex-wrap:wrap;margin-left:-60px;margin-top:60px}@media (max-width: 743px){.c-kSnMQa{margin-left:0;margin-top:var(--space-8)}}.c-kXnqgD{width:calc((100% / 2) - 60px);margin-left:60px;margin-bottom:60px}@media (max-width: 743px){.c-kXnqgD{width:100%;margin-left:0;margin-bottom:var(--space-10)}}.c-fgXcmv{max-width:650px}.c-fgXcmv img{width:100%;height:auto}@media (max-width: 1024px){.c-fgXcmv{margin:0 auto}}.c-gPAtaN{text-align:center;padding-top:100px}.c-dMnFVR{display:flex;flex-wrap:wrap;margin-left:-40px;margin-top:100px}@media (max-width: 1024px){.c-dMnFVR{justify-content:center}}@media (max-width: 743px){.c-dMnFVR{margin-left:0}}.c-kHrbHn{width:calc((100% / 4) - 40px);margin-left:40px;margin-bottom:60px}@media (max-width: 1024px){.c-kHrbHn{width:calc((100% / 2) - 40px);margin-bottom:40px}}@media (max-width: 743px){.c-kHrbHn{width:100%;margin-left:0}}.c-kHrbHn .card-content{margin-top:var(--space-6)}@media (max-width: 743px){.c-kHrbHn .card-content{text-align:left;margin-top:0}}.c-hJzboU{-webkit-mask-size:100%;mask-size:100%;-webkit-mask-repeat:no-repeat}.c-hJzboU img{object-fit:contain;height:100%;width:100%}@media (max-width: 743px){.c-hJzboU{width:100px;min-width:100px;margin-right:var(--space-6)}}.c-ghrOTq{overflow:hidden;width:100%;height:auto;padding-top:4px;display:flex;flex-direction:column}.c-kBJMsw{width:100%;height:auto;scale:1.02}.c-fcTZTi{min-height:100px;display:flex;flex-direction:column;background-color:#673AB7}.c-gvTmKt{margin-top:auto}.c-FsNLu{display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;color:var(--colors-white);padding-top:27px;padding-bottom:27px;border-bottom:1px solid var(--colors-white100)}.c-jgJYki{margin-right:var(--space-9)}.c-jgJYki img{display:block}@media (max-width: 743px){.c-jgJYki img{margin:0 auto}}@media (max-width: 1440px){.c-jgJYki{margin-right:var(--space-6)}}@media (max-width: 743px){.c-jgJYki{width:100%;margin-right:0;margin-bottom:var(--space-1);text-align:center}}.c-bFmrFE{display:flex;margin-right:auto}.c-bFmrFE li:not(:last-child){margin-right:var(--space-9)}@media (max-width: 1440px){.c-bFmrFE li:not(:last-child){margin-right:var(--space-4)}}.c-bFmrFE li{transition:var(--transitions-main)}.c-bFmrFE li:hover{opacity:.6}@media (max-width: 1024px){.c-bFmrFE{order:3;width:100%;margin-top:var(--space-7)}}@media (max-width: 1024px){.c-bFmrFE li:last-child{margin-right:0}}@media (max-width: 743px){.c-bFmrFE{order:2;width:100%;margin-bottom:var(--space-2);flex-direction:column;text-align:center}}@media (max-width: 743px){.c-bFmrFE li{margin-bottom:var(--space-6)}}@media (max-width: 743px){.c-bFmrFE li:not(:last-child){margin-right:0}}.c-MNZuo{display:flex}.c-MNZuo li:not(:last-child){margin-right:var(--space-6)}.c-MNZuo li a{transition:var(--transitions-main)}.c-MNZuo li a:hover{opacity:.6}@media (max-width: 743px){.c-MNZuo{margin:0 auto;order:3}}.c-cChYXC{padding-top:var(--space-4);padding-bottom:var(--space-4);color:var(--colors-white500);text-align:center}.c-hPoOJn{padding-top:100px;padding-bottom:100px}.c-kbuvzy{flex:1;box-shadow:var(--shadows-4)}.c-kbuvzy table{border-collapse:collapse;border-radius:var(--radii-2);table-layout:fixed;margin-bottom:var(--space-4);display:flex}.c-kbuvzy td{border:1px solid #FAFAFA;width:290px;height:60px;text-align:left;vertical-align:middle;padding-left:50px;padding-right:50px}.c-kbuvzy th{border:1px solid #FAFAFA;width:290px;height:100px;text-align:center;padding:20px}.c-kbuvzy tr:nth-child(even){background-color:#FAFAFA}.c-kbuvzy td:nth-child(3){box-shadow:0 0 24px 0px #0B2F6133;-webkit-clip-path:inset(0px -15px 0px -15px);clip-path:inset(0px -15px 0px -15px)}.c-kbuvzy th:nth-child(3){box-shadow:0 0 24px 0px #0B2F6133;-webkit-clip-path:inset(0px -15px 0px -15px);clip-path:inset(0px -15px 0px -15px)}.c-kbuvzy tr:nth-child(2){font-weight:bold}.c-kbuvzy tr:nth-child(3){font-weight:bold}.c-kbuvzy td:first-child{font-weight:bold}.c-cXVAvf{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:40px}@media (max-width: 1024px){.c-cXVAvf{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-cXVAvf{gap:20px;grid-template-columns:repeat(1,minmax(0,1fr))}}.c-dsvWej{padding:10px;border:1px solid #E2D7EB;border-radius:var(--radii-2);transition:var(--transitions-main);height:100%}.c-dsvWej:hover{background-color:#F3F5F9}.c-cmpvrW{position:relative}.c-crdKmK{width:100%;object-fit:cover;height:200px;min-height:200px}.c-jpbidf{padding:20px 10px 0px 10px;position:relative}.c-jFppbe{margin-bottom:var(--space-2);display:flex;align-items:center;color:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe .icon{margin-right:var(--space-1);fill:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe:hover{color:var(--colors-darkGray)}.c-jFppbe:hover .icon{fill:var(--colors-darkGray)}.c-hnRRWM{display:flex;align-items:center;justify-content:flex-end}.c-hnRRWM .icon{margin-right:var(--space-1);fill:var(--colors-grey)}.c-hQOWqi{margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-jiSXep{display:flex;align-items:center;justify-content:center}.c-jiSXep li{height:44px;width:44px;font-size:var(--fontSizes-1);font-weight:var(--fontWeights-4);text-align:center;margin-right:var(--space-1)}.c-jiSXep li a{display:block;line-height:44px;transition:var(--transitions-main)}.c-jiSXep li a.active{background-color:var(--colors-primary);color:var(--colors-white)}.c-jiSXep li a:hover:not(.active){color:var(--colors-primary)}.c-jiSXep li a:hover .icon{fill:var(--colors-primary)}}--sxs{--sxs:3 c-bMqEGV-jIiqPO-size-2 c-bMqEGV-fTPcDy-variant-community c-dhzjXW-iTKOFX-direction-column c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-PJLV-ifRTQw-size-10 c-PJLV-dnvIlH-size-4 c-PJLV-ohsET-size-8 c-dhzjXW-ejCoEP-direction-row c-PJLV-ypqAk-size-3 c-PJLV-cBLpOj-size-2 c-PJLV-EDrvg-size-1 c-dhzjXW-irEjuD-align-stretch c-dhzjXW-dOBaZS-gap-12 c-PJLV-fQcqgK-size-5 c-bMqEGV-hZKDTB-variant-secondary_outline c-bMqEGV-dxfqfJ-variant-primary c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true c-PJLV-cllNQK-lineClamp-true}@media{.c-bMqEGV-jIiqPO-size-2{height:32px;line-height:32px;font-size:var(--fontSizes-1);padding-left:var(--space-2);padding-right:var(--space-2)}.c-bMqEGV-fTPcDy-variant-community{background-color:#5865F2;color:var(--colors-white);border:1px solid #5865F2}.c-dhzjXW-iTKOFX-direction-column{flex-direction:column}.c-dhzjXW-jroWjL-align-center{align-items:center}.c-dhzjXW-awKDG-justify-start{justify-content:flex-start}.c-dhzjXW-kVNAnR-wrap-noWrap{flex-wrap:nowrap}.c-PJLV-ifRTQw-size-10{font-size:var(--fontSizes-10);font-weight:var(--fontWeights-6);line-height:1.25}@media (max-width: 1024px){.c-PJLV-ifRTQw-size-10{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-ifRTQw-size-10{font-size:var(--fontSizes-7)}}.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-4)}@media (max-width: 1024px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-3)}}@media (max-width: 743px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-2)}}.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-5);line-height:1.5}@media (max-width: 743px){.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-7)}}.c-dhzjXW-ejCoEP-direction-row{flex-direction:row}.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-3)}@media (max-width: 743px){.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-2)}}.c-PJLV-cBLpOj-size-2{font-size:var(--fontSizes-2)}.c-PJLV-EDrvg-size-1{font-size:var(--fontSizes-1)}.c-dhzjXW-irEjuD-align-stretch{align-items:stretch}.c-dhzjXW-dOBaZS-gap-12{gap:var(--space-12)}.c-PJLV-fQcqgK-size-5{font-size:var(--fontSizes-5);font-weight:var(--fontWeights-3)}.c-bMqEGV-hZKDTB-variant-secondary_outline{background-color:var(--colors-white);color:var(--colors-primary);border:1px solid var(--colors-primary)}.c-bMqEGV-hZKDTB-variant-secondary_outline:hover{background-color:var(--colors-primaryLightHover)}.c-bMqEGV-dxfqfJ-variant-primary{background-color:var(--colors-primary);color:var(--colors-white)}.c-bMqEGV-dxfqfJ-variant-primary:hover{background-color:var(--colors-primaryHover)}.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-4)}}.c-PJLV-iMgaFX-truncate-true{max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.c-PJLV-cllNQK-lineClamp-true{display:-webkit-box;-webkit-line-clamp:var(---lineClamp);-webkit-box-orient:vertical;overflow:hidden}}--sxs{--sxs:4 c-dhzjXW-lelvPF-direction-columnReverse c-dhzjXW-cLKPGv-direction-row c-dhzjXW-ewYpoe-align-start c-dhzjXW-dAvekr-direction-column}@media{@media (max-width: 1024px){.c-dhzjXW-lelvPF-direction-columnReverse{flex-direction:column-reverse}}@media (max-width: 743px){.c-dhzjXW-cLKPGv-direction-row{flex-direction:row}}@media (max-width: 743px){.c-dhzjXW-ewYpoe-align-start{align-items:flex-start}}@media (max-width: 743px){.c-dhzjXW-dAvekr-direction-column{flex-direction:column}}}--sxs{--sxs:6 c-hCkBfr-ilkBNdM-css c-kPczbf-igSvvfr-css c-kPczbf-idOghFk-css c-bMqEGV-ifNxlSP-css c-fOPBY-ieBuAdh-css c-dhzjXW-ielgvHS-css c-PJLV-ietOTGQ-css c-PJLV-ibrRWTZ-css c-hCkBfr-ikYMMba-css c-PJLV-igHOLBX-css c-PJLV-ijskIFF-css c-PJLV-ijpcBqa-css c-hJzboU-ieOwGK-css c-PJLV-idgasAY-css c-PJLV-ieFlRcC-css c-PJLV-iciGHnC-css c-hJzboU-icdsEyN-css c-hJzboU-iYSetQ-css c-hJzboU-iebhThe-css c-hJzboU-ifZMaZh-css c-hJzboU-ieVlAUk-css c-dhzjXW-ieHGUxU-css c-PJLV-iiKgbon-css c-bMqEGV-iioXsEe-css c-PJLV-iejLhMq-css c-PJLV-idKitzl-css c-PJLV-ieRZfPH-css c-PJLV-iUazGY-css c-PJLV-icsdZpM-css}@media{.c-hCkBfr-ilkBNdM-css{height:100%}.c-kPczbf-igSvvfr-css{display:none}@media (max-width: 1024px){.c-kPczbf-igSvvfr-css{display:block;padding:var(--space-5)}}.c-kPczbf-idOghFk-css{flex:1}@media (max-width: 1024px){.c-kPczbf-idOghFk-css{display:none}}.c-bMqEGV-ifNxlSP-css{float:right}.c-fOPBY-ieBuAdh-css{margin-left:auto;padding:var(--space-3)}.c-dhzjXW-ielgvHS-css{padding-top:80px;padding-bottom:80px}.c-PJLV-ietOTGQ-css{margin-bottom:var(--space-6)}.c-PJLV-ibrRWTZ-css{max-width:848px}.c-hCkBfr-ikYMMba-css{position:relative;z-index:2}@media (max-width: 1024px){.c-PJLV-igHOLBX-css{text-align:center}}.c-PJLV-ijskIFF-css{font-weight:var(--fontWeights-3);padding:var(--space-3) 0 var(--space-2)}.c-PJLV-ijpcBqa-css{color:var(--colors-white700)}.c-hJzboU-ieOwGK-css{-webkit-mask-image:url(/images/static/about-us/shapes/1.svg);mask-image:url(/images/static/about-us/shapes/1.svg)}.c-PJLV-idgasAY-css{font-weight:var(--fontWeights-4)}.c-PJLV-ieFlRcC-css{padding-top:var(--space-2);padding-bottom:var(--space-2);font-weight:var(--fontWeights-3)}.c-PJLV-iciGHnC-css{color:var(--colors-black700)}.c-hJzboU-icdsEyN-css{-webkit-mask-image:url(/images/static/about-us/shapes/2.svg);mask-image:url(/images/static/about-us/shapes/2.svg)}.c-hJzboU-iYSetQ-css{-webkit-mask-image:url(/images/static/about-us/shapes/3.svg);mask-image:url(/images/static/about-us/shapes/3.svg)}.c-hJzboU-iebhThe-css{-webkit-mask-image:url(/images/static/about-us/shapes/5.svg);mask-image:url(/images/static/about-us/shapes/5.svg)}.c-hJzboU-ifZMaZh-css{-webkit-mask-image:url(/images/static/about-us/shapes/6.svg);mask-image:url(/images/static/about-us/shapes/6.svg)}.c-hJzboU-ieVlAUk-css{-webkit-mask-image:url(/images/static/about-us/shapes/7.svg);mask-image:url(/images/static/about-us/shapes/7.svg)}.c-dhzjXW-ieHGUxU-css{padding-bottom:104px}.c-PJLV-iiKgbon-css{margin-bottom:var(--space-9)}.c-bMqEGV-iioXsEe-css{margin-top:var(--space-4)}.c-PJLV-iejLhMq-css{margin-bottom:var(--space-4)}.c-PJLV-idKitzl-css{margin-bottom:var(--space-12)}.c-PJLV-ieRZfPH-css{text-align:center;margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-PJLV-iUazGY-css{display:flex;align-items:center}.c-PJLV-icsdZpM-css{margin-top:var(--space-4);margin-bottom:var(--space-4);---lineClamp:3;font-family:var(--fonts-Lora)}}</style><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/cc5e782db96f5926.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/cc5e782db96f5926.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-46696d78b9933bc6.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-64ec93240796860d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-408fe53479a88159.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/962-688f32d50bed486d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bcategory%5D-c13ffc473bcb306e.js" defer="" crossorigin=""></script><script src="/_next/static/7hvt0DoxCki5tk9cAyukf/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/7hvt0DoxCki5tk9cAyukf/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><main class="c-iSkjJi __className_d65c78"><div class="c-guYHoi"><header class="c-ckOUFN"><div class="c-hCkBfr c-hCkBfr-ilkBNdM-css"><div class="c-ePqJJt"><div class="c-jafYUQ"><a class="logo" href="/"><img alt="AimStack" loading="lazy" width="156" height="37" decoding="async" data-nimg="1" class="logo-image next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 156 37&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/images/static/main/logo.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/static/main/logo.svg"/></a></div><nav class="c-BqIMD"><div class="nav-inner"><ul class="nav-list"><li><a target="_self" class="" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" class="" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" class="" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" class="" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" class="" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" class="active" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" class="" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" class="" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><div class="c-kPczbf c-kPczbf-igSvvfr-css"><a class="c-bMqEGV c-bMqEGV-jIiqPO-size-2 c-bMqEGV-fTPcDy-variant-community community-btn" href="https://community.aimstack.io"><div class="c-fSprKn"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/>Join Community</div></a></div><div class="c-kPczbf c-kPczbf-igSvvfr-css"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div></div><ul class="c-lizetl"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></nav><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><a class="c-bMqEGV c-bMqEGV-jIiqPO-size-2 c-bMqEGV-fTPcDy-variant-community c-bMqEGV-ifNxlSP-css community-btn" href="https://community.aimstack.io"><div class="c-fSprKn"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/>Join Community</div></a></div><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div><button type="button" aria-label="menu" class="c-fOPBY c-fOPBY-ieBuAdh-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:black" d="M0 146.286c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.289 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM0 512.008c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.29 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM60.952 816.748c-33.663 0-60.952 27.29-60.952 60.956 0 33.661 27.289 60.951 60.952 60.951h902.097c33.661 0 60.951-27.29 60.951-60.951 0-33.667-27.29-60.956-60.951-60.956h-902.097z"></path></svg></button></div></div></header><div class="c-cZmHrB"><div style="padding-block:100px" class="c-hCkBfr"><h1 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-ieRZfPH-css title">Category: <!-- -->Tutorials</h1><ul class="c-cXVAvf"><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/log-insights-from-your-ml-experiments-why-reports-matter"><img alt="Log insights from your ML experiments: Why Reports matter" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/screen-shot-2024-10-23-at-10.26.17.png 1x, /images/dynamic/screen-shot-2024-10-23-at-10.26.17.png 2x" src="/images/dynamic/screen-shot-2024-10-23-at-10.26.17.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/log-insights-from-your-ml-experiments-why-reports-matter">Log insights from your ML experiments: Why Reports matter</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/log-insights-from-your-ml-experiments-why-reports-matter">Iterating over analyses is an essential part of model training. Reports make it easier to track the progress and see how models improve. That&#x27;s why we created it for you! 🎉</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/reproducibility-in-ml"><img alt="Reproducibility in ML" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/medium-2-.png 1x, /images/dynamic/medium-2-.png 2x" src="/images/dynamic/medium-2-.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/reproducibility-in-ml">Reproducibility in ML</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/reproducibility-in-ml">What is Reproducibility in ML? What makes it challenging? Tools and platforms to address different aspects of reproducibility: Data Management, Experiment Tracking, Version Control.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/keep-your-model-up-to-date-for-question-answering"><img alt="Keep your model up to date for Question Answering " loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/medium-1-.png 1x, /images/dynamic/medium-1-.png 2x" src="/images/dynamic/medium-1-.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/keep-your-model-up-to-date-for-question-answering">Keep your model up to date for Question Answering </a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/keep-your-model-up-to-date-for-question-answering">In this article, we&#x27;ll show you how to keep your language model up to date for question answering tasks. We&#x27;ll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. </a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/training-models-to-predict-heart-attack"><img alt="Training Models to Predict Heart Attack" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/medium.png 1x, /images/dynamic/medium.png 2x" src="/images/dynamic/medium.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/training-models-to-predict-heart-attack">Training Models to Predict Heart Attack</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/training-models-to-predict-heart-attack">Explore heart attack prediction using machine learning.This article breaks down key factors and insights from the Heart Attack Prediction dataset. With the help of Aim we keep track of model performance.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/reproducible-forecasting-with-prophet-and-aim"><img alt="Reproducible forecasting with Prophet and Aim" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="https://miro.medium.com/v2/resize:fit:384/format:webp/quality:75/1*ZaIDQvLbqy0KyVG22D0epw.png 1x, https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*ZaIDQvLbqy0KyVG22D0epw.png 2x" src="https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*ZaIDQvLbqy0KyVG22D0epw.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/reproducible-forecasting-with-prophet-and-aim">Reproducible forecasting with Prophet and Aim</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/reproducible-forecasting-with-prophet-and-aim">You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. </a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim"><img alt="Hugging the Chaos: Connecting Datasets to Trainings with Hugging Face and Aim" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="https://miro.medium.com/v2/resize:fit:384/format:webp/quality:75/1*-luoNtZBpA1SGkTsHzCrkA.jpeg 1x, https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*-luoNtZBpA1SGkTsHzCrkA.jpeg 2x" src="https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*-luoNtZBpA1SGkTsHzCrkA.jpeg"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim">Hugging the Chaos: Connecting Datasets to Trainings with Hugging Face and Aim</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim">Combining Hugging Face and Aim to make machine learning experiments traceable, reproducible and easier to compare.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/aim-and-mlflow-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer"><img alt="Aim and MLflow — Choosing Experiment Tracker for Zero-Shot Cross-Lingual Transfer" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="https://miro.medium.com/v2/resize:fit:384/format:webp/quality:75/1*v64PbdBn6kBvsH3t5bkv8w.png 1x, https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*v64PbdBn6kBvsH3t5bkv8w.png 2x" src="https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*v64PbdBn6kBvsH3t5bkv8w.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/aim-and-mlflow-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer">Aim and MLflow — Choosing Experiment Tracker for Zero-Shot Cross-Lingual Transfer</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/aim-and-mlflow-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer">The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user...</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/how-to-integrate-aimlflow-with-your-remote-mlflow"><img alt="How to integrate aimlflow with your remote MLflow" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="https://miro.medium.com/v2/resize:fit:384/format:webp/quality:75/1*rSCbDO5InRlBXTxzD3NVwQ.png 1x, https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*rSCbDO5InRlBXTxzD3NVwQ.png 2x" src="https://miro.medium.com/v2/resize:fit:640/format:webp/quality:75/1*rSCbDO5InRlBXTxzD3NVwQ.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/how-to-integrate-aimlflow-with-your-remote-mlflow">How to integrate aimlflow with your remote MLflow</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/how-to-integrate-aimlflow-with-your-remote-mlflow">We are thrilled to unveil aimlflow, a tool that allows for a smooth integration of a robust experiment tracking UI with MLflow logs! 🚀</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/tutorials/exploring-mlflow-experiments-with-a-powerful-ui"><img alt="Exploring MLflow experiments with a powerful UI" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="https://miro.medium.com/max/384/1*YMlI66d-QPxI3fcQCoPSlw.webp 1x, https://miro.medium.com/max/640/1*YMlI66d-QPxI3fcQCoPSlw.webp 2x" src="https://miro.medium.com/max/640/1*YMlI66d-QPxI3fcQCoPSlw.webp"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/tutorials/exploring-mlflow-experiments-with-a-powerful-ui">Exploring MLflow experiments with a powerful UI</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/tutorials/exploring-mlflow-experiments-with-a-powerful-ui">We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉</a></p><div class="c-hnRRWM"></div></div></div></li></ul><nav class="c-hQOWqi"><ul class="c-jiSXep"><li><a class="active" href="/blog/tutorials?page=1">1</a></li><li><a class="" href="/blog/tutorials?page=2">2</a></li><li><a href="/blog/tutorials?page=2"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M663.040 548.198l36.198-36.198-289.638-289.638-72.397 72.397 217.19 217.242-217.19 217.242 72.397 72.397 253.44-253.44z"></path></svg></a></li></ul></nav></div></div><footer class="c-ghrOTq"><img alt="footer" fetchpriority="high" width="1440" height="246" decoding="async" data-nimg="1" class="c-kBJMsw next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1440 246&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/bg.eb38a857.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/bg.eb38a857.svg"/><div class="c-fcTZTi"><div class="c-hCkBfr c-gvTmKt"><div class="c-FsNLu"><div class="c-jgJYki"><a class="logo" href="/"><picture><source height="26" width="109" media="(max-width: 1199px)" srcSet="[object Object]"/><img alt="Aimstack" loading="lazy" width="26" height="26" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 26 26&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/aim-logo.e9fed5b2.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/aim-logo.e9fed5b2.svg"/></picture></a></div><ul class="c-bFmrFE"><li><a target="_self" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><ul class="c-MNZuo"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></div><div class="c-cChYXC"><p class="c-PJLV c-PJLV-EDrvg-size-1">Copyright © <!-- -->2024<!-- --> Aimstack</p></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"posts":[{"title":"3D Spleen Segmentation with MONAI and Aim","date":"2022-06-27T20:55:12.121Z","author":"Gev Soghomonian","description":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from http://medicaldecathlon.com/. This is a","slug":"3d-spleen-segmentation-with-monai-and-aim","image":"/images/dynamic/image-7-.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from [](http://medicaldecathlon.com/)\u003chttp://medicaldecathlon.com/\u003e.\n\nThis is a longer form of the [3D spleen segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb) on the MONAI tutorials repo.\n\nFor a complete in-depth overview of the code and the methodology please follow [this tutorial](https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb) directly or our very own [Aim from Zero to Hero tutorial](https://aimstack.io/blog/tutorials/aim-from-zero-to-hero).\n\n## ***Tracking the basics***\n\nAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\n\n### ***Transformations Matter***\n\nThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the`Single Run Page`.\n\n![](/images/dynamic/image-10-.png)\n\nYou can further search/filter through the runs based on these transformations using Aims’ very own pythonic search language `AimQL`. An Advanced usage looks like this\n\n![](/images/dynamic/image-11-.png)\n\n### *Lets talk about models and optimizers*\n\nSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic `dict`-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from`AimQL`\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n```\n\n![](/images/dynamic/image-12-.png)\n\n## What about Losses?\n\nMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\n\nFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image-5-.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the [complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html) or simply play around in our [interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n## *A picture is worth a thousand words*\n\nIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at `Aim`. You can track images as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\n\n```\naim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n```\n\nAll the grouping/filtering/aggregation functional presented above are also available for Images.\n\n![](/images/dynamic/image-6-.jpeg)\n\n![](/images/dynamic/image-7-.jpeg)\n\n## *A figure is worth a thousand pictures*\n\nWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\n\n```\naim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n```\n\nWithin *`Aim`* you can access all the tracked Figures from within the `Single Run Page`\n\n``\n\n![](/images/dynamic/image-2-.gif)\n\nAnother interesting thing that one can [track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\n## ***The curtain falls***\n\nIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with *`1-2`* lines of code. You can also play around with the completed MONAI integration results with our [interactive demo](http://play.aimstack.io:10005/).\n\nIf you have any bug reports please follow up on our [github repository](https://github.com/aimhubio/aim/issues/new/choose). Feel free to join our growing [Slack community](https://slack.aimstack.io/) and ask questions directly to the developers and seasoned users.\n\nIf you find Aim useful, [please stop by](https://github.com/aimhubio/aim) and drop us a star.","html":"\u003cp\u003eIn this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from \u003ca href=\"http://medicaldecathlon.com/\"\u003e\u003c/a\u003e\u003ca href=\"http://medicaldecathlon.com/\"\u003ehttp://medicaldecathlon.com/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is a longer form of the \u003ca href=\"https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb\"\u003e3D spleen segmentation tutorial\u003c/a\u003e on the MONAI tutorials repo.\u003c/p\u003e\n\u003cp\u003eFor a complete in-depth overview of the code and the methodology please follow \u003ca href=\"https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb\"\u003ethis tutorial\u003c/a\u003e directly or our very own \u003ca href=\"https://aimstack.io/blog/tutorials/aim-from-zero-to-hero\"\u003eAim from Zero to Hero tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eTracking the basics\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\u003c/p\u003e\n\u003ch3\u003e\u003cem\u003e\u003cstrong\u003eTransformations Matter\u003c/strong\u003e\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the\u003ccode\u003eSingle Run Page\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-10-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can further search/filter through the runs based on these transformations using Aims’ very own pythonic search language \u003ccode\u003eAimQL\u003c/code\u003e. An Advanced usage looks like this\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-11-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cem\u003eLets talk about models and optimizers\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic \u003ccode\u003edict\u003c/code\u003e-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from\u003ccode\u003eAimQL\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-12-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eWhat about Losses?\u003c/h2\u003e\n\u003cp\u003eMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\u003c/p\u003e\n\u003cp\u003eFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter grouping, we end up with a visualization akin to this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-5-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003ecomplete documentation\u003c/a\u003e or simply play around in our \u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive Demo\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eA picture is worth a thousand words\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at \u003ccode\u003eAim\u003c/code\u003e. You can track images as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll the grouping/filtering/aggregation functional presented above are also available for Images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-6-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-7-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eA figure is worth a thousand pictures\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithin \u003cem\u003e\u003ccode\u003eAim\u003c/code\u003e\u003c/em\u003e you can access all the tracked Figures from within the \u003ccode\u003eSingle Run Page\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e``\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-2-.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAnother interesting thing that one can \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\"\u003etrack is distributions\u003c/a\u003e. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDistributions can be accessed from the Single Run Page as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-6-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eThe curtain falls\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with \u003cem\u003e\u003ccode\u003e1-2\u003c/code\u003e\u003c/em\u003e lines of code. You can also play around with the completed MONAI integration results with our \u003ca href=\"http://play.aimstack.io:10005/\"\u003einteractive demo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you have any bug reports please follow up on our \u003ca href=\"https://github.com/aimhubio/aim/issues/new/choose\"\u003egithub repository\u003c/a\u003e. Feel free to join our growing \u003ca href=\"https://slack.aimstack.io/\"\u003eSlack community\u003c/a\u003e and ask questions directly to the developers and seasoned users.\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, \u003ca href=\"https://github.com/aimhubio/aim\"\u003eplease stop by\u003c/a\u003e and drop us a star.\u003c/p\u003e"},"_id":"posts/3d-spleen-segmentation-with-monai-and-aim.md","_raw":{"sourceFilePath":"posts/3d-spleen-segmentation-with-monai-and-aim.md","sourceFileName":"3d-spleen-segmentation-with-monai-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/3d-spleen-segmentation-with-monai-and-aim"},"type":"Post"},{"title":"Aim and MLflow — Choosing Experiment Tracker for Zero-Shot Cross-Lingual Transfer","date":"2023-02-14T06:41:14.310Z","author":"Hovhannes Tamoyan","description":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user...","slug":"aim-and-mlflow-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v64PbdBn6kBvsH3t5bkv8w.png","draft":false,"categories":["Tutorials"],"body":{"raw":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.\n\nThe question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.\n\nIn this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.\n\n# Task Setup\n\nThe task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.\n\nParticularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.\n\nZero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.\n\nWe will explore two techniques in our experimentation:\n\n* Fine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.\n* Feature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.\n\nIn both techniques, we will undertake training utilizing the `en` subset of the `XNLI` dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from the `XNLI` dataset, including English(`en`), German(`de`), French(`fr`), Spanish(`es`), Chinese(`zh`), and Arabic(`ar`).\n\n# The Datasets\n\nWe will utilize the `XNLI` (cross-lingual `NLI`) dataset, which is a selection of a few thousand examples from the `MNLI` (multi `NLI`) dataset, translated into 14 different languages, including some with limited resources.\n\nThe template of the `NLI` task is as follows. Given a pair of sentences, a `premise` and a `hypothesis` need to determine whether a `hypothesis` is true (entailment), false (contradiction), or undetermined (neutral) given a `premise`.\n\nLet’s take a look at a few samples to get hang of it. Say the given hypothesis is `“Issues in Data Synthesis.”` and the premise is `“Problems in data synthesis.”`. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.\n\nAnother example this time of a neutral pair of sentences is the following: the hypothesis is `“She was so happy she couldn't stop smiling.”` and the premise is `“She smiled back.”`. The first sentence doesn’t imply the second one, however, it doesn’t contradict it as well. Thus they are neutral.\n\nAn instance of contradiction, the hypothesis is `“The analysis proves that there is no link between PM and bronchitis.”` and the premise is `“This analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.”`. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.\n\nFor more examples please explore the HuggingFace Datasets page powered by Streamlit: \u003chttps://huggingface.co/datasets/viewer/\u003e.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png)\n\n# The Models\n\n\n\nIn our experiments, we will utilize the following set of pre-trained multilingual language models:\n\n![](/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png)\n\nWe will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.\n\n# Setting up Training Environment\n\nBefore beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:\n\n\n\n```\naim-and-mlflow-usecase\n├── logs\n│   ├── aim_callback\n│   │   └── .aim\n│   ├── aimlflow\n│   │   └── .aim\n│   ├── checkpoints\n│   └── mlruns\n└── main.py\n```\n\nLet’s start off by creating the main directory, we named it `aim-and-mlflow-usecase`, you can simply name anything you want. After which we need to download the `main.py` from the following source: \u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\u003e. The code explanation and sample usage can be found in the `README.md` file of the directory. We will be using this script to run our experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png)\n\nThe `logs` directory as the name suggests stores the logs. In the `checkpoints` folder, all the model states will be saved. The `mlruns` is the repository for MLflow experiments. The `aim_callback` will store the repository of Aim runs tracked using Aim’s built-in callback for Hugging Face Transformers, meanwhile, the `aimlflow` will store the runs converted from MLflow using the aimlflow tool.\n\n\u003e *It is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.*\n\nIn order to start the training process, we will be using the following command. But first, let’s navigate to the directory where our script is located (`aim-and-mlflow-usecase` in our case).\n\n```\npython main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nWhere `{PATH_TO}` is the absolute path of the `aim-and-mlflow-usecase` directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:\n\n```\npython main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nGo grab some snacks, trainings take a while 🍫 ☺️.\n\n# Using aimlflow\n\nMeanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use the `aimlflow` to simply convert and view our experiments live on Aim. Let’s first show how this can be done after which tackle the question.\n\nInstal `aimlflow` on your machine via `pip`, if it is not already installed:\n\n```\n$ pip install aimlflow\n```\n\n```\n$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n```\n\nThis command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.\n\nMore on how the `aimlflow` can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:\n\n* **[Exploring MLflow experiments with a powerful UI](https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e)**\n* **[How to integrate aimlflow with your remote MLflow](https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf)**\n\nThis is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aim’s built-in callback, `aim.hugging_face.AimCallback`, which is specifically designed for t`ransformers.Trainer` functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.\n\n# Unlocking the Power of Data Analysis\n\n\n\nOnce the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, let’s launch both tools’ UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:\n\n```\n$ mlflow ui\n```\n\n```\n$ aim up\n```\n\n\n\n\u003e *Note that for the best possible experience, we will be using the* `aim_callback/.aim` *repository in this demonstration, as it has deeper integration with the* `Trainer`*.*\n\nThe UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visiting `http://127.0.0.1:5000` for MLflow and `http://127.0.0.1:43800` for Aim.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png \"The user interface of MLflow on first look\")\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png \"The user interface of Aim on first look\")\n\nIn order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit the `Compare Runs` page:\n\nBy selecting all the experiments, navigate to the comparison page by clicking the `Compare` button.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png)\n\nThe `Run details` section presents the run metadata, including the start and end time and duration of the run. The `Parameters` section displays the hyperparameters used for the run, such as the optimizer and architecture. The `Metrics` section showcases the latest values for each metric.\n\nHaving access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.\n\nTo organize the parameters into meaningful groups for our experiment, simply go to Aim’s `Metrics Explorer` page and follow a few straightforward steps:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif)\n\n\n\n\n\n# Gaining insights\n\n\n\nLet’s examine the charts more closely and uncover valuable insights from our experiments.\n\nA quick examination of the charts reveals the following observations:\n\n* The fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.\n* The graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.\n\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png)\n\n\n\n* Even though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only by `train_batch_size`. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png)\n\n\n\n* The charts unmistakably show that the `bert-base-multilingual-cased` model achieved the best accuracy results, with the highest score observed for the `en` subset, as the model was trained on that subset. Subsequently, `es`, `fr`, `de`, `zh`, and `ar` followed. Unsurprisingly the scores for the `zh` and `ar` datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png)\n\n\n\n* Let us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.\n* ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png)\n\n\n\n  One can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. The `Parameter Explorer` will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how the `Parameter Explorer` works, let’s pick the following parameters: `train_batch_size`, `learning_rate`,`_name_or_path`, `loss`, and the accuracies of `sub_dataset`s. The following chart will be observed after clicking the `Search` button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses the `bert-base-multilingual-cased` model, with 5·10⁻⁵ `learning_rate` and the `batch_size` is 8.\n\n  Taking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.\n\n  # Conclusion\n\n  This blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aim’s powerful user interface to obtain valuable insights from the experiments.\n\n  We also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.","html":"\u003cp\u003eThe release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.\u003c/p\u003e\n\u003cp\u003eThe question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.\u003c/p\u003e\n\u003cp\u003eIn this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.\u003c/p\u003e\n\u003ch1\u003eTask Setup\u003c/h1\u003e\n\u003cp\u003eThe task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.\u003c/p\u003e\n\u003cp\u003eParticularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.\u003c/p\u003e\n\u003cp\u003eZero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.\u003c/p\u003e\n\u003cp\u003eWe will explore two techniques in our experimentation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.\u003c/li\u003e\n\u003cli\u003eFeature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn both techniques, we will undertake training utilizing the \u003ccode\u003een\u003c/code\u003e subset of the \u003ccode\u003eXNLI\u003c/code\u003e dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from the \u003ccode\u003eXNLI\u003c/code\u003e dataset, including English(\u003ccode\u003een\u003c/code\u003e), German(\u003ccode\u003ede\u003c/code\u003e), French(\u003ccode\u003efr\u003c/code\u003e), Spanish(\u003ccode\u003ees\u003c/code\u003e), Chinese(\u003ccode\u003ezh\u003c/code\u003e), and Arabic(\u003ccode\u003ear\u003c/code\u003e).\u003c/p\u003e\n\u003ch1\u003eThe Datasets\u003c/h1\u003e\n\u003cp\u003eWe will utilize the \u003ccode\u003eXNLI\u003c/code\u003e (cross-lingual \u003ccode\u003eNLI\u003c/code\u003e) dataset, which is a selection of a few thousand examples from the \u003ccode\u003eMNLI\u003c/code\u003e (multi \u003ccode\u003eNLI\u003c/code\u003e) dataset, translated into 14 different languages, including some with limited resources.\u003c/p\u003e\n\u003cp\u003eThe template of the \u003ccode\u003eNLI\u003c/code\u003e task is as follows. Given a pair of sentences, a \u003ccode\u003epremise\u003c/code\u003e and a \u003ccode\u003ehypothesis\u003c/code\u003e need to determine whether a \u003ccode\u003ehypothesis\u003c/code\u003e is true (entailment), false (contradiction), or undetermined (neutral) given a \u003ccode\u003epremise\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eLet’s take a look at a few samples to get hang of it. Say the given hypothesis is \u003ccode\u003e“Issues in Data Synthesis.”\u003c/code\u003e and the premise is \u003ccode\u003e“Problems in data synthesis.”\u003c/code\u003e. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.\u003c/p\u003e\n\u003cp\u003eAnother example this time of a neutral pair of sentences is the following: the hypothesis is \u003ccode\u003e“She was so happy she couldn't stop smiling.”\u003c/code\u003e and the premise is \u003ccode\u003e“She smiled back.”\u003c/code\u003e. The first sentence doesn’t imply the second one, however, it doesn’t contradict it as well. Thus they are neutral.\u003c/p\u003e\n\u003cp\u003eAn instance of contradiction, the hypothesis is \u003ccode\u003e“The analysis proves that there is no link between PM and bronchitis.”\u003c/code\u003e and the premise is \u003ccode\u003e“This analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.”\u003c/code\u003e. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.\u003c/p\u003e\n\u003cp\u003eFor more examples please explore the HuggingFace Datasets page powered by Streamlit: \u003ca href=\"https://huggingface.co/datasets/viewer/\"\u003ehttps://huggingface.co/datasets/viewer/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eThe Models\u003c/h1\u003e\n\u003cp\u003eIn our experiments, we will utilize the following set of pre-trained multilingual language models:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.\u003c/p\u003e\n\u003ch1\u003eSetting up Training Environment\u003c/h1\u003e\n\u003cp\u003eBefore beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim-and-mlflow-usecase\n├── logs\n│   ├── aim_callback\n│   │   └── .aim\n│   ├── aimlflow\n│   │   └── .aim\n│   ├── checkpoints\n│   └── mlruns\n└── main.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s start off by creating the main directory, we named it \u003ccode\u003eaim-and-mlflow-usecase\u003c/code\u003e, you can simply name anything you want. After which we need to download the \u003ccode\u003emain.py\u003c/code\u003e from the following source: \u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\u003c/a\u003e. The code explanation and sample usage can be found in the \u003ccode\u003eREADME.md\u003c/code\u003e file of the directory. We will be using this script to run our experiments.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003elogs\u003c/code\u003e directory as the name suggests stores the logs. In the \u003ccode\u003echeckpoints\u003c/code\u003e folder, all the model states will be saved. The \u003ccode\u003emlruns\u003c/code\u003e is the repository for MLflow experiments. The \u003ccode\u003eaim_callback\u003c/code\u003e will store the repository of Aim runs tracked using Aim’s built-in callback for Hugging Face Transformers, meanwhile, the \u003ccode\u003eaimlflow\u003c/code\u003e will store the runs converted from MLflow using the aimlflow tool.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eIt is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn order to start the training process, we will be using the following command. But first, let’s navigate to the directory where our script is located (\u003ccode\u003eaim-and-mlflow-usecase\u003c/code\u003e in our case).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere \u003ccode\u003e{PATH_TO}\u003c/code\u003e is the absolute path of the \u003ccode\u003eaim-and-mlflow-usecase\u003c/code\u003e directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGo grab some snacks, trainings take a while 🍫 ☺️.\u003c/p\u003e\n\u003ch1\u003eUsing aimlflow\u003c/h1\u003e\n\u003cp\u003eMeanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use the \u003ccode\u003eaimlflow\u003c/code\u003e to simply convert and view our experiments live on Aim. Let’s first show how this can be done after which tackle the question.\u003c/p\u003e\n\u003cp\u003eInstal \u003ccode\u003eaimlflow\u003c/code\u003e on your machine via \u003ccode\u003epip\u003c/code\u003e, if it is not already installed:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ pip install aimlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.\u003c/p\u003e\n\u003cp\u003eMore on how the \u003ccode\u003eaimlflow\u003c/code\u003e can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\"\u003eExploring MLflow experiments with a powerful UI\u003c/a\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf\"\u003eHow to integrate aimlflow with your remote MLflow\u003c/a\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aim’s built-in callback, \u003ccode\u003eaim.hugging_face.AimCallback\u003c/code\u003e, which is specifically designed for t\u003ccode\u003eransformers.Trainer\u003c/code\u003e functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.\u003c/p\u003e\n\u003ch1\u003eUnlocking the Power of Data Analysis\u003c/h1\u003e\n\u003cp\u003eOnce the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, let’s launch both tools’ UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ mlflow ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote that for the best possible experience, we will be using the\u003c/em\u003e \u003ccode\u003eaim_callback/.aim\u003c/code\u003e \u003cem\u003erepository in this demonstration, as it has deeper integration with the\u003c/em\u003e \u003ccode\u003eTrainer\u003c/code\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visiting \u003ccode\u003ehttp://127.0.0.1:5000\u003c/code\u003e for MLflow and \u003ccode\u003ehttp://127.0.0.1:43800\u003c/code\u003e for Aim.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png\" alt=\"\" title=\"The user interface of MLflow on first look\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png\" alt=\"\" title=\"The user interface of Aim on first look\"\u003e\u003c/p\u003e\n\u003cp\u003eIn order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit the \u003ccode\u003eCompare Runs\u003c/code\u003e page:\u003c/p\u003e\n\u003cp\u003eBy selecting all the experiments, navigate to the comparison page by clicking the \u003ccode\u003eCompare\u003c/code\u003e button.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eRun details\u003c/code\u003e section presents the run metadata, including the start and end time and duration of the run. The \u003ccode\u003eParameters\u003c/code\u003e section displays the hyperparameters used for the run, such as the optimizer and architecture. The \u003ccode\u003eMetrics\u003c/code\u003e section showcases the latest values for each metric.\u003c/p\u003e\n\u003cp\u003eHaving access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.\u003c/p\u003e\n\u003cp\u003eTo organize the parameters into meaningful groups for our experiment, simply go to Aim’s \u003ccode\u003eMetrics Explorer\u003c/code\u003e page and follow a few straightforward steps:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eGaining insights\u003c/h1\u003e\n\u003cp\u003eLet’s examine the charts more closely and uncover valuable insights from our experiments.\u003c/p\u003e\n\u003cp\u003eA quick examination of the charts reveals the following observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.\u003c/li\u003e\n\u003cli\u003eThe graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEven though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only by \u003ccode\u003etrain_batch_size\u003c/code\u003e. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe charts unmistakably show that the \u003ccode\u003ebert-base-multilingual-cased\u003c/code\u003e model achieved the best accuracy results, with the highest score observed for the \u003ccode\u003een\u003c/code\u003e subset, as the model was trained on that subset. Subsequently, \u003ccode\u003ees\u003c/code\u003e, \u003ccode\u003efr\u003c/code\u003e, \u003ccode\u003ede\u003c/code\u003e, \u003ccode\u003ezh\u003c/code\u003e, and \u003ccode\u003ear\u003c/code\u003e followed. Unsurprisingly the scores for the \u003ccode\u003ezh\u003c/code\u003e and \u003ccode\u003ear\u003c/code\u003e datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eLet us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOne can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. The \u003ccode\u003eParameter Explorer\u003c/code\u003e will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how the \u003ccode\u003eParameter Explorer\u003c/code\u003e works, let’s pick the following parameters: \u003ccode\u003etrain_batch_size\u003c/code\u003e, \u003ccode\u003elearning_rate\u003c/code\u003e,\u003ccode\u003e_name_or_path\u003c/code\u003e, \u003ccode\u003eloss\u003c/code\u003e, and the accuracies of \u003ccode\u003esub_dataset\u003c/code\u003es. The following chart will be observed after clicking the \u003ccode\u003eSearch\u003c/code\u003e button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses the \u003ccode\u003ebert-base-multilingual-cased\u003c/code\u003e model, with 5·10⁻⁵ \u003ccode\u003elearning_rate\u003c/code\u003e and the \u003ccode\u003ebatch_size\u003c/code\u003e is 8.\u003c/p\u003e\n\u003cp\u003eTaking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eThis blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aim’s powerful user interface to obtain valuable insights from the experiments.\u003c/p\u003e\n\u003cp\u003eWe also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/aim-and-mlflow-—-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","_raw":{"sourceFilePath":"posts/aim-and-mlflow-—-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileName":"aim-and-mlflow-—-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-and-mlflow-—-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Discover how to leverage Aim's context and subplot features for effortless comparison of validation and test metrics, enhancing your ML experiment analysis.","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that on [Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\nUse the [aim.track](https://github.com/aimhubio/aim#track) context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n```\nimport aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n```\n\nOnce the training is ran, execute `aim up` in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n\u003e **\\*Note:** The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLet’s go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.** Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.** Add the `bleu` metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.** Search by `context.subset` to show both `test` and `val` `loss` and `bleu` metrics. Divide into subplots further by `context.subset` too so Aim UI shows `test` and `val` metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\n\n\nHere is a full summary [video](https://youtu.be/DGI8S7SUfEk) on how to do it on the UI.\n\n# Learn More\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://slack.aimstack.io/) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003cp\u003eResearchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\u003c/p\u003e\n\u003cp\u003eThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\u003c/p\u003e\n\u003cp\u003eThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\u003c/p\u003e\n\u003cp\u003eModels are usually ran on validation subset after each epoch.\u003cbr\u003e\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\u003c/p\u003e\n\u003cp\u003eThere is a need to collect and effectively compare all these metrics.\u003c/p\u003e\n\u003cp\u003eHere is how to do that on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003eUsing context to track for different subsets?\u003c/h1\u003e\n\u003cp\u003eUse the \u003ca href=\"https://github.com/aimhubio/aim#track\"\u003eaim.track\u003c/a\u003e context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the training is ran, execute \u003ccode\u003eaim up\u003c/code\u003e in your terminal and start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eUsing subplots to compare test, val loss and bleu metrics\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e*Note:\u003c/strong\u003e The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet’s go step-by-step on how to break down lots of experiments using subplots.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1.\u003c/strong\u003e Explore the runs, the context table, play with the query language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2.\u003c/strong\u003e Add the \u003ccode\u003ebleu\u003c/code\u003e metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3.\u003c/strong\u003e Search by \u003ccode\u003econtext.subset\u003c/code\u003e to show both \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003ebleu\u003c/code\u003e metrics. Divide into subplots further by \u003ccode\u003econtext.subset\u003c/code\u003e too so Aim UI shows \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e metrics on different subplots for better comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"\u003e\u003c/p\u003e\n\u003cp\u003eNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\u003c/p\u003e\n\u003ch1\u003eSummary\u003c/h1\u003e\n\u003cp\u003eHere is a full summary \u003ca href=\"https://youtu.be/DGI8S7SUfEk\"\u003evideo\u003c/a\u003e on how to do it on the UI.\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim from Zero to Hero","date":"2022-03-16T16:39:52.780Z","author":"Gev Soghomonian","description":"In this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various","slug":"aim-from-zero-to-hero","image":"/images/dynamic/image.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"In this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\n\n## *Starting from basics: how Aim tracks machine learning experiments*\n\nMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with `matplotlib`. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\n\nTo track the losses, simply create an experiment run and add a tracking common like this\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n```\n\nYou are going to end up with a visualization of this kind.\n\n![](/images/dynamic/image-2-.png)\n\nA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the [complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html) or simply play around in our [interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n- - -\n\n## *Rising beyond watching losses*\n\nAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic dict or dictionary-like object, i.e.\n\n```\naim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n```\n\nIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can [group with respect to the parameters/hyperparameters](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter) that you saved with aim.\n\nViewing the [standalone parameters](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page) is fast for each separate Single Run\n\n![](/images/dynamic/image.png)\n\n[Filtering/grouping your runs using the selected trackable](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter) can be accessed and looked through in our [interactive tutorial](http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n![](/images/dynamic/image-1-.png)\n\nFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language [AimQL](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2), by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\n\n![](/images/dynamic/image-4-.png)\n\n## ***What if numbers are simply not enough?***\n\nVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\n\nAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\n\n```\naim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n```\n\nYou can find the complete guide to tracking in the [official documentation](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#).\n\nAll the grouping/filtering/aggregation functional presented above is also available for Images.\n\n![](/images/dynamic/image-2-.jpeg)\n\n![](/images/dynamic/image-3-.jpeg)\n\n![](/images/dynamic/image-5-.png)\n\nThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\n\n![](/images/dynamic/image.gif)\n\nThe grouping is rather flexible with a [multitude of options](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer).\n\nAnother interesting thing that one can [track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\nYou can play around with the image and single run explorers and see all the trackables across our numerous Demos ([FS2](http://play.aimstack.io:10004/), [Spleen Segmentation](http://play.aimstack.io:10005/), [Lightweight GAN](http://play.aimstack.io:10002/), [Machine Translation](http://play.aimstack.io:10001/)).\n\n***One Run to rule them all***\n\nThere are times when a researcher would need to focus upon only a [single run of the experiment](https://aimstack.io/blog/new-releases/aim-3-7-revamped-run-single-page-and-aim-docker-image), where he can iterate through a complete list of all the things he tracked. Aim has a dedicated [Single Run Page](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page) for this very purpose.\n\nYou can view all the trackables in the following Tabs:\n\n* Parameters/ Hyperparametrs – Everything tracked regarding the experiment\n* Metrics – All the Tracked Metrics/Losses etc.\n* System – All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\n* Distributions – All the distributions tracked (i.e. Flowing gradients and weights)\n* Images – All the Image objects saved\n* Audios – All the Audio objects saved\n* Texts – All the raw text tracked during experimentation\n* Figures – All the `matplotlin`/`Plotly` etc. Figures\n* Settings – Settings that runs share\n\nLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\n\n![](/images/dynamic/image-7-.png)\n\n![](/images/dynamic/image-8-.png)\n\n![](/images/dynamic/image-9-.png)\n\nYou can look around the individual runs in one of our interactive Demos ([FS2](http://play.aimstack.io:10004/), [Spleen Segmentation](http://play.aimstack.io:10005/), [Lightweight GAN](http://play.aimstack.io:10002/), [Machine Translation](http://play.aimstack.io:10001/)).\n\n- - -\n\n## ***No More localhost. Track experiments remotely***\n\nAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\n\n```\naim init\n# Tringgering aim server\naim server --repo \u003cREPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo \u003cREPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n```\n\nIntegrating this newly created [remote tracking server](https://aimstack.io/blog/new-releases/aim-3-4-remote-tracking-alpha-sorting-deleting-runs) within your experimentation is even easier.\n\n```\n# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n```\n\nAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\n\n![](/images/dynamic/image-4-.jpeg)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling  and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003eIn this blog post, we show how to use Aim’s basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eStarting from basics: how Aim tracks machine learning experiments\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with \u003ccode\u003ematplotlib\u003c/code\u003e. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\u003c/p\u003e\n\u003cp\u003eTo track the losses, simply create an experiment run and add a tracking common like this\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou are going to end up with a visualization of this kind.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-2-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter grouping, we end up with a visualization akin to this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003ecomplete documentation\u003c/a\u003e or simply play around in our \u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive Demo\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\u003cem\u003eRising beyond watching losses\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic dict or dictionary-like object, i.e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\"\u003egroup with respect to the parameters/hyperparameters\u003c/a\u003e that you saved with aim.\u003c/p\u003e\n\u003cp\u003eViewing the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\"\u003estandalone parameters\u003c/a\u003e is fast for each separate Single Run\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\"\u003eFiltering/grouping your runs using the selected trackable\u003c/a\u003e can be accessed and looked through in our \u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-1-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2\"\u003eAimQL\u003c/a\u003e, by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-4-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eWhat if numbers are simply not enough?\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\u003c/p\u003e\n\u003cp\u003eAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can find the complete guide to tracking in the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#\"\u003eofficial documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAll the grouping/filtering/aggregation functional presented above is also available for Images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-2-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-3-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-5-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe grouping is rather flexible with a \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer\"\u003emultitude of options\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAnother interesting thing that one can \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\"\u003etrack is distributions\u003c/a\u003e. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDistributions can be accessed from the Single Run Page as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-6-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can play around with the image and single run explorers and see all the trackables across our numerous Demos (\u003ca href=\"http://play.aimstack.io:10004/\"\u003eFS2\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10005/\"\u003eSpleen Segmentation\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10002/\"\u003eLightweight GAN\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10001/\"\u003eMachine Translation\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eOne Run to rule them all\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThere are times when a researcher would need to focus upon only a \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-7-revamped-run-single-page-and-aim-docker-image\"\u003esingle run of the experiment\u003c/a\u003e, where he can iterate through a complete list of all the things he tracked. Aim has a dedicated \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\"\u003eSingle Run Page\u003c/a\u003e for this very purpose.\u003c/p\u003e\n\u003cp\u003eYou can view all the trackables in the following Tabs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eParameters/ Hyperparametrs – Everything tracked regarding the experiment\u003c/li\u003e\n\u003cli\u003eMetrics – All the Tracked Metrics/Losses etc.\u003c/li\u003e\n\u003cli\u003eSystem – All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\u003c/li\u003e\n\u003cli\u003eDistributions – All the distributions tracked (i.e. Flowing gradients and weights)\u003c/li\u003e\n\u003cli\u003eImages – All the Image objects saved\u003c/li\u003e\n\u003cli\u003eAudios – All the Audio objects saved\u003c/li\u003e\n\u003cli\u003eTexts – All the raw text tracked during experimentation\u003c/li\u003e\n\u003cli\u003eFigures – All the \u003ccode\u003ematplotlin\u003c/code\u003e/\u003ccode\u003ePlotly\u003c/code\u003e etc. Figures\u003c/li\u003e\n\u003cli\u003eSettings – Settings that runs share\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-7-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-8-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-9-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can look around the individual runs in one of our interactive Demos (\u003ca href=\"http://play.aimstack.io:10004/\"\u003eFS2\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10005/\"\u003eSpleen Segmentation\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10002/\"\u003eLightweight GAN\u003c/a\u003e, \u003ca href=\"http://play.aimstack.io:10001/\"\u003eMachine Translation\u003c/a\u003e).\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eNo More localhost. Track experiments remotely\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim init\n# Tringgering aim server\naim server --repo \u0026#x3C;REPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo \u0026#x3C;REPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIntegrating this newly created \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-4-remote-tracking-alpha-sorting-deleting-runs\"\u003eremote tracking server\u003c/a\u003e within your experimentation is even easier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-4-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling  and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community,\u003c/a\u003e share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-from-zero-to-hero.md","_raw":{"sourceFilePath":"posts/aim-from-zero-to-hero.md","sourceFileName":"aim-from-zero-to-hero.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-from-zero-to-hero"},"type":"Post"},{"title":"Aim tutorial for Weights and Biases users","date":"2022-09-07T21:13:32.862Z","author":"Hovhannes Tamoyan","description":"Using Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026 Biases runs into the native format \u0026 explore them via Aim UI.","slug":"aim-tutorial-for-weights-and-biases-users","image":"https://miro.medium.com/max/1400/1*PBF_k6VevuUrquadJ86vfA.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\n\nI have been using [Aim](https://github.com/aimhubio/aim) for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\n\nThere are lots of converters already available to migrate or use along with other experiment trackers: [Aim Converters](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html).\n\nUsing Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026 Biases (wandb) runs into the native format and explore them via [Aim UI](https://aimstack.readthedocs.io/en/latest/ui/overview.html).\n\nIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\n\n# Example Project Setup\n\nLet’s take a look at a concrete example. We will be using `keras-tuner` to train a `CNN` on `Cifar10` dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be `sample-team`:\n\n```\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n```\n\nModel creation, data loading and other parts can be done as:\n\n```\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n```\n\nThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let’s convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\n\n\u003e *Note: During this project I noticed that the* `wandb.keras.WandbCallback` *was tracking the metrics on epoch end. For better experience we recommend using the* `aim.keras_tuner.AimCallback`*: to track all the metrics on batch end.*\n\n# Converting runs from Wandb to Aim\n\nTo be able to explore Weights \u0026 Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\n\nPick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\n\n```\n$ aim init\n```\n\nTo start converting wandb experiments from entity/team: `sample-team` and project: `my-awesome-project` run:\n\n```\n$ aim convert wandb --entity sample-team --project my-awesome-project\n```\n\nThe converter will iterate over all the experiments in the project `my-awesome-project` and create a distinct Aim run for each experiment.\n\nFor more please see the [“Show Weights and Biases logs in Aim”](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim) section in [Aim docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nNow that we have our Aim repository initialized, we simply need to do:\n\n```\n$ aim up\n```\n\nto start the Aim UI.\n\n# Exploring the differences in Aim and wandb UIs\n\nTo see the configs, metadata and more on wandb we need to navigate to the “Overview” page:\n\n![](https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp)\n\nUnder the Config section, our logged configurations are shown. On the Summary page the metrics’ latest results.\n\nThe wandb run “Overview” page counterpart on Aim UI is the individual run page, which can be accessed from the “Runs” page:\n\n![](https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp)\n\nMore information can be found when opening an individual run’s overview page.\n\n![](https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp)\n\nOn this page we can see the params, even the `wandb_run_id` and the `wandb_run_name` which are extracted from wandb runs during the conversion process.\n\nMore detailed information about the run can be found on each of the tabs, e.g. “Run Params”, “Metrics”, “System” etc.\n\n# Filtering and Grouping the Metrics\n\nOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\n\nTo see the tracked metrics and compare them with other runs we need to open the “Workspace” page on wandb:\n\n![](https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp)\n\nTo see the metrics on Aim you need to navigate to the “Metrics” page. By default you will see an empty page with “no results” message (which I hope they make this experience better soon), this is because we haven’t selected any metrics to show. To do that we can use the “+ Metrics” button to open up the list of available metrics and select the desired metrics, e.g. “loss” and “accuracy”:\n\n![](https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp)\n\nThis is the view we will get after selecting to show the loss and accuracy:\n\n![](https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp)\n\nNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan’t to see the plots by metric name, so each plot will represent a metric’s values.\n\n![](https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp)\n\nWe simply need to access the `metric.name` .\n\n![](https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp)\n\nTo have unique coloring for each run, we can use the “Group by color” functionality, and provide the run.hash. Aim assigns a unique hash to each run.\n\n\u003e *Note: in this example the Aim UI automatically assigns individual colors for each run.*\n\n\n\nNote: in this example the Aim UI automatically assigns individual colors for each run.)\n\n\\\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the “advanced search mode” click on the pen button under the search button:\n\n![](https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp)\n\nthis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\n\n```\n((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n```\n\nThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\n\n```\nmetric.name in [\"accuracy\", \"loss\"]\n```\n\nFor more please see the “[Search Runs](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs)” page on docs.\n\n# Hyperparameter Explorer\n\nTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the “Add Panel” button, and add the parameters and the metrics we need, e.g.:\n\n![](https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp)\n\nAim counterpart parameters explorer is on the page of “Params”. Where by using the “+ Run Params” button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\n\n![](https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp)\n\n## Plot Operations\n\nThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‘readable’ and explicit. On wandb one needs to click on “Edit panel” sign to see this view:\n\n![](https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp)\n\n\\\nOn Aim you can find the operations on the right sidebar of the Metrics page:\n\n![](https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp)\n\n# Conclusion\n\nAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I’ve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please [read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.","html":"\u003cp\u003eI am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\u003c/p\u003e\n\u003cp\u003eI have been using \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\u003c/p\u003e\n\u003cp\u003eThere are lots of converters already available to migrate or use along with other experiment trackers: \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html\"\u003eAim Converters\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUsing Aim’s Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026#x26; Biases (wandb) runs into the native format and explore them via \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\"\u003eAim UI\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\u003c/p\u003e\n\u003ch1\u003eExample Project Setup\u003c/h1\u003e\n\u003cp\u003eLet’s take a look at a concrete example. We will be using \u003ccode\u003ekeras-tuner\u003c/code\u003e to train a \u003ccode\u003eCNN\u003c/code\u003e on \u003ccode\u003eCifar10\u003c/code\u003e dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be \u003ccode\u003esample-team\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eModel creation, data loading and other parts can be done as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let’s convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote: During this project I noticed that the\u003c/em\u003e \u003ccode\u003ewandb.keras.WandbCallback\u003c/code\u003e \u003cem\u003ewas tracking the metrics on epoch end. For better experience we recommend using the\u003c/em\u003e \u003ccode\u003eaim.keras_tuner.AimCallback\u003c/code\u003e\u003cem\u003e: to track all the metrics on batch end.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eConverting runs from Wandb to Aim\u003c/h1\u003e\n\u003cp\u003eTo be able to explore Weights \u0026#x26; Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\u003c/p\u003e\n\u003cp\u003ePick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo start converting wandb experiments from entity/team: \u003ccode\u003esample-team\u003c/code\u003e and project: \u003ccode\u003emy-awesome-project\u003c/code\u003e run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim convert wandb --entity sample-team --project my-awesome-project\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe converter will iterate over all the experiments in the project \u003ccode\u003emy-awesome-project\u003c/code\u003e and create a distinct Aim run for each experiment.\u003c/p\u003e\n\u003cp\u003eFor more please see the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim\"\u003e“Show Weights and Biases logs in Aim”\u003c/a\u003e section in \u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNow that we have our Aim repository initialized, we simply need to do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eExploring the differences in Aim and wandb UIs\u003c/h1\u003e\n\u003cp\u003eTo see the configs, metadata and more on wandb we need to navigate to the “Overview” page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eUnder the Config section, our logged configurations are shown. On the Summary page the metrics’ latest results.\u003c/p\u003e\n\u003cp\u003eThe wandb run “Overview” page counterpart on Aim UI is the individual run page, which can be accessed from the “Runs” page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eMore information can be found when opening an individual run’s overview page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOn this page we can see the params, even the \u003ccode\u003ewandb_run_id\u003c/code\u003e and the \u003ccode\u003ewandb_run_name\u003c/code\u003e which are extracted from wandb runs during the conversion process.\u003c/p\u003e\n\u003cp\u003eMore detailed information about the run can be found on each of the tabs, e.g. “Run Params”, “Metrics”, “System” etc.\u003c/p\u003e\n\u003ch1\u003eFiltering and Grouping the Metrics\u003c/h1\u003e\n\u003cp\u003eOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\u003c/p\u003e\n\u003cp\u003eTo see the tracked metrics and compare them with other runs we need to open the “Workspace” page on wandb:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo see the metrics on Aim you need to navigate to the “Metrics” page. By default you will see an empty page with “no results” message (which I hope they make this experience better soon), this is because we haven’t selected any metrics to show. To do that we can use the “+ Metrics” button to open up the list of available metrics and select the desired metrics, e.g. “loss” and “accuracy”:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the view we will get after selecting to show the loss and accuracy:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan’t to see the plots by metric name, so each plot will represent a metric’s values.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe simply need to access the \u003ccode\u003emetric.name\u003c/code\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo have unique coloring for each run, we can use the “Group by color” functionality, and provide the run.hash. Aim assigns a unique hash to each run.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote: in this example the Aim UI automatically assigns individual colors for each run.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNote: in this example the Aim UI automatically assigns individual colors for each run.)\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the “advanced search mode” click on the pen button under the search button:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003ethis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emetric.name in [\"accuracy\", \"loss\"]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more please see the “\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs\"\u003eSearch Runs\u003c/a\u003e” page on docs.\u003c/p\u003e\n\u003ch1\u003eHyperparameter Explorer\u003c/h1\u003e\n\u003cp\u003eTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the “Add Panel” button, and add the parameters and the metrics we need, e.g.:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim counterpart parameters explorer is on the page of “Params”. Where by using the “+ Run Params” button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePlot Operations\u003c/h2\u003e\n\u003cp\u003eThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‘readable’ and explicit. On wandb one needs to click on “Edit panel” sign to see this view:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nOn Aim you can find the operations on the right sidebar of the Metrics page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I’ve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please \u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eread the docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.\u003c/p\u003e"},"_id":"posts/aim-tutorial-for-weights-and-biases-users.md","_raw":{"sourceFilePath":"posts/aim-tutorial-for-weights-and-biases-users.md","sourceFileName":"aim-tutorial-for-weights-and-biases-users.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-tutorial-for-weights-and-biases-users"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":"Integration: Aim Logger with XGBoost Library. Learn how to seamlessly use Aim's capabilities in XGBoost experiments. Enhance analysis and insights today! ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\n\n[Aim](https://github.com/aimhubio/aim) is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n## What is XGBoost?\n\n[XGBoost](https://github.com/dmlc/xgboost) is an optimized gradient boosting library with highly  *efficient*,  *flexible,*  and  *portable* design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\n## How to use Aim with XGBoost?\n\nCheck out end-to-end [Aim integration](https://aimstack.io/blog/new-releases/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements) examples with multiple frameworks [here](https://github.com/aimhubio/aim/tree/main). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\n\n```\n# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n```\n\nAs you can see on line 49, AimCallback is imported from `aim.xgboost` and passed to `xgb.train` as one of the callbacks. Aim session can open and close by the AimCallback and the metrics and hparamsstore by XGBoost. In addition to that, thesystem measures pass to Aim as well.\n\n## What it looks like?\n\nAfter you run the experiment and the `aim up` command in the `aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png \"Aim UI dashboard page\")\n\nTo explore the run, we should:\n\n* Choose the `xgboost_test`experiment.\n* Select the metrics to explore.\n* Divide into charts by metrics.\n\nFor example, the gif below illustrates the steps above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif)\n\n\\\n So this is what the final result looks like.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png)\n\nAs easy as that, we can analyze the runs and the system usage.\n\n## Learn More\n\n\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nIf you find Aim useful, support us and star [the project](https://github.com/aimhubio/aim) on GitHub. Also, join [Aim Community ](https://community.aimstack.io/)and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is Aim?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\u003c/p\u003e\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dmlc/xgboost\"\u003eXGBoost\u003c/a\u003e is an optimized gradient boosting library with highly  \u003cem\u003eefficient\u003c/em\u003e,  \u003cem\u003eflexible,\u003c/em\u003e  and  \u003cem\u003eportable\u003c/em\u003e design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\u003c/p\u003e\n\u003ch2\u003eHow to use Aim with XGBoost?\u003c/h2\u003e\n\u003cp\u003eCheck out end-to-end \u003ca href=\"https://aimstack.io/blog/new-releases/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements\"\u003eAim integration\u003c/a\u003e examples with multiple frameworks \u003ca href=\"https://github.com/aimhubio/aim/tree/main\"\u003ehere\u003c/a\u003e. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs you can see on line 49, AimCallback is imported from \u003ccode\u003eaim.xgboost\u003c/code\u003e and passed to \u003ccode\u003exgb.train\u003c/code\u003e as one of the callbacks. Aim session can open and close by the AimCallback and the metrics and hparamsstore by XGBoost. In addition to that, thesystem measures pass to Aim as well.\u003c/p\u003e\n\u003ch2\u003eWhat it looks like?\u003c/h2\u003e\n\u003cp\u003eAfter you run the experiment and the \u003ccode\u003eaim up\u003c/code\u003e command in the \u003ccode\u003eaim_logs\u003c/code\u003edirectory, Aim UI will be running. When first opened, the dashboard page will come up.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png\" alt=\"\" title=\"Aim UI dashboard page\"\u003e\u003c/p\u003e\n\u003cp\u003eTo explore the run, we should:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoose the \u003ccode\u003exgboost_test\u003c/code\u003eexperiment.\u003c/li\u003e\n\u003cli\u003eSelect the metrics to explore.\u003c/li\u003e\n\u003cli\u003eDivide into charts by metrics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, the gif below illustrates the steps above.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nSo this is what the final result looks like.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs easy as that, we can analyze the runs and the system usage.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, support us and star \u003ca href=\"https://github.com/aimhubio/aim\"\u003ethe project\u003c/a\u003e on GitHub. Also, join \u003ca href=\"https://community.aimstack.io/\"\u003eAim Community \u003c/a\u003eand share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"Exploring MLflow experiments with a powerful UI","date":"2023-01-17T21:52:43.983Z","author":"Gor Arakelyan","description":"We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉","slug":"exploring-mlflow-experiments-with-a-powerful-ui","image":"https://miro.medium.com/max/1400/1*YMlI66d-QPxI3fcQCoPSlw.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"\n\nWe are excited to announce the release of [aimlflow](https://github.com/aimhubio/aimlflow), an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉\n\n[MLflow](https://github.com/mlflow/mlflow) is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments. [Aim](https://github.com/aimhubio/aim) is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\n\n![](https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif)\n\n**With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim’s powerful features, leading to deeper understanding and more effective decision-making.**\n\n![](https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp)\n\nIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let’s dive in and see how to make it happen.\n\n\u003e Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\n\u003e\n\u003e View more on GitHub: \u003chttps://github.com/aimhubio/aim\u003e\n\n# Project overview\n\nWe use a simple project that trains a CNN using [PyTorch](https://github.com/pytorch/pytorch) and [Ray Tune](https://github.com/ray-project/ray/tree/master/python/ray/tune) on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\n\n* **PyTorch** for building and training the model\n* **Ray Tune** for hyper-parameters tuning\n* **MLflow** for experiment tracking\n\nFind the full project code on GitHub: \u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003e\n\n![](https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp)\n\nRun the trainings by downloading and executing `tune.py` python file:\n\n```\npython tune.py\n```\n\nYou should see a similar output, meaning the trainings are successfully initiated:\n\n![](https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp)\n\n## Getting started with aimlflow\n\nAfter the hyper-parameter tuning is ran, let’s see how aimlflow can help us to explore the tracked experiments via UI.\n\nTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a `.aim` repo located on the file system.\n\n**This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn’t it amazing? 🤩**\n\n## 1. Install aimlflow on your machine\n\nIt is super easy to install aimlflow, simply run the following command:\n\n```\npip3 install aim-mlflow\n```\n\n## 2. Sync MLflow logs with Aim\n\nPick any directory on your file system and initialize a `.aim` repo:\n\n```\naim init\n```\n\nRun the `aimlflow sync` command to sync MLflow experiments with the Aim repo:\n\n```\naimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n```\n\n## 3. Run Aim\n\nNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\n\n```\naim up --repo=AIM_REPO_PATH\n```\n\nYou will see the following message on the terminal output:\n\n![](https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp)\n\nCongratulations! Now you can explore the training logs with Aim. 🎉\n\n# Quick tour of Aim for MLflow users\n\nIn this section, we will take a quick tour of Aim’s features, including:\n\n* Exploring hyper-parameters tuning results\n* Comparing tracked metrics\n* As well as, taking a look at the other capabilities Aim provides\n\n## Exploring MLflow experiments\n\nNow then Aim is set up and running, we navigate to the project overview page at `127.0.0.1:43800`, where the summary of the project is displayed:\n\n* The number of tracked training runs and experiments\n* Statistics on the amount of tracked metadata\n* A list of experiments and tags, with the ability to quickly explore selected items\n* A calendar and feed of contributions\n* A table of in-progress trainings\n\n![](https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp \"Project overview\")\n\n\\\nTo view the results of the trainings, let’s navigate to the runs dashboard at `127.0.0.1:43800/runs`. Here, you can see hyper-parameters and metrics results all of the trainings.\n\n![](https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp \" Runs dashboard\")\n\nWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\n\n![](https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp \"Run page\")\n\nOn this page, we can view the tracked hparams, including the `mlflow_run_id` and the `mlflow_run_name` which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\n\n## Comparing metrics\n\nComparing metrics across several runs is super easy with Aim:\n\n* Open the metrics page from the left sidebar\n* Select desired metrics by clicking on `+ Metrics` button\n* Pressing `Search` button on the top right corner\n\nWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\n\n![](https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp)\n\nAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\n\n* **by color** — each group of metrics will be filled in with its unique color\n* **by stroke style** — each group will have a unique stroke style (solid, dashed, etc)\n* **by facet** — each group of metrics will be displayed in a separate subplot\n\nTo learn which set of trials performed the best, let’s apply several groupings:\n\n* Group by `run.hparams.l1` hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\n* Group by `metric.name` to divide losses and accuracies into separate subplots (this grouping is applied by default)\n\n![](https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp)\n\n**Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.**\n\nFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\n\n```\nrun.metrics['training_iteration'].last \u003e 1\n```\n\n\n\n\u003e Read more about Aim query language capabilities in the docs: \u003chttps://aimstack.readthedocs.io/en/latest/using/search.html\u003e\n\nThis will result in querying and displaying 9 matched runs:\n\n![](https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp)\n\nLet’s aggregate the groups to see which one performed the best:\n\n![](https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp)\n\nFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. 🎉\n\n\u003e For more please see Aim official docs here: \u003chttps://aimstack.readthedocs.io/en/latest/\u003e\n\n## Last, but not least: a closer look at Aim’s key features\n\n* Use powerful pythonic search to select the runs you want to analyze:\n\n![](https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp)\n\nGroup metrics by hyperparameters to analyze hyperparameters’ influence on run performance:\n\n![](https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp)\n\nSelect multiple metrics and analyze them side by side:\n\n![](https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp)\n\nAggregate metrics by std.dev, std.err, conf.interval:\n\n* ![](https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp)\n\n  Align x axis by any other metric:\n\n  * ![](https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp)\n\n  Scatter plots to learn correlations and trends:\n\n  ![](https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp)\n\n  High dimensional data visualization via parallel coordinate plot:\n\n  ![](https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp)\n\n  Explore media metadata, such as images and audio objects via Aim Explorers:\n\n  * ![](https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp \"Audio Explorer\")\n\n  ![](https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp \"Images Explorer\")\n\n  # Conclusion\n\n  In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\n\n  In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\n\n  We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please [read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\n  # Learn more\n\n  If you have any questions join [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. 🙌\n\n  Show some love by dropping a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"\u003cp\u003eWe are excited to announce the release of \u003ca href=\"https://github.com/aimhubio/aimlflow\"\u003eaimlflow\u003c/a\u003e, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! 🎉\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mlflow/mlflow\"\u003eMLflow\u003c/a\u003e is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments. \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim’s powerful features, leading to deeper understanding and more effective decision-making.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let’s dive in and see how to make it happen.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\u003c/p\u003e\n\u003cp\u003eView more on GitHub: \u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eWe use a simple project that trains a CNN using \u003ca href=\"https://github.com/pytorch/pytorch\"\u003ePyTorch\u003c/a\u003e and \u003ca href=\"https://github.com/ray-project/ray/tree/master/python/ray/tune\"\u003eRay Tune\u003c/a\u003e on the \u003ca href=\"https://www.cs.toronto.edu/~kriz/cifar.html\"\u003eCIFAR-10\u003c/a\u003e dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePyTorch\u003c/strong\u003e for building and training the model\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRay Tune\u003c/strong\u003e for hyper-parameters tuning\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMLflow\u003c/strong\u003e for experiment tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFind the full project code on GitHub: \u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eRun the trainings by downloading and executing \u003ccode\u003etune.py\u003c/code\u003e python file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython tune.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should see a similar output, meaning the trainings are successfully initiated:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eGetting started with aimlflow\u003c/h2\u003e\n\u003cp\u003eAfter the hyper-parameter tuning is ran, let’s see how aimlflow can help us to explore the tracked experiments via UI.\u003c/p\u003e\n\u003cp\u003eTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a \u003ccode\u003e.aim\u003c/code\u003e repo located on the file system.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn’t it amazing? 🤩\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e1. Install aimlflow on your machine\u003c/h2\u003e\n\u003cp\u003eIt is super easy to install aimlflow, simply run the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install aim-mlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e2. Sync MLflow logs with Aim\u003c/h2\u003e\n\u003cp\u003ePick any directory on your file system and initialize a \u003ccode\u003e.aim\u003c/code\u003e repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the \u003ccode\u003eaimlflow sync\u003c/code\u003e command to sync MLflow experiments with the Aim repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e3. Run Aim\u003c/h2\u003e\n\u003cp\u003eNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up --repo=AIM_REPO_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will see the following message on the terminal output:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eCongratulations! Now you can explore the training logs with Aim. 🎉\u003c/p\u003e\n\u003ch1\u003eQuick tour of Aim for MLflow users\u003c/h1\u003e\n\u003cp\u003eIn this section, we will take a quick tour of Aim’s features, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExploring hyper-parameters tuning results\u003c/li\u003e\n\u003cli\u003eComparing tracked metrics\u003c/li\u003e\n\u003cli\u003eAs well as, taking a look at the other capabilities Aim provides\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExploring MLflow experiments\u003c/h2\u003e\n\u003cp\u003eNow then Aim is set up and running, we navigate to the project overview page at \u003ccode\u003e127.0.0.1:43800\u003c/code\u003e, where the summary of the project is displayed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe number of tracked training runs and experiments\u003c/li\u003e\n\u003cli\u003eStatistics on the amount of tracked metadata\u003c/li\u003e\n\u003cli\u003eA list of experiments and tags, with the ability to quickly explore selected items\u003c/li\u003e\n\u003cli\u003eA calendar and feed of contributions\u003c/li\u003e\n\u003cli\u003eA table of in-progress trainings\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp\" alt=\"\" title=\"Project overview\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nTo view the results of the trainings, let’s navigate to the runs dashboard at \u003ccode\u003e127.0.0.1:43800/runs\u003c/code\u003e. Here, you can see hyper-parameters and metrics results all of the trainings.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp\" alt=\"\" title=\" Runs dashboard\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp\" alt=\"\" title=\"Run page\"\u003e\u003c/p\u003e\n\u003cp\u003eOn this page, we can view the tracked hparams, including the \u003ccode\u003emlflow_run_id\u003c/code\u003e and the \u003ccode\u003emlflow_run_name\u003c/code\u003e which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\u003c/p\u003e\n\u003ch2\u003eComparing metrics\u003c/h2\u003e\n\u003cp\u003eComparing metrics across several runs is super easy with Aim:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen the metrics page from the left sidebar\u003c/li\u003e\n\u003cli\u003eSelect desired metrics by clicking on \u003ccode\u003e+ Metrics\u003c/code\u003e button\u003c/li\u003e\n\u003cli\u003ePressing \u003ccode\u003eSearch\u003c/code\u003e button on the top right corner\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eby color\u003c/strong\u003e — each group of metrics will be filled in with its unique color\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eby stroke style\u003c/strong\u003e — each group will have a unique stroke style (solid, dashed, etc)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eby facet\u003c/strong\u003e — each group of metrics will be displayed in a separate subplot\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo learn which set of trials performed the best, let’s apply several groupings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGroup by \u003ccode\u003erun.hparams.l1\u003c/code\u003e hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\u003c/li\u003e\n\u003cli\u003eGroup by \u003ccode\u003emetric.name\u003c/code\u003e to divide losses and accuracies into separate subplots (this grouping is applied by default)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.metrics['training_iteration'].last \u003e 1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRead more about Aim query language capabilities in the docs: \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/search.html\"\u003ehttps://aimstack.readthedocs.io/en/latest/using/search.html\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis will result in querying and displaying 9 matched runs:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eLet’s aggregate the groups to see which one performed the best:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. 🎉\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFor more please see Aim official docs here: \u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003ehttps://aimstack.readthedocs.io/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eLast, but not least: a closer look at Aim’s key features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUse powerful pythonic search to select the runs you want to analyze:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eGroup metrics by hyperparameters to analyze hyperparameters’ influence on run performance:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSelect multiple metrics and analyze them side by side:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregate metrics by std.dev, std.err, conf.interval:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAlign x axis by any other metric:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp\" alt=\"\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eScatter plots to learn correlations and trends:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHigh dimensional data visualization via parallel coordinate plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eExplore media metadata, such as images and audio objects via Aim Explorers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp\" alt=\"\" title=\"Audio Explorer\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp\" alt=\"\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eIn conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\u003c/p\u003e\n\u003cp\u003eIn this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\u003c/p\u003e\n\u003cp\u003eWe covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please \u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eread the docs\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. 🙌\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you think Aim is useful.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","_raw":{"sourceFilePath":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileName":"exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-mlflow-experiments-with-a-powerful-ui"},"type":"Post"},{"title":"How to integrate aimlflow with your remote MLflow","date":"2023-01-30T06:25:58.163Z","author":"Hovhannes Tamoyan","description":"We are thrilled to unveil aimlflow, a tool that allows for a smooth integration of a robust experiment tracking UI with MLflow logs! 🚀","slug":"how-to-integrate-aimlflow-with-your-remote-mlflow","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSCbDO5InRlBXTxzD3NVwQ.png","draft":false,"categories":["Tutorials"],"body":{"raw":"With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim’s powerful features, leading to deeper understanding and more effective decision-making.\n\nWe have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:\n\nRunning Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here: \u003chttps://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\u003e\n\nIn this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.\n\n# Project overview\n\nWe will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.\n\nWe will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.\n\nFind the full project code on GitHub: \u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003e\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png)\n\n# Server-side/Remote Configuration\n\nLet’s create a separate directory for the demo and name it `mlflow-demo-remote`. After which download and run the `tune.py` python script from the Github repo to conduct the training sessions:\n\n```\n$ python tune.py\n```\n\nRay Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png)\n\nOnce started, mlflow will commence recording the results in the `mlruns` directory. Our remote directory will have the following structure:\n\n```\nmlflow-demo-remote\n├── tune.py\n└── mlruns\n    ├── ...\n```\n\nLet’s open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from the `mlflow-demo-remote` directory:\n\n```\n$ mlflow ui --host 0.0.0.0\n```\n\nBy default, the `--host` is set to `127.0.0.1`, limiting access to the service to the local machine only. To expose it to external machines, set the host to `0.0.0.0`.\n\nBy default, the system listens on port `5000`.\n\nOne can set `--backend-store-uri` param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path of `mlruns` directory.\n\nUpon navigating to `http://127.0.0.1:5000`, you will be presented with a page that looks similar to this:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png)\n\n# Synchronising MLflow Runs with Aim\n\n\n\nAfter successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.\n\nFirst, let’s move forward with the installation process of aimlflow. It’s incredibly easy to set up on your device, just execute the following command:\n\n```\n$ pip install aim-mlflow\n```\n\nAfter successfully installing aimlflow on your machine let’s create a directory named `mlflow-demo-local` where the `.aim`repository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:\n\n```\n$ aim init\n```\n\nThis will establish an Aim repository in the present directory and it will be named `.aim`.\n\nThis is how our local system directory will look like:\n\n```\nmlflow-demo-local\n└── .aim\n    ├── ...\n```\n\nIn order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into the `.aim` repository.\n\nTo begin the process of converting MLflow experiments from the the hosted url `YOUR_REMOTE_IP:5000` into the Aim repository `.aim`, execute the following command from our local `mlflow-demo-local` directory:\n\n```\n$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n```\n\nThe converter will go through all experiments within the project and create a unique `Aim` run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.\n\nThis means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? ☺️\n\nNow that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:\n\n```\n$ aim up\n```\n\nto open the user interface and explore our metrics and other information.\n\nFor further reading please referee to [Aim documentation](https://aimstack.readthedocs.io/en/latest/) where you will learn more about the superpowers of Aim.\n\n# Conclusion\n\n\n\nTo sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.\n\nIn this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.\n\nThe added value of Aim makes installing aimlflow and enabling the additional capability well worth it.\n\n# Learn more\n\nIf you have any questions join [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. 🙌\n\nShow some love by dropping a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"\u003cp\u003eWith aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim’s powerful features, leading to deeper understanding and more effective decision-making.\u003c/p\u003e\n\u003cp\u003eWe have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:\u003c/p\u003e\n\u003cp\u003eRunning Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here: \u003ca href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\"\u003ehttps://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.\u003c/p\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eWe will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.\u003c/p\u003e\n\u003cp\u003eWe will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.\u003c/p\u003e\n\u003cp\u003eFind the full project code on GitHub: \u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eServer-side/Remote Configuration\u003c/h1\u003e\n\u003cp\u003eLet’s create a separate directory for the demo and name it \u003ccode\u003emlflow-demo-remote\u003c/code\u003e. After which download and run the \u003ccode\u003etune.py\u003c/code\u003e python script from the Github repo to conduct the training sessions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ python tune.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRay Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOnce started, mlflow will commence recording the results in the \u003ccode\u003emlruns\u003c/code\u003e directory. Our remote directory will have the following structure:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emlflow-demo-remote\n├── tune.py\n└── mlruns\n    ├── ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from the \u003ccode\u003emlflow-demo-remote\u003c/code\u003e directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ mlflow ui --host 0.0.0.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default, the \u003ccode\u003e--host\u003c/code\u003e is set to \u003ccode\u003e127.0.0.1\u003c/code\u003e, limiting access to the service to the local machine only. To expose it to external machines, set the host to \u003ccode\u003e0.0.0.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eBy default, the system listens on port \u003ccode\u003e5000\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOne can set \u003ccode\u003e--backend-store-uri\u003c/code\u003e param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path of \u003ccode\u003emlruns\u003c/code\u003e directory.\u003c/p\u003e\n\u003cp\u003eUpon navigating to \u003ccode\u003ehttp://127.0.0.1:5000\u003c/code\u003e, you will be presented with a page that looks similar to this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eSynchronising MLflow Runs with Aim\u003c/h1\u003e\n\u003cp\u003eAfter successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.\u003c/p\u003e\n\u003cp\u003eFirst, let’s move forward with the installation process of aimlflow. It’s incredibly easy to set up on your device, just execute the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ pip install aim-mlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter successfully installing aimlflow on your machine let’s create a directory named \u003ccode\u003emlflow-demo-local\u003c/code\u003e where the \u003ccode\u003e.aim\u003c/code\u003erepository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will establish an Aim repository in the present directory and it will be named \u003ccode\u003e.aim\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis is how our local system directory will look like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emlflow-demo-local\n└── .aim\n    ├── ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into the \u003ccode\u003e.aim\u003c/code\u003e repository.\u003c/p\u003e\n\u003cp\u003eTo begin the process of converting MLflow experiments from the the hosted url \u003ccode\u003eYOUR_REMOTE_IP:5000\u003c/code\u003e into the Aim repository \u003ccode\u003e.aim\u003c/code\u003e, execute the following command from our local \u003ccode\u003emlflow-demo-local\u003c/code\u003e directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe converter will go through all experiments within the project and create a unique \u003ccode\u003eAim\u003c/code\u003e run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.\u003c/p\u003e\n\u003cp\u003eThis means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? ☺️\u003c/p\u003e\n\u003cp\u003eNow that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto open the user interface and explore our metrics and other information.\u003c/p\u003e\n\u003cp\u003eFor further reading please referee to \u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003eAim documentation\u003c/a\u003e where you will learn more about the superpowers of Aim.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eTo sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.\u003c/p\u003e\n\u003cp\u003eIn this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.\u003c/p\u003e\n\u003cp\u003eThe added value of Aim makes installing aimlflow and enabling the additional capability well worth it.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. 🙌\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you think Aim is useful.\u003c/p\u003e"},"_id":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","_raw":{"sourceFilePath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileName":"how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\n\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\n\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\n\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance. One thing you do not need when tuning hyper-parameters is variance.\n\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\n\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using [Aim](https://github.com/aimhubio/aim).\n\n## **How to fix the seed in PyTorch Lightning**\n\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\n\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use `pl.seed_everything(seed)` . See the docs [here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\n\n\u003e Note: in other libraries you would use something like: `np.random.seed()` or `torch.manual_seed()` \n\n## **Implementation**\n\nFind the full code for this and other tutorials [here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\n\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n```\n\n## Analyzing the Training Runs\n\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after [Aim 1.3.5 release](https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment)) and have multiple charts of different metrics on the same screen.\n\nDo the following steps to see the different effects of the optimizers\n\n* Go to dashboard, explore by experiment\n* Add loss to `SELECT` and divide into subplots by metric\n* Group by experiment to make all metrics of similar color\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\n\nHere is how it looks on Aim:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png)\n\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\n\nIf you apply the same settings to the learning rate, this is the result:\n\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\n\n* Remove both previous groupings\n* Group by color by learning rate\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\n\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\n\nOn top of this, if we also add grouping by style by optimizer:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png)\n\nNow, it is obvious that the the runs with SGD optimizer and `lr=0.01` (green, broken lines) are the best choices for all the seeds we have tried.\n\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\n\n## Learn More\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://community.aimstack.io/) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is a random seed and how is it important?\u003c/h2\u003e\n\u003cp\u003eThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\u003c/li\u003e\n\u003cli\u003eDropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\u003c/li\u003e\n\u003cli\u003eAugmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance. One thing you do not need when tuning hyper-parameters is variance.\u003c/p\u003e\n\u003cp\u003eThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\u003c/p\u003e\n\u003cp\u003eLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eHow to fix the seed in PyTorch Lightning\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python\u003ccode\u003erandom.seed(seed)\u003c/code\u003e, but it is not enough for PL.\u003c/p\u003e\n\u003cp\u003ePytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use \u003ccode\u003epl.seed_everything(seed)\u003c/code\u003e . See the docs \u003ca href=\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: in other libraries you would use something like: \u003ccode\u003enp.random.seed()\u003c/code\u003e or \u003ccode\u003etorch.manual_seed()\u003c/code\u003e \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFind the full code for this and other tutorials \u003ca href=\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAnalyzing the Training Runs\u003c/h2\u003e\n\u003cp\u003eAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after \u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment\"\u003eAim 1.3.5 release\u003c/a\u003e) and have multiple charts of different metrics on the same screen.\u003c/p\u003e\n\u003cp\u003eDo the following steps to see the different effects of the optimizers\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGo to dashboard, explore by experiment\u003c/li\u003e\n\u003cli\u003eAdd loss to \u003ccode\u003eSELECT\u003c/code\u003e and divide into subplots by metric\u003c/li\u003e\n\u003cli\u003eGroup by experiment to make all metrics of similar color\u003c/li\u003e\n\u003cli\u003eGroup by style by optimizer to see different optimizers on loss and accuracy and its effects\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere is how it looks on Aim:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\u003c/p\u003e\n\u003cp\u003eIf you apply the same settings to the learning rate, this is the result:\u003c/p\u003e\n\u003ch2\u003eFor the next step to analyze how learning rate affects the experiments, do the following steps:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemove both previous groupings\u003c/li\u003e\n\u003cli\u003eGroup by color by learning rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\u003c/p\u003e\n\u003cp\u003eWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\u003c/p\u003e\n\u003cp\u003eOn top of this, if we also add grouping by style by optimizer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, it is obvious that the the runs with SGD optimizer and \u003ccode\u003elr=0.01\u003c/code\u003e (green, broken lines) are the best choices for all the seeds we have tried.\u003c/p\u003e\n\u003cp\u003eFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"},{"title":"How YerevaNN researchers use Aim to compare 100s of models","date":"2022-05-31T16:29:36.844Z","author":"Gev Soghomonian","description":"Explore YerevaNN's use of Aim: Comparing 100s of models for cross-lingual transfer in NLP. Reproducing studies, model selection, and insights revealed. Learn more!","slug":"how-yerevann-researchers-use-aim-to-compare-100s-of-models","image":"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*lnmBXL_nhBGxOvjNO56-Rw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"*Thanks to [Knarik Mheryan](https://medium.com/u/309b97b32187?source=post_page-----1a34ced2ced0--------------------------------) and [Hrant Khachatrian](https://medium.com/u/83126d4b3c1b?source=post_page-----1a34ced2ced0--------------------------------)for co-writing this blogpost.*\n\n[YerevaNN](https://yerevann.com/) is a non-profit computer science and mathematics research lab based in Yerevan, Armenia. The lab’s main research interests are focused around machine learning algorithms and machine learning for biomedical data. YerevaNN also spends significant effort on supervision of student projects in order to support the up and coming Armenian scholars.\n\n[YerevaNN ](https://yerevann.com/)collaborates with University of Southern California — focused on low-resource learning problems, ENSAE / ParisTech on theoretical statistics problems, biologists and chemists from around the world on ML applications in various biomedical problems. YerevaNN has publications at ACL, CVPR, WMT, Annals of Statistics and Nature Scientific Data among others.\n\nWe traditionally onboard new teammates with a task to reproduce a well-known paper. This allows us to both introduce them to our internal systems, codebase, resources as well as it’s a great leeway into the problems they are going to focus on as part of their research at YerevaNN. This is a story about how Knarik Mheryan has gotten started at YerevaNN and how [Aim](https://github.com/aimhubio/aim/) has helped her in the process.\n\nThe Challenges\n\n## Model selection for cross-lingual transfer\n\nDeep learning has enabled high-accuracy solutions to a wide variety of NLP tasks.. As these models are trained primarily on English corpora, they aren’t readily applicable or available for other human languages (due to the lack of labeled data).\n\nOne approach to solve this problem is to pre-train a model on an unlabeled multilingual dataset in an unsupervised fashion, then fine-tune it only on English labeled data for the given NLP task. It turns out the model’s performance on the target languages is strong, even though the model is fine-tuned only on English data.\n\nModel selection is a crucial step in this approach. When fine-tuning on English data with different hyperparams and seeds we are left with a set of models. The challenge is to select the model with the highest performance (e.g. F1 score) in the target language.\n\n**In 2021, Yang Chen and Alan Ritter, have published a [paper in EMNLP](https://arxiv.org/abs/2010.06127) that tries to address this problem. Knarik Mheryan’s first task at YerevaNN has been to reproduce this work as part of the [ML Reproducibility Challenge 2021](https://paperswithcode.com/rc2021).**\n\n## Reproducing the study\n\nA common tactic has been to select models that perform well on the English data. This paper shows that the accuracy of models on English data does not always correlate well with the target language performance. To reproduce this, we have fine-tuned 240 models on a Named Entity Recognition task.\n\nThe dimensionality of these experiments is high. So analyzing several 100 models is not a trivial task and it can take hours when comparing them with traditional tools. At YerevaNN we have been using Aim as it allows us to compare 1000s of high-dimensional (all metrics, hyperparams and other metadata) experiments in a few clicks.\n\n# The Solution\n\n## Using Params Explorer to compare models\n\nParams explorer allows us to directly compare the models with one view and with a few clicks. We have tracked the F1 scores for all models, languages. Using Aim’s [context](https://aimstack.readthedocs.io/en/latest/understanding/concepts.html?highlight=context#sequence-context) makes it easy to add additional metadata to the metrics (such as lang=”fr”) so there is no need to bake that data into the name.\n\nWe have selected two hyperparameters of interest and F1 scores of 5 languages of interest. This one view shows all the models (each curve is a model) at once and we can compare them relative to selected hyperparams and metrics. (see Figure 1)\n\nMore importantly, the plot below shows that the best performing model on English data (F1 lang=”en”) certainly doesn’t perform well on German, Spanish, Dutch and Chinese. **The red bold curve represents the best model on the English dataset.**\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMiTQD1jlsycSLCgPV0hfw.png)\n\nFigure 1: Aim’s Params plot for hundreds of fine-tuned models. Each curve corresponds to a single model with some hyperparameters and random seeds. Each vertical axis shows the model’s hyperparameters or the F1 scores for some language (German, Spanish, Dutch, Chinese, English).\n\n\\\nIt took us only a few clicks to see this key insight.\n\n**And this makes a couple of hours of work into something that’s less than a minute.**\n\n# Qualitative metrics comparison with\n\n# [Metrics Explorer](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)\n\n## Catastrophic Forgetting on the best Eng-based model\n\nIn our pursuit of understanding why models behave that way, we have tracked F1 scores across 60 epochs for the model that performs best on the English dataset. The Figure 2 below shows the behavior of F1 scores for 5 different languages (German, Spanish, Dutch, Chinese and English).\n\nThe red line on Figure 2 represents the evaluation on English data. There is a clear difference in performance between the languages. Effectively Aim shows that the model overfits on English and what the ‘Catastrophic forgetting’ looks like.\n\nNotably, the Chinese (orange) gets worse faster than the other languages. Possibly because there are major linguistic differences between Chinese and the rest of the languages.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTYunp3onfPgzAorAUed9A.png)\n\nFigure 2: Visualization of the Catastrophic forgetting phenomenon. Evaluations on 5 languages development datasets for 60 epochs after fine-tuning the model on the English training data. The languages are English (red), Dutch (blue), German (green), Spanish (violet), and Chinese (orange).\n\n## Proposed model selection algorithm analysis\n\nWe reproduce the paper in two phases.\n\n1. First, we have created candidate models by fine-tuning them on English labeled data and checked their performance on the proposed 5 target languages.\n2. Second, we are going to build the algorithm for model selection — proposed in the paper and verify if it works better than the conventional methods.\n\n**Analyzing the Learning Rates**\n\nAim’s Metric Explorer has allowed us to analyze the fine-tuning process through all the runs. In Figure 3, each line is a run with a specific target language, hyperparameter and random seed. Here we group runs by learning rate giving them the same color.\n\nWith Aim we frequently group runs into different charts. In this case we have grouped the metrics into charts by language . This allows us to visualize the fine-tuning with different learning rates per language.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab3o77BJ0R8hbEezh7NrcA.png)\n\nFigure 3: Visualization of the fine-tuning process with different learning rates and random seeds for Spanish, German, Chinese, Dutch and English languages.\n\nWith one more click, we aggregate runs of the same group (grouped by learning rate through all seeds). The aggregation helps us to see the trends of the F1 metrics collected across epochs.\n\nFrom Figure 4 it is clear that all models with very small learning rates (red) barely learn anything in 20 epochs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiam-hSc66ISC3deKgMoUg.png)\n\nFigure 4: Visualization of the fine-tuning process aggregated by learning rates through different random seeds. Lines through highlighted intervals are medians from runs with different seeds. Each chart plots the fine-tuning process for a particular language (Spanish, German, Chinese, Dutch and English)\n\nIn this paper the authors suggest an alternative strategy for model selection named Learned Model Selection (LMS). LMS is a neural network based model that learns to score the compatibility between a fine-tuned model and a target language. According to the paper, this method consistently selects better models than English development data across the 4 target languages.\n\nFor this final step we have also calculated the LMS scores for each model and have used Aim Params Explorer to compare the new and old methods for their efficiency. By just selecting the params and metrics we have produced the Figure 5 on Aim. Each highlighted curve in Figure 5 corresponds to one of the model candidates. With the old method (with the English axis) the best candidate F1 on the English axis is 0.699. On the other hand, if we select by the new algorithm (with LMS score axis), the F1 score would be 0.71, which is indeed higher than with the English candidate model.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7Jpwdraw_XlSji_GTk2yA.png)\n\nFigure 5: Visualization of the final process of model selection. The most left axis is the model number (like id) from 0 to 59. ‘F1 lang=”English”‘ is the F1 score on the English development data. ‘F1 lang=”German”‘ is the F1 score on the German test data. ‘LMS score’ is the ranking score provided by LMS, the algorithm proposed in the paper, the higher the better.\n\nWith Aim our experiments analysis at YerevaNN has become order of magnitude faster due to intuitive, beautiful and fast UI and the Aim explorers. The team at YerevaNN uses Aim on a daily basis to analyze their experiments.\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring 🙌\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nThis article was originally published on [Aim Blog](https://aimstack.io/blog). Find more in depth guides and details of the newest releases there.","html":"\u003cp\u003e\u003cem\u003eThanks to \u003ca href=\"https://medium.com/u/309b97b32187?source=post_page-----1a34ced2ced0--------------------------------\"\u003eKnarik Mheryan\u003c/a\u003e and \u003ca href=\"https://medium.com/u/83126d4b3c1b?source=post_page-----1a34ced2ced0--------------------------------\"\u003eHrant Khachatrian\u003c/a\u003efor co-writing this blogpost.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://yerevann.com/\"\u003eYerevaNN\u003c/a\u003e is a non-profit computer science and mathematics research lab based in Yerevan, Armenia. The lab’s main research interests are focused around machine learning algorithms and machine learning for biomedical data. YerevaNN also spends significant effort on supervision of student projects in order to support the up and coming Armenian scholars.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://yerevann.com/\"\u003eYerevaNN \u003c/a\u003ecollaborates with University of Southern California — focused on low-resource learning problems, ENSAE / ParisTech on theoretical statistics problems, biologists and chemists from around the world on ML applications in various biomedical problems. YerevaNN has publications at ACL, CVPR, WMT, Annals of Statistics and Nature Scientific Data among others.\u003c/p\u003e\n\u003cp\u003eWe traditionally onboard new teammates with a task to reproduce a well-known paper. This allows us to both introduce them to our internal systems, codebase, resources as well as it’s a great leeway into the problems they are going to focus on as part of their research at YerevaNN. This is a story about how Knarik Mheryan has gotten started at YerevaNN and how \u003ca href=\"https://github.com/aimhubio/aim/\"\u003eAim\u003c/a\u003e has helped her in the process.\u003c/p\u003e\n\u003cp\u003eThe Challenges\u003c/p\u003e\n\u003ch2\u003eModel selection for cross-lingual transfer\u003c/h2\u003e\n\u003cp\u003eDeep learning has enabled high-accuracy solutions to a wide variety of NLP tasks.. As these models are trained primarily on English corpora, they aren’t readily applicable or available for other human languages (due to the lack of labeled data).\u003c/p\u003e\n\u003cp\u003eOne approach to solve this problem is to pre-train a model on an unlabeled multilingual dataset in an unsupervised fashion, then fine-tune it only on English labeled data for the given NLP task. It turns out the model’s performance on the target languages is strong, even though the model is fine-tuned only on English data.\u003c/p\u003e\n\u003cp\u003eModel selection is a crucial step in this approach. When fine-tuning on English data with different hyperparams and seeds we are left with a set of models. The challenge is to select the model with the highest performance (e.g. F1 score) in the target language.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIn 2021, Yang Chen and Alan Ritter, have published a \u003ca href=\"https://arxiv.org/abs/2010.06127\"\u003epaper in EMNLP\u003c/a\u003e that tries to address this problem. Knarik Mheryan’s first task at YerevaNN has been to reproduce this work as part of the \u003ca href=\"https://paperswithcode.com/rc2021\"\u003eML Reproducibility Challenge 2021\u003c/a\u003e.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003eReproducing the study\u003c/h2\u003e\n\u003cp\u003eA common tactic has been to select models that perform well on the English data. This paper shows that the accuracy of models on English data does not always correlate well with the target language performance. To reproduce this, we have fine-tuned 240 models on a Named Entity Recognition task.\u003c/p\u003e\n\u003cp\u003eThe dimensionality of these experiments is high. So analyzing several 100 models is not a trivial task and it can take hours when comparing them with traditional tools. At YerevaNN we have been using Aim as it allows us to compare 1000s of high-dimensional (all metrics, hyperparams and other metadata) experiments in a few clicks.\u003c/p\u003e\n\u003ch1\u003eThe Solution\u003c/h1\u003e\n\u003ch2\u003eUsing Params Explorer to compare models\u003c/h2\u003e\n\u003cp\u003eParams explorer allows us to directly compare the models with one view and with a few clicks. We have tracked the F1 scores for all models, languages. Using Aim’s \u003ca href=\"https://aimstack.readthedocs.io/en/latest/understanding/concepts.html?highlight=context#sequence-context\"\u003econtext\u003c/a\u003e makes it easy to add additional metadata to the metrics (such as lang=”fr”) so there is no need to bake that data into the name.\u003c/p\u003e\n\u003cp\u003eWe have selected two hyperparameters of interest and F1 scores of 5 languages of interest. This one view shows all the models (each curve is a model) at once and we can compare them relative to selected hyperparams and metrics. (see Figure 1)\u003c/p\u003e\n\u003cp\u003eMore importantly, the plot below shows that the best performing model on English data (F1 lang=”en”) certainly doesn’t perform well on German, Spanish, Dutch and Chinese. \u003cstrong\u003eThe red bold curve represents the best model on the English dataset.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMiTQD1jlsycSLCgPV0hfw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 1: Aim’s Params plot for hundreds of fine-tuned models. Each curve corresponds to a single model with some hyperparameters and random seeds. Each vertical axis shows the model’s hyperparameters or the F1 scores for some language (German, Spanish, Dutch, Chinese, English).\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nIt took us only a few clicks to see this key insight.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnd this makes a couple of hours of work into something that’s less than a minute.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eQualitative metrics comparison with\u003c/h1\u003e\n\u003ch1\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003eMetrics Explorer\u003c/a\u003e\u003c/h1\u003e\n\u003ch2\u003eCatastrophic Forgetting on the best Eng-based model\u003c/h2\u003e\n\u003cp\u003eIn our pursuit of understanding why models behave that way, we have tracked F1 scores across 60 epochs for the model that performs best on the English dataset. The Figure 2 below shows the behavior of F1 scores for 5 different languages (German, Spanish, Dutch, Chinese and English).\u003c/p\u003e\n\u003cp\u003eThe red line on Figure 2 represents the evaluation on English data. There is a clear difference in performance between the languages. Effectively Aim shows that the model overfits on English and what the ‘Catastrophic forgetting’ looks like.\u003c/p\u003e\n\u003cp\u003eNotably, the Chinese (orange) gets worse faster than the other languages. Possibly because there are major linguistic differences between Chinese and the rest of the languages.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTYunp3onfPgzAorAUed9A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 2: Visualization of the Catastrophic forgetting phenomenon. Evaluations on 5 languages development datasets for 60 epochs after fine-tuning the model on the English training data. The languages are English (red), Dutch (blue), German (green), Spanish (violet), and Chinese (orange).\u003c/p\u003e\n\u003ch2\u003eProposed model selection algorithm analysis\u003c/h2\u003e\n\u003cp\u003eWe reproduce the paper in two phases.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFirst, we have created candidate models by fine-tuning them on English labeled data and checked their performance on the proposed 5 target languages.\u003c/li\u003e\n\u003cli\u003eSecond, we are going to build the algorithm for model selection — proposed in the paper and verify if it works better than the conventional methods.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eAnalyzing the Learning Rates\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim’s Metric Explorer has allowed us to analyze the fine-tuning process through all the runs. In Figure 3, each line is a run with a specific target language, hyperparameter and random seed. Here we group runs by learning rate giving them the same color.\u003c/p\u003e\n\u003cp\u003eWith Aim we frequently group runs into different charts. In this case we have grouped the metrics into charts by language . This allows us to visualize the fine-tuning with different learning rates per language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab3o77BJ0R8hbEezh7NrcA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 3: Visualization of the fine-tuning process with different learning rates and random seeds for Spanish, German, Chinese, Dutch and English languages.\u003c/p\u003e\n\u003cp\u003eWith one more click, we aggregate runs of the same group (grouped by learning rate through all seeds). The aggregation helps us to see the trends of the F1 metrics collected across epochs.\u003c/p\u003e\n\u003cp\u003eFrom Figure 4 it is clear that all models with very small learning rates (red) barely learn anything in 20 epochs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiam-hSc66ISC3deKgMoUg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 4: Visualization of the fine-tuning process aggregated by learning rates through different random seeds. Lines through highlighted intervals are medians from runs with different seeds. Each chart plots the fine-tuning process for a particular language (Spanish, German, Chinese, Dutch and English)\u003c/p\u003e\n\u003cp\u003eIn this paper the authors suggest an alternative strategy for model selection named Learned Model Selection (LMS). LMS is a neural network based model that learns to score the compatibility between a fine-tuned model and a target language. According to the paper, this method consistently selects better models than English development data across the 4 target languages.\u003c/p\u003e\n\u003cp\u003eFor this final step we have also calculated the LMS scores for each model and have used Aim Params Explorer to compare the new and old methods for their efficiency. By just selecting the params and metrics we have produced the Figure 5 on Aim. Each highlighted curve in Figure 5 corresponds to one of the model candidates. With the old method (with the English axis) the best candidate F1 on the English axis is 0.699. On the other hand, if we select by the new algorithm (with LMS score axis), the F1 score would be 0.71, which is indeed higher than with the English candidate model.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7Jpwdraw_XlSji_GTk2yA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 5: Visualization of the final process of model selection. The most left axis is the model number (like id) from 0 to 59. ‘F1 lang=”English”‘ is the F1 score on the English development data. ‘F1 lang=”German”‘ is the F1 score on the German test data. ‘LMS score’ is the ranking score provided by LMS, the algorithm proposed in the paper, the higher the better.\u003c/p\u003e\n\u003cp\u003eWith Aim our experiments analysis at YerevaNN has become order of magnitude faster due to intuitive, beautiful and fast UI and the Aim explorers. The team at YerevaNN uses Aim on a daily basis to analyze their experiments.\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eThis article was originally published on \u003ca href=\"https://aimstack.io/blog\"\u003eAim Blog\u003c/a\u003e. Find more in depth guides and details of the newest releases there.\u003c/p\u003e"},"_id":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","_raw":{"sourceFilePath":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","sourceFileName":"how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models"},"type":"Post"},{"title":"Hugging the Chaos: Connecting Datasets to Trainings with Hugging Face and Aim","date":"2023-02-17T07:05:28.709Z","author":"Gor Arakelyan","description":"Combining Hugging Face and Aim to make machine learning experiments traceable, reproducible and easier to compare.","slug":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-luoNtZBpA1SGkTsHzCrkA.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"# The cost of neglecting experiments management\n\n\n\nWorking with large and frequently changing datasets is hard!\\\nYou can easily end up in a mess if you don’t have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.\n\nIn this article, we will explore how you can combine [Hugging Face Datasets](https://github.com/huggingface/datasets) and [Aim](https://github.com/aimhubio/aim) to make machine learning experiments traceable, reproducible and easier to compare.\n\nLet’s dive in and get started!\n\n# Hugging Face Datasets + Aim = ❤\n\n[Hugging Face Datasets](https://github.com/huggingface/datasets) is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, you’ll never have to worry about manually loading and versioning your data again.\n\n[Aim](https://github.com/aimhubio/aim), on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png)\n\nHugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.\n\n# Project overview\n\nLet’s go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.\n\nHere’s what we’ll be using:\n\n* [Hugging Face Datasets](https://github.com/huggingface/datasets) to load and manage the dataset.\n* [Hugging Face Hub](http://huggingface.co/) to host the dataset.\n* [PyTorch](https://github.com/pytorch/pytorch) to build and train the model.\n* [Aim](https://github.com/aimhubio/aim) to keep track of all the model and dataset metadata.\n\nOur dataset is going to be called “A-MNIST” — a version of the “MNIST” dataset with extra samples added. We’ll start with the original “MNIST” dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.\n\n# Dataset preparation\n\n## Uploading the dataset to Hub\n\n\n\nLet’s head over to Hugging Face Hub and create a new dataset repository called [“A-MNIST”](https://huggingface.co/datasets/gorar/A-MNIST) to store our dataset.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png)\n\nCurrently, the repository is empty, so let’s upload the initial version of the dataset.\n\nWe’ll use the original MNIST dataset, as the first version v1.0.0. To do this, we’ll need to upload the dataset files along with the dataset loading script `A-MNIST.py`. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:\n\n```\nclass AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n```\n\n*See the full script here: \u003chttps://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\u003e*\n\nWe’ll have the following setup, including all the necessary git configurations and the dataset card:\n\n```\n- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n```\n\nLet’s commit the changes and push the dataset to the Hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png)\n\nAwesome! Now we’ve got the first version of “A-MNIST” hosted on Hugging Face Hub!\n\n## Augmenting the dataset\n\n\n\nNext up, let’s add more images to the MNIST dataset using augmentation techniques.\n\nWe will rotate all the train images by 20 degree and append to the dataset:\n\n```\nrotated_images[i] = rotate(images[i], angle=20, reshape=False)\n```\n\nAs well as, update the “A-MNIST” dataset version to 1.1.0:\n\n```\nBUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n```\n\nAfter preparing the new train dataset, let’s commit the changes and push the upgraded version to the hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png)\n\nPerfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. 🎉\n\n# Training setup\n\nLet’s load the dataset using datasets python package. It’s really simple, just one line of code:\n\n```\ndataset = load_dataset(\"gorar/A-MNIST\")\n```\n\nOne of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:\n\n```\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n```\n\nWe’re ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:\n\n```\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n```\n\n*See the full code here: \u003chttps://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\u003e*\n\nWe’ve got everything ready now. Let’s kick off the training by running:\n\n```\npython train.py\n```\n\nHurray, the training is underway! 🏃\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png)\n\nIt would be tough to monitor my training progress on terminal. This is where Aim’s superpowers come into play.\n\n# Integrating Aim\n\n\n\n## Trainings tracking\n\n\n\nAim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:\n\n```\nimport aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n```\n\nFurthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.\n\nSince v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply import `HFDataset` module from Aim and you can track all of your dataset metadata with just a single line of code!\n\n```\nfrom aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n```\n\nAim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the dataset’s description, version, features, etc.\n\n# Experimentation\n\n## Conducting trainings\n\n\n\nWe’ll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, we’ll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.\n\nDatasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:\n\n```\n# Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n```\n\nWe will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.\n\n## Exploring training results via Aim\n\n\n\nNow that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aim’s UI, we need to run the `aim up` command. A following message would be printed on the terminal output meaning the UI is successfully running:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png)\n\nTo access the UI, let’s navigate to `127.0.0.1:43800` in the browser.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png)\n\nWe can see our trainings on the contributions map and in the activity feed. Let’s take a closer look at an individual run details by navigating to the run’s page on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png)\n\nAll of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.\n\nWe can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png)\n\nThe UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.\n\n## Comparing trainings via Aim\n\nLet’s find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics Explorer `127.0.0.1:43800/metrics`.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png)\n\nAim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.\n\nLet’s group by `run.dataset.dataset.meta.version` to see how the models performed based on the dataset version they were trained on.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png)\n\n\\\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.\n\nNow, to improve visibility and better evaluate performance, let’s go ahead and smooth out these lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png)\n\nIt appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, let’s group metrics by the learning rate to learn its impact on models performance.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png)\n\nIt seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png)\n\nTo sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.\n\nWith just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! 🎉\n\n# Reproducibility\n\nHave you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.\n\nAim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!\n\n# Conclusion\n\nIn this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.\n\nWe compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.\n\nSimplifying our daily work with efficient stack helps us focus on what really matters — getting the best results from our machine learning experiments.\n\n# Learn more\n\n\n\nCheck out the Aim + Datasets integration docs [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\\\nDatasets repo: \u003chttps://github.com/huggingface/datasets\u003e\\\nAim repo: \u003chttps://github.com/aimhubio/aim\u003e\n\nIf you have questions, join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You’re most welcome! 🙌\n\nDrop a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003ch1\u003eThe cost of neglecting experiments management\u003c/h1\u003e\n\u003cp\u003eWorking with large and frequently changing datasets is hard!\u003cbr\u003e\nYou can easily end up in a mess if you don’t have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.\u003c/p\u003e\n\u003cp\u003eIn this article, we will explore how you can combine \u003ca href=\"https://github.com/huggingface/datasets\"\u003eHugging Face Datasets\u003c/a\u003e and \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e to make machine learning experiments traceable, reproducible and easier to compare.\u003c/p\u003e\n\u003cp\u003eLet’s dive in and get started!\u003c/p\u003e\n\u003ch1\u003eHugging Face Datasets + Aim = ❤\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/huggingface/datasets\"\u003eHugging Face Datasets\u003c/a\u003e is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, you’ll never have to worry about manually loading and versioning your data again.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.\u003c/p\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eLet’s go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.\u003c/p\u003e\n\u003cp\u003eHere’s what we’ll be using:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/huggingface/datasets\"\u003eHugging Face Datasets\u003c/a\u003e to load and manage the dataset.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://huggingface.co/\"\u003eHugging Face Hub\u003c/a\u003e to host the dataset.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/pytorch/pytorch\"\u003ePyTorch\u003c/a\u003e to build and train the model.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e to keep track of all the model and dataset metadata.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOur dataset is going to be called “A-MNIST” — a version of the “MNIST” dataset with extra samples added. We’ll start with the original “MNIST” dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.\u003c/p\u003e\n\u003ch1\u003eDataset preparation\u003c/h1\u003e\n\u003ch2\u003eUploading the dataset to Hub\u003c/h2\u003e\n\u003cp\u003eLet’s head over to Hugging Face Hub and create a new dataset repository called \u003ca href=\"https://huggingface.co/datasets/gorar/A-MNIST\"\u003e“A-MNIST”\u003c/a\u003e to store our dataset.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eCurrently, the repository is empty, so let’s upload the initial version of the dataset.\u003c/p\u003e\n\u003cp\u003eWe’ll use the original MNIST dataset, as the first version v1.0.0. To do this, we’ll need to upload the dataset files along with the dataset loading script \u003ccode\u003eA-MNIST.py\u003c/code\u003e. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eSee the full script here: \u003ca href=\"https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\"\u003ehttps://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe’ll have the following setup, including all the necessary git configurations and the dataset card:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s commit the changes and push the dataset to the Hub.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAwesome! Now we’ve got the first version of “A-MNIST” hosted on Hugging Face Hub!\u003c/p\u003e\n\u003ch2\u003eAugmenting the dataset\u003c/h2\u003e\n\u003cp\u003eNext up, let’s add more images to the MNIST dataset using augmentation techniques.\u003c/p\u003e\n\u003cp\u003eWe will rotate all the train images by 20 degree and append to the dataset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erotated_images[i] = rotate(images[i], angle=20, reshape=False)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs well as, update the “A-MNIST” dataset version to 1.1.0:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter preparing the new train dataset, let’s commit the changes and push the upgraded version to the hub.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003ePerfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. 🎉\u003c/p\u003e\n\u003ch1\u003eTraining setup\u003c/h1\u003e\n\u003cp\u003eLet’s load the dataset using datasets python package. It’s really simple, just one line of code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = load_dataset(\"gorar/A-MNIST\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOne of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe’re ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eSee the full code here: \u003ca href=\"https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\"\u003ehttps://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe’ve got everything ready now. Let’s kick off the training by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython train.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHurray, the training is underway! 🏃\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIt would be tough to monitor my training progress on terminal. This is where Aim’s superpowers come into play.\u003c/p\u003e\n\u003ch1\u003eIntegrating Aim\u003c/h1\u003e\n\u003ch2\u003eTrainings tracking\u003c/h2\u003e\n\u003cp\u003eAim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFurthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.\u003c/p\u003e\n\u003cp\u003eSince v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply import \u003ccode\u003eHFDataset\u003c/code\u003e module from Aim and you can track all of your dataset metadata with just a single line of code!\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the dataset’s description, version, features, etc.\u003c/p\u003e\n\u003ch1\u003eExperimentation\u003c/h1\u003e\n\u003ch2\u003eConducting trainings\u003c/h2\u003e\n\u003cp\u003eWe’ll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, we’ll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.\u003c/p\u003e\n\u003cp\u003eDatasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.\u003c/p\u003e\n\u003ch2\u003eExploring training results via Aim\u003c/h2\u003e\n\u003cp\u003eNow that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aim’s UI, we need to run the \u003ccode\u003eaim up\u003c/code\u003e command. A following message would be printed on the terminal output meaning the UI is successfully running:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo access the UI, let’s navigate to \u003ccode\u003e127.0.0.1:43800\u003c/code\u003e in the browser.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see our trainings on the contributions map and in the activity feed. Let’s take a closer look at an individual run details by navigating to the run’s page on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAll of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.\u003c/p\u003e\n\u003cp\u003eWe can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.\u003c/p\u003e\n\u003ch2\u003eComparing trainings via Aim\u003c/h2\u003e\n\u003cp\u003eLet’s find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics Explorer \u003ccode\u003e127.0.0.1:43800/metrics\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.\u003c/p\u003e\n\u003cp\u003eLet’s group by \u003ccode\u003erun.dataset.dataset.meta.version\u003c/code\u003e to see how the models performed based on the dataset version they were trained on.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.\u003c/p\u003e\n\u003cp\u003eNow, to improve visibility and better evaluate performance, let’s go ahead and smooth out these lines.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIt appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, let’s group metrics by the learning rate to learn its impact on models performance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIt seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.\u003c/p\u003e\n\u003cp\u003eWith just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! 🎉\u003c/p\u003e\n\u003ch1\u003eReproducibility\u003c/h1\u003e\n\u003cp\u003eHave you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.\u003c/p\u003e\n\u003cp\u003eAim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eIn this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.\u003c/p\u003e\n\u003cp\u003eWe compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.\u003c/p\u003e\n\u003cp\u003eSimplifying our daily work with efficient stack helps us focus on what really matters — getting the best results from our machine learning experiments.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eCheck out the Aim + Datasets integration docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\"\u003ehere\u003c/a\u003e.\u003cbr\u003e\nDatasets repo: \u003ca href=\"https://github.com/huggingface/datasets\"\u003ehttps://github.com/huggingface/datasets\u003c/a\u003e\u003cbr\u003e\nAim repo: \u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have questions, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. You’re most welcome! 🙌\u003c/p\u003e\n\u003cp\u003eDrop a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","_raw":{"sourceFilePath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileName":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim"},"type":"Post"},{"title":"Keep your model up to date for Question Answering ","date":"2024-02-28T18:24:47.359Z","author":"Hovhannes Tamoyan","description":"In this article, we'll show you how to keep your language model up to date for question answering tasks. We'll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. ","slug":"keep-your-model-up-to-date-for-question-answering","image":"/images/dynamic/medium-1-.png","draft":false,"categories":["Tutorials"],"body":{"raw":"In this article, we'll show you how to keep your language model up to date for question answering tasks. We'll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. We will Aim for tracking our progress and insights.\n\n## Importing Necessary Modules\n\nLet's kick off by importing the necessary modules for our experiments:\n\n```\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    DefaultDataCollator,\n    Trainer,\n    TrainingArguments,\n)\nfrom aim.hugging_face import AimCallback\n\n```\n\n## Preprocessing and Tokenization\n\nTo prepare our data for fine-tuning, we need to preprocess and tokenize it appropriately. We employ the DistilBERT tokenizer and define a preprocessing function to handle this task efficiently:\n\n```\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n    # Tokenize questions and context\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Process answers and calculate start/end positions\n    # ...\n    # Code snippet for answer processing goes here (omitted for brevity)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n```\n\n## Loading and Preparing the Dataset\n\n\n\nWith our preprocessing function ready, we proceed to load the SQuAD dataset and apply the preprocessing function to it. Additionally, we instantiate a classic data collator to be used as an argument for the Trainer:\n\n```\nsquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.train_test_split(test_size=0.2)\n\ntokenized_squad = squad.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=squad[\"train\"].column_names,\n)\n\ndata_collator = DefaultDataCollator()\n\n```\n\n## Defining Training Arguments\n\nNext, we define the training arguments for our fine-tuning process, including parameters such as batch size, learning rate, and number of epochs:\n\n```\nbatch_size = 32\nlr = 4e-5\n\ntraining_args = TrainingArguments(\n    output_dir=f\"qa_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    fp16=True,\n)\n\n```\n\n## Integrating Aim for Experiment Tracking\n\n\n\nIntegrating Aim into our experiments allows us to track metadata effectively. We instantiate the AimCallback, specifying the repository or directory for Aim:\n\n```\naim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:43700\", experiment='squad_huggingface_experiment'\n)\n\n```\n\nSimply provide the \\`aim_callback\\` as a callback in the Trainer arguments, and Aim will handle everything behind the scenes.\n\n## Initializing the Trainer\n\nWe then load the pre-trained DistilBERT model and define our Trainer instance, incorporating the training arguments, dataset, tokenizer, data collator, and AimCallback:\n\n```\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_squad[\"train\"],\n    eval_dataset=tokenized_squad[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[aim_callback],\n)\n\n```\n\n## Training and Evaluation\n\nWith all components set up, we are now ready to commence the training process followed by evaluating our fine-tuned model:\n\n```\ntrainer.train()\ntrainer.evaluate()\n```\n\nBy following these steps, we can effectively fine-tune a pre-trained language model for question answering while leveraging Aim for experiment tracking and metadata analysis. This approach enables us to derive meaningful insights and enhance the performance of our models efficiently.\n\n\n\n## Exploring the runs on Aim\n\n\n\nAfter completing our fine-tuning pipeline, it's super easy to explore and analyze the runs using Aim. We conducted experiments with two batch sizes (16, 32) and three learning rates (0.004, 0.0004, 0.00004) to compare their performance and derive insights in real-time.\n\nTo visualize and interpret the experiment results, we utilize the Aim command-line interface. By running the following command:\n\n```\naim up\n```\n\nWe can access the Aim dashboard via the provided URL, where we'll encounter a comprehensive overview of our experiments and their corresponding metadata. Let's delve into what we'll observe in the Runs Explorer page:\n\n![Runs explorer](/images/dynamic/screenshot-2024-02-20-at-11.34.35 pm.png \"Runs explorer\")\n\nEach run's page provides details on hyperparameters, tracked metrics, system metrics, environment variables, and package versions.\n\n![Run overview](/images/dynamic/screenshot-2024-02-20-at-11.35.20 pm.png \"Run overview\")\n\nAdditionally, you can review the run status in the Logs tab.\n\n![Logs tab](/images/dynamic/screenshot-2024-02-20-at-11.37.51 pm.png \"Logs tab\")\n\n## Experiments Analysis\n\n\n\nThe traditional method for comparing machine learning experiments is through observing learning curves.\n\n![Metrics explorer](/images/dynamic/screenshot-2024-02-20-at-11.40.34 pm.png \"Metrics explorer\")\n\nWe plot the loss by differentiating curves based on batch size and learning rate combinations. Evaluation subset curves are shown in solid lines, while training curves are in dashed lines.\n\nMoreover to track hyperparameters and compare them easily, we can utilize the Params Explorer. This involves observing how loss depends on the evaluation subset in relation to batch size and learning rate.\n\n![Param explorer](/images/dynamic/screenshot-2024-02-20-at-11.38.32 pm.png \"Params explorer\")\n\nBased on this analysis, we observe that the best performing hyperparameter combination is batch size: 32 and learning rate: 0.00004.\n\n## Answering questions\n\nNow let’s use the latest checkpoint of the best model to answer some questions and track the questions and the answers:\n\n```\nfrom transformers import pipeline\n\nmodel_ckpt = \"PATH_TO_BEST_MODEL_CKPT\"\nquestion_answerer = pipeline(\"question-answering\", model=model_ckpt)\n\nquestions = [\n    {\n        \"question\": \"How many countries are members of the United Nations?\",\n        \"context\": \"The United Nations has 195 member countries.\",\n    },\n    {\n        \"question\": \"What is the population of China?\",\n        \"context\": \"China has a population of around 1.4 billion.\",\n    },\n    {\n        \"question\": \"Who is the current president of the United States?\",\n        \"context\": \"As of my last update in 2024, Joe Biden is the current President of the United States, succeeding Donald Trump.\",\n    },\n    {\n        \"question\": \"How many planets are there in the solar system?\",\n        \"context\": \"There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\",\n    },\n    {\n        \"question\": \"What is the capital city of Australia?\",\n        \"context\": \"The capital city of Australia is Canberra.\",\n    },\n]\n\nfor question in questions:\n    output = question_answerer(\n        question=question[\"question\"], context=question[\"context\"]\n    )\n\n    aim_callback.experiment.track(\n        Text(question[\"question\"]),\n        name=\"question\",\n        context={\"context\": question[\"context\"]},\n    )\n\n    aim_callback.experiment.track(\n        Text(output[\"answer\"]),\n        name=\"answer\",\n        context={\"context\": question[\"context\"]},\n    )\n```\n\nTo observe the responses we can use the Text Explorer, by grouping by “texts.context.context” in the rows, and “texts.name” in the columns:\n\n![Text explorer](/images/dynamic/screenshot-2024-02-20-at-11.47.57 pm.png \"Text explorer\")\n\n## Conclusion\n\nTo sum up, this blog post outlined how to fine-tune a DistilBERT model for question answering with the SQuAD dataset, while leveraging Aim for experiment tracking. We started by preprocessing the data and setting up the Trainer with AimCallback integration. Through systematic experimentation, we discovered the best hyperparameter combinations and monitored our model's performance using Aim's user-friendly interface.\n\n\n\nMoreover, we demonstrated the versatility of exploration beyond mere model training by analyzing text inputs and outputs with Aim. By visualizing learning curves, comparing hyperparameter combinations, and monitoring model responses to questions, we gained valuable insights into our model's behavior.\n\n\n\n# Learn more\n\n\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html) 🙌\n\n\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aim) if you think Aim is useful. ⭐️","html":"\u003cp\u003eIn this article, we'll show you how to keep your language model up to date for question answering tasks. We'll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. We will Aim for tracking our progress and insights.\u003c/p\u003e\n\u003ch2\u003eImporting Necessary Modules\u003c/h2\u003e\n\u003cp\u003eLet's kick off by importing the necessary modules for our experiments:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    DefaultDataCollator,\n    Trainer,\n    TrainingArguments,\n)\nfrom aim.hugging_face import AimCallback\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePreprocessing and Tokenization\u003c/h2\u003e\n\u003cp\u003eTo prepare our data for fine-tuning, we need to preprocess and tokenize it appropriately. We employ the DistilBERT tokenizer and define a preprocessing function to handle this task efficiently:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n    # Tokenize questions and context\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Process answers and calculate start/end positions\n    # ...\n    # Code snippet for answer processing goes here (omitted for brevity)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLoading and Preparing the Dataset\u003c/h2\u003e\n\u003cp\u003eWith our preprocessing function ready, we proceed to load the SQuAD dataset and apply the preprocessing function to it. Additionally, we instantiate a classic data collator to be used as an argument for the Trainer:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.train_test_split(test_size=0.2)\n\ntokenized_squad = squad.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=squad[\"train\"].column_names,\n)\n\ndata_collator = DefaultDataCollator()\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDefining Training Arguments\u003c/h2\u003e\n\u003cp\u003eNext, we define the training arguments for our fine-tuning process, including parameters such as batch size, learning rate, and number of epochs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebatch_size = 32\nlr = 4e-5\n\ntraining_args = TrainingArguments(\n    output_dir=f\"qa_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    fp16=True,\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eIntegrating Aim for Experiment Tracking\u003c/h2\u003e\n\u003cp\u003eIntegrating Aim into our experiments allows us to track metadata effectively. We instantiate the AimCallback, specifying the repository or directory for Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:43700\", experiment='squad_huggingface_experiment'\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimply provide the `aim_callback` as a callback in the Trainer arguments, and Aim will handle everything behind the scenes.\u003c/p\u003e\n\u003ch2\u003eInitializing the Trainer\u003c/h2\u003e\n\u003cp\u003eWe then load the pre-trained DistilBERT model and define our Trainer instance, incorporating the training arguments, dataset, tokenizer, data collator, and AimCallback:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_squad[\"train\"],\n    eval_dataset=tokenized_squad[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[aim_callback],\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTraining and Evaluation\u003c/h2\u003e\n\u003cp\u003eWith all components set up, we are now ready to commence the training process followed by evaluating our fine-tuned model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etrainer.train()\ntrainer.evaluate()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy following these steps, we can effectively fine-tune a pre-trained language model for question answering while leveraging Aim for experiment tracking and metadata analysis. This approach enables us to derive meaningful insights and enhance the performance of our models efficiently.\u003c/p\u003e\n\u003ch2\u003eExploring the runs on Aim\u003c/h2\u003e\n\u003cp\u003eAfter completing our fine-tuning pipeline, it's super easy to explore and analyze the runs using Aim. We conducted experiments with two batch sizes (16, 32) and three learning rates (0.004, 0.0004, 0.00004) to compare their performance and derive insights in real-time.\u003c/p\u003e\n\u003cp\u003eTo visualize and interpret the experiment results, we utilize the Aim command-line interface. By running the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can access the Aim dashboard via the provided URL, where we'll encounter a comprehensive overview of our experiments and their corresponding metadata. Let's delve into what we'll observe in the Runs Explorer page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.34.35%E2%80%AFpm.png\" alt=\"Runs explorer\" title=\"Runs explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eEach run's page provides details on hyperparameters, tracked metrics, system metrics, environment variables, and package versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.35.20%E2%80%AFpm.png\" alt=\"Run overview\" title=\"Run overview\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, you can review the run status in the Logs tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.37.51%E2%80%AFpm.png\" alt=\"Logs tab\" title=\"Logs tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eExperiments Analysis\u003c/h2\u003e\n\u003cp\u003eThe traditional method for comparing machine learning experiments is through observing learning curves.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.40.34%E2%80%AFpm.png\" alt=\"Metrics explorer\" title=\"Metrics explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eWe plot the loss by differentiating curves based on batch size and learning rate combinations. Evaluation subset curves are shown in solid lines, while training curves are in dashed lines.\u003c/p\u003e\n\u003cp\u003eMoreover to track hyperparameters and compare them easily, we can utilize the Params Explorer. This involves observing how loss depends on the evaluation subset in relation to batch size and learning rate.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.38.32%E2%80%AFpm.png\" alt=\"Param explorer\" title=\"Params explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eBased on this analysis, we observe that the best performing hyperparameter combination is batch size: 32 and learning rate: 0.00004.\u003c/p\u003e\n\u003ch2\u003eAnswering questions\u003c/h2\u003e\n\u003cp\u003eNow let’s use the latest checkpoint of the best model to answer some questions and track the questions and the answers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom transformers import pipeline\n\nmodel_ckpt = \"PATH_TO_BEST_MODEL_CKPT\"\nquestion_answerer = pipeline(\"question-answering\", model=model_ckpt)\n\nquestions = [\n    {\n        \"question\": \"How many countries are members of the United Nations?\",\n        \"context\": \"The United Nations has 195 member countries.\",\n    },\n    {\n        \"question\": \"What is the population of China?\",\n        \"context\": \"China has a population of around 1.4 billion.\",\n    },\n    {\n        \"question\": \"Who is the current president of the United States?\",\n        \"context\": \"As of my last update in 2024, Joe Biden is the current President of the United States, succeeding Donald Trump.\",\n    },\n    {\n        \"question\": \"How many planets are there in the solar system?\",\n        \"context\": \"There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\",\n    },\n    {\n        \"question\": \"What is the capital city of Australia?\",\n        \"context\": \"The capital city of Australia is Canberra.\",\n    },\n]\n\nfor question in questions:\n    output = question_answerer(\n        question=question[\"question\"], context=question[\"context\"]\n    )\n\n    aim_callback.experiment.track(\n        Text(question[\"question\"]),\n        name=\"question\",\n        context={\"context\": question[\"context\"]},\n    )\n\n    aim_callback.experiment.track(\n        Text(output[\"answer\"]),\n        name=\"answer\",\n        context={\"context\": question[\"context\"]},\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo observe the responses we can use the Text Explorer, by grouping by “texts.context.context” in the rows, and “texts.name” in the columns:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.47.57%E2%80%AFpm.png\" alt=\"Text explorer\" title=\"Text explorer\"\u003e\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eTo sum up, this blog post outlined how to fine-tune a DistilBERT model for question answering with the SQuAD dataset, while leveraging Aim for experiment tracking. We started by preprocessing the data and setting up the Trainer with AimCallback integration. Through systematic experimentation, we discovered the best hyperparameter combinations and monitored our model's performance using Aim's user-friendly interface.\u003c/p\u003e\n\u003cp\u003eMoreover, we demonstrated the versatility of exploration beyond mere model training by analyzing text inputs and outputs with Aim. By visualizing learning curves, comparing hyperparameter combinations, and monitoring model responses to questions, we gained valuable insights into our model's behavior.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e if you think Aim is useful. ⭐️\u003c/p\u003e"},"_id":"posts/keep-your-model-up-to-date-for-question-answering.md","_raw":{"sourceFilePath":"posts/keep-your-model-up-to-date-for-question-answering.md","sourceFileName":"keep-your-model-up-to-date-for-question-answering.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/keep-your-model-up-to-date-for-question-answering"},"type":"Post"},{"title":"Log insights from your ML experiments: Why Reports matter","date":"2024-10-23T19:42:39.685Z","author":"Tatyana Manaseryan","description":"Iterating over analyses is an essential part of model training. Reports make it easier to track the progress and see how models improve. That's why we created it for you! 🎉","slug":"log-insights-from-your-ml-experiments-why-reports-matter","image":"/images/dynamic/screen-shot-2024-10-23-at-10.26.17.png","draft":false,"categories":["Tutorials"],"body":{"raw":"Iterating over analyses is an essential part of model training. ML engineers run many experiments, tweaking models and tuning hyperparameters. Reports make it easier to track the progress and see how models improve.\n\nFor teams, interactive reports are even more important. Without them, insights can become scattered as different members work on separate data parts.\n\n![](/images/dynamic/11.jpg)\n\n**And yes, this slows down the entire development process..⏳**\n\nReports unite key findings, making it easy to track who analyzed what and how decisions were made, fostering better communication and alignment across the team.\n\n**That's why Reports are essential and we created it for you! 🎉**\n\nUse Aim reports to show your work and iterate over the knowledge around it. And of course, do it with your collaborators.\n\n* Process tracked metadata and produce new plots directly within the Aim UI, saving time by eliminating the need to write custom plotting functions.\n\n\n\n## How to use Aim reports.\n\n\n\nReports work as a real-time markdown editor where you can embed queries to the data via regular python as well as use all the markdown abilities to document around.\n\nImagine a scenario where you’ve noticed the best performing run and want to make sure that you catch and store all the information about this specific run.\n\nHow?\n\nNavigate to **Reports page** (Button is located on the side bar, scroll it and you’ll see 👀) and click on ` +New `.\n\n![Reports page](/images/dynamic/screenrecording2024-10-22at17.52.11-ezgif.com-video-to-gif-converter.gif \"Reports page\")\n\n### The Best Run - Overview\n\n\n\nThis snippet will generate the best run overview:\n\n````\n```aim\nmetrics_of_interest = 'metric.name in (\"Loss\", \"__system__cpu\")'\nrun_of_interest = 'run.hash == \"d9e89aa7875e44b2ba85612a\"'\nmetrics = repo.fetch_metrics(metrics_of_interest + ' and ' + run_of_interest)\nJSON(metrics[0]['run']['train_config'])\n````\n\n![Reports page](/images/dynamic/screen-shot-2024-10-22-at-18.00.02.png \"Reports page\")\n\nNow let’s move on to the next task.\n\n## Visualization objects and methods\n\n\n\n#### Loss and System CPU metrics\n\nWith Aim reports, it’s quite easy to plot the data, write the assumptions and save for a later view.\n\nWe are to try out other batch sizes with the hope to reduce the total loss to 1.03. \n\n**Run this snippet and you’ll get the line charts in less than 2 seconds.**\n\n````\n```aim\nmetrics_of_interest = 'metric.name in (\"Loss\", \"__system__cpu\")'\nrun_of_interest = 'run.hash == \"d9e89aa7875e44b2ba85612a\"'\nmetrics = repo.fetch_metrics(metrics_of_interest + ' and ' + run_of_interest)\n\nlinechart = LineChart(metrics)\nlinechart.group('column', ['metric.name'])\nlinechart.group('row', ['metric.context.type'])\n```\n````\n\n![Loss and System CPU metrics](/images/dynamic/new-ezgif.com-video-to-gif-converter.gif \"Loss and System CPU metrics\")\n\n![Loss and System CPU metrics](/images/dynamic/screen-shot-2024-10-23-at-10.26.17.png \"Loss and System CPU metrics\")\n\n## ⏯🖼️You can also retrieve images and audios from Aim storage.\n\nJust run simple commands like: `fetch_images` , `fetch_audios`\n\n![](/images/dynamic/screen-shot-2024-10-23-at-10.29.01.png)\n\nYou can also retrieve metrics, figures and texts from Aim storage and group them by  `color`, ` stroke_style`, ` row`, `column` .\n\nThe `group` method can be applied multiple times sequentially.\n\n![](/images/dynamic/grouping-example-1-.png)\n\nWhen you finish, don’t forget to save your report! 📑\n\n![Save your report](/images/dynamic/screen-shot-2024-10-23-at-10.50.57-1-.png \"Save your report\")\n\n**A report not only allows you to customize widgets for your analysis but also keeps a persistent set of runs visible each time you open it.**\n\n🔍This makes it easy to return and review experiments anytime.\n\nWith Aim reports:\n\n* it’s simple to visualize data, \n* make informed assumptions, \n* and document your analysis in one place. \n\n\n\nYou can save these reports for future reference, ensuring that your insights remain accessible for team collaboration or review. This keeps the workflow smooth and makes it easier to track experiments and their outcomes over time.\n\n\n\nAnd yes, **each report comes with a shareable URL**—perfect for keeping your teammates in the loop! We know you’ve been waiting for this… 🤟\n\n## Learn More\n\n\n\nBut that’s not all—there's so much more you can do with Aim reports.\n\nCheck out the [docs](https://aimstack.readthedocs.io/en/latest/ui/pages/reports.html) and feel free to reach out to us on our Discord community if you have any questions!\n\n[Join the Aim community](https://community.aimstack.io/) 💜","html":"\u003cp\u003eIterating over analyses is an essential part of model training. ML engineers run many experiments, tweaking models and tuning hyperparameters. Reports make it easier to track the progress and see how models improve.\u003c/p\u003e\n\u003cp\u003eFor teams, interactive reports are even more important. Without them, insights can become scattered as different members work on separate data parts.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/11.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnd yes, this slows down the entire development process..⏳\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eReports unite key findings, making it easy to track who analyzed what and how decisions were made, fostering better communication and alignment across the team.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThat's why Reports are essential and we created it for you! 🎉\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eUse Aim reports to show your work and iterate over the knowledge around it. And of course, do it with your collaborators.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProcess tracked metadata and produce new plots directly within the Aim UI, saving time by eliminating the need to write custom plotting functions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eHow to use Aim reports.\u003c/h2\u003e\n\u003cp\u003eReports work as a real-time markdown editor where you can embed queries to the data via regular python as well as use all the markdown abilities to document around.\u003c/p\u003e\n\u003cp\u003eImagine a scenario where you’ve noticed the best performing run and want to make sure that you catch and store all the information about this specific run.\u003c/p\u003e\n\u003cp\u003eHow?\u003c/p\u003e\n\u003cp\u003eNavigate to \u003cstrong\u003eReports page\u003c/strong\u003e (Button is located on the side bar, scroll it and you’ll see 👀) and click on \u003ccode\u003e+New\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenrecording2024-10-22at17.52.11-ezgif.com-video-to-gif-converter.gif\" alt=\"Reports page\" title=\"Reports page\"\u003e\u003c/p\u003e\n\u003ch3\u003eThe Best Run - Overview\u003c/h3\u003e\n\u003cp\u003eThis snippet will generate the best run overview:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e```aim\nmetrics_of_interest = 'metric.name in (\"Loss\", \"__system__cpu\")'\nrun_of_interest = 'run.hash == \"d9e89aa7875e44b2ba85612a\"'\nmetrics = repo.fetch_metrics(metrics_of_interest + ' and ' + run_of_interest)\nJSON(metrics[0]['run']['train_config'])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2024-10-22-at-18.00.02.png\" alt=\"Reports page\" title=\"Reports page\"\u003e\u003c/p\u003e\n\u003cp\u003eNow let’s move on to the next task.\u003c/p\u003e\n\u003ch2\u003eVisualization objects and methods\u003c/h2\u003e\n\u003ch4\u003eLoss and System CPU metrics\u003c/h4\u003e\n\u003cp\u003eWith Aim reports, it’s quite easy to plot the data, write the assumptions and save for a later view.\u003c/p\u003e\n\u003cp\u003eWe are to try out other batch sizes with the hope to reduce the total loss to 1.03.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRun this snippet and you’ll get the line charts in less than 2 seconds.\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e```aim\nmetrics_of_interest = 'metric.name in (\"Loss\", \"__system__cpu\")'\nrun_of_interest = 'run.hash == \"d9e89aa7875e44b2ba85612a\"'\nmetrics = repo.fetch_metrics(metrics_of_interest + ' and ' + run_of_interest)\n\nlinechart = LineChart(metrics)\nlinechart.group('column', ['metric.name'])\nlinechart.group('row', ['metric.context.type'])\n```\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/new-ezgif.com-video-to-gif-converter.gif\" alt=\"Loss and System CPU metrics\" title=\"Loss and System CPU metrics\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2024-10-23-at-10.26.17.png\" alt=\"Loss and System CPU metrics\" title=\"Loss and System CPU metrics\"\u003e\u003c/p\u003e\n\u003ch2\u003e⏯🖼️You can also retrieve images and audios from Aim storage.\u003c/h2\u003e\n\u003cp\u003eJust run simple commands like: \u003ccode\u003efetch_images\u003c/code\u003e , \u003ccode\u003efetch_audios\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2024-10-23-at-10.29.01.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can also retrieve metrics, figures and texts from Aim storage and group them by  \u003ccode\u003ecolor\u003c/code\u003e, \u003ccode\u003e stroke_style\u003c/code\u003e, \u003ccode\u003e row\u003c/code\u003e, \u003ccode\u003ecolumn\u003c/code\u003e .\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003egroup\u003c/code\u003e method can be applied multiple times sequentially.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/grouping-example-1-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen you finish, don’t forget to save your report! 📑\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2024-10-23-at-10.50.57-1-.png\" alt=\"Save your report\" title=\"Save your report\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA report not only allows you to customize widgets for your analysis but also keeps a persistent set of runs visible each time you open it.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e🔍This makes it easy to return and review experiments anytime.\u003c/p\u003e\n\u003cp\u003eWith Aim reports:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit’s simple to visualize data,\u003c/li\u003e\n\u003cli\u003emake informed assumptions,\u003c/li\u003e\n\u003cli\u003eand document your analysis in one place.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can save these reports for future reference, ensuring that your insights remain accessible for team collaboration or review. This keeps the workflow smooth and makes it easier to track experiments and their outcomes over time.\u003c/p\u003e\n\u003cp\u003eAnd yes, \u003cstrong\u003eeach report comes with a shareable URL\u003c/strong\u003e—perfect for keeping your teammates in the loop! We know you’ve been waiting for this… 🤟\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eBut that’s not all—there's so much more you can do with Aim reports.\u003c/p\u003e\n\u003cp\u003eCheck out the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/reports.html\"\u003edocs\u003c/a\u003e and feel free to reach out to us on our Discord community if you have any questions!\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://community.aimstack.io/\"\u003eJoin the Aim community\u003c/a\u003e 💜\u003c/p\u003e"},"_id":"posts/log-insights-from-your-ml-experiments-why-reports-matter.md","_raw":{"sourceFilePath":"posts/log-insights-from-your-ml-experiments-why-reports-matter.md","sourceFileName":"log-insights-from-your-ml-experiments-why-reports-matter.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/log-insights-from-your-ml-experiments-why-reports-matter"},"type":"Post"},{"title":"Reproducibility in ML","date":"2024-04-02T07:02:11.308Z","author":"Tatyana Manaseryan","description":"What is Reproducibility in ML? What makes it challenging? Tools and platforms to address different aspects of reproducibility: Data Management, Experiment Tracking, Version Control.","slug":"reproducibility-in-ml","image":"/images/dynamic/medium-2-.png","draft":false,"categories":["Tutorials"],"body":{"raw":"Imagine spending hours looking over a machine learning paper. Fascinated by its approach and promising results, you want to replicate the study, and possibly build upon its findings. Yet, despite your efforts, the results you achieve fall significantly short of those documented in the publication.\n\nAlternatively, consider a scenario where, upon reviewing your colleague's code on GitHub, you notice that the performance metrics of your model do not match with the results reported. Even after double-checking your work, the inconsistency remains.\n\nThese challenges show how hard it is to replicate ML research and is known as reproducibility challenge.\n\n## What is Reproducibility in the context of ML?\n\nLet’s start from understanding the problem. Reproducibility in machine learning (ML) refers to the ability to obtain the same results using the same dataset and algorithm as in the original work.\n\n![](/images/dynamic/page-1.png)\n\n### Why reproduce and get the same results as the original research?\n\nFrom the standpoint of large-scale deployments, reproducibility is essential. It also plays a key role in confirming the validity of research findings and conclusions.\n\nReproducibility is important not only for verifying the accuracy but also for transparency as other researchers may want to run the same ML models. This could become a great opportunity for new experiments and collaborations. \n\nData, code and environment are core elements of any model but often the published research misses the original data and code. Changes in data and source code will affect the outcome significantly. Often, because of data privacy issues, researchers keep the data private. Another important aspect is the environment, it’s important to know which frameworks have been used to avoid problems such as different versions of the library, it can simply break the code.\n\n## Barriers of reproducibility in ML\n\nWhat makes the reproducibility challenging or even impossible? \n\n* **No proper record tracking**\n\nWhile training the models, new experiments are happening and while experimenting it’s important to keep track of changes in parameters, inputs and decisions. \n\nAdditionally, the lack of standardized practices for documenting experiments and results, further complicates the problem. Without clear, comprehensive documentation, replicating and validating findings become more challenging.\n\n* **Changes in datasets**\n\nFrequently, the dataset undergoes random shuffling, leading to variations in the composition of training, testing, and validation sets with each execution. If the sets are predetermined, shuffling within the training set can alter the sequence of sample iteration, impacting the model's learning process.\n\n* **Hyperparameter inconsistency**\n\nEach model trained has its set of hyperparameters and changes in hyperparameters will lead to different results. It’s important to specify the range of the hyperparameters considered and the method employed to select the best hyperparameters.\n\n* **Non-deterministic GPU floating-point calculations**\n\nFloating point numbers have to deal with some odd scenarios such as infinities, different types of zeros, what happens when rounding things which can lead to different results. Changes in GPU and different results from floating-point makes the reproducibility hard.\n\n* **Changes in ML frameworks**\n\nThis can result in different behaviors between versions and inconsistencies across different frameworks.\n\n* **Noisy hidden layers**\n\nNumerous neural network architectures incorporate layers that exhibit random behavior. For instance, the dropout layer randomly eliminates neurons based on a probability (p), resulting in a unique architecture in each instance.\n\n## How to address the reproducibility challenge\n\nOn the brighter side, the ML community has recognized these challenges and is actively working towards solutions. Various tools and platforms have emerged to address different aspects of reproducibility.\n\n### Data Management\n\nTo enhance reproducibility, researchers are encouraged to share their datasets, code, and the specifics of their computational environments. Moreover, using the values to control randomness and maintaining version control of the datasets and ML models can help.\n\n![Data management](/images/dynamic/data-management-diagram.svg \"Data management\")\n\nFor instance, [Docker](https://www.docker.com/) and other containerization technologies allow researchers to package their software with all its dependencies, ensuring that it runs consistently across different computing environments. [Jupyter Notebook](https://jupyter.org/) provides a means for sharing code alongside its output, blending code, visualization, and documentation in a single, interactive environment.\n\n### Experiment Tracking\n\nAnother solution of the problem is the usage of ML experiment tracking tools like [MLflow](https://mlflow.org/), [TensorBoard](https://www.tensorflow.org/tensorboard), [Weights \u0026 Biases](https://wandb.ai/site), [Aim](https://aimstack.io/). These tools help researchers log experiments, track metrics, and compare different runs, providing a structured way to manage and analyze the bunch of experiments that typify ML research. It’s possible to track every change that happens through experiment tracking. \n\nDuring the research there is always a need to query and compare the metadata. Aim is an open-source tool. It logs all the AI metadata (experiments, prompts, etc) enables a UI to compare \u0026 observe them and SDK to query them programmatically.\n\nExploring the metrics of a run: \n\n![Aim: Metrics explorer](/images/dynamic/select_form_dropdown.png \"Aim: Metrics explorer\")\n\nExploring the parameters of run:\n\n![Aim UI: Run Params](/images/dynamic/screen-shot-2024-03-22-at-16.28.34.png \"Aim UI: Run Params\")\n\nAim also supports artifacts logging which makes the models easy to be reproduced. Examples of such artifacts are the model checkpoints, training run configuration files, etc. Aim provides the logging API for artifacts, and the UI for showing artifact's metadata.\n\n![Artifacts logging](/images/dynamic/run-overview-artifacts.png \"Artifacts logging\")\n\n### Version Control\n\nVersion control systems like [Git](https://git-scm.com/), coupled with platforms such as [GitHub](https://github.com/) and [GitLab](https://about.gitlab.com/), enable the tracking of changes in code and documentation, facilitating collaboration and ensuring transparency.\n\n![Version control](/images/dynamic/page-8.png \"Version control\")\n\nFor controlling randomness in experiments, setting and sharing seed values for random number generators is a simple yet effective practice. This ensures that the random elements of ML processes, such as data shuffling and initialization of weights, are consistent across runs.\n\n## To wrap up\n\nWhile challenges to reproducibility in ML are significant, the community's concerted efforts towards developing tools and practices are in the right direction. By embracing these solutions, researchers and practitioners can enhance the reliability and validity of their work, fostering an environment of trasparency and collaboration in the ML field. The journey towards fully reproducible ML is ongoing, but with these tools and practices, we are well on our way.\n\n## Learn More\n\nJoin the [Aim Community](https://discord.com/invite/zXq2NfVdtF) for more AI/ML discussions, share your knowledge, and learn from other ML folks.","html":"\u003cp\u003eImagine spending hours looking over a machine learning paper. Fascinated by its approach and promising results, you want to replicate the study, and possibly build upon its findings. Yet, despite your efforts, the results you achieve fall significantly short of those documented in the publication.\u003c/p\u003e\n\u003cp\u003eAlternatively, consider a scenario where, upon reviewing your colleague's code on GitHub, you notice that the performance metrics of your model do not match with the results reported. Even after double-checking your work, the inconsistency remains.\u003c/p\u003e\n\u003cp\u003eThese challenges show how hard it is to replicate ML research and is known as reproducibility challenge.\u003c/p\u003e\n\u003ch2\u003eWhat is Reproducibility in the context of ML?\u003c/h2\u003e\n\u003cp\u003eLet’s start from understanding the problem. Reproducibility in machine learning (ML) refers to the ability to obtain the same results using the same dataset and algorithm as in the original work.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/page-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eWhy reproduce and get the same results as the original research?\u003c/h3\u003e\n\u003cp\u003eFrom the standpoint of large-scale deployments, reproducibility is essential. It also plays a key role in confirming the validity of research findings and conclusions.\u003c/p\u003e\n\u003cp\u003eReproducibility is important not only for verifying the accuracy but also for transparency as other researchers may want to run the same ML models. This could become a great opportunity for new experiments and collaborations.\u003c/p\u003e\n\u003cp\u003eData, code and environment are core elements of any model but often the published research misses the original data and code. Changes in data and source code will affect the outcome significantly. Often, because of data privacy issues, researchers keep the data private. Another important aspect is the environment, it’s important to know which frameworks have been used to avoid problems such as different versions of the library, it can simply break the code.\u003c/p\u003e\n\u003ch2\u003eBarriers of reproducibility in ML\u003c/h2\u003e\n\u003cp\u003eWhat makes the reproducibility challenging or even impossible?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNo proper record tracking\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhile training the models, new experiments are happening and while experimenting it’s important to keep track of changes in parameters, inputs and decisions.\u003c/p\u003e\n\u003cp\u003eAdditionally, the lack of standardized practices for documenting experiments and results, further complicates the problem. Without clear, comprehensive documentation, replicating and validating findings become more challenging.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eChanges in datasets\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFrequently, the dataset undergoes random shuffling, leading to variations in the composition of training, testing, and validation sets with each execution. If the sets are predetermined, shuffling within the training set can alter the sequence of sample iteration, impacting the model's learning process.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameter inconsistency\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach model trained has its set of hyperparameters and changes in hyperparameters will lead to different results. It’s important to specify the range of the hyperparameters considered and the method employed to select the best hyperparameters.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNon-deterministic GPU floating-point calculations\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFloating point numbers have to deal with some odd scenarios such as infinities, different types of zeros, what happens when rounding things which can lead to different results. Changes in GPU and different results from floating-point makes the reproducibility hard.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eChanges in ML frameworks\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis can result in different behaviors between versions and inconsistencies across different frameworks.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNoisy hidden layers\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNumerous neural network architectures incorporate layers that exhibit random behavior. For instance, the dropout layer randomly eliminates neurons based on a probability (p), resulting in a unique architecture in each instance.\u003c/p\u003e\n\u003ch2\u003eHow to address the reproducibility challenge\u003c/h2\u003e\n\u003cp\u003eOn the brighter side, the ML community has recognized these challenges and is actively working towards solutions. Various tools and platforms have emerged to address different aspects of reproducibility.\u003c/p\u003e\n\u003ch3\u003eData Management\u003c/h3\u003e\n\u003cp\u003eTo enhance reproducibility, researchers are encouraged to share their datasets, code, and the specifics of their computational environments. Moreover, using the values to control randomness and maintaining version control of the datasets and ML models can help.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/data-management-diagram.svg\" alt=\"Data management\" title=\"Data management\"\u003e\u003c/p\u003e\n\u003cp\u003eFor instance, \u003ca href=\"https://www.docker.com/\"\u003eDocker\u003c/a\u003e and other containerization technologies allow researchers to package their software with all its dependencies, ensuring that it runs consistently across different computing environments. \u003ca href=\"https://jupyter.org/\"\u003eJupyter Notebook\u003c/a\u003e provides a means for sharing code alongside its output, blending code, visualization, and documentation in a single, interactive environment.\u003c/p\u003e\n\u003ch3\u003eExperiment Tracking\u003c/h3\u003e\n\u003cp\u003eAnother solution of the problem is the usage of ML experiment tracking tools like \u003ca href=\"https://mlflow.org/\"\u003eMLflow\u003c/a\u003e, \u003ca href=\"https://www.tensorflow.org/tensorboard\"\u003eTensorBoard\u003c/a\u003e, \u003ca href=\"https://wandb.ai/site\"\u003eWeights \u0026#x26; Biases\u003c/a\u003e, \u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e. These tools help researchers log experiments, track metrics, and compare different runs, providing a structured way to manage and analyze the bunch of experiments that typify ML research. It’s possible to track every change that happens through experiment tracking.\u003c/p\u003e\n\u003cp\u003eDuring the research there is always a need to query and compare the metadata. Aim is an open-source tool. It logs all the AI metadata (experiments, prompts, etc) enables a UI to compare \u0026#x26; observe them and SDK to query them programmatically.\u003c/p\u003e\n\u003cp\u003eExploring the metrics of a run:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/select_form_dropdown.png\" alt=\"Aim: Metrics explorer\" title=\"Aim: Metrics explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eExploring the parameters of run:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2024-03-22-at-16.28.34.png\" alt=\"Aim UI: Run Params\" title=\"Aim UI: Run Params\"\u003e\u003c/p\u003e\n\u003cp\u003eAim also supports artifacts logging which makes the models easy to be reproduced. Examples of such artifacts are the model checkpoints, training run configuration files, etc. Aim provides the logging API for artifacts, and the UI for showing artifact's metadata.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/run-overview-artifacts.png\" alt=\"Artifacts logging\" title=\"Artifacts logging\"\u003e\u003c/p\u003e\n\u003ch3\u003eVersion Control\u003c/h3\u003e\n\u003cp\u003eVersion control systems like \u003ca href=\"https://git-scm.com/\"\u003eGit\u003c/a\u003e, coupled with platforms such as \u003ca href=\"https://github.com/\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://about.gitlab.com/\"\u003eGitLab\u003c/a\u003e, enable the tracking of changes in code and documentation, facilitating collaboration and ensuring transparency.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/page-8.png\" alt=\"Version control\" title=\"Version control\"\u003e\u003c/p\u003e\n\u003cp\u003eFor controlling randomness in experiments, setting and sharing seed values for random number generators is a simple yet effective practice. This ensures that the random elements of ML processes, such as data shuffling and initialization of weights, are consistent across runs.\u003c/p\u003e\n\u003ch2\u003eTo wrap up\u003c/h2\u003e\n\u003cp\u003eWhile challenges to reproducibility in ML are significant, the community's concerted efforts towards developing tools and practices are in the right direction. By embracing these solutions, researchers and practitioners can enhance the reliability and validity of their work, fostering an environment of trasparency and collaboration in the ML field. The journey towards fully reproducible ML is ongoing, but with these tools and practices, we are well on our way.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eJoin the \u003ca href=\"https://discord.com/invite/zXq2NfVdtF\"\u003eAim Community\u003c/a\u003e for more AI/ML discussions, share your knowledge, and learn from other ML folks.\u003c/p\u003e"},"_id":"posts/reproducibility-in-ml.md","_raw":{"sourceFilePath":"posts/reproducibility-in-ml.md","sourceFileName":"reproducibility-in-ml.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/reproducibility-in-ml"},"type":"Post"},{"title":"Reproducible forecasting with Prophet and Aim","date":"2023-03-14T07:19:42.802Z","author":"Davit Grigoryan","description":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. ","slug":"reproducible-forecasting-with-prophet-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaIDQvLbqy0KyVG22D0epw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.\n\n# What is Aim?\n\n[Aim](https://aimstack.io/) is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n# What is Prophet?\n\n[Prophet](https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/) is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.\n\nA key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.\n\n# Tracking Prophet experiments with Aim\n\nProphet isn’t trained like neural networks, so you can’t track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost we’re going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check out [the repo](https://github.com/aimhubio/aim/tree/main/examples).\n\nIn the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.\n\n```\nfrom aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n```\n\nAdditionally, if you’re working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Here’s a simple code snippet doing just that:\n\n```\n# Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n```\n\nNow, the extra feature(s) will be tracked as a hyperparameter called **extra_regressors**.\n\nTake a look at a simple, end-to-end example [here](https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py).\n\n# Viewing experiment logs\n\nAfter running the experiment, we can view the logs by executing `aim up` from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png)\n\nAdditionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif)\n\nThese features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.\n\n# Using Aim’s Pythonic search to filter metrics and hyperparameters\n\n\n\nGiven the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if we’re working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aim’s *pythonic search* functionality can be super useful. Say we only want to see the models with MAE ≤ 0.25. We can go to the metrics section and search exactly like we would in Python, say `((metric.name == \"mae\") and (metric.last \u003c= 0.25))` (the **last** part is meant for neural networks, where you might want to see the metric at the last epoch). Here’s a visual demonstration of this feature:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png)\n\nAs you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.\n\n# Conclusion\n\n\n\nTo sum up, Aim’s integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with its *pythonic search* functionality. All these features can make forecasting with Prophet a breeze!\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring 🙌\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs.","html":"\u003cp\u003eYou can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.\u003c/p\u003e\n\u003ch1\u003eWhat is Aim?\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.\u003c/p\u003e\n\u003ch1\u003eWhat is Prophet?\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/\"\u003eProphet\u003c/a\u003e is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.\u003c/p\u003e\n\u003cp\u003eA key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.\u003c/p\u003e\n\u003ch1\u003eTracking Prophet experiments with Aim\u003c/h1\u003e\n\u003cp\u003eProphet isn’t trained like neural networks, so you can’t track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost we’re going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check out \u003ca href=\"https://github.com/aimhubio/aim/tree/main/examples\"\u003ethe repo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdditionally, if you’re working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Here’s a simple code snippet doing just that:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, the extra feature(s) will be tracked as a hyperparameter called \u003cstrong\u003eextra_regressors\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTake a look at a simple, end-to-end example \u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eViewing experiment logs\u003c/h1\u003e\n\u003cp\u003eAfter running the experiment, we can view the logs by executing \u003ccode\u003eaim up\u003c/code\u003e from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThese features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.\u003c/p\u003e\n\u003ch1\u003eUsing Aim’s Pythonic search to filter metrics and hyperparameters\u003c/h1\u003e\n\u003cp\u003eGiven the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if we’re working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aim’s \u003cem\u003epythonic search\u003c/em\u003e functionality can be super useful. Say we only want to see the models with MAE ≤ 0.25. We can go to the metrics section and search exactly like we would in Python, say \u003ccode\u003e((metric.name == \"mae\") and (metric.last \u0026#x3C;= 0.25))\u003c/code\u003e (the \u003cstrong\u003elast\u003c/strong\u003e part is meant for neural networks, where you might want to see the metric at the last epoch). Here’s a visual demonstration of this feature:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eTo sum up, Aim’s integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with its \u003cem\u003epythonic search\u003c/em\u003e functionality. All these features can make forecasting with Prophet a breeze!\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs.\u003c/p\u003e"},"_id":"posts/reproducible-forecasting-with-prophet-and-aim.md","_raw":{"sourceFilePath":"posts/reproducible-forecasting-with-prophet-and-aim.md","sourceFileName":"reproducible-forecasting-with-prophet-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/reproducible-forecasting-with-prophet-and-aim"},"type":"Post"},{"title":"Training Models to Predict Heart Attack","date":"2024-02-14T16:36:40.128Z","author":"Hovhannes Tamoyan","description":"Explore heart attack prediction using machine learning.This article breaks down key factors and insights from the Heart Attack Prediction dataset. With the help of Aim we keep track of model performance.","slug":"training-models-to-predict-heart-attack","image":"/images/dynamic/medium.png","draft":false,"categories":["Tutorials"],"body":{"raw":"In today's blog post, we'll explore heart attack prediction using the Heart Attack Prediction dataset. Our aim is to understand the key factors behind heart attacks with Machine Learning algorithms. We're going also to use the Aim open-source experiment tracking tool, renowned for its great UI and lightning-fast insights.\n\nLet's dive straight into the action by setting up our environment and importing the essential libraries:\n\n```\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom aim import Image, Run\n```\n\nLet's dive into the dataset. We'll start by loading it to see the main features:\n\n```\ndf = pd.read_csv(\"heart.csv\")\ndf.head()\n```\n\nThis dataset gives us key information on heart health, with the \"target\" column indicating a heart attack occurrence (1) or absence (0). Here are the dataset's columns briefly:\n\n* age: Age of the individual\n* sex: Gender of the individual (1 for male, 0 for female)\n* cp: Chest pain type\n* trestbps: Resting blood pressure\n* chol: Serum cholesterol level\n* fbs: Fasting blood sugar level\n* restecg: Resting electrocardiographic results\n* thalach: Maximum heart rate achieved\n* exang: Exercise-induced angina (1 for yes, 0 for no)\n* oldpeak: ST depression induced by exercise relative to rest\n* slope: Slope of the peak exercise ST segment\n* ca: Number of major vessels colored by fluoroscopy\n* thal: Thalassemia\n* target: Level of heart attack (0.1 for lesser and 0.9 for stronger)\n\n\n\nNow, armed with this understanding, let's kickstart our Aim run to track our discoveries:\n\n```\nrun = Run()\n```\n\nIt's as simple as that! Let's begin by logging some key insights about our dataset into our Aim run:\n\n```\nrun[\"dataset_info\"] = {\n    \"name\": \"Heart Attack Prediction\",\n    \"features\": df.describe().to_dict(),\n}\n\n```\n\nEasy, isn't it? But as they say, simplicity often conceals complexity, and we're just scratching the surface of our exploration.\n\nTo observe these and the future metadata that we will track, let’s execute the following command:\n\n```\naim up\n```\n\nThe resulting view will provide us with:\n\n![Overview](/images/dynamic/untitled-2-.png \"Overview\")\n\nAnd by navigating to the Runs page we will see our newly created single run.\n\n![Runs Explorer](/images/dynamic/untitled-3-.png \"Runs Explorer\")\n\nClick on the run name/hash to see the details of this run:\n\n![Run: Overview](/images/dynamic/untitled-4-.png \"Run: Overview\")\n\n## Exploratory Data Analysis\n\nUpon a preliminary examination of the data, let's delve into our first qualitative analysis to explore the correlations among various metrics:\n\n```\ncorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5)\nplt.title(\"Correlation Matrix Heatmap\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Correlation Matrix heatmap\")\n```\n\nWe've generated a correlation matrix heatmap representing the correlation coefficients between different features in the heart attack dataset. Each cell contains the correlation coefficient between the corresponding row and column variables. Here are some key observations:\n\n* **Positive Correlations:**\n\n  * cp (Chest Pain Type) correlates positively with thalach (Maximum heart rate achieved) and slope.\n  * thalach correlates positively with slope.\n* **Negative Correlations:**\n\n  * age correlates negatively with thalach.\n  * sex correlates negatively with the target variable.\n* **Strong Correlations:** \n\n  * The absolute correlation coefficient between cp and the target is relatively high (0.42), indicating a moderate positive correlation.\n  * The absolute correlation coefficient between thalach and the target is also relatively high (0.40).\n* **Other Observations:**\n\n  * exang (Exercise induced angina) exhibits a strong negative correlation with thalach.\n  * oldpeak (ST depression induced by exercise relative to rest) correlates negatively with thalach.\n\n\n\nCorrelation coefficients range from -1 to 1, where 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\n\n![Correlation Matrix heatmap ](/images/dynamic/untitled-5-.png \"Correlation Matrix heatmap\")\n\nNext, let's visualize the distribution of the target variable and track it in Aim:\n\nIt would also be interesting to see how is the target variable distributed and again track it in Aim:\n\n```\nfig = plt.figure(figsize=(8, 6))\nsns.histplot(df[\"target\"], kde=True, color=\"skyblue\", bins=20)\nplt.title(\"Distribution of Target Variable\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Frequency\")\n\naim_img = Image(fig)\nrun.track(aim_img, name=\"Distribution of Target Variable\")\n```\n\n![Distribution of Target Variable](/images/dynamic/untitled-6-.png \"Distribution of Target Variable\")\n\nAlso let’s plot the distribution of the features such as age, trestbps, chol, thalach and oldpeak that are gonna help us predict the target value:\n\n```\nnumerical_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n\nfor i, feature in enumerate(numerical_features, 1):\n    fig = plt.figure(figsize=(10, 8))\n    sns.boxplot(x=df[feature], color=\"skyblue\")\n    aim_img = Image(fig)\n    run.track(aim_img, name=f\"Box Plots of {feature}\")\n```\n\nThe centre of distribution chol \u0026 oldpeak is positively skewed, because the whisker and half-box are longer on the right side of the median than on the left side. Distribution of trestbps is approximately symmetric, because both half-boxes are almost the same length. It’s the most concentrated distribution because the interquartile range. The centre of distribution thalach \u0026 age is negatively skewed because the whisker and half-box are longer on the left side of the median than on the right side.\n\n**To check out the boxplots of each feature we can use the Images Explorer:**\n\n![Images Explorer](/images/dynamic/screenshot-2024-02-13-at-4.36.58 pm.png \"Images Explorer\")\n\nNow let’s normalize our features and check out their importance and track everything on Aim, starting with Impact of Age Categories on Heart Attack Risk:\n\n```\ndf[\"target\"] = df[\"target\"].apply(lambda x: 1 if x \u003e= 0.5 else 0)\n\ndf[\"age_category\"] = pd.cut(df[\"age\"], bins=[29, 45, 60, 100], labels=[\"Young\", \"Middle-aged\", \"Senior\"], right=False,)\nfig = plt.figure(figsize=(10, 6))\nsns.countplot(x=\"age_category\", hue=\"target\", data=df, palette=\"viridis\")\nplt.title(\"Impact of Age Categories on Heart Attack Risk\")\nplt.xlabel(\"Age Category\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Age Categories on Heart Attack Risk\")\n```\n\n![Impact of Age Categories on Heart Attack Risk](/images/dynamic/untitled-7-.png \"Impact of Age Categories on Heart Attack Risk\")\n\nUnderstanding the combined effect of blood pressure (trestbps) and cholesterol (chol) levels on heart attack risk is crucial. We computed the total cardiovascular risk by summing these two variables. The box plot below illustrates the distribution of total cardiovascular risk among individuals with and without heart attacks:\n\n```\ndf[\"total_risk\"] = df[\"trestbps\"] + df[\"chol\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"total_risk\", data=df)\nplt.title(\"Impact of Total Cardiovascular Risk on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Total Cardiovascular Risk on Heart Attack\")\n\n```\n\n![Impact of Total Cardiovascular Risk on Heart Attack](/images/dynamic/untitled-8-.png \"Impact of Total Cardiovascular Risk on Heart Attack\")\n\nExercise-induced angina, characterized by chest pain during physical exertion, could be a significant indicator of heart health. In this visualization, we explore how the presence of exercise-induced angina, coupled with maximum heart rate achieved (thalach), correlates with the occurrence of heart attacks:\n\n```\ndf[\"exercise_angina\"] = (df[\"exang\"] == 1) \u0026 (df[\"thalach\"] \u003e 150)\nfig = plt.figure(figsize=(10, 8))\nsns.countplot(x=\"exercise_angina\", hue=\"target\", data=df)\nplt.title(\"Impact of Exercise-induced Angina on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-induced Angina on Heart Attack\")\n\n```\n\n![Impact of Exercise-induced Angina on Heart Attack](/images/dynamic/untitled-9-.png \"Impact of Exercise-induced Angina on Heart Attack\")\n\nThe ratio of total cholesterol (chol) to high-density lipoprotein (HDL) cholesterol is a critical marker of cardiovascular health. A higher ratio may indicate an increased risk of heart disease. Here, we analyze the distribution of cholesterol-to-HDL ratios among individuals with and without heart attacks:\n\n```\ndf[\"cholesterol_hdl_ratio\"] = df[\"chol\"] / df[\"thalach\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"cholesterol_hdl_ratio\", data=df)\nplt.title(\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\n\n```\n\nThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\n\n![Impact of Cholesterol-to-HDL Ratio on Heart Attack](/images/dynamic/untitled-11-.png \"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\n\nExercise-induced angina can be a symptom of underlying heart conditions. In this analysis, we explore how the presence or absence of exercise-induced angina affects the likelihood of experiencing a heart attack:\n\n```\nfig = plt.figure(figsize=(8, 6))\nsns.countplot(x=\"exang\", hue=\"target\", data=df, palette=\"Set2\")\nplt.title(\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\nplt.xlabel(\"Exercise-Induced Angina (exang)\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\n\n```\n\nThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\n\n![Impact of Exercise-Induced Angina on Heart Attack Risk](/images/dynamic/untitled-12-.png \"Impact of Exercise-Induced Angina in Heart Attack Risk\")\n\n## Training Models to Predict Heart Attack\n\n\n\nIn our pursuit to predict heart attack risk, we’re going to train three simple machine learning algorithms using the provided features from the dataset.\n\n```\nX = df[[\"age\", \"sex\", \"chol\", \"trestbps\", \"thalach\", \"exang\", \"oldpeak\"]]\ny = df[\"target\"]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n```\n\nLet's dive into each model and examine their performance.\n\n### Logistic Regression\n\n\n\nWe kickstart our analysis with the classic Logistic Regression model:\n\n```\nLr_model = LogisticRegression(random_state=42)\nLr_model.fit(X_train_scaled, y_train)\ny_pred = Lr_model.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"logistic_regression\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: LogisticRegression\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"logistic_regression\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-logistic_regression\"] = classification_report(y_test, y_pred, output_dict=True)\n\n```\n\n![Run Params](/images/dynamic/untitled-13-.png \"Run Params\")\n\n### Random Forest Algorithm\n\nMoving forward, let's explore the Random Forest algorithm:\n\n```\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfmodel = RandomForestClassifier(random_state=42)\nrfmodel.fit(X_train, y_train)\ny_pred = rfmodel.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"random_forest_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: RandomForestClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"random_forest_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-random_forest\"] = classification_report(y_test, y_pred, output_dict=True)\n\n```\n\n![Run Params](/images/dynamic/untitled-14-.png \"Run Params\")\n\n### Decision Tree\n\nLastly, let's delve into the Decision Tree algorithm:\n\n```\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"decision_tree_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: DecisionTreeClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"decision_tree_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-decision_tree\"] = classification_report(y_test, y_pred, output_dict=True)\n\n```\n\n![Run Params](/images/dynamic/untitled-15-.png \"Run Params\")\n\n## Post-Training Analysis\n\nTo assess the accuracies of the three models we've trained, we can use the Metrics Explorer:\n\n![Metrics Explorer](/images/dynamic/untitled-16-.png \"Metrics Explorer\")\n\nAdditionally, we can compare the confusion matrices of each method using the Images Explorer:\n\n![Images Explorer](/images/dynamic/untitled-17-.png \"Images Explorer\")\n\nTo comprehend the importance of each feature provided by the Random Forest classifier, we conduct the following analysis:\n\n```\nfeature_importances = rfmodel.feature_importances_\n\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame(\n    {\"Feature\": feature_names, \"Importance\": feature_importances}\n)\n\nfeature_importance_df = feature_importance_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\nfig = plt.figure(figsize=(10, 6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\nplt.title(\"Feature Importances in Predicting Heart Attacks\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Feature Importances in Predicting Heart Attacks\")\n\n```\n\n![Feature Importances in Predicting Heart Attacks](/images/dynamic/untitled-18-.png \"Feature Importances in Predicting Heart Attacks\")\n\nFor more details, the process can be reviewed in the 'Logs' tab. For long and expensive experiments, the benefits are greater:\n\n![Logs](/images/dynamic/untitled-19-.png \"Logs\")\n\n## Conclusion\n\nIn wrapping up our exploration of heart attack prediction, we dove deep into the factors affecting heart health through machine learning.\n\nOur study showed how picking the right features and checking our models are super important for guessing heart attack chances well. We used Aim, a cool tool that helps track experiments. It's known for being easy to use and giving quick results. With Aim, we kept track of what we found while studying. The things we learned could help doctors and policymakers make better plans to help patients avoid heart attacks.\n\nWe used Aim to track every step and insight, showcasing its value in biomedical research. For more on Aim's features, check out [aimstack.io](https://aimstack.io/) or [GitHub repository](https://github.com/aimhubio/aim).\n\n# Learn more\n\nAim is on a mission to democratize AI dev tools. 🙌\n\nTry out Aim, join the [Aim community](https://community.aimstack.io/), share your feedback.","html":"\u003cp\u003eIn today's blog post, we'll explore heart attack prediction using the Heart Attack Prediction dataset. Our aim is to understand the key factors behind heart attacks with Machine Learning algorithms. We're going also to use the Aim open-source experiment tracking tool, renowned for its great UI and lightning-fast insights.\u003c/p\u003e\n\u003cp\u003eLet's dive straight into the action by setting up our environment and importing the essential libraries:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom aim import Image, Run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's dive into the dataset. We'll start by loading it to see the main features:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf = pd.read_csv(\"heart.csv\")\ndf.head()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis dataset gives us key information on heart health, with the \"target\" column indicating a heart attack occurrence (1) or absence (0). Here are the dataset's columns briefly:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eage: Age of the individual\u003c/li\u003e\n\u003cli\u003esex: Gender of the individual (1 for male, 0 for female)\u003c/li\u003e\n\u003cli\u003ecp: Chest pain type\u003c/li\u003e\n\u003cli\u003etrestbps: Resting blood pressure\u003c/li\u003e\n\u003cli\u003echol: Serum cholesterol level\u003c/li\u003e\n\u003cli\u003efbs: Fasting blood sugar level\u003c/li\u003e\n\u003cli\u003erestecg: Resting electrocardiographic results\u003c/li\u003e\n\u003cli\u003ethalach: Maximum heart rate achieved\u003c/li\u003e\n\u003cli\u003eexang: Exercise-induced angina (1 for yes, 0 for no)\u003c/li\u003e\n\u003cli\u003eoldpeak: ST depression induced by exercise relative to rest\u003c/li\u003e\n\u003cli\u003eslope: Slope of the peak exercise ST segment\u003c/li\u003e\n\u003cli\u003eca: Number of major vessels colored by fluoroscopy\u003c/li\u003e\n\u003cli\u003ethal: Thalassemia\u003c/li\u003e\n\u003cli\u003etarget: Level of heart attack (0.1 for lesser and 0.9 for stronger)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, armed with this understanding, let's kickstart our Aim run to track our discoveries:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun = Run()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt's as simple as that! Let's begin by logging some key insights about our dataset into our Aim run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun[\"dataset_info\"] = {\n    \"name\": \"Heart Attack Prediction\",\n    \"features\": df.describe().to_dict(),\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEasy, isn't it? But as they say, simplicity often conceals complexity, and we're just scratching the surface of our exploration.\u003c/p\u003e\n\u003cp\u003eTo observe these and the future metadata that we will track, let’s execute the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe resulting view will provide us with:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-2-.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd by navigating to the Runs page we will see our newly created single run.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-3-.png\" alt=\"Runs Explorer\" title=\"Runs Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eClick on the run name/hash to see the details of this run:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-4-.png\" alt=\"Run: Overview\" title=\"Run: Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eExploratory Data Analysis\u003c/h2\u003e\n\u003cp\u003eUpon a preliminary examination of the data, let's delve into our first qualitative analysis to explore the correlations among various metrics:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5)\nplt.title(\"Correlation Matrix Heatmap\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Correlation Matrix heatmap\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe've generated a correlation matrix heatmap representing the correlation coefficients between different features in the heart attack dataset. Each cell contains the correlation coefficient between the corresponding row and column variables. Here are some key observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePositive Correlations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecp (Chest Pain Type) correlates positively with thalach (Maximum heart rate achieved) and slope.\u003c/li\u003e\n\u003cli\u003ethalach correlates positively with slope.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eNegative Correlations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eage correlates negatively with thalach.\u003c/li\u003e\n\u003cli\u003esex correlates negatively with the target variable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStrong Correlations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe absolute correlation coefficient between cp and the target is relatively high (0.42), indicating a moderate positive correlation.\u003c/li\u003e\n\u003cli\u003eThe absolute correlation coefficient between thalach and the target is also relatively high (0.40).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOther Observations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexang (Exercise induced angina) exhibits a strong negative correlation with thalach.\u003c/li\u003e\n\u003cli\u003eoldpeak (ST depression induced by exercise relative to rest) correlates negatively with thalach.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCorrelation coefficients range from -1 to 1, where 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-5-.png\" alt=\"Correlation Matrix heatmap \" title=\"Correlation Matrix heatmap\"\u003e\u003c/p\u003e\n\u003cp\u003eNext, let's visualize the distribution of the target variable and track it in Aim:\u003c/p\u003e\n\u003cp\u003eIt would also be interesting to see how is the target variable distributed and again track it in Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efig = plt.figure(figsize=(8, 6))\nsns.histplot(df[\"target\"], kde=True, color=\"skyblue\", bins=20)\nplt.title(\"Distribution of Target Variable\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Frequency\")\n\naim_img = Image(fig)\nrun.track(aim_img, name=\"Distribution of Target Variable\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-6-.png\" alt=\"Distribution of Target Variable\" title=\"Distribution of Target Variable\"\u003e\u003c/p\u003e\n\u003cp\u003eAlso let’s plot the distribution of the features such as age, trestbps, chol, thalach and oldpeak that are gonna help us predict the target value:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enumerical_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n\nfor i, feature in enumerate(numerical_features, 1):\n    fig = plt.figure(figsize=(10, 8))\n    sns.boxplot(x=df[feature], color=\"skyblue\")\n    aim_img = Image(fig)\n    run.track(aim_img, name=f\"Box Plots of {feature}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe centre of distribution chol \u0026#x26; oldpeak is positively skewed, because the whisker and half-box are longer on the right side of the median than on the left side. Distribution of trestbps is approximately symmetric, because both half-boxes are almost the same length. It’s the most concentrated distribution because the interquartile range. The centre of distribution thalach \u0026#x26; age is negatively skewed because the whisker and half-box are longer on the left side of the median than on the right side.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTo check out the boxplots of each feature we can use the Images Explorer:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-13-at-4.36.58%E2%80%AFpm.png\" alt=\"Images Explorer\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eNow let’s normalize our features and check out their importance and track everything on Aim, starting with Impact of Age Categories on Heart Attack Risk:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"target\"] = df[\"target\"].apply(lambda x: 1 if x \u003e= 0.5 else 0)\n\ndf[\"age_category\"] = pd.cut(df[\"age\"], bins=[29, 45, 60, 100], labels=[\"Young\", \"Middle-aged\", \"Senior\"], right=False,)\nfig = plt.figure(figsize=(10, 6))\nsns.countplot(x=\"age_category\", hue=\"target\", data=df, palette=\"viridis\")\nplt.title(\"Impact of Age Categories on Heart Attack Risk\")\nplt.xlabel(\"Age Category\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Age Categories on Heart Attack Risk\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-7-.png\" alt=\"Impact of Age Categories on Heart Attack Risk\" title=\"Impact of Age Categories on Heart Attack Risk\"\u003e\u003c/p\u003e\n\u003cp\u003eUnderstanding the combined effect of blood pressure (trestbps) and cholesterol (chol) levels on heart attack risk is crucial. We computed the total cardiovascular risk by summing these two variables. The box plot below illustrates the distribution of total cardiovascular risk among individuals with and without heart attacks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"total_risk\"] = df[\"trestbps\"] + df[\"chol\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"total_risk\", data=df)\nplt.title(\"Impact of Total Cardiovascular Risk on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Total Cardiovascular Risk on Heart Attack\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-8-.png\" alt=\"Impact of Total Cardiovascular Risk on Heart Attack\" title=\"Impact of Total Cardiovascular Risk on Heart Attack\"\u003e\u003c/p\u003e\n\u003cp\u003eExercise-induced angina, characterized by chest pain during physical exertion, could be a significant indicator of heart health. In this visualization, we explore how the presence of exercise-induced angina, coupled with maximum heart rate achieved (thalach), correlates with the occurrence of heart attacks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"exercise_angina\"] = (df[\"exang\"] == 1) \u0026#x26; (df[\"thalach\"] \u003e 150)\nfig = plt.figure(figsize=(10, 8))\nsns.countplot(x=\"exercise_angina\", hue=\"target\", data=df)\nplt.title(\"Impact of Exercise-induced Angina on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-induced Angina on Heart Attack\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-9-.png\" alt=\"Impact of Exercise-induced Angina on Heart Attack\" title=\"Impact of Exercise-induced Angina on Heart Attack\"\u003e\u003c/p\u003e\n\u003cp\u003eThe ratio of total cholesterol (chol) to high-density lipoprotein (HDL) cholesterol is a critical marker of cardiovascular health. A higher ratio may indicate an increased risk of heart disease. Here, we analyze the distribution of cholesterol-to-HDL ratios among individuals with and without heart attacks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"cholesterol_hdl_ratio\"] = df[\"chol\"] / df[\"thalach\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"cholesterol_hdl_ratio\", data=df)\nplt.title(\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-11-.png\" alt=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\" title=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\"\u003e\u003c/p\u003e\n\u003cp\u003eExercise-induced angina can be a symptom of underlying heart conditions. In this analysis, we explore how the presence or absence of exercise-induced angina affects the likelihood of experiencing a heart attack:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efig = plt.figure(figsize=(8, 6))\nsns.countplot(x=\"exang\", hue=\"target\", data=df, palette=\"Set2\")\nplt.title(\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\nplt.xlabel(\"Exercise-Induced Angina (exang)\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-12-.png\" alt=\"Impact of Exercise-Induced Angina on Heart Attack Risk\" title=\"Impact of Exercise-Induced Angina in Heart Attack Risk\"\u003e\u003c/p\u003e\n\u003ch2\u003eTraining Models to Predict Heart Attack\u003c/h2\u003e\n\u003cp\u003eIn our pursuit to predict heart attack risk, we’re going to train three simple machine learning algorithms using the provided features from the dataset.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eX = df[[\"age\", \"sex\", \"chol\", \"trestbps\", \"thalach\", \"exang\", \"oldpeak\"]]\ny = df[\"target\"]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's dive into each model and examine their performance.\u003c/p\u003e\n\u003ch3\u003eLogistic Regression\u003c/h3\u003e\n\u003cp\u003eWe kickstart our analysis with the classic Logistic Regression model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLr_model = LogisticRegression(random_state=42)\nLr_model.fit(X_train_scaled, y_train)\ny_pred = Lr_model.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"logistic_regression\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: LogisticRegression\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"logistic_regression\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-logistic_regression\"] = classification_report(y_test, y_pred, output_dict=True)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-13-.png\" alt=\"Run Params\" title=\"Run Params\"\u003e\u003c/p\u003e\n\u003ch3\u003eRandom Forest Algorithm\u003c/h3\u003e\n\u003cp\u003eMoving forward, let's explore the Random Forest algorithm:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.ensemble import RandomForestClassifier\n\nrfmodel = RandomForestClassifier(random_state=42)\nrfmodel.fit(X_train, y_train)\ny_pred = rfmodel.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"random_forest_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: RandomForestClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"random_forest_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-random_forest\"] = classification_report(y_test, y_pred, output_dict=True)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-14-.png\" alt=\"Run Params\" title=\"Run Params\"\u003e\u003c/p\u003e\n\u003ch3\u003eDecision Tree\u003c/h3\u003e\n\u003cp\u003eLastly, let's delve into the Decision Tree algorithm:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"decision_tree_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: DecisionTreeClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"decision_tree_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-decision_tree\"] = classification_report(y_test, y_pred, output_dict=True)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-15-.png\" alt=\"Run Params\" title=\"Run Params\"\u003e\u003c/p\u003e\n\u003ch2\u003ePost-Training Analysis\u003c/h2\u003e\n\u003cp\u003eTo assess the accuracies of the three models we've trained, we can use the Metrics Explorer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-16-.png\" alt=\"Metrics Explorer\" title=\"Metrics Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, we can compare the confusion matrices of each method using the Images Explorer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-17-.png\" alt=\"Images Explorer\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eTo comprehend the importance of each feature provided by the Random Forest classifier, we conduct the following analysis:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efeature_importances = rfmodel.feature_importances_\n\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame(\n    {\"Feature\": feature_names, \"Importance\": feature_importances}\n)\n\nfeature_importance_df = feature_importance_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\nfig = plt.figure(figsize=(10, 6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\nplt.title(\"Feature Importances in Predicting Heart Attacks\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Feature Importances in Predicting Heart Attacks\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-18-.png\" alt=\"Feature Importances in Predicting Heart Attacks\" title=\"Feature Importances in Predicting Heart Attacks\"\u003e\u003c/p\u003e\n\u003cp\u003eFor more details, the process can be reviewed in the 'Logs' tab. For long and expensive experiments, the benefits are greater:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-19-.png\" alt=\"Logs\" title=\"Logs\"\u003e\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn wrapping up our exploration of heart attack prediction, we dove deep into the factors affecting heart health through machine learning.\u003c/p\u003e\n\u003cp\u003eOur study showed how picking the right features and checking our models are super important for guessing heart attack chances well. We used Aim, a cool tool that helps track experiments. It's known for being easy to use and giving quick results. With Aim, we kept track of what we found while studying. The things we learned could help doctors and policymakers make better plans to help patients avoid heart attacks.\u003c/p\u003e\n\u003cp\u003eWe used Aim to track every step and insight, showcasing its value in biomedical research. For more on Aim's features, check out \u003ca href=\"https://aimstack.io/\"\u003eaimstack.io\u003c/a\u003e or \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub repository\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eAim is on a mission to democratize AI dev tools. 🙌\u003c/p\u003e\n\u003cp\u003eTry out Aim, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e"},"_id":"posts/training-models-to-predict-heart-attack.md","_raw":{"sourceFilePath":"posts/training-models-to-predict-heart-attack.md","sourceFileName":"training-models-to-predict-heart-attack.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/training-models-to-predict-heart-attack"},"type":"Post"}]},"__N_SSG":true},"page":"/blog/[category]","query":{"category":"tutorials"},"buildId":"7hvt0DoxCki5tk9cAyukf","isFallback":false,"gsp":true,"scriptLoader":[{"src":"https://www.googletagmanager.com/gtag/js?id=G-G8YKCN2HLS","strategy":"afterInteractive"},{"id":"google-analytics","strategy":"afterInteractive","children":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){window.dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config','G-G8YKCN2HLS');\n            "}]}</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-G8YKCN2HLS"
            height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript></body></html>