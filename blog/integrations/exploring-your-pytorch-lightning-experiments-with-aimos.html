<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Exploring your PyTorch Lightning experiments with AimOS"/><meta name="twitter:description" content="Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS."/><meta name="twitter:image" content="https://aimstack.io/images/dynamic/banner1.png"/><meta name="twitter:site" content="@aimstack"/><meta name="twitter:creator" content="@aimstack"/><title>Exploring your PyTorch Lightning experiments with AimOS</title><meta name="robots" content="index,follow"/><meta name="description" content="Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS."/><meta property="og:title" content="Exploring your PyTorch Lightning experiments with AimOS"/><meta property="og:description" content="Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS."/><meta property="og:url" content="https://aimstack.io/blog/integrations/exploring-your-pytorch-lightning-experiments-with-aimos"/><meta property="og:type" content="website"/><meta property="og:image" content="/images/dynamic/banner1.png"/><meta property="og:image:alt" content="Exploring your PyTorch Lightning experiments with AimOS"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1224"/><meta property="og:image:height" content="724"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="AimStack"/><link rel="canonical" href="https://aimstack.io/blog/integrations/exploring-your-pytorch-lightning-experiments-with-aimos"/><link rel="preload" href="/_next/static/media/bg.eb38a857.svg" as="image" fetchpriority="high"/><meta name="next-head-count" content="24"/><style id="stitches">--sxs{--sxs:0 t-dSaWrp}@media{:root,.t-dSaWrp{--fonts-base:inherit;--colors-blue:#1093F2;--colors-blueHover:#0F7AC8;--colors-lightBlue:rgba(16, 147, 242, 0.1);--colors-lightBlueHover:rgba(16, 147, 242, 0.2);--colors-darkBlue:#0C1031;--colors-darkBlue500:rgba(12,16,49,0.5);--colors-bigStone:#191D3C;--colors-bigStoneHover:#313551;--colors-green:#14C89D;--colors-black:#000000;--colors-black003:rgba(0,0,0,0.03);--colors-black700:rgba(0,0,0,0.7);--colors-black800:rgba(0,0,0,0.8);--colors-white:#ffffff;--colors-white100:rgba(255,255,255,0.1);--colors-white500:rgba(255,255,255,0.5);--colors-white700:rgba(255,255,255,0.7);--colors-red:#CC231A;--colors-lightGrey:#F4F7F9;--colors-lightGreyHover:#ECEEF0;--colors-grey:#CFD3D6;--colors-darkGrey:#737379;--colors-darkGreyHover:#393940;--colors-primary:var(--colors-blue);--colors-primaryHover:var(--colors-blueHover);--colors-primaryLight:var(--colors-lightBlue);--colors-primaryLightHover:var(--colors-lightBlueHover);--colors-secondary:var(--colors-green);--colors-textColor:var(--colors-black);--colors-bgColor:var(--colors-darkBlue);--colors-danger:var(--colors-red);--fontSizes-1:14px;--fontSizes-2:16px;--fontSizes-3:18px;--fontSizes-4:20px;--fontSizes-5:22px;--fontSizes-6:24px;--fontSizes-7:32px;--fontSizes-8:44px;--fontSizes-9:56px;--fontSizes-10:64px;--fontSizes-11:68px;--fontSizes-baseSize:var(--fontSizes-2);--space-1:4px;--space-2:8px;--space-3:12px;--space-4:16px;--space-5:20px;--space-6:24px;--space-7:28px;--space-8:32px;--space-9:36px;--space-10:40px;--space-11:44px;--space-12:48px;--space-13:52px;--space-14:56px;--fontWeights-1:400;--fontWeights-2:500;--fontWeights-3:600;--fontWeights-4:700;--fontWeights-5:800;--fontWeights-6:900;--shadows-1:0px 60px 66px -65px rgba(11, 47, 97, 0.2);--shadows-2:0px 96px 66px -65px rgba(11, 47, 97, 0.2);--shadows-3:1px 1px 10px 3px  rgb(0, 0, 0, 10%);--shadows-4:0px 10px 24px -10px rgba(11, 47, 97, 0.4);--shadows-5:0px 10px 32px -10px rgba(11, 47, 97, 0.2);--radii-1:6px;--radii-2:8px;--transitions-main:0.2s ease-out}}--sxs{--sxs:1 iPqvQa k-iKtTFk}@media{html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,main,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section{display:block}*[hidden]{display:none}*{box-sizing:border-box}ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:none}table{border-spacing:0}strong{font-weight:var(--fontWeights-5)}body{background:var(--colors-white);color:var(--colors-textColor);font-family:var(--fonts-base);font-size:var(--fontSizes-baseSize);line-height:1.35;overscroll-behavior:none}img{user-drag:none;-webkit-user-drag:none}a{text-decoration:none;color:inherit}a.link{color:var(--colors-primary);font-weight:var(--fontWeights-4);transition:var(--transitions-main)}a.link:hover{color:var(--colors-primaryHover)}.text-center{text-align:center}@keyframes k-iKtTFk{0%{opacity:0}10%{opacity:1}90%{opacity:1}100%{opacity:0}}}--sxs{--sxs:2 c-iSkjJi c-dhzjXW c-guYHoi c-ckOUFN c-hCkBfr c-ePqJJt c-jafYUQ c-BqIMD c-iCFsHS c-kPczbf c-lizetl c-fOPBY c-cZmHrB c-dtSTJL c-caMtBg c-PJLV c-bMqEGV c-ioCbLy c-hIPllP c-kfcfix c-fXQtST c-iHpEPN c-jHDeUH c-fYCBFy c-gDDtX c-kpTGUt c-hdgScR c-kskmfB c-idhSbv c-fixGjY c-kHKySA c-iWPTnC c-dJzRen c-juttUG c-TJTRd c-foiehy c-hoGqzL c-QFqsL c-kBJMsw c-coAScu c-gPhuwe c-eJZVxt c-joiLKa c-IuYqv c-bCTsjq c-EjvQF c-bcXfYW c-kOFnHV c-bosfyl c-gswrlc c-jVwFOG c-eYYUfS c-bBICiS c-kDFBqB c-ghrOTq c-fcTZTi c-gvTmKt c-FsNLu c-jgJYki c-bFmrFE c-MNZuo c-cChYXC c-cXVAvf c-dsvWej c-cmpvrW c-crdKmK c-jpbidf c-jFppbe c-hnRRWM c-hQOWqi c-jiSXep c-bOGecN c-bkAuHq c-bzbvcg c-cnJPJj c-bjrmKQ c-hdKdh c-eVsMjz}@media{.c-iSkjJi{position:relative}.c-iSkjJi .bg-top{z-index:2}.c-iSkjJi .bg-bottom{z-index:1;position:absolute;bottom:300px;left:0;right:0;object-fit:contain;width:100%;height:60%}.c-dhzjXW{display:flex}.c-guYHoi{position:relative;z-index:3;display:flex;flex-direction:column;min-height:100vh}.c-ckOUFN{height:72px;position:fixed;top:0px;left:0px;right:0px;z-index:99;transition:var(--transitions-main);background-color:transparent}.c-ckOUFN.dark{background-color:var(--colors-darkBlue)}.c-ckOUFN.dark a{color:var(--colors-white)}.c-ckOUFN.dark.fixed a{color:var(--colors-textColor)}.c-ckOUFN.fixed{box-shadow:var(--shadows-3);background-color:var(--colors-white)}.c-hCkBfr{margin-left:auto;margin-right:auto;padding-left:var(--space-6);padding-right:var(--space-6);width:100%;max-width:1300px}.c-ePqJJt{display:flex;align-items:center;height:100%}.c-jafYUQ{margin-right:50px}.c-jafYUQ .logo{max-width:158px;width:100%;display:block}.c-jafYUQ .logo-image{display:block}@media (max-width: 1024px){.c-jafYUQ{position:relative;z-index:11}}.c-BqIMD{display:flex;justify-content:center}.c-BqIMD .nav-list{display:flex}.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-6)}@media (max-width: 1440px){.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-4)}}.c-BqIMD .nav-list li a{-webkit-backface-visibility:hidden;backface-visibility:hidden;display:inline-flex}.c-BqIMD .nav-list li a .text{transition:var(--transitions-main)}.c-BqIMD .nav-list li a:hover .text{opacity:.6}@media (max-width: 1024px){.c-BqIMD{position:fixed;top:72px;right:0;bottom:0;left:0;overflow-y:auto;z-index:10;height:0;transition:height 0.5s;background-color:var(--colors-white);flex-direction:column;justify-content:space-between}}@media (max-width: 1024px){.open .c-BqIMD{height:calc(100% - 72px)}}@media (max-width: 1024px){.c-BqIMD .nav-inner{width:100%;flex-direction:column;align-items:inherit;justify-content:space-between;padding-top:16px}}@media (max-width: 1024px){.c-BqIMD .nav-list{flex-direction:column;align-items:flex-start}}@media (max-width: 1024px){.c-BqIMD .nav-list > li{font-size:var(--fontSizes-2);font-weight:var(--fontWeights-3);width:100%}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a{display:block;padding:var(--space-3) var(--space-5)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a:active{background-color:var(--colors-primaryLight)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li:not(:last-child){margin-right:0}}.c-iCFsHS{display:inline-block;height:18px;line-height:18px;padding:0 4px;border-radius:2px;background-color:var(--colors-primary);color:var(--colors-white);font-weight:700;font-size:10px;margin-left:6px}.c-kPczbf{margin-left:auto}.c-kPczbf span span{display:flex;align-items:center}.c-kPczbf.desktop-btn span span{justify-content:flex-end}.c-lizetl{display:none}@media (max-width: 1024px){.c-lizetl{display:flex;justify-content:center;padding-top:var(--space-6);padding-bottom:var(--space-6)}}@media (max-width: 1024px){.c-lizetl > li{margin-right:var(--space-6)}}.c-fOPBY{display:none}@media (max-width: 1024px){.c-fOPBY{position:relative;z-index:11;display:inline-block;-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer}}.c-cZmHrB{flex:1}.c-dtSTJL{background-image:url(/images/static/hero/dots.svg), url(/images/static/hero/bg.svg);background-repeat:no-repeat;background-size:cover, contain;background-position:center, right;position:relative;height:100%;min-height:100vh;display:flex;text-align:center;overflow:hidden;padding:150px 0}@media (max-width: 1024px){.c-dtSTJL{padding:120px 0}}@media (max-width: 743px){.c-dtSTJL{padding:120px 0 24px}}.c-caMtBg{width:100%;position:relative;display:flex}@media (max-width: 1024px){.c-caMtBg{flex-direction:column;align-items:center;height:100%}}.c-caMtBg .hero-button-container{display:flex}@media (max-width: 1024px){.c-caMtBg .hero-button-container{justify-content:center}}@media (max-width: 425px){.c-caMtBg .hero-button-container{flex-direction:column}}@media (max-width: 425px){.c-caMtBg .hero-button-container .hero-try-demo{margin-bottom:var(--space-4)}}.c-caMtBg .hero-button-container .hero-quick-start{margin-left:var(--space-4)}@media (max-width: 425px){.c-caMtBg .hero-button-container .hero-quick-start{margin-left:0}}.c-caMtBg .hero-content{max-width:550px;min-width:550px;text-align:left}@media (max-width: 1024px){.c-caMtBg .hero-content{text-align:center;min-width:unset}}.c-bMqEGV{-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer;border-radius:var(--radii-1);display:inline-block;transition:var(--transitions-main)}.c-ioCbLy{position:absolute;top:50%;transform:translateY(-50%);right:-300px;border-radius:12px;border:1px solid #D8DADF;width:866px;height:auto;aspect-ratio:auto}@media (max-width: 1440px){.c-ioCbLy{width:700px}}@media (max-width: 1024px){.c-ioCbLy{position:unset;transform:unset;width:500px;margin-top:var(--space-12)}}@media (max-width: 743px){.c-ioCbLy{width:90%}}@media (max-width: 425px){.c-ioCbLy{display:none}}.c-hIPllP{padding:80px 0}@media (max-width: 1024px){.c-hIPllP{padding:60px 0}}@media (max-width: 743px){.c-hIPllP{padding:60px 0 24px}}.c-kfcfix{height:110px;display:flex;background-color:white}.c-fXQtST{background-color:white}.c-iHpEPN{display:flex;align-items:center;overflow:visible}.c-iHpEPN img{width:100%;height:auto}.c-jHDeUH{position:relative;overflow:hidden;padding:150px 0}@media (max-width: 1024px){.c-jHDeUH{padding:80px 0 24px}}.c-fYCBFy{object-fit:cover;z-index:-1}.c-gDDtX{display:flex;justify-content:space-between;align-items:center}@media (max-width: 1024px){.c-gDDtX{flex-direction:column}}.c-kpTGUt{display:flex;flex-direction:column;max-width:500px}@media (max-width: 1024px){.c-kpTGUt{text-align:center;align-items:center}}.c-kpTGUt .integrations-title{margin-bottom:var(--space-6);font-weight:800}.c-kpTGUt .integrations-subtitle{line-height:30px;margin-bottom:var(--space-14)}@media (max-width: 1024px){.c-kpTGUt .integrations-subtitle{margin-bottom:var(--space-11)}}.c-kpTGUt a{width:-moz-fit-content;width:fit-content}@media (max-width: 425px){.c-kpTGUt a{width:100%}}.c-hdgScR{width:50%;max-width:540px;height:auto}@media (max-width: 1024px){.c-hdgScR{margin-top:var(--space-7);width:100%}}@media (max-width: 425px){.c-hdgScR{display:none}}.c-kskmfB{position:relative;overflow:hidden;padding:150px 0}@media (max-width: 743px){.c-kskmfB{padding:80px 0 24px;text-align:center}}.c-kskmfB .features-title{margin-bottom:var(--space-6);font-weight:800}@media (max-width: 743px){.c-kskmfB .features-title{text-align:center}}.c-kskmfB .features-subtitle{line-height:30px;margin-bottom:var(--space-14)}@media (max-width: 743px){.c-kskmfB .features-subtitle{margin-bottom:var(--space-11)}}.c-idhSbv{object-fit:contain;z-index:-1}.c-fixGjY{display:flex;flex-direction:column}.c-kHKySA{display:flex;flex-direction:column;max-width:600px;margin:0 auto;text-align:center}@media (max-width: 743px){.c-kHKySA{max-width:100%}}.c-iWPTnC{display:flex;flex-direction:column;justify-content:space-between;width:100%}.c-dJzRen{display:flex;align-items:center;justify-content:space-between;padding-bottom:120px}@media (max-width: 743px){.c-dJzRen{flex-direction:column}}.c-juttUG{text-align:left;width:400px;min-width:300px}@media (max-width: 743px){.c-juttUG{width:100%}}.c-juttUG .step-title{display:flex;align-items:center;text-align:left;margin-bottom:var(--space-6)}.c-juttUG .step-title .badge{color:var(--colors-white);font-size:10px;padding:6px 4px;background:#5865F2;border-radius:4px;margin-left:var(--space-4)}.c-juttUG .step-description{margin-bottom:var(--space-5)}@media (max-width: 743px){.c-juttUG .step-description{text-align:left}}@media (max-width: 743px){.c-juttUG ul{text-align:left}}.c-juttUG ul li{margin-bottom:var(--space-3);position:relative;padding-left:24px}.c-juttUG ul li:before{content:"‚Ä¢";position:absolute;left:0;top:-5px;font-size:24px;color:#5865F2}.c-TJTRd{padding:12px;display:inline-flex;border-radius:8px;margin-bottom:24px}.c-foiehy{max-width:800px;min-width:380px;height:auto;display:block;margin-left:60px}@media (max-width: 743px){.c-foiehy{display:none}}.c-foiehy.step-banner-image-mobile{display:none}@media (max-width: 743px){.c-foiehy.step-banner-image-mobile{margin:0 auto var(--space-10);display:block;min-width:unset;width:100%}}.c-hoGqzL{border-top:1px solid #E4E7EB;display:flex;align-items:flex-start;width:100%}@media (max-width: 743px){.c-hoGqzL{flex-direction:column}}.c-hoGqzL .step-coming-soon{flex:1;max-width:360px;padding-right:40px;margin-right:60px;padding:60px 0}@media (max-width: 743px){.c-hoGqzL .step-coming-soon{padding-right:0;margin-right:0}}.c-QFqsL{display:flex;flex-direction:column;overflow:hidden;position:relative;padding-bottom:100px}@media (max-width: 743px){.c-QFqsL{padding:80px 0 24px}}.c-QFqsL .top-bg{object-fit:cover}.c-QFqsL .bottom-bg{object-fit:cover}.c-kBJMsw{width:100%;height:auto;scale:1.02}.c-coAScu{background-color:#04062E;padding:100px 0}.c-coAScu .quickstart-button{margin-top:var(--space-14);font-weight:var(--fontWeights-3)}@media (max-width: 1024px){.c-coAScu .quickstart-button{display:none}}.c-coAScu .quickstart-button-mobile{display:none;text-align:center;margin-top:var(--space-6)}@media (max-width: 1024px){.c-coAScu .quickstart-button-mobile{display:inline-block;margin-inline-start:auto;margin-inline-end:auto}}.c-gPhuwe{overflow:hidden;display:flex;justify-content:space-between}@media (max-width: 1024px){.c-gPhuwe{flex-direction:column;margin:0 auto}}.c-eJZVxt{max-width:480px}@media (max-width: 1024px){.c-eJZVxt{margin-bottom:var(--space-14)}}.c-eJZVxt .quickstart-title{color:var(--colors-white);margin-bottom:var(--space-6)}.c-eJZVxt .quickstart-subtitle{color:var(--colors-white)}.c-eJZVxt .quickstart-subtitle li{margin-bottom:var(--space-3);position:relative;padding-left:24px;line-height:26px}.c-eJZVxt .quickstart-subtitle li:before{content:"‚Ä¢";position:absolute;left:0;top:-5px;font-size:24px;color:#1093F2}.c-joiLKa{margin-left:80px}@media (max-width: 1024px){.c-joiLKa{margin-left:0}}.c-IuYqv{padding-top:var(--space-5)}.c-IuYqv code{font-family:var(--fonts-Inconsolata)}.c-bCTsjq{position:relative;width:500px}@media (max-width: 1440px){.c-bCTsjq{width:unset}}.c-bCTsjq.light .hljs{background-color:var(--colors-white);color:var(--colors-textColor)}.c-bCTsjq button{position:absolute;right:12px;top:6px;background:none;border:none;cursor:pointer;height:42px;width:42px;transition:var(--transitions-main)}.c-bCTsjq button:hover{opacity:.6}.c-bCTsjq .copied{display:block;opacity:0;position:absolute;right:0;top:-20px;animation:k-iKtTFk 2.5s ease 0s alternate;color:var(--colors-secondary);font-size:var(--fontSizes-1);font-weight:var(--fontWeights-2);transition:var(--transitions-main);z-index:10}.c-bCTsjq .hljs{display:block;overflow-x:auto;color:#abb2bf;background:#0c0d34;margin-bottom:20px;border-radius:var(--radii-1);border:1px solid #3D3D5D;padding:var(--space-4)}.c-bCTsjq .hljs-comment,.c-bCTsjq .hljs-quote{color:#5c6370;font-style:italic}.c-bCTsjq .hljs-doctag,.c-bCTsjq .hljs-keyword,.c-bCTsjq .hljs-formula{color:#c678dd}.c-bCTsjq .hljs-section,.c-bCTsjq .hljs-name,.c-bCTsjq .hljs-selector-tag,.c-bCTsjq .hljs-deletion,.c-bCTsjq .hljs-subst{color:#e06c75}.c-bCTsjq .hljs-literal{color:#56b6c2}.c-bCTsjq .hljs-string,.c-bCTsjq .hljs-regexp,.c-bCTsjq .hljs-addition,.c-bCTsjq .hljs-attribute,.c-bCTsjq .hljs-meta-string{color:#98c379}.c-bCTsjq .hljs-built_in,.c-bCTsjq .hljs-class .hljs-title{color:#e6c07b}.c-bCTsjq .hljs-attr,.c-bCTsjq .hljs-variable,.c-bCTsjq .hljs-template-variable,.c-bCTsjq .hljs-type,.c-bCTsjq .hljs-selector-class,.c-bCTsjq .hljs-selector-attr,.c-bCTsjq .hljs-selector-pseudo,.c-bCTsjq .hljs-number{color:#d19a66}.c-bCTsjq .hljs-symbol,.c-bCTsjq .hljs-bullet,.c-bCTsjq .hljs-link,.c-bCTsjq .hljs-meta,.c-bCTsjq .hljs-selector-id,.c-bCTsjq .hljs-title{color:#61aeee}.c-bCTsjq .hljs-emphasis{font-style:italic}.c-bCTsjq .hljs-strong{font-weight:bold}.c-bCTsjq .hljs-link{text-decoration:underline}.c-EjvQF{background-image:linear-gradient(transparent, #d0cafe40, transparent);position:relative;overflow:hidden;padding:150px 0}@media (max-width: 743px){.c-EjvQF{padding:80px 0 24px}}.c-EjvQF .demos-title{text-align:center;margin-bottom:var(--space-6)}.c-EjvQF .demos-subtitle{text-align:center;margin-bottom:var(--space-14)}.c-bcXfYW{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:16px}@media (max-width: 1024px){.c-bcXfYW{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-bcXfYW{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 575px){.c-bcXfYW{grid-template-columns:repeat(1,minmax(0,1fr))}}.c-kOFnHV{min-width:260px;height:340px;display:inline-flex}@media (max-width: 575px){.c-kOFnHV{width:100%;max-width:360px;margin:0 auto}}.c-kOFnHV a{display:block;width:100%;height:100%;border-radius:var(--radii-2);transition:var(--transitions-main);border:1px solid #E2D7EB;background-color:var(--colors-white)}.c-kOFnHV a .demo-inner{padding:var(--space-5) var(--space-6)}.c-kOFnHV a:hover{background-color:#F3F5F9}.c-bosfyl{width:100%;height:66%;display:block;border-radius:var(--radii-1) var(--radii-1) 0 0;object-fit:cover;padding:10px;overflow:hidden}.c-gswrlc{padding:100px 0}.c-gswrlc .input{max-width:400px}@media (max-width: 1024px){.c-gswrlc{padding:80px 0;text-align:center}}@media (max-width: 1024px){.c-gswrlc .input{margin:0 auto}}@media (max-width: 743px){.c-gswrlc{padding:60px 0}}.c-jVwFOG{display:flex;align-items:center;justify-content:space-between}.c-eYYUfS{flex:1;margin-right:80px}@media (max-width: 1024px){.c-eYYUfS{margin-right:0}}.c-bBICiS{position:relative;display:block}.c-bBICiS .error-message{position:absolute;top:100%;transform:translateY(2px);color:var(--colors-danger);font-size:var(--fontSizes-1)}.c-bBICiS input{border:none;border-radius:var(--radii-1);width:100%;transition:var(--transitions-main)}.c-kDFBqB{flex:1}.c-kDFBqB img{max-width:100%;width:100%;height:auto}@media (max-width: 1024px){.c-kDFBqB{display:none}}.c-ghrOTq{overflow:hidden;width:100%;height:auto;padding-top:4px;display:flex;flex-direction:column}.c-fcTZTi{min-height:100px;display:flex;flex-direction:column;background-color:#673AB7}.c-gvTmKt{margin-top:auto}.c-FsNLu{display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;color:var(--colors-white);padding-top:27px;padding-bottom:27px;border-bottom:1px solid var(--colors-white100)}.c-jgJYki{margin-right:var(--space-9)}.c-jgJYki img{display:block}@media (max-width: 743px){.c-jgJYki img{margin:0 auto}}@media (max-width: 1440px){.c-jgJYki{margin-right:var(--space-6)}}@media (max-width: 743px){.c-jgJYki{width:100%;margin-right:0;margin-bottom:var(--space-1);text-align:center}}.c-bFmrFE{display:flex;margin-right:auto}.c-bFmrFE li:not(:last-child){margin-right:var(--space-9)}@media (max-width: 1440px){.c-bFmrFE li:not(:last-child){margin-right:var(--space-4)}}.c-bFmrFE li{transition:var(--transitions-main)}.c-bFmrFE li:hover{opacity:.6}@media (max-width: 1024px){.c-bFmrFE{order:3;width:100%;margin-top:var(--space-7)}}@media (max-width: 1024px){.c-bFmrFE li:last-child{margin-right:0}}@media (max-width: 743px){.c-bFmrFE{order:2;width:100%;margin-bottom:var(--space-2);flex-direction:column;text-align:center}}@media (max-width: 743px){.c-bFmrFE li{margin-bottom:var(--space-6)}}@media (max-width: 743px){.c-bFmrFE li:not(:last-child){margin-right:0}}.c-MNZuo{display:flex}.c-MNZuo li:not(:last-child){margin-right:var(--space-6)}.c-MNZuo li a{transition:var(--transitions-main)}.c-MNZuo li a:hover{opacity:.6}@media (max-width: 743px){.c-MNZuo{margin:0 auto;order:3}}.c-cChYXC{padding-top:var(--space-4);padding-bottom:var(--space-4);color:var(--colors-white500);text-align:center}.c-cXVAvf{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:40px}@media (max-width: 1024px){.c-cXVAvf{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-cXVAvf{gap:20px;grid-template-columns:repeat(1,minmax(0,1fr))}}.c-dsvWej{padding:10px;border:1px solid #E2D7EB;border-radius:var(--radii-2);transition:var(--transitions-main);height:100%}.c-dsvWej:hover{background-color:#F3F5F9}.c-cmpvrW{position:relative}.c-crdKmK{width:100%;object-fit:cover;height:200px;min-height:200px}.c-jpbidf{padding:20px 10px 0px 10px;position:relative}.c-jFppbe{margin-bottom:var(--space-2);display:flex;align-items:center;color:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe .icon{margin-right:var(--space-1);fill:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe:hover{color:var(--colors-darkGray)}.c-jFppbe:hover .icon{fill:var(--colors-darkGray)}.c-hnRRWM{display:flex;align-items:center;justify-content:flex-end}.c-hnRRWM .icon{margin-right:var(--space-1);fill:var(--colors-grey)}.c-hQOWqi{margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-jiSXep{display:flex;align-items:center;justify-content:center}.c-jiSXep li{height:44px;width:44px;font-size:var(--fontSizes-1);font-weight:var(--fontWeights-4);text-align:center;margin-right:var(--space-1)}.c-jiSXep li a{display:block;line-height:44px;transition:var(--transitions-main)}.c-jiSXep li a.active{background-color:var(--colors-primary);color:var(--colors-white)}.c-jiSXep li a:hover:not(.active){color:var(--colors-primary)}.c-jiSXep li a:hover .icon{fill:var(--colors-primary)}.c-bOGecN{position:relative;display:flex}.c-bkAuHq{object-fit:contain;min-height:100%;width:100%;height:100%;max-width:75%;margin-inline-start:auto;margin-inline-end:auto}.c-bzbvcg{margin:var(--space-10) auto;line-height:1.7;overflow-wrap:break-word}.c-bzbvcg h1,.c-bzbvcg h2,.c-bzbvcg h3,.c-bzbvcg h4,.c-bzbvcg h5,.c-bzbvcg h6{margin-top:var(--space-8);margin-bottom:var(--space-5)}.c-bzbvcg.blog-inner p,.c-bzbvcg.blog-inner ol,.c-bzbvcg.blog-inner ul,.c-bzbvcg.blog-inner a{font-family:var(--fonts-Lora)}.c-bzbvcg.blog-inner .blog-image{position:static !important}.c-bzbvcg h1{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-3);border-bottom:1px solid var(--colors-grey)}@media (max-width: 743px){.c-bzbvcg h1{font-size:var(--fontSizes-7)}}.c-bzbvcg h2{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-bzbvcg h2{font-size:var(--fontSizes-6)}}.c-bzbvcg h3{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-bzbvcg h3{font-size:var(--fontSizes-4)}}.c-bzbvcg p{margin-bottom:var(--space-5);font-size:var(--fontSizes-4);color:var(--colors-black800)}.c-bzbvcg ol,.c-bzbvcg ul{margin-bottom:var(--space-5);padding-left:var(--space-10);font-size:var(--fontSizes-3)}.c-bzbvcg ol li >ul,.c-bzbvcg ul li >ul{list-style-type:circle}.c-bzbvcg ol li:not(:last-child),.c-bzbvcg ul li:not(:last-child){margin-bottom:var(--space-2)}.c-bzbvcg ul{list-style-type:disc}.c-bzbvcg ul :has(input){list-style-type:none}.c-bzbvcg a{text-decoration:underline;word-break:break-all}.c-bzbvcg a:hover{color:var(--colors-primary)}.c-bzbvcg img{max-width:100%;height:auto}.c-bzbvcg strong{font-weight:var(--fontWeights-4)}.c-bzbvcg blockquote{padding-left:var(--space-9);border-left:2px solid var(--colors-grey);font-size:var(--fontSizes-2);font-style:italic}.c-bzbvcg em,.c-bzbvcg q{font-style:italic}.c-bzbvcg pre{font-size:var(--fontSizes-2);font-family:var(--fonts-Inconsolata);line-height:1.7;overflow:auto;white-space:pre;word-break:normal;margin:0 0 var(--space-5);padding:var(--space-10);color:var(--colors-darkGreyHover);background-color:var(--colors-lightGrey);border:none;border-radius:2px}.c-bzbvcg code{white-space:pre-wrap}.c-bzbvcg table{margin-bottom:var(--space-4)}.c-bzbvcg table,.c-bzbvcg th,.c-bzbvcg td{border:1px solid var(--colors-grey)}.c-bzbvcg th,.c-bzbvcg td{padding:6px 13px}.c-bzbvcg iframe{max-width:100%}.c-cnJPJj{display:flex;justify-content:flex-end;margin-bottom:var(--space-10)}.c-cnJPJj button{height:40px;width:40px;background-color:var(--colors-lightGrey) !important;border-radius:var(--radii-1);margin-left:var(--space-3);transition:var(--transitions-main)}.c-cnJPJj button:hover[aria-label=twitter]{background-color:#43b1ff !important}.c-cnJPJj button:hover[aria-label=linkedin]{background-color:#0A66C2 !important}.c-cnJPJj button:hover[aria-label=facebook]{background-color:#6e8dd0 !important}.c-cnJPJj button:hover[aria-label=reddit]{background-color:#fb411c !important}.c-cnJPJj button:hover .icon{fill:var(--colors-white)}.c-bjrmKQ{padding:50px 0;border-top:2px solid var(--colors-lightGrey);border-bottom:2px solid var(--colors-lightGrey);margin-bottom:50px}.c-bjrmKQ a{transition:var(--transitions-main)}.c-bjrmKQ a .text,.c-bjrmKQ a .chevron-text{transition:var(--transitions-main)}.c-bjrmKQ a:hover .text{color:var(--colors-primary)}.c-hdKdh{flex:1;padding-right:var(--space-3);border-right:2px solid var(--colors-lightGrey)}.c-hdKdh a:hover .chevron-text{transform:translateX(15px)}@media (max-width: 743px){.c-hdKdh a:hover .chevron-text{transform:translateX(0)}}.c-eVsMjz{flex:1;text-align:right;padding-left:var(--space-3)}.c-eVsMjz a:hover .chevron-text{transform:translateX(-15px)}@media (max-width: 743px){.c-eVsMjz a:hover .chevron-text{transform:translateX(0)}}}--sxs{--sxs:3 c-dhzjXW-iTKOFX-direction-column c-dhzjXW-jroWjL-align-center c-dhzjXW-bICGYT-justify-center c-dhzjXW-kVNAnR-wrap-noWrap c-PJLV-bWcAUL-size-9 c-PJLV-ypqAk-size-3 c-bMqEGV-kwxnul-size-1 c-bMqEGV-gPhMRO-variant-outline c-bMqEGV-dxfqfJ-variant-primary c-PJLV-jszWhv-size-7 c-PJLV-hlFDyt-size-6 c-PJLV-dnvIlH-size-4 c-PJLV-cBLpOj-size-2 c-bBICiS-RBfUm-size-1 c-PJLV-EDrvg-size-1 c-PJLV-iMgaFX-truncate-true c-PJLV-cllNQK-lineClamp-true c-dhzjXW-ejCoEP-direction-row c-dhzjXW-awKDG-justify-start c-dhzjXW-irEjuD-align-stretch c-dhzjXW-knmidH-justify-between c-dhzjXW-eKWVTQ-gap-5 c-dhzjXW-kdofoX-gap-2 c-dhzjXW-bZmKkd-justify-end}@media{.c-dhzjXW-iTKOFX-direction-column{flex-direction:column}.c-dhzjXW-jroWjL-align-center{align-items:center}.c-dhzjXW-bICGYT-justify-center{justify-content:center}.c-dhzjXW-kVNAnR-wrap-noWrap{flex-wrap:nowrap}.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-9);font-weight:var(--fontWeights-6);line-height:normal}@media (max-width: 1024px){.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-7)}}.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-3)}@media (max-width: 743px){.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-2)}}.c-bMqEGV-kwxnul-size-1{height:53px;line-height:53px;font-size:var(--fontSizes-3);padding-left:var(--space-6);padding-right:var(--space-6)}@media (max-width: 1024px){.c-bMqEGV-kwxnul-size-1{height:50px;line-height:50px}}.c-bMqEGV-gPhMRO-variant-outline{background-color:var(--colors-white);color:#5865F2;border:1px solid #5865F2}.c-bMqEGV-gPhMRO-variant-outline:hover{background-color:#DEE0FC;border-color:var(--colors-primaryHover)}.c-bMqEGV-dxfqfJ-variant-primary{background-color:var(--colors-primary);color:var(--colors-white)}.c-bMqEGV-dxfqfJ-variant-primary:hover{background-color:var(--colors-primaryHover)}.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-2)}@media (max-width: 743px){.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-6)}}.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-4)}}.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-4)}@media (max-width: 1024px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-3)}}@media (max-width: 743px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-2)}}.c-PJLV-cBLpOj-size-2{font-size:var(--fontSizes-2)}.c-bBICiS-RBfUm-size-1 input{padding:17px 24px;font-size:var(--fontSizes-2);background-color:var(--colors-lightGrey)}.c-bBICiS-RBfUm-size-1 input:hover{background-color:var(--colors-lightGreyHover)}.c-PJLV-EDrvg-size-1{font-size:var(--fontSizes-1)}.c-PJLV-iMgaFX-truncate-true{max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.c-PJLV-cllNQK-lineClamp-true{display:-webkit-box;-webkit-line-clamp:var(---lineClamp);-webkit-box-orient:vertical;overflow:hidden}.c-dhzjXW-ejCoEP-direction-row{flex-direction:row}.c-dhzjXW-awKDG-justify-start{justify-content:flex-start}.c-dhzjXW-irEjuD-align-stretch{align-items:stretch}.c-dhzjXW-knmidH-justify-between{justify-content:space-between}.c-dhzjXW-eKWVTQ-gap-5{gap:var(--space-5)}.c-dhzjXW-kdofoX-gap-2{gap:var(--space-2)}.c-dhzjXW-bZmKkd-justify-end{justify-content:flex-end}}--sxs{--sxs:6 c-dhzjXW-iguWOGj-css c-hCkBfr-ilkBNdM-css c-kPczbf-igSvvfr-css c-kPczbf-idOghFk-css c-fOPBY-ieBuAdh-css c-PJLV-iftFgSo-css c-PJLV-idSlxtB-css c-PJLV-ibWHAEn-css c-fXQtST-ijfuMJQ-css c-TJTRd-ihKsdEP-css c-bMqEGV-ihZnFyC-css c-TJTRd-ihYCfXG-css c-TJTRd-igEZsbb-css c-PJLV-ifPSeYF-css c-PJLV-ibZBPXN-css c-PJLV-iejLhMq-css c-PJLV-ietOTGQ-css c-bMqEGV-icGsXGD-css c-PJLV-ieRZfPH-css c-PJLV-iUazGY-css c-PJLV-icsdZpM-css c-dhzjXW-icNlJTv-css c-PJLV-ihjojsL-css c-hCkBfr-ibrRWTZ-css c-dhzjXW-igyJvzA-css c-PJLV-ikaqJbg-css c-dhzjXW-ietOTGQ-css c-PJLV-iduEtjy-css c-PJLV-idnFJmc-css}@media{.c-dhzjXW-iguWOGj-css{height:100vh}.c-hCkBfr-ilkBNdM-css{height:100%}.c-kPczbf-igSvvfr-css{display:none}@media (max-width: 1024px){.c-kPczbf-igSvvfr-css{display:block;padding:var(--space-5)}}.c-kPczbf-idOghFk-css{flex:1}@media (max-width: 1024px){.c-kPczbf-idOghFk-css{display:none}}.c-fOPBY-ieBuAdh-css{margin-left:auto;padding:var(--space-3)}.c-PJLV-iftFgSo-css{margin-bottom:var(--space-6);width:100%}.c-PJLV-idSlxtB-css{margin-bottom:56px;width:100%}.c-PJLV-ibWHAEn-css{margin-bottom:var(--space-6);font-weight:700;text-align:center}.c-fXQtST-ijfuMJQ-css{opacity:0;transition:var(--transitions-main)}.c-TJTRd-ihKsdEP-css{background:#DBEEFE}.c-bMqEGV-ihZnFyC-css{margin-top:var(--space-6);font-weight:var(--fontWeights-3)}.c-TJTRd-ihYCfXG-css{background:#E1DCFE}.c-TJTRd-igEZsbb-css{background:#FADAFE}.c-PJLV-ifPSeYF-css{color:var(--colors-white)}.c-PJLV-ibZBPXN-css{margin-bottom:var(--space-2);font-weight:var(--fontWeights-4)}.c-PJLV-iejLhMq-css{margin-bottom:var(--space-4)}.c-PJLV-ietOTGQ-css{margin-bottom:var(--space-6)}.c-bMqEGV-icGsXGD-css{margin-top:var(--space-14)}.c-PJLV-ieRZfPH-css{text-align:center;margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-PJLV-iUazGY-css{display:flex;align-items:center}.c-PJLV-icsdZpM-css{margin-top:var(--space-4);margin-bottom:var(--space-4);---lineClamp:3;font-family:var(--fonts-Lora)}.c-dhzjXW-icNlJTv-css{margin-top:var(--space-10)}.c-PJLV-ihjojsL-css{font-weight:var(--fontWeights-3)}.c-hCkBfr-ibrRWTZ-css{max-width:848px}.c-dhzjXW-igyJvzA-css{margin-top:var(--space-6)}.c-PJLV-ikaqJbg-css{margin-top:var(--space-6);margin-bottom:var(--space-6);font-weight:var(--fontWeights-4)}.c-dhzjXW-ietOTGQ-css{margin-bottom:var(--space-6)}.c-PJLV-iduEtjy-css{font-weight:var(--fontWeights-2)}.c-PJLV-idnFJmc-css{margin-top:var(--space-3);---lineClamp:2}}</style><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/83b8838004fca89f.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/83b8838004fca89f.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-c90e30c2b72653d5.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-64ec93240796860d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-f217a9f47deeaa28.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/962-688f32d50bed486d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/279-cdd5a11c3c2335fe.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bcategory%5D/%5Bslug%5D-8a5c73c99d96fd9e.js" defer="" crossorigin=""></script><script src="/_next/static/sv29o-HT1NsDRREfQxVOj/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/sv29o-HT1NsDRREfQxVOj/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><main class="c-iSkjJi __className_aaf875"><div class="c-guYHoi"><header class="c-ckOUFN"><div class="c-hCkBfr c-hCkBfr-ilkBNdM-css"><div class="c-ePqJJt"><div class="c-jafYUQ"><a class="logo" href="/"><img alt="AimStack" loading="lazy" width="156" height="37" decoding="async" data-nimg="1" class="logo-image next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 156 37&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/images/static/main/logo.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/static/main/logo.svg"/></a></div><nav class="c-BqIMD"><div class="nav-inner"><ul class="nav-list"><li><a target="_self" class="" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" class="" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" class="" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" class="" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" class="" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" class="active" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" class="" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" class="" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><div class="c-kPczbf c-kPczbf-igSvvfr-css"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div></div><ul class="c-lizetl"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></nav><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div><button type="button" aria-label="menu" class="c-fOPBY c-fOPBY-ieBuAdh-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:black" d="M0 146.286c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.289 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM0 512.008c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.29 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM60.952 816.748c-33.663 0-60.952 27.29-60.952 60.956 0 33.661 27.289 60.951 60.952 60.951h902.097c33.661 0 60.951-27.29 60.951-60.951 0-33.667-27.29-60.956-60.951-60.956h-902.097z"></path></svg></button></div></div></header><div class="c-cZmHrB"><div class="c-PJLV"><div style="padding-top:100px" class="c-hCkBfr"><a href="/blog"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-icNlJTv-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M640 213.333c23.56 0.005 42.658 19.106 42.658 42.667 0 11.78-4.774 22.445-12.492 30.165l-225.832 225.835 225.835 225.835c7.607 7.701 12.306 18.289 12.306 29.975 0 23.562-19.1 42.662-42.662 42.662-11.687 0-22.276-4.699-29.981-12.311l0.004 0.004-256-256c-7.723-7.719-12.5-18.385-12.5-30.167s4.777-22.448 12.5-30.166l256-256c7.7-7.722 18.35-12.499 30.115-12.499 0.018 0 0.036 0 0.054 0l-0.003-0z"></path></svg><p class="c-PJLV c-PJLV-ypqAk-size-3 c-PJLV-ihjojsL-css">Go Back</p></div></a></div><div class="c-hCkBfr c-hCkBfr-ibrRWTZ-css"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-irEjuD-align-stretch c-dhzjXW-knmidH-justify-between c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-eKWVTQ-gap-5 c-dhzjXW-igyJvzA-css"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-kdofoX-gap-2"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M658.744 749.256l-210.744-210.746v-282.51h128v229.49l173.256 173.254zM512 0c-282.77 0-512 229.23-512 512s229.23 512 512 512 512-229.23 512-512-229.23-512-512-512zM512 896c-212.078 0-384-171.922-384-384s171.922-384 384-384c212.078 0 384 171.922 384 384s-171.922 384-384 384z"></path></svg><p class="c-PJLV c-PJLV-EDrvg-size-1">Jan 11, 2024</p></div></div><h1 class="c-PJLV c-PJLV-jszWhv-size-7 c-PJLV-ikaqJbg-css title">Exploring your PyTorch Lightning experiments with AimOS</h1><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-kdofoX-gap-2 c-dhzjXW-ietOTGQ-css"><span class="c-PJLV c-PJLV-cBLpOj-size-2">Author:</span><span class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-iduEtjy-css author">Hovhannes Tamoyan</span></div></div><div class="c-bOGecN"><img alt="Exploring your PyTorch Lightning experiments with AimOS" title="Exploring your PyTorch Lightning experiments with AimOS" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="c-bkAuHq" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/banner1.png 1x, /images/dynamic/banner1.png 2x" src="/images/dynamic/banner1.png"/></div><div class="c-hCkBfr c-hCkBfr-ibrRWTZ-css"><div class="c-bzbvcg blog-inner"><div><p>Discover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.</p><h2 id="introduction">Introduction</h2><p><a href="https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems">AimOS</a>, through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let&#x27;s dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.</p><blockquote><p>AimOS üîç ‚Äî An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.</p></blockquote><h2 id="running-a-sample-training-with-pytorch-lightning-a-step-by-step-guide">Running a Sample Training with PyTorch Lightning: A Step-by-Step Guide</h2><p>Let&#x27;s take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.</p>
<h3 id="setting-the-stage">Setting the Stage</h3><p>Before diving into the script, ensure that AimOS is installed. If not, simply run: `pip3 install aimos`</p><h3 id="1-importing-required-modules">1. Importing Required Modules</h3><p>Start by importing the necessary modules.</p><pre><code>import os

import lightning.pytorch as pl
import torch
from aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger
from torch import nn, optim, utils
from torch.nn import functional as F
from torchmetrics import Accuracy
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor</code></pre><h3 id="2-preparing-the-dataset-and-the-dataloaders">2. Preparing the Dataset and the DataLoaders</h3><p>Download the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.</p><pre><code>dataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())
train_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])
test_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())

train_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)
val_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)
test_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)</code></pre><h3 id="3-instantiate-the-model">3. Instantiate the Model</h3><p>Let&#x27;s create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.</p><pre><code>class LitModule(pl.LightningModule):
    def __init__(self, hidden_size=64, lr=5e-3):
        super().__init__()
        self.hidden_size = hidden_size
        self.lr = lr
        self.num_classes = 10
        self.dims = (1, 28, 28)
        channels, width, height = self.dims

        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(channels * width * height, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, self.num_classes),
        )

        self.accuracy = Accuracy(task=&quot;multiclass&quot;, num_classes=self.num_classes)

    def forward(self, x):
        x = self.model(x)
        return F.log_softmax(x, dim=1)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)
        self.log(&quot;train_loss&quot;, loss)

        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)
        preds = torch.argmax(logits, dim=1)
        self.accuracy(preds, y)

        self.log(&quot;val_loss&quot;, loss, prog_bar=True)
        self.log(&quot;val_acc&quot;, self.accuracy, prog_bar=True)

        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)
        preds = torch.argmax(logits, dim=1)
        self.accuracy(preds, y)

        self.log(&quot;test_loss&quot;, loss, prog_bar=True)
        self.log(&quot;test_acc&quot;, self.accuracy, prog_bar=True)

        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        return optimizer</code></pre><p>To log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.</p><h3 id="4-initialize-aimos-callback">4. Initialize AimOS Callback</h3><p>Initialize the AimOS callback by providing the experiment name.</p><pre><code>aim_logger = AimLogger(
    experiment_name=&quot;example_experiment&quot;
)</code></pre><p>Make sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.</p><pre><code>aimos server</code></pre><h3 id="5-training">5. Training</h3><p>Instantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. üöÄ</p><pre><code>model = LitModule(lr=lr)
aim_logger.experiment.set(&quot;lr&quot;, lr, strict=False)

trainer = pl.Trainer(accelerator=&quot;gpu&quot;, devices=1, max_epochs=5, logger=aim_logger)
trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)

trainer.test(dataloaders=test_loader)</code></pre><p>Let‚Äôs use these learning rates to have more understanding about the learning curve of the task: <code>[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]</code>.</p><p>After completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the <code>.aim</code>repository is initialized and execute:</p><pre><code>aimos ui</code></pre><p>This is the view you&#x27;ll see after following the provided URL:</p><p><img alt="blog image" title="AimOS apps" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png 1x, /images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png 2x" src="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png"/></p><h2 id="interpreting-experiment-results">Interpreting Experiment Results</h2><p>Now let‚Äôs dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment&#x27;s performance.</p>
<h2 id="aimos-overview-page">AimOS Overview Page</h2><p>Head over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.</p><p><img alt="blog image" title="Overview" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png 1x, /images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png 2x" src="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png"/></p><h2 id="aimos-runs-page">AimOS Runs Page</h2><p>Visit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.</p><p><img alt="blog image" title="Runs page" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/dec-21-screenshot-from-aimos.png 1x, /images/dynamic/dec-21-screenshot-from-aimos.png 2x" src="/images/dynamic/dec-21-screenshot-from-aimos.png"/></p><h2 id="run-overview">Run Overview</h2><p>By navigating to each run‚Äôs individual page, you&#x27;ll have a detailed view of the activities within that run.</p><p>Navigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.</p><p><img alt="blog image" title="Run page" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png 1x, /images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png 2x" src="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png"/></p><h2 id="params-tab">Params Tab</h2><p>For a more detailed view of all parameters, navigate to the Params tab.</p><p><img alt="blog image" title="Params tan" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png 1x, /images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png 2x" src="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png"/></p><h2 id="metrics-tab">Metrics Tab</h2><p>In the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let&#x27;s apply color grouping to the runs based on their context.</p><p><img alt="blog image" title="Metrics tab" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png 1x, /images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png 2x" src="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png"/></p><p>In the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.</p><h2 id="comparing-all-runs">Comparing all Runs</h2><p>Let&#x27;s use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the <code>loss</code> and <code>acc</code>metrics, apply the stroke by <code>metric.context.subset</code>, and use a log-scale on the y-axis.</p><p><img alt="blog image" title="Metrics explorer" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" class="blog-image" style="color:transparent;object-fit:contain;aspect-ratio:auto;width:100%;height:100%;background-size:contain;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1200 600&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png 1x, /images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png 2x" src="/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png"/></p><h2 id="wrapping-up">Wrapping up</h2><p>AimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!</p>
<p>For further insights into AI system monitoring and observability of software lifecycle, check out our latest article on¬†<a href="https://aimstack.io/blog">AimStack blog.</a></p><h2 id="learn-more">Learn more</h2><p><a href="https://aimos.readthedocs.io/en/latest/apps/overview.html">AimOS is on a mission to democratize AI Systems logging tools</a>.¬†üôå</p><p>Try out¬†<a href="https://github.com/aimhubio/aimos">AimOS,</a> join the¬†<a href="https://community.aimstack.io/">Aim community</a>¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.</p><p>Don‚Äôt forget to leave us a star on¬†GitHub if you think <a href="https://github.com/aimhubio/aimos">AimOS</a> is useful, and here is the repository of¬†<a href="https://github.com/aimhubio/aim">Aim</a>,¬†an easy-to-use &amp; supercharged open-source experiment tracker.‚≠êÔ∏è</p></div></div><div class="c-cnJPJj"><button aria-label="twitter" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M322.14 927.976c386.322 0 597.681-320.14 597.681-597.678 0-9-0.199-18.2-0.604-27.2 41.116-29.734 76.601-66.565 104.782-108.76-38.292 17.037-78.95 28.164-120.579 33 43.833-26.275 76.654-67.553 92.381-116.18-41.24 24.439-86.339 41.679-133.363 50.98-31.685-33.666-73.577-55.957-119.199-63.427s-92.44 0.299-133.206 22.103c-40.766 21.805-73.211 56.432-92.324 98.527s-23.826 89.314-13.411 134.357c-83.5-4.19-165.188-25.881-239.767-63.667s-140.386-90.823-193.153-155.673c-26.819 46.239-35.025 100.955-22.952 153.027s43.521 97.594 87.952 127.313c-33.356-1.059-65.981-10.040-95.18-26.2v2.6c-0.030 48.525 16.745 95.562 47.475 133.116 30.729 37.552 73.515 63.309 121.085 72.886-30.899 8.451-63.328 9.685-94.78 3.6 13.424 41.731 39.54 78.228 74.706 104.405 35.166 26.171 77.626 40.712 121.454 41.591-74.408 58.449-166.322 90.155-260.94 90.004-16.78-0.027-33.543-1.056-50.2-3.083 96.122 61.666 207.937 94.424 322.14 94.359z"></path></svg></button><button aria-label="linkedin" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M948.201 0h-872.601c-41.8 0-75.6 33-75.6 73.8v876.199c0 40.801 33.8 74.001 75.6 74.001h872.601c41.796 0 75.799-33.2 75.799-73.802v-876.398c0-40.8-34.002-73.8-75.799-73.8zM303.8 872.602h-152v-488.802h152v488.802zM227.8 317.2c-48.8 0-88.2-39.4-88.2-88s39.4-88 88.2-88c48.6 0 88 39.4 88 88 0 48.4-39.4 88-88 88zM872.602 872.602h-151.802v-237.602c0-56.599-1.001-129.6-79.002-129.6-78.998 0-90.998 61.8-90.998 125.6v241.601h-151.6v-488.802h145.6v66.8h2c20.2-38.4 69.802-79 143.598-79 153.805 0 182.204 101.2 182.204 232.799v268.203z"></path></svg></button><button aria-label="facebook" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M1024 512c0-282.77-229.228-512-512-512-282.77 0-512 229.23-512 512 0 255.551 187.23 467.371 432 505.782v-357.78h-130v-148.002h130v-112.8c0-128.32 76.44-199.2 193.391-199.2 56.001 0 114.608 10 114.608 10v126h-64.558c-63.602 0-83.445 39.47-83.445 80v96h142l-22.699 148.002h-119.302v357.78c244.77-38.411 432.003-250.231 432.003-505.782z"></path></svg></button><button aria-label="reddit" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M1024 502.571c0-62.251-50.859-112.853-113.365-112.853-30.507 0-58.155 12.203-78.507 31.829-77.227-50.816-181.717-83.157-297.429-87.296l63.275-199.211 171.349 40.149-0.256 2.475c0 50.901 41.6 92.288 92.757 92.288 51.115 0 92.672-41.387 92.672-92.288s-41.6-92.331-92.672-92.331c-39.253 0-72.704 24.491-86.229 58.837l-184.704-43.307c-8.064-1.963-16.256 2.688-18.773 10.624l-70.571 222.165c-121.088 1.451-230.784 34.048-311.467 86.4-20.224-18.688-47.061-30.379-76.757-30.379-62.507 0-113.323 50.645-113.323 112.896 0 41.387 22.741 77.269 56.192 96.896-2.219 12.032-3.669 24.192-3.669 36.565 0 166.869 205.141 302.635 457.344 302.635s457.387-135.765 457.387-302.635c0-11.691-1.237-23.211-3.2-34.56 35.499-19.072 59.947-55.979 59.947-98.901zM289.109 580.053c0-37.035 30.293-67.2 67.499-67.2s67.456 30.165 67.456 67.2-30.251 67.157-67.456 67.157-67.499-30.123-67.499-67.157zM675.712 779.264c-34.005 33.835-87.381 50.304-163.157 50.304l-0.555-0.128-0.555 0.128c-75.819 0-129.195-16.469-163.157-50.304-6.187-6.144-6.187-16.171 0-22.315 6.187-6.187 16.256-6.187 22.443 0 27.733 27.605 73.771 41.003 140.715 41.003l0.555 0.128 0.555-0.128c66.944 0 112.981-13.44 140.715-41.045 6.187-6.187 16.256-6.144 22.443 0 6.187 6.187 6.187 16.171 0 22.357zM667.648 647.211c-37.205 0-67.456-30.123-67.456-67.157s30.251-67.2 67.456-67.2 67.456 30.165 67.456 67.2-30.251 67.157-67.456 67.157z"></path></svg></button></div></div><div class="c-bjrmKQ"><div class="c-hCkBfr c-hCkBfr-ibrRWTZ-css"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-irEjuD-align-stretch c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap"><div class="c-hdKdh"><a rel="prev" href="/blog/tutorials/exploring-mlflow-experiments-with-a-powerful-ui"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M360.96 475.802l-36.198 36.198 289.638 289.638 72.397-72.397-217.19-217.242 217.19-217.242-72.397-72.397z"></path></svg><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-ihjojsL-css chevron-text">PREVIOUS POST</p></div><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-cllNQK-lineClamp-true c-PJLV-idnFJmc-css text">Exploring MLflow experiments with a powerful UI</p></a></div><div class="c-eVsMjz"><a rel="next" href="/blog/tutorials/how-to-integrate-aimlflow-with-your-remote-mlflow"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-bZmKkd-justify-end c-dhzjXW-kVNAnR-wrap-noWrap"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-ihjojsL-css chevron-text">NEXT POST</p><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M663.040 548.198l36.198-36.198-289.638-289.638-72.397 72.397 217.19 217.242-217.19 217.242 72.397 72.397 253.44-253.44z"></path></svg></div><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-cllNQK-lineClamp-true c-PJLV-idnFJmc-css text">How to integrate aimlflow with your remote MLflow</p></a></div></div></div></div></div></div><footer class="c-ghrOTq"><img alt="footer" fetchpriority="high" width="1440" height="246" decoding="async" data-nimg="1" class="c-kBJMsw next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1440 246&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/bg.eb38a857.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/bg.eb38a857.svg"/><div class="c-fcTZTi"><div class="c-hCkBfr c-gvTmKt"><div class="c-FsNLu"><div class="c-jgJYki"><a class="logo" href="/"><picture><source height="26" width="109" media="(max-width: 1199px)" srcSet="[object Object]"/><img alt="Aimstack" loading="lazy" width="26" height="26" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 26 26&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/aim-logo.e9fed5b2.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/aim-logo.e9fed5b2.svg"/></picture></a></div><ul class="c-bFmrFE"><li><a target="_self" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><ul class="c-MNZuo"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></div><div class="c-cChYXC"><p class="c-PJLV c-PJLV-EDrvg-size-1">Copyright ¬© <!-- -->2024<!-- --> Aimstack</p></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"post":{"title":"Exploring your PyTorch Lightning experiments with AimOS","date":"2024-01-11T13:41:33.464Z","author":"Hovhannes Tamoyan","description":"Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS.","slug":"exploring-your-pytorch-lightning-experiments-with-aimos","image":"/images/dynamic/banner1.png","draft":false,"categories":["Integrations"],"body":{"raw":"Discover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.\n\n## Introduction\n\n\n\n[AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems), through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let's dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.\n\n\u003e AimOS üîç ‚Äî An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.\n\n## Running a Sample Training with PyTorch Lightning: A Step-by-Step Guide\n\n\n\nLet's take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.\n\n\n\n### Setting the Stage\n\n\n\nBefore diving into the script, ensure that AimOS is installed. If not, simply run: \\`pip3 install aimos\\`\n\n### 1. Importing Required Modules\n\n\n\nStart by importing the necessary modules.\n\n```\nimport os\n\nimport lightning.pytorch as pl\nimport torch\nfrom aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger\nfrom torch import nn, optim, utils\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n```\n\n### 2. Preparing the Dataset and the DataLoaders\n\nDownload the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.\n\n```\ndataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)\nval_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)\ntest_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)\n```\n\n### 3. Instantiate the Model\n\nLet's create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.\n\n```\nclass LitModule(pl.LightningModule):\n    def __init__(self, hidden_size=64, lr=5e-3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lr = lr\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n```\n\nTo log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.\n\n### 4. Initialize AimOS Callback\n\n\n\nInitialize the AimOS callback by providing the experiment name.\n\n```\naim_logger = AimLogger(\n    experiment_name=\"example_experiment\"\n)\n```\n\nMake sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.\n\n```\naimos server\n```\n\n### 5. Training\n\nInstantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. üöÄ\n\n```\nmodel = LitModule(lr=lr)\naim_logger.experiment.set(\"lr\", lr, strict=False)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=aim_logger)\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\ntrainer.test(dataloaders=test_loader)\n```\n\nLet‚Äôs use these learning rates to have more understanding about the learning curve of the task: `[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]`.\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the `.aim `repository is initialized and execute:\n\n```\naimos ui\n```\n\nThis is the view you'll see after following the provided URL:\n\n![AimOS apps](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png \"AimOS apps\")\n\n## Interpreting Experiment Results\n\nNow let‚Äôs dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment's performance.\n\n\n\n## AimOS Overview Page\n\nHead over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\n\n![Overview](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png \"Overview\")\n\n## AimOS Runs Page\n\nVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\n\n![Runs page](/images/dynamic/dec-21-screenshot-from-aimos.png \"Runs page\")\n\n## Run Overview\n\nBy navigating to each run‚Äôs individual page, you'll have a detailed view of the activities within that run.\n\nNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\n\n![Run page](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png \"Run page\")\n\n## Params Tab\n\nFor a more detailed view of all parameters, navigate to the Params tab.\n\n![Params tab](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png \"Params tan\")\n\n## Metrics Tab\n\nIn the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let's apply color grouping to the runs based on their context.\n\n![Metrics tab](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png \"Metrics tab\")\n\nIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\n\n## Comparing all Runs\n\n\n\nLet's use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the `loss` and `acc `metrics, apply the stroke by `metric.context.subset `, and use a log-scale on the y-axis.\n\n![Metrics explorer](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png \"Metrics explorer\")\n\n## Wrapping up\n\nAimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!\n\n\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on¬†[AimStack blog.](https://aimstack.io/blog)\n\n## Learn more\n\n\n\n[AimOS is on a mission to democratize AI Systems logging tools](https://aimos.readthedocs.io/en/latest/apps/overview.html).¬†üôå\n\nTry out¬†[AimOS,](https://github.com/aimhubio/aimos) join the¬†[Aim community](https://community.aimstack.io/)¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon‚Äôt forget to leave us a star on¬†GitHub if you think [AimOS](https://github.com/aimhubio/aimos) is useful, and here is the repository of¬†[Aim](https://github.com/aimhubio/aim),¬†an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003cp\u003eDiscover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e, through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let's dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAimOS üîç ‚Äî An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eRunning a Sample Training with PyTorch Lightning: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eLet's take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.\u003c/p\u003e\n\u003ch3\u003eSetting the Stage\u003c/h3\u003e\n\u003cp\u003eBefore diving into the script, ensure that AimOS is installed. If not, simply run: `pip3 install aimos`\u003c/p\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eStart by importing the necessary modules.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\n\nimport lightning.pytorch as pl\nimport torch\nfrom aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger\nfrom torch import nn, optim, utils\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Preparing the Dataset and the DataLoaders\u003c/h3\u003e\n\u003cp\u003eDownload the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)\nval_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)\ntest_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Instantiate the Model\u003c/h3\u003e\n\u003cp\u003eLet's create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass LitModule(pl.LightningModule):\n    def __init__(self, hidden_size=64, lr=5e-3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lr = lr\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.\u003c/p\u003e\n\u003ch3\u003e4. Initialize AimOS Callback\u003c/h3\u003e\n\u003cp\u003eInitialize the AimOS callback by providing the experiment name.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_logger = AimLogger(\n    experiment_name=\"example_experiment\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Training\u003c/h3\u003e\n\u003cp\u003eInstantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. üöÄ\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = LitModule(lr=lr)\naim_logger.experiment.set(\"lr\", lr, strict=False)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=aim_logger)\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\ntrainer.test(dataloaders=test_loader)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet‚Äôs use these learning rates to have more understanding about the learning curve of the task: \u003ccode\u003e[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the \u003ccode\u003e.aim \u003c/code\u003erepository is initialized and execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the view you'll see after following the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png\" alt=\"AimOS apps\" title=\"AimOS apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting Experiment Results\u003c/h2\u003e\n\u003cp\u003eNow let‚Äôs dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment's performance.\u003c/p\u003e\n\u003ch2\u003eAimOS Overview Page\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eAimOS Runs Page\u003c/h2\u003e\n\u003cp\u003eVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/dec-21-screenshot-from-aimos.png\" alt=\"Runs page\" title=\"Runs page\"\u003e\u003c/p\u003e\n\u003ch2\u003eRun Overview\u003c/h2\u003e\n\u003cp\u003eBy navigating to each run‚Äôs individual page, you'll have a detailed view of the activities within that run.\u003c/p\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png\" alt=\"Run page\" title=\"Run page\"\u003e\u003c/p\u003e\n\u003ch2\u003eParams Tab\u003c/h2\u003e\n\u003cp\u003eFor a more detailed view of all parameters, navigate to the Params tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png\" alt=\"Params tab\" title=\"Params tan\"\u003e\u003c/p\u003e\n\u003ch2\u003eMetrics Tab\u003c/h2\u003e\n\u003cp\u003eIn the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let's apply color grouping to the runs based on their context.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png\" alt=\"Metrics tab\" title=\"Metrics tab\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\u003c/p\u003e\n\u003ch2\u003eComparing all Runs\u003c/h2\u003e\n\u003cp\u003eLet's use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003eacc \u003c/code\u003emetrics, apply the stroke by \u003ccode\u003emetric.context.subset \u003c/code\u003e, and use a log-scale on the y-axis.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png\" alt=\"Metrics explorer\" title=\"Metrics explorer\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eAimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on¬†\u003ca href=\"https://aimstack.io/blog\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools\u003c/a\u003e.¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS,\u003c/a\u003e join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†GitHub if you think \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e is useful, and here is the repository of¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e,¬†an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/exploring-your-pytorch-lightning-experiments-with-aimos.md","_raw":{"sourceFilePath":"posts/exploring-your-pytorch-lightning-experiments-with-aimos.md","sourceFileName":"exploring-your-pytorch-lightning-experiments-with-aimos.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-your-pytorch-lightning-experiments-with-aimos"},"type":"Post"},"posts":[{"title":"3D Spleen Segmentation with MONAI and Aim","date":"2022-06-27T20:55:12.121Z","author":"Gev Soghomonian","description":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from http://medicaldecathlon.com/. This is a","slug":"3d-spleen-segmentation-with-monai-and-aim","image":"/images/dynamic/image-7-.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"In this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from¬†[](http://medicaldecathlon.com/)\u003chttp://medicaldecathlon.com/\u003e.\n\nThis is a longer form of the¬†[3D spleen segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb)¬†on the MONAI tutorials repo.\n\nFor a complete in-depth overview of the code and the methodology please follow¬†[this tutorial](https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb)¬†directly or our very own¬†[Aim from Zero to Hero tutorial](https://aimstack.io/blog/tutorials/aim-from-zero-to-hero).\n\n## ***Tracking the basics***\n\nAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\n\n### ***Transformations Matter***\n\nThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the`Single Run Page`.\n\n![](/images/dynamic/image-10-.png)\n\nYou can further search/filter through the runs based on these transformations using Aims‚Äô very own pythonic search language¬†`AimQL`. An Advanced usage looks like this\n\n![](/images/dynamic/image-11-.png)\n\n### *Lets talk about models and optimizers*\n\nSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic¬†`dict`-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from`AimQL`\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n```\n\n![](/images/dynamic/image-12-.png)\n\n## What about Losses?\n\nMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\n\nFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\n\n```\naim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image-5-.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the¬†[complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html)¬†or simply play around in our¬†[interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n## *A picture is worth a thousand words*\n\nIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at¬†`Aim`. You can¬†track images¬†as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\n\n```\naim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n```\n\nAll the grouping/filtering/aggregation functional presented above are also available for Images.\n\n![](/images/dynamic/image-6-.jpeg)\n\n![](/images/dynamic/image-7-.jpeg)\n\n## *A figure is worth a thousand pictures*\n\nWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\n\n```\naim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n```\n\nWithin¬†*`Aim`*¬†you can access all the tracked Figures from within the¬†`Single Run Page`\n\n``\n\n![](/images/dynamic/image-2-.gif)\n\nAnother interesting thing that one can¬†[track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\n## ***The curtain falls***\n\nIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with¬†*`1-2`*¬†lines of code. You can also play around with the completed MONAI integration results with our¬†[interactive demo](http://play.aimstack.io:10005/).\n\nIf you have any bug reports please follow up on our¬†[github repository](https://github.com/aimhubio/aim/issues/new/choose). Feel free to join our growing¬†[Slack community](https://slack.aimstack.io/)¬†and ask questions directly to the developers and seasoned users.\n\nIf you find Aim useful,¬†[please stop by](https://github.com/aimhubio/aim)¬†and drop us a star.","html":"\u003cp\u003eIn this tutorial we will show how to use Aim and MONAI for 3d Spleen segmentation on a medical dataset from¬†\u003ca href=\"http://medicaldecathlon.com/\"\u003e\u003c/a\u003e\u003ca href=\"http://medicaldecathlon.com/\"\u003ehttp://medicaldecathlon.com/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is a longer form of the¬†\u003ca href=\"https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb\"\u003e3D spleen segmentation tutorial\u003c/a\u003e¬†on the MONAI tutorials repo.\u003c/p\u003e\n\u003cp\u003eFor a complete in-depth overview of the code and the methodology please follow¬†\u003ca href=\"https://github.com/osoblanco/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb\"\u003ethis tutorial\u003c/a\u003e¬†directly or our very own¬†\u003ca href=\"https://aimstack.io/blog/tutorials/aim-from-zero-to-hero\"\u003eAim from Zero to Hero tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eTracking the basics\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAs we are dealing with an application in a BioMedical domain, we would like to be able to track all possible transformations, hyperparameters and architectural metadata. We need the latter to analyze our model and results with a greater level of granularity.\u003c/p\u003e\n\u003ch3\u003e\u003cem\u003e\u003cstrong\u003eTransformations Matter\u003c/strong\u003e\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eThe way we preprocess and augment the data has massive implications for the robustness of the eventual model. Thus carefully tracking and filtering through the transformations is obligatory. Aim helps us to seamlessly integrate this into the experiment through the\u003ccode\u003eSingle Run Page\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-10-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can further search/filter through the runs based on these transformations using Aims‚Äô very own pythonic search language¬†\u003ccode\u003eAimQL\u003c/code\u003e. An Advanced usage looks like this\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-11-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cem\u003eLets talk about models and optimizers\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eSimilar to the augmentations, tracking the architectural and optimization choices during model creation can be essential for analyzing further feasibility and the outcome of the experiment. It is seamless to track; as long the (hyper)parameters conform to a pythonic¬†\u003ccode\u003edict\u003c/code\u003e-like object, you can incorporate it within the trackables and enjoy the complete searchability functional from\u003ccode\u003eAimQL\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run  = aim.Run(experiment = 'some experiment name')\n\naim_run['hparams'] = tansformations_dict_train\n...\nUNet_meatdata = dict(spatial_dims=3,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n    norm=Norm.BATCH)\n\naim_run['UNet_meatdata'] = UNet_meatdata\n\n...\n\naim_run['Optimizer_metadata'] = Optimizer_metadata\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-12-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eWhat about Losses?\u003c/h2\u003e\n\u003cp\u003eMajority of people working within the machine learning landscape have tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with matplotlib. Aim allows for simple tracking of such losses (The ease of integration and use is an inherent theme).\u003c/p\u003e\n\u003cp\u003eFurthermore, a natural question would be, what if we have numerous experiments with a variety of losses and metrics to track. Aim has you covered here with the ease of tracking and grouping. Tracking a set of varying losses can be completed in this fashion\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run  = aim.Run(experiment = 'some experiment name')\n\n#Some Preprocessing\n...\n\n# Training Pipeline\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n\n    aim_run.track(loss_1 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(loss_2 , name = \"Loss_name\", context = {'type':'loss_type'})\n    aim_run.track(metric_1 , name = \"Metric_name\", context = {'type':'metric_type'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter grouping, we end up with a visualization akin to this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-5-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregating, grouping, decoupling and customizing the way you want to visualise your experimental metrics is rather intuitive. You can also view the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003ecomplete documentation\u003c/a\u003e¬†or simply play around in our¬†\u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive Demo\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eA picture is worth a thousand words\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eIn the context of BioMedical Imaging, particularly 3D spleen segmentation, we would like to be able to view how does the model improve over the training iterations. Previously this would have been a challenging task, however, we came up with a rather neat solution at¬†\u003ccode\u003eAim\u003c/code\u003e. You can¬†track images¬†as easily as losses using our framework, meaning the predictions masks on the validation set can be tracked per slice of the 3D segmentation task.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run.track(aim.Image(val_inputs[0, 0, :, :, slice_to_track], \\\n                        caption=f'Input Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Input'})\n\naim_run.track(aim.Image(val_labels[0, 0, :, :, slice_to_track], \\\n                        caption=f'Label Image: {index}'), \\\n                name = 'Validation',  context = {'type':'Label'})\naim_run.track(aim.Image(output,caption=f'Predicted Label: {index}'), \\\n                name = 'Predictions',  context = {'type':'labels'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll the grouping/filtering/aggregation functional presented above are also available for Images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-6-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-7-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eA figure is worth a thousand pictures\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eWe decided to go beyond simply comparing images and try to incorporate the complete scale of 3D spleen segmentation. For this very purpose created an optimized animation with plotly. It showcases the complete (or sampled) slices from the 3d objects in succession. Thus, integrating any kind of Figure is simple as always\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run.track(\n    aim.Figure(figure),\n    name='some_name',\n    context={'context_key':'context_value'}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithin¬†\u003cem\u003e\u003ccode\u003eAim\u003c/code\u003e\u003c/em\u003e¬†you can access all the tracked Figures from within the¬†\u003ccode\u003eSingle Run Page\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e``\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-2-.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAnother interesting thing that one can¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\"\u003etrack is distributions\u003c/a\u003e. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.pytorch import track_params_dists, track_gradients_dists\n\n....\n\ntrack_gradients_dists(model, aim_run)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDistributions can be accessed from the Single Run Page as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-6-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eThe curtain falls\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eIn this blogpost we saw that it is possible to easily integrate Aim in both ever-day experiment tracking and highly advanced granular research with¬†\u003cem\u003e\u003ccode\u003e1-2\u003c/code\u003e\u003c/em\u003e¬†lines of code. You can also play around with the completed MONAI integration results with our¬†\u003ca href=\"http://play.aimstack.io:10005/\"\u003einteractive demo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you have any bug reports please follow up on our¬†\u003ca href=\"https://github.com/aimhubio/aim/issues/new/choose\"\u003egithub repository\u003c/a\u003e. Feel free to join our growing¬†\u003ca href=\"https://slack.aimstack.io/\"\u003eSlack community\u003c/a\u003e¬†and ask questions directly to the developers and seasoned users.\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful,¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eplease stop by\u003c/a\u003e¬†and drop us a star.\u003c/p\u003e"},"_id":"posts/3d-spleen-segmentation-with-monai-and-aim.md","_raw":{"sourceFilePath":"posts/3d-spleen-segmentation-with-monai-and-aim.md","sourceFileName":"3d-spleen-segmentation-with-monai-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/3d-spleen-segmentation-with-monai-and-aim"},"type":"Post"},{"title":"AI Observability with AimOS: A Deep Dive into the LangChain Debugger","date":"2023-11-24T20:58:19.490Z","author":"Hovhannes Tamoyan","description":"Explore step-by-step guide for LangChain Debugger, a tool designed for in-depth tracking and visualization of LangChain scripts. The guide features a hands-on example emphasizing its abilities to log LLMs prompts, tool inputs/outputs, and chain metadata, providing a clear understanding of its functionalities.","slug":"ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger","image":"/images/dynamic/langchain.png","draft":false,"categories":["Integrations"],"body":{"raw":"Imagine being able to track and visualize every part of your LangChain script in detail. That's what LangChain Debugger does, working smoothly with AimOS. \n\nWe'll look at a sample script to see how well LangChain Debugger and the scripts work together. It will highlight the main features of LangChain Debugger which logs LLMs prompts and generations, tools inputs/outputs, and chains metadata.\n\nIn this guide, we will prompt \"What are the number of parameters in OpenAI‚Äôs GPT5 (based on rumors) and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\n\n\u003e [AimOS üîç ](https://github.com/aimhubio/aimos)‚Äî A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\n\n## Executing LangChain Debugger: A Step-by-Step Guide\n\nLet's do a practical example using a script to interact with LangChain to show how well it works with AimOS and LangChain Debugger.\n\n### Setting Stage\n\nBefore diving into the script, ensure that LangChain Debugger is installed and properly configured. If not, head over to the AimOS GitHub page for installation instructions: https://github.com/aimhubio/aimos.\n\n### Sample Script\n\n### 1. Importing Required Modules\n\nWe begin by importing the necessary modules for LangChain and AimOS interaction.\n\n```\nfrom aimstack.langchain_debugger.callback_handlers import GenericCallbackHandler\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n```\n\n### 2. Configuring Callbacks and LangChain LLM\n\nEstablish a callback mechanism using LangChain Debugger's **`GenericCallbackHandler`** and set up the LangChain LLM (Large Language Model) using OpenAI.\n\n```\naim_callback = GenericCallbackHandler(repo_path=\"aim://0.0.0.0:53800\")\ncallbacks = [aim_callback]\nllm = OpenAI(temperature=0, callbacks=callbacks)\n```\n\nEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server effortlessly with the following command:\n\n```\naimos server\n```\n\n### 3. Loading Tools and Initializing Agent\n\nLoad the required tools for your LangChain script, initialize the agent, and set the agent type as **`ZERO_SHOT_REACT_DESCRIPTION`**.\n\n```\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callbacks=callbacks)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    callbacks=callbacks,\n)\n```\n\n### 4. Running LangChain Script\n\nExecute your LangChain script within the agent using the **`run`** method.\n\n```\nagent.run(\n    \"What are the number of parameters in GPT5 and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\"\n)\n```\n\n### 5. Flushing Callbacks\n\nEnsure that callbacks are flushed after running the script to capture the trace.\n\n```\naim_callback.flush()\n```\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the **`.aim`** repository is initialized and simply execute:\n\n```\naimos ui\n```\n\nüòä Enjoy this preview of what you'll see once you visit the provided URL:\n\n![AimOS Apps](/images/dynamic/navigating-depths-debugger-1-1-.png \"AimOS Apps\")\n\n## Interpreting the results\n\nHead over to the AimOS Traces page to witness the detailed trace of your LangChain script. This page provides a comprehensive overview of every action and interaction within your LangChain environment.\n\nHere you'll find a new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show the details about the script‚Äôs execution, token use, and the costs involved.\n\n![LangChain Debugger](/images/dynamic/screenshot-2023-11-23-at-11.57.51‚ÄØpm-1-.png \"LangChain Debugger\")\n\n## Overview: Key Elements\n\nNavigate through the Overview tab to grasp essential information:\n\n* Trace: A unique identifier for each trace, allowing you to easily reference and analyze specific interactions. \n\n* Total Steps: The number of actions or queries performed in a trace, providing a quick measure of complexity\n\n* Total Tokens: The total number of tokens is represented to understand the computational load of your queries, ensuring optimal performance.\n\n* Cost: This represents the total cost of the trace which is a critical factor in resource management, the cost breakdown helps you make informed decisions.\n\n![Overview tab](/images/dynamic/screenshot-2023-11-23-at-11.58.15‚ÄØpm-1-.png \"Overview tab\")\n\n## Steps: Exploring the Process\n\nThe Steps tab provides a detailed walkthrough of the sequence of actions undertaken throughout the pipeline to achieve the specified goal. In our example, the Agent executed a total of 18 steps to successfully accomplish the task.\n\nHere's a breakdown of the pivotal actions:\n\n1. LLM Response: The Language Model (LLM) initially determines that, for the given question, it needs to utilize the Search tool to identify the person in question.\n2. Search Tool Execution: The LLM activates the Search tool to acquire the necessary information, successfully obtaining the desired output.\n3. LLM Decision: Subsequently, the LLM decides to employ the Search tool once again to retrieve additional details, such as the the number of parameters in GPT5 (based on rumors) and GPT4.\n4. Search Tool Execution: The Agent, following the LLM's decision, triggers the Search tool once more, fetching the required information.\n5. Calculator Tool Usage: With the obtained number, the Agent employs the Calculator tool to calculate the logarithm (base e) of the difference between the number of parameters.\n6. Final Answer: Lastly, armed with all the necessary information, the LLM crafts the complete answer to the original question and returns it.\n\n![Steps tab](/images/dynamic/screenshot-2023-11-23-at-11.58.42‚ÄØpm-1-.png \"Steps tab\")\n\n![Steps tab](/images/dynamic/screenshot-2023-11-24-at-12.00.36‚ÄØam-1-.png \"Steps tab\")\n\n## Cost: Evaluating Resource Usage\n\nIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LangChain activities.\n\n* On the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\n* On the y-axis, you can see the number of tokens sent or received.\n\n![Cost tab](/images/dynamic/screenshot-2023-11-24-at-12.00.17‚ÄØam-1-.png \"Cost tab\")\n\n## Wrapping up\n\nThe LangChain Debugger in AimOS is a powerful tool for deepening your understanding and gaining in-depth insights into your LangChain script. It logs LLMs prompts and generations, tools inputs/outputs, and chains metadata. It's great for both experienced users and beginners, offering a comprehensive observability.\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the¬†[AimStack blog.](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\n## Learn more\n\n[AimOS is on a mission to democratize AI Systems logging tools.¬†](https://aimos.readthedocs.io/en/latest/apps/overview.html)üôå\n\nTry out¬†[AimOS](https://github.com/aimhubio/aimos), join the¬†[Aim community](https://community.aimstack.io/),¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aimos/)¬†if you think AimOS is useful, and here is the repository of¬†[Aim](https://github.com/aimhubio/aim),¬†an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003cp\u003eImagine being able to track and visualize every part of your LangChain script in detail. That's what LangChain Debugger does, working smoothly with AimOS.\u003c/p\u003e\n\u003cp\u003eWe'll look at a sample script to see how well LangChain Debugger and the scripts work together. It will highlight the main features of LangChain Debugger which logs LLMs prompts and generations, tools inputs/outputs, and chains metadata.\u003c/p\u003e\n\u003cp\u003eIn this guide, we will prompt \"What are the number of parameters in OpenAI‚Äôs GPT5 (based on rumors) and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS üîç \u003c/a\u003e‚Äî A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eExecuting LangChain Debugger: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eLet's do a practical example using a script to interact with LangChain to show how well it works with AimOS and LangChain Debugger.\u003c/p\u003e\n\u003ch3\u003eSetting Stage\u003c/h3\u003e\n\u003cp\u003eBefore diving into the script, ensure that LangChain Debugger is installed and properly configured. If not, head over to the AimOS GitHub page for installation instructions: https://github.com/aimhubio/aimos.\u003c/p\u003e\n\u003ch3\u003eSample Script\u003c/h3\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eWe begin by importing the necessary modules for LangChain and AimOS interaction.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aimstack.langchain_debugger.callback_handlers import GenericCallbackHandler\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Configuring Callbacks and LangChain LLM\u003c/h3\u003e\n\u003cp\u003eEstablish a callback mechanism using LangChain Debugger's \u003cstrong\u003e\u003ccode\u003eGenericCallbackHandler\u003c/code\u003e\u003c/strong\u003e and set up the LangChain LLM (Large Language Model) using OpenAI.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = GenericCallbackHandler(repo_path=\"aim://0.0.0.0:53800\")\ncallbacks = [aim_callback]\nllm = OpenAI(temperature=0, callbacks=callbacks)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server effortlessly with the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Loading Tools and Initializing Agent\u003c/h3\u003e\n\u003cp\u003eLoad the required tools for your LangChain script, initialize the agent, and set the agent type as \u003cstrong\u003e\u003ccode\u003eZERO_SHOT_REACT_DESCRIPTION\u003c/code\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callbacks=callbacks)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    callbacks=callbacks,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4. Running LangChain Script\u003c/h3\u003e\n\u003cp\u003eExecute your LangChain script within the agent using the \u003cstrong\u003e\u003ccode\u003erun\u003c/code\u003e\u003c/strong\u003e method.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eagent.run(\n    \"What are the number of parameters in GPT5 and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Flushing Callbacks\u003c/h3\u003e\n\u003cp\u003eEnsure that callbacks are flushed after running the script to capture the trace.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback.flush()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the \u003cstrong\u003e\u003ccode\u003e.aim\u003c/code\u003e\u003c/strong\u003e repository is initialized and simply execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eüòä Enjoy this preview of what you'll see once you visit the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/navigating-depths-debugger-1-1-.png\" alt=\"AimOS Apps\" title=\"AimOS Apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting the results\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Traces page to witness the detailed trace of your LangChain script. This page provides a comprehensive overview of every action and interaction within your LangChain environment.\u003c/p\u003e\n\u003cp\u003eHere you'll find a new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show the details about the script‚Äôs execution, token use, and the costs involved.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-23-at-11.57.51%E2%80%AFpm-1-.png\" alt=\"LangChain Debugger\" title=\"LangChain Debugger\"\u003e\u003c/p\u003e\n\u003ch2\u003eOverview: Key Elements\u003c/h2\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTrace: A unique identifier for each trace, allowing you to easily reference and analyze specific interactions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTotal Steps: The number of actions or queries performed in a trace, providing a quick measure of complexity\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTotal Tokens: The total number of tokens is represented to understand the computational load of your queries, ensuring optimal performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCost: This represents the total cost of the trace which is a critical factor in resource management, the cost breakdown helps you make informed decisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-23-at-11.58.15%E2%80%AFpm-1-.png\" alt=\"Overview tab\" title=\"Overview tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eSteps: Exploring the Process\u003c/h2\u003e\n\u003cp\u003eThe Steps tab provides a detailed walkthrough of the sequence of actions undertaken throughout the pipeline to achieve the specified goal. In our example, the Agent executed a total of 18 steps to successfully accomplish the task.\u003c/p\u003e\n\u003cp\u003eHere's a breakdown of the pivotal actions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLLM Response: The Language Model (LLM) initially determines that, for the given question, it needs to utilize the Search tool to identify the person in question.\u003c/li\u003e\n\u003cli\u003eSearch Tool Execution: The LLM activates the Search tool to acquire the necessary information, successfully obtaining the desired output.\u003c/li\u003e\n\u003cli\u003eLLM Decision: Subsequently, the LLM decides to employ the Search tool once again to retrieve additional details, such as the the number of parameters in GPT5 (based on rumors) and GPT4.\u003c/li\u003e\n\u003cli\u003eSearch Tool Execution: The Agent, following the LLM's decision, triggers the Search tool once more, fetching the required information.\u003c/li\u003e\n\u003cli\u003eCalculator Tool Usage: With the obtained number, the Agent employs the Calculator tool to calculate the logarithm (base e) of the difference between the number of parameters.\u003c/li\u003e\n\u003cli\u003eFinal Answer: Lastly, armed with all the necessary information, the LLM crafts the complete answer to the original question and returns it.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-23-at-11.58.42%E2%80%AFpm-1-.png\" alt=\"Steps tab\" title=\"Steps tab\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-24-at-12.00.36%E2%80%AFam-1-.png\" alt=\"Steps tab\" title=\"Steps tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eCost: Evaluating Resource Usage\u003c/h2\u003e\n\u003cp\u003eIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LangChain activities.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOn the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\u003c/li\u003e\n\u003cli\u003eOn the y-axis, you can see the number of tokens sent or received.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-24-at-12.00.17%E2%80%AFam-1-.png\" alt=\"Cost tab\" title=\"Cost tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eThe LangChain Debugger in AimOS is a powerful tool for deepening your understanding and gaining in-depth insights into your LangChain script. It logs LLMs prompts and generations, tools inputs/outputs, and chains metadata. It's great for both experienced users and beginners, offering a comprehensive observability.\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools.¬†\u003c/a\u003eüôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e,¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aimos/\"\u003eGitHub\u003c/a\u003e¬†if you think AimOS is useful, and here is the repository of¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e,¬†an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger.md","_raw":{"sourceFilePath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger.md","sourceFileName":"ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger"},"type":"Post"},{"title":"AI Observability with AimOS: A Deep Dive into the LlamaIndex Observer App","date":"2023-11-16T18:12:14.363Z","author":"Hovhannes Tamoyan","description":"Discover AI observability with AimOS's LlamaIndex Observer. Essential insights for efficient AI model tracking and analysis. Check out our guide for observability solutions in the world of AI.","slug":"ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app","image":"/images/dynamic/llamaindex.png","draft":false,"categories":["Integrations"],"body":{"raw":"Welcome to the world of seamless tracking and visualization! LlamaIndex Observer, a tool that unites the robust capabilities of LlamaIndex with the blazingly fast [AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems). This integration offers a full-featured platform for effectively monitoring and understanding every detail in your LlamaIndex environment.\n\nIn this guide, we're going to highlight the main aspects of the LlamaIndex Observer application, with a special focus on its Trace page, a key component within the LlamaIndex Observer package in Aim.\n\n# LlamaIndex + AimOS = ‚ù§\n\nImagine being able to see everything that happens in your LlamaIndex system from a high-level perspective. **The LlamaIndex Observer** makes this possible by using AimOS to provide a detailed record of each interaction.\n\n\u003e AimOS üîç ‚Äî A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\n\nWith [AimOS](https://github.com/aimhubio/aimos), you can set up various types of AI monitoring applications, like experiment tracking, production monitoring, and usage analysis.\n\nNow, let‚Äôs see how it works with an example!\n\n## Putting LlamaIndex Observer into Action: A Step-by-Step Guide\n\nWe'll now look at a hands-on example. We'll use a Python script to interact with LlamaIndex and observe how LlamaIndex Tracker records each action. This example demonstrates the smooth interaction between AimOS and LlamaIndex.\n\nBefore starting with the script, ensure AimOS is installed and set up. If you need to install it, visit [AimOS repository](https://github.com/aimhubio/aimos) for the guidelines.\n\n## Sample Script\n\n### 1. Importing Required Modules\n\nFirst we‚Äôll start by importing the necessary Python modules for file manipulation, HTTP requests, and working with LlamaIndex and AimOS.\n\n```\nimport os\nfrom pathlib import Path\nimport requests\nfrom aimstack.llamaindex_observer.callback_handlers import GenericCallbackHandler\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.callbacks.base import CallbackManager\n```\n\n### 2. Downloading Paul Graham's Essay\n\nIn this step, the script downloads Paul Graham's essay from a given URL and saves it locally in a specified directory. \n\nIt‚Äôs important to ensure that the essay is successfully downloaded before moving on to the next steps. Next, we will be asking questions referring this essay.\n\n```\npaul_graham_essay_url = \"\u003chttps://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\u003e\"\nresponse = requests.get(paul_graham_essay_url)\noutput_dir = Path(\"aimos/examples/paul_graham_essay/data/\")\n\nif response.status_code == 200:\n    os.system(f\"mkdir -p {output_dir}\")\n    with open(output_dir.joinpath(\"Paul_Graham_Essay.txt\"), \"wb\") as file:\n        file.write(response.content)\nelse:\n    print(\"Failed to download the file.\")\n```\n\n### 3. Loading Document into LlamaIndex\n\nThe script reads the downloaded document using LlamaIndex's **`SimpleDirectoryReader`** and loads it into the system as a document.\n\n```\ndocuments = SimpleDirectoryReader(output_dir).load_data()\n```\n\n### 4. Setting Up AimOS Callback for LlamaIndex Observer\n\n```\naim_callback = GenericCallbackHandler(repo=\"aim://0.0.0.0:53800\")\ncallback_manager = CallbackManager([aim_callback])\n```\n\nEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server with this simple command command:\n\n```\naimos server\n```\n\n### 5. Creating LlamaIndex Service Context\n\nHere, we configure the LlamaIndex service context, create a vector store index using the loaded documents, and obtain a query engine for interacting with the index.\n\n```\nservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\nquery_engine = index.as_query_engine()\n```\n\n### 6. Performing Queries and Flushing Callbacks\n\nThis section triggers queries on the LlamaIndex using the query engine and ensures that the callbacks are flushed, capturing the corresponding traces.\n\n```\naim_callback.flush()\nquery_engine.query(\"How does Graham address the topic of competition and the importance (or lack thereof) of being the first mover in a market?\")\naim_callback.flush()\nquery_engine.query(\"What are Paul Graham's notable projects or companies?\")\naim_callback.flush()\nquery_engine.query(\"What problems did the author encounter with the early AI programs?\")\naim_callback.flush()\n```\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the .aim repository is initialized and simply run.\n\n```\naimos ui\n```\n\nüòä Enjoy this preview of what you'll see once you visit the provided URL:\n\n![AimOS Overview](/images/dynamic/1-.png \"AimOS Overview\")\n\nNavigate to the **`llamaindex_observer`** package under AI Systems Debugging.\n\nNow, we will delve into the tracked information.\n\n## Interpreting the Results\n\nOnce you‚Äôve run the script, go to the Traces page in AimOS. There, you'll find a  new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show you details about the script‚Äôs execution, token use, and the costs involved.\n\n![AimOS Traces](/images/dynamic/llamaindex-insights-2-.png \"AimOS Traces\")\n\n### Overview Tab: A Quick Summary\n\nThe Overview tab in AimOS gives you a snapshot of what happened. Here are the main things you can see:\n\n**Trace ID:** A unique identifier for each trace, allowing you to easily reference and analyze specific interactions.\n\n**Date and Time:** Timestamps help you understand when actions took place, facilitating a chronological understanding of your LlamaIndex usage.\n\n**Total Steps:** The number of actions or queries performed in a trace, providing a quick measure of complexity.\n\n**Total Tokens:** Understand the computational load of your queries, ensuring optimal performance.\n\n**Cost:** A critical factor in resource management, the cost breakdown helps you make informed decisions.\n\n![Trace ID](/images/dynamic/llamaindex-insights-3.png \"Trace ID\")\n\n## Understanding the Process\n\nAt the top of the Steps tab, you'll find a slider labeled \"Select the step,\" enabling seamless navigation through the LlamaIndex processes. These steps are logged using the 'flush()' command.\n\n### Step 1: Chunking and Embedding\n\nIn the initial step, the provided essay document undergoes a chunking process, dividing it into multiple segments. Simultaneously, embeddings are extracted for each chunk, forming a foundational representation of the document's content.\n\n![LlamaIndex Observer insights](/images/dynamic/llamaindex-insights-4.png \"LlamaIndex Observer insights\")\n\n### Step 2: Query Processing\n\n**Question Formulation:** A specific question is asked about the document, such as \"How does Graham address the topic of competition and the importance of being the first mover in a market?\"\n\n![LlamaIndex Observer question formulation](/images/dynamic/llamaindex-insights-5.png \"LlamaIndex Observer question formulation\")\n\n**Query Initiation:** LlamaIndex initiates a query, retrieving the most relevant chunks from the previously embedded document.\n\n![LlamaIndex Observer query initiation](/images/dynamic/6.png \"LlamaIndex Observer query initiation\")\n\n**Prompt Generation:** A prompt for the language model is crafted. This involves combining the context strings (document chunks) and the question to create an input for the language model.\n\n![LlamaIndex Observer prompt generation](/images/dynamic/7.png \"LlamaIndex Observer prompt generation\")\n\n**Language Model Response:** The language model processes the prompt, generating a response that addresses the asked question.\n\n![LlamaIndex Observer - Language model response](/images/dynamic/8.png \"LlamaIndex Observer - Language model response\")\n\n### Step 3: Iterative Questioning\n\nWhile Step 1 is a one-time occurrence, our example involves asking three distinct questions. The process for generating responses to the second and third questions aligns with the methodology outlined in Step 2, utilizing the previously embedded chunks of the document.\n\nThe additional questions asked are:\n\n* \"What are Paul Graham's notable projects or companies?\" \n\n![Prompt](/images/dynamic/9.png \"Prompt\")\n\nGenerated response:\n\n![Generated response](/images/dynamic/10.png \"Generated response\")\n\n* \"What problems did the author encounter with the early AI programs?\"\n\n![Prompt](/images/dynamic/11.png \"Prompt\")\n\nGenerated response:\n\n![Generated response](/images/dynamic/12.png)\n\n## Cost: Evaluating Resource Usage\n\nIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LlamaIndex activities.\n\n* On the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\n* On the y-axis, you can see the number of tokens sent or received.\n\n![Token-usage](/images/dynamic/13.png \"Cost tab\")\n\n## Wrapping up\n\nThe ‚ÄúLlamaIndex Observer‚Äù in AimOS is a powerful tool for deepening your understanding of how your LlamaIndex models operate. It's great for both experienced users and beginners, offering a comprehensive observability of model performance.\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the [AimStack blog](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems). \n\n## Learn more\n\n[AimOS is on a mission to democratize AI Systems logging tools. ](https://aimos.readthedocs.io/en/latest/apps/overview.html)üôå\n\nTry out¬†**[AimOS](https://github.com/aimhubio/aimos)**, join the¬†**[Aim community](https://community.aimstack.io/),** and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aimos/tree/main)¬†if you think AimOS is useful, and here is the repository of¬†[Aim](https://github.com/aimhubio/aim),¬†an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003cp\u003eWelcome to the world of seamless tracking and visualization! LlamaIndex Observer, a tool that unites the robust capabilities of LlamaIndex with the blazingly fast \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e. This integration offers a full-featured platform for effectively monitoring and understanding every detail in your LlamaIndex environment.\u003c/p\u003e\n\u003cp\u003eIn this guide, we're going to highlight the main aspects of the LlamaIndex Observer application, with a special focus on its Trace page, a key component within the LlamaIndex Observer package in Aim.\u003c/p\u003e\n\u003ch1\u003eLlamaIndex + AimOS = ‚ù§\u003c/h1\u003e\n\u003cp\u003eImagine being able to see everything that happens in your LlamaIndex system from a high-level perspective. \u003cstrong\u003eThe LlamaIndex Observer\u003c/strong\u003e makes this possible by using AimOS to provide a detailed record of each interaction.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAimOS üîç ‚Äî A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWith \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, you can set up various types of AI monitoring applications, like experiment tracking, production monitoring, and usage analysis.\u003c/p\u003e\n\u003cp\u003eNow, let‚Äôs see how it works with an example!\u003c/p\u003e\n\u003ch2\u003ePutting LlamaIndex Observer into Action: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eWe'll now look at a hands-on example. We'll use a Python script to interact with LlamaIndex and observe how LlamaIndex Tracker records each action. This example demonstrates the smooth interaction between AimOS and LlamaIndex.\u003c/p\u003e\n\u003cp\u003eBefore starting with the script, ensure AimOS is installed and set up. If you need to install it, visit \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS repository\u003c/a\u003e for the guidelines.\u003c/p\u003e\n\u003ch2\u003eSample Script\u003c/h2\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eFirst we‚Äôll start by importing the necessary Python modules for file manipulation, HTTP requests, and working with LlamaIndex and AimOS.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\nfrom pathlib import Path\nimport requests\nfrom aimstack.llamaindex_observer.callback_handlers import GenericCallbackHandler\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.callbacks.base import CallbackManager\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Downloading Paul Graham's Essay\u003c/h3\u003e\n\u003cp\u003eIn this step, the script downloads Paul Graham's essay from a given URL and saves it locally in a specified directory.\u003c/p\u003e\n\u003cp\u003eIt‚Äôs important to ensure that the essay is successfully downloaded before moving on to the next steps. Next, we will be asking questions referring this essay.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epaul_graham_essay_url = \"\u0026#x3C;https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\u003e\"\nresponse = requests.get(paul_graham_essay_url)\noutput_dir = Path(\"aimos/examples/paul_graham_essay/data/\")\n\nif response.status_code == 200:\n    os.system(f\"mkdir -p {output_dir}\")\n    with open(output_dir.joinpath(\"Paul_Graham_Essay.txt\"), \"wb\") as file:\n        file.write(response.content)\nelse:\n    print(\"Failed to download the file.\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Loading Document into LlamaIndex\u003c/h3\u003e\n\u003cp\u003eThe script reads the downloaded document using LlamaIndex's \u003cstrong\u003e\u003ccode\u003eSimpleDirectoryReader\u003c/code\u003e\u003c/strong\u003e and loads it into the system as a document.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocuments = SimpleDirectoryReader(output_dir).load_data()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4. Setting Up AimOS Callback for LlamaIndex Observer\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = GenericCallbackHandler(repo=\"aim://0.0.0.0:53800\")\ncallback_manager = CallbackManager([aim_callback])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server with this simple command command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Creating LlamaIndex Service Context\u003c/h3\u003e\n\u003cp\u003eHere, we configure the LlamaIndex service context, create a vector store index using the loaded documents, and obtain a query engine for interacting with the index.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\nquery_engine = index.as_query_engine()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e6. Performing Queries and Flushing Callbacks\u003c/h3\u003e\n\u003cp\u003eThis section triggers queries on the LlamaIndex using the query engine and ensures that the callbacks are flushed, capturing the corresponding traces.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback.flush()\nquery_engine.query(\"How does Graham address the topic of competition and the importance (or lack thereof) of being the first mover in a market?\")\naim_callback.flush()\nquery_engine.query(\"What are Paul Graham's notable projects or companies?\")\naim_callback.flush()\nquery_engine.query(\"What problems did the author encounter with the early AI programs?\")\naim_callback.flush()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the .aim repository is initialized and simply run.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eüòä Enjoy this preview of what you'll see once you visit the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1-.png\" alt=\"AimOS Overview\" title=\"AimOS Overview\"\u003e\u003c/p\u003e\n\u003cp\u003eNavigate to the \u003cstrong\u003e\u003ccode\u003ellamaindex_observer\u003c/code\u003e\u003c/strong\u003e package under AI Systems Debugging.\u003c/p\u003e\n\u003cp\u003eNow, we will delve into the tracked information.\u003c/p\u003e\n\u003ch2\u003eInterpreting the Results\u003c/h2\u003e\n\u003cp\u003eOnce you‚Äôve run the script, go to the Traces page in AimOS. There, you'll find a  new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show you details about the script‚Äôs execution, token use, and the costs involved.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-2-.png\" alt=\"AimOS Traces\" title=\"AimOS Traces\"\u003e\u003c/p\u003e\n\u003ch3\u003eOverview Tab: A Quick Summary\u003c/h3\u003e\n\u003cp\u003eThe Overview tab in AimOS gives you a snapshot of what happened. Here are the main things you can see:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrace ID:\u003c/strong\u003e A unique identifier for each trace, allowing you to easily reference and analyze specific interactions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDate and Time:\u003c/strong\u003e Timestamps help you understand when actions took place, facilitating a chronological understanding of your LlamaIndex usage.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTotal Steps:\u003c/strong\u003e The number of actions or queries performed in a trace, providing a quick measure of complexity.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTotal Tokens:\u003c/strong\u003e Understand the computational load of your queries, ensuring optimal performance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCost:\u003c/strong\u003e A critical factor in resource management, the cost breakdown helps you make informed decisions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-3.png\" alt=\"Trace ID\" title=\"Trace ID\"\u003e\u003c/p\u003e\n\u003ch2\u003eUnderstanding the Process\u003c/h2\u003e\n\u003cp\u003eAt the top of the Steps tab, you'll find a slider labeled \"Select the step,\" enabling seamless navigation through the LlamaIndex processes. These steps are logged using the 'flush()' command.\u003c/p\u003e\n\u003ch3\u003eStep 1: Chunking and Embedding\u003c/h3\u003e\n\u003cp\u003eIn the initial step, the provided essay document undergoes a chunking process, dividing it into multiple segments. Simultaneously, embeddings are extracted for each chunk, forming a foundational representation of the document's content.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-4.png\" alt=\"LlamaIndex Observer insights\" title=\"LlamaIndex Observer insights\"\u003e\u003c/p\u003e\n\u003ch3\u003eStep 2: Query Processing\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eQuestion Formulation:\u003c/strong\u003e A specific question is asked about the document, such as \"How does Graham address the topic of competition and the importance of being the first mover in a market?\"\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-5.png\" alt=\"LlamaIndex Observer question formulation\" title=\"LlamaIndex Observer question formulation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eQuery Initiation:\u003c/strong\u003e LlamaIndex initiates a query, retrieving the most relevant chunks from the previously embedded document.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/6.png\" alt=\"LlamaIndex Observer query initiation\" title=\"LlamaIndex Observer query initiation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt Generation:\u003c/strong\u003e A prompt for the language model is crafted. This involves combining the context strings (document chunks) and the question to create an input for the language model.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/7.png\" alt=\"LlamaIndex Observer prompt generation\" title=\"LlamaIndex Observer prompt generation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Model Response:\u003c/strong\u003e The language model processes the prompt, generating a response that addresses the asked question.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/8.png\" alt=\"LlamaIndex Observer - Language model response\" title=\"LlamaIndex Observer - Language model response\"\u003e\u003c/p\u003e\n\u003ch3\u003eStep 3: Iterative Questioning\u003c/h3\u003e\n\u003cp\u003eWhile Step 1 is a one-time occurrence, our example involves asking three distinct questions. The process for generating responses to the second and third questions aligns with the methodology outlined in Step 2, utilizing the previously embedded chunks of the document.\u003c/p\u003e\n\u003cp\u003eThe additional questions asked are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"What are Paul Graham's notable projects or companies?\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/9.png\" alt=\"Prompt\" title=\"Prompt\"\u003e\u003c/p\u003e\n\u003cp\u003eGenerated response:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/10.png\" alt=\"Generated response\" title=\"Generated response\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"What problems did the author encounter with the early AI programs?\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/11.png\" alt=\"Prompt\" title=\"Prompt\"\u003e\u003c/p\u003e\n\u003cp\u003eGenerated response:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/12.png\" alt=\"Generated response\"\u003e\u003c/p\u003e\n\u003ch2\u003eCost: Evaluating Resource Usage\u003c/h2\u003e\n\u003cp\u003eIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LlamaIndex activities.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOn the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\u003c/li\u003e\n\u003cli\u003eOn the y-axis, you can see the number of tokens sent or received.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/13.png\" alt=\"Token-usage\" title=\"Cost tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eThe ‚ÄúLlamaIndex Observer‚Äù in AimOS is a powerful tool for deepening your understanding of how your LlamaIndex models operate. It's great for both experienced users and beginners, offering a comprehensive observability of model performance.\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimStack blog\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools. \u003c/a\u003eüôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003cstrong\u003e\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e\u003c/strong\u003e, join the¬†\u003cstrong\u003e\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e,\u003c/strong\u003e and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aimos/tree/main\"\u003eGitHub\u003c/a\u003e¬†if you think AimOS is useful, and here is the repository of¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e,¬†an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app.md","_raw":{"sourceFilePath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app.md","sourceFileName":"ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app"},"type":"Post"},{"title":"Aim 1.3.5 ‚Äî Activity View and X-axis alignment","date":"2021-02-05T11:16:41.748Z","author":"Gev Soghomonian","description":"Aim v1.3.5 is now available. Activity View on the Dashboard, X-axis alignment by epoch, Ordering runs both on Explore and on Dashboard. Check out!","slug":"aim-1-3-5-activity-view-and-x-axis-alignment","image":"/images/dynamic/1st.gif","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n## Aim Release ‚Äî Activity View and X-axis alignment\n\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is now available. Thanks to the incredible [Aim community](https://discord.com/invite/zXq2NfVdtF) for the feedback and support on building democratized open source AI dev tools.\n\nCheck out the new features at play.aimstack.io\n\nHere are the highlights of the features:\n\n## Activity View on the Dashboard\n\nThe **Activity View** shows the daily experiment runs. With one click you can search each day‚Äôs runs and explore them straight away\n\n**Statistics** displays the overall count of [Experiments and Runs. ](https://github.com/aimhubio/aim#concepts)\n\n![](/images/dynamic/1st.gif \"The new dashboard in action!\")\n\n## **X-axis alignment by epoch**\n\nX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss ‚Äî 1000 steps, validation loss ‚Äî 15 steps) then you can align them by epoch, relative time or absolute time.\n\n![](/images/dynamic/2nd.gif \"No more misaligned metrics!\")\n\n## **Ordering runs both on Explore and on Dashboard**\n\n\n\nNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\n\nOn Explore, when sorted by a hyperparam that‚Äôs also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\n\n![](/images/dynamic/3.gif \"Sort the runs both on Dashboard and Explore!\")\n\n## Learn More\n\n\n\nWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\n\nWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month¬†üòä). Try out Aim, join the¬†[Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support¬†üôå.","html":"\u003ch2\u003eAim Release ‚Äî Activity View and X-axis alignment\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e v1.3.5 is now available. Thanks to the incredible \u003ca href=\"https://discord.com/invite/zXq2NfVdtF\"\u003eAim community\u003c/a\u003e for the feedback and support on building democratized open source AI dev tools.\u003c/p\u003e\n\u003cp\u003eCheck out the new features at play.aimstack.io\u003c/p\u003e\n\u003cp\u003eHere are the highlights of the features:\u003c/p\u003e\n\u003ch2\u003eActivity View on the Dashboard\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eActivity View\u003c/strong\u003e shows the daily experiment runs. With one click you can search each day‚Äôs runs and explore them straight away\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStatistics\u003c/strong\u003e displays the overall count of \u003ca href=\"https://github.com/aimhubio/aim#concepts\"\u003eExperiments and Runs. \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1st.gif\" alt=\"\" title=\"The new dashboard in action!\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eX-axis alignment by epoch\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss ‚Äî 1000 steps, validation loss ‚Äî 15 steps) then you can align them by epoch, relative time or absolute time.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/2nd.gif\" alt=\"\" title=\"No more misaligned metrics!\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eOrdering runs both on Explore and on Dashboard\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\u003c/p\u003e\n\u003cp\u003eOn Explore, when sorted by a hyperparam that‚Äôs also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/3.gif\" alt=\"\" title=\"Sort the runs both on Dashboard and Explore!\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\u003c/p\u003e\n\u003cp\u003eWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month¬†üòä). Try out Aim, join the¬†\u003ca href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support¬†üôå.\u003c/p\u003e"},"_id":"posts/aim-1-3-5-‚Äî-activity-view-and-x-axis-alignment.md","_raw":{"sourceFilePath":"posts/aim-1-3-5-‚Äî-activity-view-and-x-axis-alignment.md","sourceFileName":"aim-1-3-5-‚Äî-activity-view-and-x-axis-alignment.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-1-3-5-‚Äî-activity-view-and-x-axis-alignment"},"type":"Post"},{"title":"Aim 1.3.8 ‚Äî Enhanced Context Table and Advanced Group Coloring","date":"2021-03-01T11:09:06.858Z","author":"Gev Soghomonian","description":"Discover Aim 1.3.8's Exciting Features: Advanced Group Coloring \u0026 Enhanced Context Table. Streamlined AI metrics visualization for efficient analysis. Explore now!","slug":"aim-1-3-8-enhanced-context-table-and-advanced-group-coloring","image":"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n[Aim](https://aimstack.io/)¬†1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\n\nCheck out the new features at¬†[play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=).\n\nHere are the more notable changes:\n\n### Enhanced Context Table\n\n\n\nThe context table used to not use the screen real-estate effectively ‚Äî an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\n\nSo we have made 3 changes to tackle this problem\n\n### New table groups view\n\nBelow is the new modified look of the table. Here is what‚Äôs changed:\n\n* The empty per group is removed\n* In addition to the group details popover, there is an in-place list group config is available for instant lookup\n\n![](https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp \"New improved Aim Context Table on Explore\")\n\n\n\n* **\\[In Progress for next release]**¬†Column resize ‚Äî this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\n\n### Column Management\n\n\n\nThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\n\nYou can also re-order the columns by dragging the mid-section up/down.\n\nThis is super-handy when dealing with 100s of columns on the table for parameters.\n\n![](https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp \"Drag columns left and right to pin, search and disable less important ones\")\n\n\n\n### Advanced Group Coloring\n\n\n\nThis is the latest iteration over the group coloring.\n\n* Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\n* You can shuffle the colors in case they aren‚Äôt pick to your liking\n* You can pick two different color palette for better distribution of the colors.\n\n  ![](https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp \"Effective coloring of the groups is important!\")\n\n  ### Thanks to\n* [mahnerak](https://github.com/mahnerak), Lars,¬†[Joe](https://github.com/jafioti)¬†for detailed feedback ‚Äî helped us immensely in this iteration.\n\n  ### Learn More\n\n  We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\n\n  Try out¬†[Aim](https://aimstack.io/), join the¬†[Aim community](https://slack.aimstack.io/), share your feedback.\n\n  We are building the next generation of democratized AI dev tools.\n\n  And don‚Äôt forget to leave¬†[Aim](https://aimstack.io/)¬†a star on GitHub for support üôå.","html":"\u003cp\u003e\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e¬†1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\u003c/p\u003e\n\u003cp\u003eCheck out the new features at¬†\u003ca href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere are the more notable changes:\u003c/p\u003e\n\u003ch3\u003eEnhanced Context Table\u003c/h3\u003e\n\u003cp\u003eThe context table used to not use the screen real-estate effectively ‚Äî an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\u003c/p\u003e\n\u003cp\u003eSo we have made 3 changes to tackle this problem\u003c/p\u003e\n\u003ch3\u003eNew table groups view\u003c/h3\u003e\n\u003cp\u003eBelow is the new modified look of the table. Here is what‚Äôs changed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe empty per group is removed\u003c/li\u003e\n\u003cli\u003eIn addition to the group details popover, there is an in-place list group config is available for instant lookup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp\" alt=\"\" title=\"New improved Aim Context Table on Explore\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e[In Progress for next release]\u003c/strong\u003e¬†Column resize ‚Äî this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eColumn Management\u003c/h3\u003e\n\u003cp\u003eThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\u003c/p\u003e\n\u003cp\u003eYou can also re-order the columns by dragging the mid-section up/down.\u003c/p\u003e\n\u003cp\u003eThis is super-handy when dealing with 100s of columns on the table for parameters.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp\" alt=\"\" title=\"Drag columns left and right to pin, search and disable less important ones\"\u003e\u003c/p\u003e\n\u003ch3\u003eAdvanced Group Coloring\u003c/h3\u003e\n\u003cp\u003eThis is the latest iteration over the group coloring.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNow Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can shuffle the colors in case they aren‚Äôt pick to your liking\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can pick two different color palette for better distribution of the colors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp\" alt=\"\" title=\"Effective coloring of the groups is important!\"\u003e\u003c/p\u003e\n\u003ch3\u003eThanks to\u003c/h3\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mahnerak\"\u003emahnerak\u003c/a\u003e, Lars,¬†\u003ca href=\"https://github.com/jafioti\"\u003eJoe\u003c/a\u003e¬†for detailed feedback ‚Äî helped us immensely in this iteration.\u003c/p\u003e\n\u003ch3\u003eLearn More\u003c/h3\u003e\n\u003cp\u003eWe have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eWe are building the next generation of democratized AI dev tools.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support üôå.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/aim-1-3-8-‚Äî-enhanced-context-table-and-advanced-group-coloring.md","_raw":{"sourceFilePath":"posts/aim-1-3-8-‚Äî-enhanced-context-table-and-advanced-group-coloring.md","sourceFileName":"aim-1-3-8-‚Äî-enhanced-context-table-and-advanced-group-coloring.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-1-3-8-‚Äî-enhanced-context-table-and-advanced-group-coloring"},"type":"Post"},{"title":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","date":"2021-05-18T13:58:29.035Z","author":"Gev Soghomonian","description":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","slug":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)¬†2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\n\nCheck out the updated Aim at¬†[play.aimstack.io](http://play.aimstack.io:10001/).\n\nFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes¬†[here](https://github.com/aimhubio/aim/milestone/6?closed=1).\n\n## XGBoost Integration\n\nTo begin with, ¬†`aim.callback`¬†is now available that exports¬†`AimCallback`¬†to be passed to the¬†`xgb.train`¬†as a callback to log the experiments.\n\nCheck out this¬†[blogpost](https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library)¬†for additional details on how to integrate Aim to your XGBoost code.\n\n## Confidence Interval as the aggregation method\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyjcE9k4-cZGl5g03F3Mjg.png)\n\nNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.¬†\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nCheck out the latest¬†[Aim integration](https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration)!\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim at¬†\u003ca href=\"http://play.aimstack.io:10001/\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes¬†\u003ca href=\"https://github.com/aimhubio/aim/milestone/6?closed=1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eXGBoost Integration\u003c/h2\u003e\n\u003cp\u003eTo begin with, ¬†\u003ccode\u003eaim.callback\u003c/code\u003e¬†is now available that exports¬†\u003ccode\u003eAimCallback\u003c/code\u003e¬†to be passed to the¬†\u003ccode\u003exgb.train\u003c/code\u003e¬†as a callback to log the experiments.\u003c/p\u003e\n\u003cp\u003eCheck out this¬†\u003ca href=\"https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library\"\u003eblogpost\u003c/a\u003e¬†for additional details on how to integrate Aim to your XGBoost code.\u003c/p\u003e\n\u003ch2\u003eConfidence Interval as the aggregation method\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyjcE9k4-cZGl5g03F3Mjg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups.¬†\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eCheck out the latest¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration\"\u003eAim integration\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","_raw":{"sourceFilePath":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileName":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements"},"type":"Post"},{"title":"Aim 2022 community report","date":"2023-01-05T21:48:26.367Z","author":"Gev Soghomonian","description":"It‚Äôs time to review the 2022. In a year, Aim has become from a cool open-source project into a full-blown product. We have shipped 12 new Aim versions..","slug":"aim-2022-community-report","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OvP3KCeUaBbqWj_jq4c5xg.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey lovely Aimers, Happy New Year! üéÜ\n\nNow that the 2022 is over, we are excited to share what has happened in 2022. We have achieved a lot together!\n\n# TLDR ‚Äî Highlights\n\nIn 2022 we have taken Aim to completely different level. It has become from a cool open-source project into a full-blown product. An inspiring community-led effort!!\n\n* We have shipped¬†[12 new Aim versions](https://github.com/aimhubio/aim/releaseshttps://github.com/aimhubio/aim/releases)¬†and countless other patches\n* Almost every week for the past 80+ weeks a new Aim version has been shipped ‚Äî¬†**a blistering pace of innovation!! üòÖ**\n* Aim contributors have doubled to 48 in 2022\n* ~1300 GitHub issues have been created in 2022 and over 1100 of them have been resolved.\n* We have reached 3K stars on GitHub. ‚≠êÔ∏è (If you like our work,¬†[drop us a star!](https://github.com/aimhubio/aim)¬†)\n* We¬†[launched](https://medium.com/aimstack/aim-community-is-moving-to-discord-da24a52169db)¬†our community on¬†[discord](https://community.aimstack.io/).\n\n# Product\n\nWe have shipped 100s of major features in 2022. It would take several chapters to talk about all of them.\n\nHowever, here are two fundamental features that we haven‚Äôt talked much about yet.\n\n## Base Explorer\n\n* A foundational work to help compare any type of renderable ML metadata at mass.\n* This is the first experimental step we have made to turn Aim into a one-stop-shop for multidimensional metadata analysis.\n\nThe Figures explorer runs on Base Explorer\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRbl_iVzy8FwRfj6eZWffA.png)\n\n## Aim Callbacks\n\nA surgical intervention into your ML runs. For everything!!\n\n* Aim Users are using the callbacks to create policies to automatically manage trainings. A major step towards reproducibility and true training management. Hours of GPU-time saved at the worst case scenario!!\n\nWe are going to talk a lot about the callbacks too! üôå\n\nMore about callbacks¬†[here](https://aimstack.readthedocs.io/en/latest/using/callbacks.html#)\n\n## Aim Roadmap\n\nHere is how the public Aim roadmap looks like now. This is just the subset of the additions made. A monumental work!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rYmolGfB_bEjGHFb4TB5Q.png)\n\n# Integrations\n\nAim is heavily integrated with the AI ecosystem now.\n\n* in 2022 Aim ecosystem integrations have grown 3x\n* ~10 more integrations are also in progress right now ‚ù§Ô∏è\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EMYy1Eh6qYqrLZo2i6UPw.png)\n\n\n\n* Lots of impactful repos have adopted Aim as well. Including these ones:¬†[HF Accelerate](https://github.com/huggingface/accelerate),¬†[FAIR Metaseq](https://github.com/facebookresearch/metaseqhttps://github.com/facebookresearch/metaseq)¬†(default tracker),¬†[FAIR Fairseq](https://github.com/facebookresearch/fairseq),¬†[Ludwig](https://github.com/ludwig-ai/ludwig)¬†etc\n* Community-requested Aim converters for Mlflow, TensorBoard and¬†[Wandb](https://medium.com/aimstack/aim-tutorial-for-weights-and-biases-users-bfbdde76f21e?source=collection_home---4------0-----------------------).\n\n# Onwards to 2023\n\nIn 2023 we are going to work hard to put Aim into the hands of as many researchers and MLOps engineers as possible!\n\nSoon we will publish the Aim roadmap of 2023! Lots to share. :)\n\n**2023 here we come! üòä**","html":"\u003cp\u003eHey lovely Aimers, Happy New Year! üéÜ\u003c/p\u003e\n\u003cp\u003eNow that the 2022 is over, we are excited to share what has happened in 2022. We have achieved a lot together!\u003c/p\u003e\n\u003ch1\u003eTLDR ‚Äî Highlights\u003c/h1\u003e\n\u003cp\u003eIn 2022 we have taken Aim to completely different level. It has become from a cool open-source project into a full-blown product. An inspiring community-led effort!!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe have shipped¬†\u003ca href=\"https://github.com/aimhubio/aim/releaseshttps://github.com/aimhubio/aim/releases\"\u003e12 new Aim versions\u003c/a\u003e¬†and countless other patches\u003c/li\u003e\n\u003cli\u003eAlmost every week for the past 80+ weeks a new Aim version has been shipped ‚Äî¬†\u003cstrong\u003ea blistering pace of innovation!! üòÖ\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eAim contributors have doubled to 48 in 2022\u003c/li\u003e\n\u003cli\u003e~1300 GitHub issues have been created in 2022 and over 1100 of them have been resolved.\u003c/li\u003e\n\u003cli\u003eWe have reached 3K stars on GitHub. ‚≠êÔ∏è (If you like our work,¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003edrop us a star!\u003c/a\u003e¬†)\u003c/li\u003e\n\u003cli\u003eWe¬†\u003ca href=\"https://medium.com/aimstack/aim-community-is-moving-to-discord-da24a52169db\"\u003elaunched\u003c/a\u003e¬†our community on¬†\u003ca href=\"https://community.aimstack.io/\"\u003ediscord\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eProduct\u003c/h1\u003e\n\u003cp\u003eWe have shipped 100s of major features in 2022. It would take several chapters to talk about all of them.\u003c/p\u003e\n\u003cp\u003eHowever, here are two fundamental features that we haven‚Äôt talked much about yet.\u003c/p\u003e\n\u003ch2\u003eBase Explorer\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA foundational work to help compare any type of renderable ML metadata at mass.\u003c/li\u003e\n\u003cli\u003eThis is the first experimental step we have made to turn Aim into a one-stop-shop for multidimensional metadata analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe Figures explorer runs on Base Explorer\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRbl_iVzy8FwRfj6eZWffA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAim Callbacks\u003c/h2\u003e\n\u003cp\u003eA surgical intervention into your ML runs. For everything!!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim Users are using the callbacks to create policies to automatically manage trainings. A major step towards reproducibility and true training management. Hours of GPU-time saved at the worst case scenario!!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe are going to talk a lot about the callbacks too! üôå\u003c/p\u003e\n\u003cp\u003eMore about callbacks¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/callbacks.html#\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eAim Roadmap\u003c/h2\u003e\n\u003cp\u003eHere is how the public Aim roadmap looks like now. This is just the subset of the additions made. A monumental work!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rYmolGfB_bEjGHFb4TB5Q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eIntegrations\u003c/h1\u003e\n\u003cp\u003eAim is heavily integrated with the AI ecosystem now.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ein 2022 Aim ecosystem integrations have grown 3x\u003c/li\u003e\n\u003cli\u003e~10 more integrations are also in progress right now ‚ù§Ô∏è\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EMYy1Eh6qYqrLZo2i6UPw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLots of impactful repos have adopted Aim as well. Including these ones:¬†\u003ca href=\"https://github.com/huggingface/accelerate\"\u003eHF Accelerate\u003c/a\u003e,¬†\u003ca href=\"https://github.com/facebookresearch/metaseqhttps://github.com/facebookresearch/metaseq\"\u003eFAIR Metaseq\u003c/a\u003e¬†(default tracker),¬†\u003ca href=\"https://github.com/facebookresearch/fairseq\"\u003eFAIR Fairseq\u003c/a\u003e,¬†\u003ca href=\"https://github.com/ludwig-ai/ludwig\"\u003eLudwig\u003c/a\u003e¬†etc\u003c/li\u003e\n\u003cli\u003eCommunity-requested Aim converters for Mlflow, TensorBoard and¬†\u003ca href=\"https://medium.com/aimstack/aim-tutorial-for-weights-and-biases-users-bfbdde76f21e?source=collection_home---4------0-----------------------\"\u003eWandb\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eOnwards to 2023\u003c/h1\u003e\n\u003cp\u003eIn 2023 we are going to work hard to put Aim into the hands of as many researchers and MLOps engineers as possible!\u003c/p\u003e\n\u003cp\u003eSoon we will publish the Aim roadmap of 2023! Lots to share. :)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2023 here we come! üòä\u003c/strong\u003e\u003c/p\u003e"},"_id":"posts/aim-2022-community-report.md","_raw":{"sourceFilePath":"posts/aim-2022-community-report.md","sourceFileName":"aim-2022-community-report.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2022-community-report"},"type":"Post"},{"title":"Aim 2022; Product Recap","date":"2023-01-20T06:16:21.056Z","author":"Gor Arakelyan","description":"A retrospective look at the past year! Significant improvements enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.","slug":"aim-2022-product-recap","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mpgFSGReVaycf5644hM6xw.png","draft":false,"categories":["New Releases"],"body":{"raw":"# A retrospective look at the past year\n\n\n\nAim underwent significant improvements in the past year. Over the course of 50 releases, including 12 minor releases and 38 patch releases, Aim received contributions from 35 total contributors, including 18 new external contributors. These contributions included 769 merged pull requests and 487 submitted issues!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhiLjCXeyDNrMIuZU5_SXg.png \"Aim contributions pulse\")\n\nThere was a significant progress in various areas of the Aim, including adding more supported platforms, taking the UI to the whole new level, adding remote tracking capabilities, revamping the CLI, extending the QL and moving towards active monitoring for optimizing large-scale trainings.\n\nIn addition to adding new features and extending Aim, improvements in performance and usability were applied to enhance the overall user experience. Furthermore, new integrations with machine learning tools were implemented to make it easier to use Aim in a broader technology stack\n\nOverall, these improvements greatly enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.\n\n# Aim Community\n\n![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GitHotOjlY9WPsvQmWaqwQ.png \"Aim contributors\")\n\n\n\n\u003e We are on a mission to democratize AI dev tools!\n\nNumerous external contributions were pushed to ‚Äúmain‚Äù in 2022. Without community support, Aim would not be where it is today.\n\nCongratulations to¬†[Sharathmk99](https://github.com/Sharathmk99),¬†[YodaEmbedding](https://github.com/YodaEmbedding),¬†[hjoonjang](https://github.com/hjoonjang),¬†[jangop](https://github.com/jangop),¬†[djwessel](https://github.com/djwessel),¬†[GeeeekExplorer](https://github.com/GeeeekExplorer),¬†[dsblank](https://github.com/dsblank),¬†[timokau](https://github.com/timokau),¬†[kumarshreshtha](https://github.com/kumarshreshtha),¬†[kage08](https://github.com/kage08),¬†[karan2801](https://github.com/karan2801),¬†[hendriks73](https://github.com/hendriks73),¬†[yeghiakoronian](https://github.com/yeghiakoronian),¬†[uduse](https://github.com/uduse),¬†[arnauddhaene](https://github.com/arnauddhaene),¬†[lukoucky](https://github.com/lukoucky),¬†[Arvind644](https://github.com/Arvind644),¬†[shrinandj](https://github.com/shrinandj)¬†for their first contributions over the past year! üéâ\n\nThese contributions have covered a range of areas, including documentation, user guides, the SDK and the UI!\n\n# Highlights\n\n* Supported platforms were expanded to include Docker, Apple M1, Conda, Google Colab and Jupyter Notebooks.\n* The UI was given a makeover, with upgraded home and run single pages, as well as brand new experiment page. The new audio and figure explorers now allow for the exploration of more types of metadata.\n* Aim remote tracking server was rolled out to enable tracking from multi-host environments.\n* Active monitoring capabilities were enabled, such as notifications for stalled runs, and the ability to programmatically define training heuristics using callbacks and notifications.\n* Aim was integrated with 9 different machine learning frameworks alongside with 3 convertors that help to migrate from other experiment trackers.\n* Last but not least, the documentation was completely revamped, including almost 40 pages of guides towards helping in effectively using and understanding Aim.\n\n# **Supported Platforms**\n\n\n\nImprovements were made to ensure Aim works seamlessly with widely used environments and platforms in ML/MLOps field, including adding support for new platforms.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYG-SSawh8Q8jte6BjyNMQ.png)\n\n\n\n* Support for running Aim on Apple M1 devices.\n* Support for using Aim inside Google Colab and Jupyter notebooks.\n* Docker images for Aim UI and server.\n* Support for running Aim in Kubernetes clusters.\n* Aim became available in Conda.\n\n# User Interface\n\n\n\nThe UI is one of key interfaces to interact with the ML training runs tracked by Aim. It‚Äôs super-powerful for comparing large number of trainings. The UI was significantly improved in the past year with new pages and explorers to help to explore more metadata types.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odqgX33xLVzYBI6sfRYRWQ.png \"Home Page\")\n\n\n\n* Revamped home page to see project‚Äôs contributions and overall statistics.\n* Revamped run page to deep-dive into an individual run.\n* Brand new experiment page to view experiment‚Äôs details, attached runs, etc.\n* New explorers, such as the ‚ÄúFigures Explorer‚Äù and ‚ÄúAudio Explorer‚Äù, to allow a cross-run comparison of Plotly figures and audio objects respectively.\n* ‚ÄúMetrics Explorer‚Äù key enhancements, including displaying chart legends and the ability of exporting charts as images.\n\n\n\n# Command Line Interface\n\n\n\nAim CLI offers a simple interface to easily manage tracked trainings. Two key groups of commands were added:\n\n* **Runs management**¬†to enable the base operations for managing trainings ‚Äî list, copy, remove, move, upload, close.\n* **Storage management**¬†to help to manage Aim data storage, such as reindexing, pruning, managing data format versions (advanced usage).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZFkXFzmz5BxGYofUgWdTg.png)\n\n# Query Language\n\nAim enables a powerful query language to filter through all the stored metadata using python expressions. A few additions were done providing a more friendly and pythonic interface for working with date-time expressions, as well as querying runs based on its metrics results.\n\n![](https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6J9gouDEt9Mkhv9Cds4dNw.png)\n\n# Remote Tracking\n\nThe Aim remote tracking server, which allows running trainings in a multi-host environment and collect tracked data in a centralized location, used to be one of the most requested features.\n\nIt has been gradually rolled out from its experimental phase to a stable version that can scale up and handle an increasing number of parallel trainings.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CizfTBF8Nh4DZcun6TP8Pg.png)\n\nSwitching from a local environment to a centralized server is as easy as pie, as zero code changes are required.‚ö°\\\nThis is because the interfaces are completely compatible!\n\n# Active Monitoring\n\nAim made it first steps towards optimizing conducting long and large scale trainings, such as training or tuning GPT-like models. It reduces human-hours spent monitoring/restarting runs, improves reproducibility and ramps up time for people who need to train these models:\n\n* Ability to notify on stalled/stuck runs.\n* Callbacks and notifications to define training heuristics programmatically.\n\nA quote from ‚ÄúScaling Laws for Generative Mixed-Modal Language Models‚Äù (Aghajanyan et al., 2023,¬†[arXiv:2301.03728](https://arxiv.org/abs/2301.03728)):\n\n\u003e We tracked all experiments using the Aim experiment tracker (Arakelyan et al., 2020). To ensure consistent training strategies across our experiments, we implemented a model restart policy using the Aim experiment tracker and callbacks. Specifically, if training perplexities do not decrease after 500 million tokens, the training run is restarted with a reduced learning rate with a factor of 0.8 of the current time step. This policy helps remove variance in the scaling laws due to differences in training procedures and allows us to scale up the number of asynchronous experiments significantly. All experiments were conducted in a two-month time frame with a cluster of 768 80GB A100 GPUs. The majority of experiments used 64 GPUs at a time.\n\n# Key Performance and Usability Optimizations\n\n* Storage: long metrics sampling and retrieval (\u003e1M steps).\n* Storage: robust locking mechanism and automatic background indexing.\n* UI: The v3.12 milestone was released, addressing over 30 usability issues.\n* UI: virtualizations in various places across the UI to improve responsiveness when displaying large amount of data.\n* UI: optimizations in stream decoding and data encoding to enhance overall performance.\n* UI: live update optimizations to effectively update and display real-time data.\n\n# Integrations\n\nAim easily integrates with a large number of widely adopted machine learning frameworks and tools, reducing barriers to get started with Aim. During the past year, a number of integrations were added:\n\n* **ML Frameworks**¬†‚Äî Pytorch Ignite, CatBoost, LightGBM, KerasTuner, fastai, MXNet, Optuna, PaddlePaddle\n* **Convertors**¬†to easily migrate from other tools¬†**‚Äî**¬†TensorBoard to Aim, MLFlow to Aim, WandB to Aim.\n* **Tools that integrated Aim**¬†‚Äî Meta‚Äôs fairseq and metaseq frameworks, HuggingFace‚Äôs accelerate, Ludwig and Kedro.\n\n![](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*YT4Yf1sJUqQLWRtiF8fJgQ.png)\n\n# Learn More\n\n\n\nIf you have any questions join¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. üôå\n\nShow some love by dropping a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003ch1\u003eA retrospective look at the past year\u003c/h1\u003e\n\u003cp\u003eAim underwent significant improvements in the past year. Over the course of 50 releases, including 12 minor releases and 38 patch releases, Aim received contributions from 35 total contributors, including 18 new external contributors. These contributions included 769 merged pull requests and 487 submitted issues!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhiLjCXeyDNrMIuZU5_SXg.png\" alt=\"\" title=\"Aim contributions pulse\"\u003e\u003c/p\u003e\n\u003cp\u003eThere was a significant progress in various areas of the Aim, including adding more supported platforms, taking the UI to the whole new level, adding remote tracking capabilities, revamping the CLI, extending the QL and moving towards active monitoring for optimizing large-scale trainings.\u003c/p\u003e\n\u003cp\u003eIn addition to adding new features and extending Aim, improvements in performance and usability were applied to enhance the overall user experience. Furthermore, new integrations with machine learning tools were implemented to make it easier to use Aim in a broader technology stack\u003c/p\u003e\n\u003cp\u003eOverall, these improvements greatly enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.\u003c/p\u003e\n\u003ch1\u003eAim Community\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GitHotOjlY9WPsvQmWaqwQ.png\" alt=\"\" title=\"Aim contributors\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNumerous external contributions were pushed to ‚Äúmain‚Äù in 2022. Without community support, Aim would not be where it is today.\u003c/p\u003e\n\u003cp\u003eCongratulations to¬†\u003ca href=\"https://github.com/Sharathmk99\"\u003eSharathmk99\u003c/a\u003e,¬†\u003ca href=\"https://github.com/YodaEmbedding\"\u003eYodaEmbedding\u003c/a\u003e,¬†\u003ca href=\"https://github.com/hjoonjang\"\u003ehjoonjang\u003c/a\u003e,¬†\u003ca href=\"https://github.com/jangop\"\u003ejangop\u003c/a\u003e,¬†\u003ca href=\"https://github.com/djwessel\"\u003edjwessel\u003c/a\u003e,¬†\u003ca href=\"https://github.com/GeeeekExplorer\"\u003eGeeeekExplorer\u003c/a\u003e,¬†\u003ca href=\"https://github.com/dsblank\"\u003edsblank\u003c/a\u003e,¬†\u003ca href=\"https://github.com/timokau\"\u003etimokau\u003c/a\u003e,¬†\u003ca href=\"https://github.com/kumarshreshtha\"\u003ekumarshreshtha\u003c/a\u003e,¬†\u003ca href=\"https://github.com/kage08\"\u003ekage08\u003c/a\u003e,¬†\u003ca href=\"https://github.com/karan2801\"\u003ekaran2801\u003c/a\u003e,¬†\u003ca href=\"https://github.com/hendriks73\"\u003ehendriks73\u003c/a\u003e,¬†\u003ca href=\"https://github.com/yeghiakoronian\"\u003eyeghiakoronian\u003c/a\u003e,¬†\u003ca href=\"https://github.com/uduse\"\u003euduse\u003c/a\u003e,¬†\u003ca href=\"https://github.com/arnauddhaene\"\u003earnauddhaene\u003c/a\u003e,¬†\u003ca href=\"https://github.com/lukoucky\"\u003elukoucky\u003c/a\u003e,¬†\u003ca href=\"https://github.com/Arvind644\"\u003eArvind644\u003c/a\u003e,¬†\u003ca href=\"https://github.com/shrinandj\"\u003eshrinandj\u003c/a\u003e¬†for their first contributions over the past year! üéâ\u003c/p\u003e\n\u003cp\u003eThese contributions have covered a range of areas, including documentation, user guides, the SDK and the UI!\u003c/p\u003e\n\u003ch1\u003eHighlights\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eSupported platforms were expanded to include Docker, Apple M1, Conda, Google Colab and Jupyter Notebooks.\u003c/li\u003e\n\u003cli\u003eThe UI was given a makeover, with upgraded home and run single pages, as well as brand new experiment page. The new audio and figure explorers now allow for the exploration of more types of metadata.\u003c/li\u003e\n\u003cli\u003eAim remote tracking server was rolled out to enable tracking from multi-host environments.\u003c/li\u003e\n\u003cli\u003eActive monitoring capabilities were enabled, such as notifications for stalled runs, and the ability to programmatically define training heuristics using callbacks and notifications.\u003c/li\u003e\n\u003cli\u003eAim was integrated with 9 different machine learning frameworks alongside with 3 convertors that help to migrate from other experiment trackers.\u003c/li\u003e\n\u003cli\u003eLast but not least, the documentation was completely revamped, including almost 40 pages of guides towards helping in effectively using and understanding Aim.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\u003cstrong\u003eSupported Platforms\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eImprovements were made to ensure Aim works seamlessly with widely used environments and platforms in ML/MLOps field, including adding support for new platforms.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYG-SSawh8Q8jte6BjyNMQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSupport for running Aim on Apple M1 devices.\u003c/li\u003e\n\u003cli\u003eSupport for using Aim inside Google Colab and Jupyter notebooks.\u003c/li\u003e\n\u003cli\u003eDocker images for Aim UI and server.\u003c/li\u003e\n\u003cli\u003eSupport for running Aim in Kubernetes clusters.\u003c/li\u003e\n\u003cli\u003eAim became available in Conda.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eUser Interface\u003c/h1\u003e\n\u003cp\u003eThe UI is one of key interfaces to interact with the ML training runs tracked by Aim. It‚Äôs super-powerful for comparing large number of trainings. The UI was significantly improved in the past year with new pages and explorers to help to explore more metadata types.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odqgX33xLVzYBI6sfRYRWQ.png\" alt=\"\" title=\"Home Page\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRevamped home page to see project‚Äôs contributions and overall statistics.\u003c/li\u003e\n\u003cli\u003eRevamped run page to deep-dive into an individual run.\u003c/li\u003e\n\u003cli\u003eBrand new experiment page to view experiment‚Äôs details, attached runs, etc.\u003c/li\u003e\n\u003cli\u003eNew explorers, such as the ‚ÄúFigures Explorer‚Äù and ‚ÄúAudio Explorer‚Äù, to allow a cross-run comparison of Plotly figures and audio objects respectively.\u003c/li\u003e\n\u003cli\u003e‚ÄúMetrics Explorer‚Äù key enhancements, including displaying chart legends and the ability of exporting charts as images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eCommand Line Interface\u003c/h1\u003e\n\u003cp\u003eAim CLI offers a simple interface to easily manage tracked trainings. Two key groups of commands were added:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRuns management\u003c/strong\u003e¬†to enable the base operations for managing trainings ‚Äî list, copy, remove, move, upload, close.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStorage management\u003c/strong\u003e¬†to help to manage Aim data storage, such as reindexing, pruning, managing data format versions (advanced usage).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZFkXFzmz5BxGYofUgWdTg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eQuery Language\u003c/h1\u003e\n\u003cp\u003eAim enables a powerful query language to filter through all the stored metadata using python expressions. A few additions were done providing a more friendly and pythonic interface for working with date-time expressions, as well as querying runs based on its metrics results.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6J9gouDEt9Mkhv9Cds4dNw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eRemote Tracking\u003c/h1\u003e\n\u003cp\u003eThe Aim remote tracking server, which allows running trainings in a multi-host environment and collect tracked data in a centralized location, used to be one of the most requested features.\u003c/p\u003e\n\u003cp\u003eIt has been gradually rolled out from its experimental phase to a stable version that can scale up and handle an increasing number of parallel trainings.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CizfTBF8Nh4DZcun6TP8Pg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSwitching from a local environment to a centralized server is as easy as pie, as zero code changes are required.‚ö°\u003cbr\u003e\nThis is because the interfaces are completely compatible!\u003c/p\u003e\n\u003ch1\u003eActive Monitoring\u003c/h1\u003e\n\u003cp\u003eAim made it first steps towards optimizing conducting long and large scale trainings, such as training or tuning GPT-like models. It reduces human-hours spent monitoring/restarting runs, improves reproducibility and ramps up time for people who need to train these models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAbility to notify on stalled/stuck runs.\u003c/li\u003e\n\u003cli\u003eCallbacks and notifications to define training heuristics programmatically.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA quote from ‚ÄúScaling Laws for Generative Mixed-Modal Language Models‚Äù (Aghajanyan et al., 2023,¬†\u003ca href=\"https://arxiv.org/abs/2301.03728\"\u003earXiv:2301.03728\u003c/a\u003e):\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe tracked all experiments using the Aim experiment tracker (Arakelyan et al., 2020). To ensure consistent training strategies across our experiments, we implemented a model restart policy using the Aim experiment tracker and callbacks. Specifically, if training perplexities do not decrease after 500 million tokens, the training run is restarted with a reduced learning rate with a factor of 0.8 of the current time step. This policy helps remove variance in the scaling laws due to differences in training procedures and allows us to scale up the number of asynchronous experiments significantly. All experiments were conducted in a two-month time frame with a cluster of 768 80GB A100 GPUs. The majority of experiments used 64 GPUs at a time.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eKey Performance and Usability Optimizations\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eStorage: long metrics sampling and retrieval (\u003e1M steps).\u003c/li\u003e\n\u003cli\u003eStorage: robust locking mechanism and automatic background indexing.\u003c/li\u003e\n\u003cli\u003eUI: The v3.12 milestone was released, addressing over 30 usability issues.\u003c/li\u003e\n\u003cli\u003eUI: virtualizations in various places across the UI to improve responsiveness when displaying large amount of data.\u003c/li\u003e\n\u003cli\u003eUI: optimizations in stream decoding and data encoding to enhance overall performance.\u003c/li\u003e\n\u003cli\u003eUI: live update optimizations to effectively update and display real-time data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eIntegrations\u003c/h1\u003e\n\u003cp\u003eAim easily integrates with a large number of widely adopted machine learning frameworks and tools, reducing barriers to get started with Aim. During the past year, a number of integrations were added:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eML Frameworks\u003c/strong\u003e¬†‚Äî Pytorch Ignite, CatBoost, LightGBM, KerasTuner, fastai, MXNet, Optuna, PaddlePaddle\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConvertors\u003c/strong\u003e¬†to easily migrate from other tools¬†\u003cstrong\u003e‚Äî\u003c/strong\u003e¬†TensorBoard to Aim, MLFlow to Aim, WandB to Aim.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTools that integrated Aim\u003c/strong\u003e¬†‚Äî Meta‚Äôs fairseq and metaseq frameworks, HuggingFace‚Äôs accelerate, Ludwig and Kedro.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1358/format:webp/1*YT4Yf1sJUqQLWRtiF8fJgQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. üôå\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ‚≠êÔ∏è on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/aim-2022-product-recap.md","_raw":{"sourceFilePath":"posts/aim-2022-product-recap.md","sourceFileName":"aim-2022-product-recap.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2022-product-recap"},"type":"Post"},{"title":"Aim 3.1 ‚Äî Images Tracker and Images Explorer","date":"2021-11-25T14:57:05.232Z","author":"Gev Soghomonian","description":"Excited to share with you the Aim 3.1! üòä A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community","slug":"aim-3-1-images-tracker-and-images-explorer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Excited to share with you the¬†Aim 3.1!¬†\n\nA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWe have added a new demo to showcase the new Explore Images feature. We have also forked the¬†[lightweight-gan from lucidrains](https://github.com/lucidrains/lightweight-gan)¬†and added the Aim integration.\n\nDemo code:¬†\u003chttps://github.com/aimhubio/lightweight-gan\u003e\\\nDemo site:¬†[play.aimstack.io:10002](http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq\u0026select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS\u0026images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK)\n\n**The notable changes in the Aim 3.1.0:**\n\n* Images tracker and explorer\n* Runs navigation from the single run page\n* Performance improvements and tests\n\nThanks to srikanth,¬†Mohamad¬†Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\n\n## Images Tracker and Explorer\n\nFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\n\nAll the regular grouping and search goodies are available on Images Explorer too¬†\n\nAdd an image tracker in your training code like this:\n\n```\nrun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n```\n\nOnce you run your training, the images will be saved and become available on the Images Explorer tab.\n\nOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\n\nHere is a quick promo video of what it can do. The rest of the details can be found in the docs[¬†here](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png)\n\nA quick demo video.\n\n![](https://youtu.be/4mtUFV8yG_o)\n\nThe Images view on the single run page is coming soon ‚Ä¶\n\n## Runs Navigation\n\nOne of the regressions from the Aim 2.x to¬†[3.0.0](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative)¬†was the ability to navigate between the runs of the same experiment from the single run page.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63UGGl2EDquqGHIY03VpQQ.png)\n\nWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\n\nSorry for the regression¬†\n\n## Performance improvements and assurances\n\nTwo groups of performance improvements have been made‚Äî on the UI and on the SDK side.\n\n### **Column virtualization on Aim UI Table**\n\nThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\n\nWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem ‚Äî and this is a key step towards ¬†it. üçª\n\n### **Storage performance tests**\n\nOne of the unique challenges in building Aim is to make sure each newly added data type won‚Äôt impact the overall performance. We have added continuously add more performance tests to ensure the new releases don‚Äôt impact the performance.\n\nIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\n\nPerformance reliability is key for Aim and this is a major iteration towards that!\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"\u003cp\u003eExcited to share with you the¬†Aim 3.1!¬†\u003c/p\u003e\n\u003cp\u003eA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\u003c/p\u003e\n\u003cp\u003eWe have added a new demo to showcase the new Explore Images feature. We have also forked the¬†\u003ca href=\"https://github.com/lucidrains/lightweight-gan\"\u003elightweight-gan from lucidrains\u003c/a\u003e¬†and added the Aim integration.\u003c/p\u003e\n\u003cp\u003eDemo code:¬†\u003ca href=\"https://github.com/aimhubio/lightweight-gan\"\u003ehttps://github.com/aimhubio/lightweight-gan\u003c/a\u003e\u003cbr\u003e\nDemo site:¬†\u003ca href=\"http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq\u0026#x26;select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS\u0026#x26;images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK\"\u003eplay.aimstack.io:10002\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe notable changes in the Aim 3.1.0:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImages tracker and explorer\u003c/li\u003e\n\u003cli\u003eRuns navigation from the single run page\u003c/li\u003e\n\u003cli\u003ePerformance improvements and tests\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThanks to srikanth,¬†Mohamad¬†Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\u003c/p\u003e\n\u003ch2\u003eImages Tracker and Explorer\u003c/h2\u003e\n\u003cp\u003eFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\u003c/p\u003e\n\u003cp\u003eAll the regular grouping and search goodies are available on Images Explorer too¬†\u003c/p\u003e\n\u003cp\u003eAdd an image tracker in your training code like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you run your training, the images will be saved and become available on the Images Explorer tab.\u003c/p\u003e\n\u003cp\u003eOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\u003c/p\u003e\n\u003cp\u003eHere is a quick promo video of what it can do. The rest of the details can be found in the docs\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003e¬†here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA quick demo video.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://youtu.be/4mtUFV8yG_o\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Images view on the single run page is coming soon ‚Ä¶\u003c/p\u003e\n\u003ch2\u003eRuns Navigation\u003c/h2\u003e\n\u003cp\u003eOne of the regressions from the Aim 2.x to¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003e3.0.0\u003c/a\u003e¬†was the ability to navigate between the runs of the same experiment from the single run page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63UGGl2EDquqGHIY03VpQQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\u003c/p\u003e\n\u003cp\u003eSorry for the regression¬†\u003c/p\u003e\n\u003ch2\u003ePerformance improvements and assurances\u003c/h2\u003e\n\u003cp\u003eTwo groups of performance improvements have been made‚Äî on the UI and on the SDK side.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eColumn virtualization on Aim UI Table\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\u003c/p\u003e\n\u003cp\u003eWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem ‚Äî and this is a key step towards ¬†it. üçª\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eStorage performance tests\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eOne of the unique challenges in building Aim is to make sure each newly added data type won‚Äôt impact the overall performance. We have added continuously add more performance tests to ensure the new releases don‚Äôt impact the performance.\u003c/p\u003e\n\u003cp\u003eIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\u003c/p\u003e\n\u003cp\u003ePerformance reliability is key for Aim and this is a major iteration towards that!\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-1-‚Äî-images-tracker-and-images-explorer.md","_raw":{"sourceFilePath":"posts/aim-3-1-‚Äî-images-tracker-and-images-explorer.md","sourceFileName":"aim-3-1-‚Äî-images-tracker-and-images-explorer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-1-‚Äî-images-tracker-and-images-explorer"},"type":"Post"},{"title":"Aim 3.10 ‚Äî Visualize terminal logs, M1 support \u0026 better query autocomplete","date":"2022-05-25T20:49:49.499Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.10 is now available! What's new? A lot! Visualize terminal logs, M1 support, better autocomplete experience, integration with CatBoost and LightGBM.","slug":"aim-3-10-visualize-terminal-logs-m1-support-better-query-autocomplete","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5HbX-z8qa0ORFDRKArmFw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.10 is now available!\n\nWe are on a mission to¬†[democratize AI dev tools](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative). Thanks to the awesome Aim community for the help and contributions.\n\nHere is what‚Äôs new in Aim 3.10:\n\n* Visualize terminal logs\n* M1 support\n* Better autocomplete experience\n* Aim citation available\n* CatBoost integration\n* LightGBM integration\n\nCheck out the 3.10 release¬†[milestone](https://github.com/aimhubio/aim/milestone/33)¬†and the¬†[GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.10.0)\n\n\u003e *Shout out to Daniel Wessel,¬†[arnauddhaene](https://github.com/arnauddhaene),[¬†uduse](https://github.com/uduse),¬†[lukoucky](https://github.com/lukoucky), Armen Aghajanyan and others for contributions and feedback. We really appreciate it.*\n\n## Visualize terminal logs\n\nWhen it comes to automating training of multiple runs with job schedulers or workload managers on a cluster, it becomes hard to track the¬†**terminal**¬†**logs**¬†of the runs.\n\nNow Aim automatically streams the terminal logs to the UI. Near-real-time.\n\nThe terminal logs can be turned off if the run instance is created with the following flag in place:\n\n```\naim_run = Run(capture_terminal_logs=False)\n```\n\nCheck out more about this feature¬†[in Aim docs](https://aimstack.readthedocs.io/en/latest/using/configure_runs.html?highlight=terminal%20logs#capturing-terminal-logs).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K60XrQq0iH6NlO0NFz44EQ.png)\n\n## M1 support\n\n[With the awesome work by the PyTorch team](https://twitter.com/PyTorch/status/1526944876478144512)¬†on enabling support for GPU-accelerated PyTorch training on Mac, there has been a huge demand to enable aim on M1 as well.\n\nAim now supports M1 too. Now you can use Aim with¬†[PyTorch](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)¬†on Mac to track and deeply compare your experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp5LKw55RgE4gnPXC1cWIg.png)\n\n## Better autocomplete experience\n\nWe have integrated a rich code autocomplete system as the Aim community loves to deeply query their training runs. This has been a highly requested improvement and a huge productivity booster. Expect more improvements here.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*vEQnAJSks5YGNCXHllDnNA.gif)\n\n## Aim citation available\n\nNow you can cite Aim from your paper if you are using Aim to compare your experiments.¬†[The citation file](https://github.com/aimhubio/aim/blob/main/CITATION.cff).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLHweR2at8tbpHiTkelbfA.png)\n\n[Thanks to the researchers from Meta AI Research for the incentive and initiative.](https://arxiv.org/abs/2205.10770)¬†¬†üôå\n\n## CatBoost integration\n\n[CatBoost](https://catboost.ai/)¬†is one of the fastest Gradient Boosting on Decision Trees library. Aim now supports CatBoost out of the box. Check out the[¬†Aim CatBoost docs here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost).\n\nHere is a short¬†[example](https://github.com/aimhubio/aim/blob/main/examples/catboost_track.py)¬†that shows how to use AimLogger for CatBoost.\n\n## LightGBM integration\n\n[LightGBM](https://lightgbm.readthedocs.io/en/latest/)¬†is one of the most widely-used and battle-tested gradient boosting frameworks. Due to high-demand we have also added an out-of-the-box Aim LightGBM integration.\n\n[Here is a short code example](https://github.com/aimhubio/aim/blob/main/examples/lightgbm_track.py)¬†and¬†[the docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm)¬†on how to use the integration.\n\n- - -\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n* [](https://twitter.com/share?url=https://aimstack.io/aim-3-10-release-catboost-integration/\u0026text=Aim%203.10%20%E2%80%94%20Visualize%20terminal%20logs%2C%20M1%20support%20%26%23038%3B%20better%20query%20autocomplete)","html":"\u003cp\u003eHey team, Aim 3.10 is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003edemocratize AI dev tools\u003c/a\u003e. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what‚Äôs new in Aim 3.10:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVisualize terminal logs\u003c/li\u003e\n\u003cli\u003eM1 support\u003c/li\u003e\n\u003cli\u003eBetter autocomplete experience\u003c/li\u003e\n\u003cli\u003eAim citation available\u003c/li\u003e\n\u003cli\u003eCatBoost integration\u003c/li\u003e\n\u003cli\u003eLightGBM integration\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the 3.10 release¬†\u003ca href=\"https://github.com/aimhubio/aim/milestone/33\"\u003emilestone\u003c/a\u003e¬†and the¬†\u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.10.0\"\u003eGitHub Release Notes\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eShout out to Daniel Wessel,¬†\u003ca href=\"https://github.com/arnauddhaene\"\u003earnauddhaene\u003c/a\u003e,\u003ca href=\"https://github.com/uduse\"\u003e¬†uduse\u003c/a\u003e,¬†\u003ca href=\"https://github.com/lukoucky\"\u003elukoucky\u003c/a\u003e, Armen Aghajanyan and others for contributions and feedback. We really appreciate it.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eVisualize terminal logs\u003c/h2\u003e\n\u003cp\u003eWhen it comes to automating training of multiple runs with job schedulers or workload managers on a cluster, it becomes hard to track the¬†\u003cstrong\u003eterminal\u003c/strong\u003e¬†\u003cstrong\u003elogs\u003c/strong\u003e¬†of the runs.\u003c/p\u003e\n\u003cp\u003eNow Aim automatically streams the terminal logs to the UI. Near-real-time.\u003c/p\u003e\n\u003cp\u003eThe terminal logs can be turned off if the run instance is created with the following flag in place:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = Run(capture_terminal_logs=False)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck out more about this feature¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/configure_runs.html?highlight=terminal%20logs#capturing-terminal-logs\"\u003ein Aim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K60XrQq0iH6NlO0NFz44EQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eM1 support\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/PyTorch/status/1526944876478144512\"\u003eWith the awesome work by the PyTorch team\u003c/a\u003e¬†on enabling support for GPU-accelerated PyTorch training on Mac, there has been a huge demand to enable aim on M1 as well.\u003c/p\u003e\n\u003cp\u003eAim now supports M1 too. Now you can use Aim with¬†\u003ca href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\"\u003ePyTorch\u003c/a\u003e¬†on Mac to track and deeply compare your experiments.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp5LKw55RgE4gnPXC1cWIg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eBetter autocomplete experience\u003c/h2\u003e\n\u003cp\u003eWe have integrated a rich code autocomplete system as the Aim community loves to deeply query their training runs. This has been a highly requested improvement and a huge productivity booster. Expect more improvements here.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*vEQnAJSks5YGNCXHllDnNA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAim citation available\u003c/h2\u003e\n\u003cp\u003eNow you can cite Aim from your paper if you are using Aim to compare your experiments.¬†\u003ca href=\"https://github.com/aimhubio/aim/blob/main/CITATION.cff\"\u003eThe citation file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLHweR2at8tbpHiTkelbfA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2205.10770\"\u003eThanks to the researchers from Meta AI Research for the incentive and initiative.\u003c/a\u003e¬†¬†üôå\u003c/p\u003e\n\u003ch2\u003eCatBoost integration\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://catboost.ai/\"\u003eCatBoost\u003c/a\u003e¬†is one of the fastest Gradient Boosting on Decision Trees library. Aim now supports CatBoost out of the box. Check out the\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\"\u003e¬†Aim CatBoost docs here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere is a short¬†\u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/catboost_track.py\"\u003eexample\u003c/a\u003e¬†that shows how to use AimLogger for CatBoost.\u003c/p\u003e\n\u003ch2\u003eLightGBM integration\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://lightgbm.readthedocs.io/en/latest/\"\u003eLightGBM\u003c/a\u003e¬†is one of the most widely-used and battle-tested gradient boosting frameworks. Due to high-demand we have also added an out-of-the-box Aim LightGBM integration.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/lightgbm_track.py\"\u003eHere is a short code example\u003c/a\u003e¬†and¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\"\u003ethe docs\u003c/a\u003e¬†on how to use the integration.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/share?url=https://aimstack.io/aim-3-10-release-catboost-integration/\u0026#x26;text=Aim%203.10%20%E2%80%94%20Visualize%20terminal%20logs%2C%20M1%20support%20%26%23038%3B%20better%20query%20autocomplete\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/aim-3-10-‚Äî-visualize-terminal-logs-m1-support-better-query-autocomplete.md","_raw":{"sourceFilePath":"posts/aim-3-10-‚Äî-visualize-terminal-logs-m1-support-better-query-autocomplete.md","sourceFileName":"aim-3-10-‚Äî-visualize-terminal-logs-m1-support-better-query-autocomplete.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-10-‚Äî-visualize-terminal-logs-m1-support-better-query-autocomplete"},"type":"Post"},{"title":"Aim 3.13‚ÄìFigures Explorer and notifications on stalled runs","date":"2022-08-24T21:08:50.210Z","author":"Gev Soghomonian","description":"In this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven‚Äôt had the chance to talk about much. ","slug":"aim-3-13figures-explorer-and-notifications-on-stalled-runs","image":"https://miro.medium.com/max/1400/1*5hUcOciwufupoI4-nBbcyA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"# Hey team, Aim 3.13 is now available!\n\nIn this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven‚Äôt had the chance to talk about much. Stay tuned on the¬†[AimStack](https://twitter.com/aimstackio)¬†twitter where we will share them and more use-cases.\n\n**Aim is evolving really quickly! üöÄ**\n\n\u003e *We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.*\n\u003e\n\u003e *Thanks to¬†[Sharathmk99 and](https://github.com/Sharathmk99)¬†[tmynn](https://github.com/tmynn)¬†for their first contributions!! üî•üî•üî•*\n\n**TLDR;**\n\nAim 3.13 is full of impactful changes. Here are the main highlights!\n\n* **Figures Explorer**\\\n  Explore and compare 100s of Plotly figures within a few clicks.\n* **Notify about stalled runs**\\\n  Configure Aim to notify your team when the run is stalled (slack, email, workspace are available)\n* **Stable Aim Remote Server**\n* **KerasTuner integration**\n* **Weights and Biases log converter**\n\nCheck out the Aim 3.13 release¬†[milestone](https://github.com/aimhubio/aim/issues?q=is%3Aissue+milestone%3Av3.13.0+is%3Aopen)¬†and the¬†[GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.13.0)¬†for more information.\n\n# Figures Explorer\n\nFigures Explorer is a new addition to the mighty Aim explorers. With this feature now you can compare 1000s of Plotly figures within a few clicks.\n\nThis is just the first iteration üòä. Expect lots of improvements of the experience over time. This is how the figures explorer works:\n\n![](https://miro.medium.com/max/1400/1*WKwbKRBJPK8fErSKVItf6Q.gif)\n\n# Notify about stalled runs\n\nOne of the ways the precious training time is prolonged / wasted when for some reasons the runs stall. The training runs can stall in so many different ways:\n\n* Hardware issues\n* exception in the code\n* driver issues\n\nAnd many other things we never anticipate. This feature allows Aim users to configure notifications to their preferred channel (slack, workspace) when the run stalls.\n\nCheck out the¬†[docs](https://aimstack.readthedocs.io/en/latest/using/run_status_notifications.html#)¬†for more info. Here is how it looks on your slack:\n\n![](https://miro.medium.com/max/1400/1*OFZh37yiEa_vo-14gv7UFw.webp)\n\n# Stable Aim Remote Server\n\nAim Remote allows to set up a remote Aim tracking server so the tracked logs can be sent to a centralized location.\n\nIt‚Äôs been a while since Aim Remote Server was an experimental feature. We have been lucky as the users have started using it and shared lots of feedbacks, issues that we have fixed.\n\nNow we are excited to announce that the Aim Remote Server is stable\n\n![](https://miro.medium.com/max/1400/1*zC-crWCgWncraYJFQRAd3g.webp)\n\n# KerasTuner integration\n\nA highly requested integration with KerasTuner.\n\nHere are the¬†[docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tunerhttps://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tuner)¬†and a short code snippet. Super-easy to integrate Aim with your KerasTuner project now.\n\n```\nfrom aim.keras_tuner import AimCallback\n\n...\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n```\n\n# Weights and Biases log converter\n\nThe W\u0026B users who also wanted to use Aim in their work, have been asking about this feature for a while now.\n\nExcited to share that now you can easily convert your W\u0026B logs to Aim.\n\nHere is how it works:\n\n```\ncd project-directory\n# Init the Aim repo\naim init \n# Run the converter to migrate logs from WandB to Aim\naim convert wandb --entity 'my_team' --project 'my_project'\n```\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)¬†üôå\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful ‚≠êÔ∏è","html":"\u003ch1\u003eHey team, Aim 3.13 is now available!\u003c/h1\u003e\n\u003cp\u003eIn this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven‚Äôt had the chance to talk about much. Stay tuned on the¬†\u003ca href=\"https://twitter.com/aimstackio\"\u003eAimStack\u003c/a\u003e¬†twitter where we will share them and more use-cases.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAim is evolving really quickly! üöÄ\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThanks to¬†\u003ca href=\"https://github.com/Sharathmk99\"\u003eSharathmk99 and\u003c/a\u003e¬†\u003ca href=\"https://github.com/tmynn\"\u003etmynn\u003c/a\u003e¬†for their first contributions!! üî•üî•üî•\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim 3.13 is full of impactful changes. Here are the main highlights!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFigures Explorer\u003c/strong\u003e\u003cbr\u003e\nExplore and compare 100s of Plotly figures within a few clicks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNotify about stalled runs\u003c/strong\u003e\u003cbr\u003e\nConfigure Aim to notify your team when the run is stalled (slack, email, workspace are available)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStable Aim Remote Server\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKerasTuner integration\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeights and Biases log converter\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the Aim 3.13 release¬†\u003ca href=\"https://github.com/aimhubio/aim/issues?q=is%3Aissue+milestone%3Av3.13.0+is%3Aopen\"\u003emilestone\u003c/a\u003e¬†and the¬†\u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.13.0\"\u003eGitHub Release Notes\u003c/a\u003e¬†for more information.\u003c/p\u003e\n\u003ch1\u003eFigures Explorer\u003c/h1\u003e\n\u003cp\u003eFigures Explorer is a new addition to the mighty Aim explorers. With this feature now you can compare 1000s of Plotly figures within a few clicks.\u003c/p\u003e\n\u003cp\u003eThis is just the first iteration üòä. Expect lots of improvements of the experience over time. This is how the figures explorer works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*WKwbKRBJPK8fErSKVItf6Q.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eNotify about stalled runs\u003c/h1\u003e\n\u003cp\u003eOne of the ways the precious training time is prolonged / wasted when for some reasons the runs stall. The training runs can stall in so many different ways:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHardware issues\u003c/li\u003e\n\u003cli\u003eexception in the code\u003c/li\u003e\n\u003cli\u003edriver issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd many other things we never anticipate. This feature allows Aim users to configure notifications to their preferred channel (slack, workspace) when the run stalls.\u003c/p\u003e\n\u003cp\u003eCheck out the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/run_status_notifications.html#\"\u003edocs\u003c/a\u003e¬†for more info. Here is how it looks on your slack:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*OFZh37yiEa_vo-14gv7UFw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eStable Aim Remote Server\u003c/h1\u003e\n\u003cp\u003eAim Remote allows to set up a remote Aim tracking server so the tracked logs can be sent to a centralized location.\u003c/p\u003e\n\u003cp\u003eIt‚Äôs been a while since Aim Remote Server was an experimental feature. We have been lucky as the users have started using it and shared lots of feedbacks, issues that we have fixed.\u003c/p\u003e\n\u003cp\u003eNow we are excited to announce that the Aim Remote Server is stable\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*zC-crWCgWncraYJFQRAd3g.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eKerasTuner integration\u003c/h1\u003e\n\u003cp\u003eA highly requested integration with KerasTuner.\u003c/p\u003e\n\u003cp\u003eHere are the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tunerhttps://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tuner\"\u003edocs\u003c/a\u003e¬†and a short code snippet. Super-easy to integrate Aim with your KerasTuner project now.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.keras_tuner import AimCallback\n\n...\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eWeights and Biases log converter\u003c/h1\u003e\n\u003cp\u003eThe W\u0026#x26;B users who also wanted to use Aim in their work, have been asking about this feature for a while now.\u003c/p\u003e\n\u003cp\u003eExcited to share that now you can easily convert your W\u0026#x26;B logs to Aim.\u003c/p\u003e\n\u003cp\u003eHere is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd project-directory\n# Init the Aim repo\naim init \n# Run the converter to migrate logs from WandB to Aim\naim convert wandb --entity 'my_team' --project 'my_project'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e¬†üôå\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful ‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/aim-3-13‚Äìfigures-explorer-and-notifications-on-stalled-runs.md","_raw":{"sourceFilePath":"posts/aim-3-13‚Äìfigures-explorer-and-notifications-on-stalled-runs.md","sourceFileName":"aim-3-13‚Äìfigures-explorer-and-notifications-on-stalled-runs.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-13‚Äìfigures-explorer-and-notifications-on-stalled-runs"},"type":"Post"},{"title":"Aim 3.14 ‚Äî Brand new Home Page!","date":"2022-11-03T21:31:15.756Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.14 is now available! We're excited to introduce Brand new Home Page, MXNet and FastAI loggers, new tooltip positioning modes and ability to edit tags on the runs table!","slug":"aim-3-14-brand-new-home-page","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEJ0_GtGyXC4sv2sZrSCRA.png","draft":false,"categories":["New Releases"],"body":{"raw":"# Hey team, Aim 3.14 is now available!\n\nWe are releasing a new Aim version every 4‚Äì5 weeks with community-contributed features and fixes.\n\n**A brand new Aim release every few weeks**. With new features and fixes! üöÄ\n\n\u003e We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\u003e\n\u003e Thanks to¬†[djwessel](https://github.com/djwessel)¬†and¬†[Vahram-aimhub](https://github.com/Vahram-aimhub)¬†for their first contributionsüî•üî•üî•\n\n# Aim 3.14 Overview\n\nHere is what‚Äôs new in the latest release:\n\n* **Brand new Home Page**\n* **MXNet and FastAI loggers**\n* **New tooltip positioning modes**\n* **Ability to edit tags on the runs table**\n\nCheck out the¬†[Aim 3.14 release milestone](https://github.com/aimhubio/aim/milestone/41)¬†and the¬†[GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.14.0)¬†for more information.\n\n\u003e Note: at the time of writing this, Aim is at¬†[3.14.3](https://github.com/aimhubio/aim/releases)¬†‚Äî check those intermediate versions too.\n\n# Brand new Home Page\n\nAfter months of requests from the community to build a quick access page, improve the onboarding, we are excited to introduce new Home page.\n\nThe home page is pretty self-descriptive. It‚Äôs an overview of whatever actions have been taken using Aim.\n\nThis is in no way the final version, we continue to iterate on it. Would love to hear your feedbacks.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhWVuIETasWSsUc3oEYdfQ.png)\n\n# MXNet and fastai loggers\n\nNow there are loggers available both for MxNet and FastAI.\n\nIt takes only a few lines to incorporate Aim to your code.\n\nHere is how it looks for MXNet in your code. More¬†[MXNet integration details here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-mxnet)\n\n\n\n```\nfrom aim.mxnet import AimLoggingHandler\n\naim_log_handler = AimLoggingHandler(repo='.', experiment_name='mxnet_example',\n                                    log_interval=1, metrics=[train_acc, train_loss, val_acc])\n\nest.fit(train_data=train_data_loader, val_data=val_data_loader,\n        epochs=num_epochs, event_handlers=[aim_log_handler])\n```\n\nThis is the code for fastai Aim integrations.¬†[More details here.](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)\n\n```\nfrom aim.fastai import AimCallback\n\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_example'))\n```\n\n## New tooltip positioning modes\n\nWe have added three buttons to the Explorers tooltip on click.\n\nThese buttons allow to fix where you‚Äôd want to see these tooltips appear when hovering over the metrics.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71V5hEnGIOg2-QIi01ok1A.png)\n\nIn this case we have chosen to place the tooltip at the top of the page.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vDsrHdRa42jxpTcG66nkg.png)\n\n# Ability to edit tags on the runs table\n\nTags have become one of the widely used features lately and we are iterating so you can make the best out of them.\n\nNow you can add tags to your runs from within the Runs Explorer.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vk0DJyFCGdw9A_TPzGDMRg.png)\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)¬†üôå\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful ‚≠êÔ∏è.","html":"\u003ch1\u003eHey team, Aim 3.14 is now available!\u003c/h1\u003e\n\u003cp\u003eWe are releasing a new Aim version every 4‚Äì5 weeks with community-contributed features and fixes.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA brand new Aim release every few weeks\u003c/strong\u003e. With new features and fixes! üöÄ\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eThanks to¬†\u003ca href=\"https://github.com/djwessel\"\u003edjwessel\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/Vahram-aimhub\"\u003eVahram-aimhub\u003c/a\u003e¬†for their first contributionsüî•üî•üî•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eAim 3.14 Overview\u003c/h1\u003e\n\u003cp\u003eHere is what‚Äôs new in the latest release:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBrand new Home Page\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMXNet and FastAI loggers\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNew tooltip positioning modes\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAbility to edit tags on the runs table\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the¬†\u003ca href=\"https://github.com/aimhubio/aim/milestone/41\"\u003eAim 3.14 release milestone\u003c/a\u003e¬†and the¬†\u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.14.0\"\u003eGitHub Release Notes\u003c/a\u003e¬†for more information.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: at the time of writing this, Aim is at¬†\u003ca href=\"https://github.com/aimhubio/aim/releases\"\u003e3.14.3\u003c/a\u003e¬†‚Äî check those intermediate versions too.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eBrand new Home Page\u003c/h1\u003e\n\u003cp\u003eAfter months of requests from the community to build a quick access page, improve the onboarding, we are excited to introduce new Home page.\u003c/p\u003e\n\u003cp\u003eThe home page is pretty self-descriptive. It‚Äôs an overview of whatever actions have been taken using Aim.\u003c/p\u003e\n\u003cp\u003eThis is in no way the final version, we continue to iterate on it. Would love to hear your feedbacks.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhWVuIETasWSsUc3oEYdfQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eMXNet and fastai loggers\u003c/h1\u003e\n\u003cp\u003eNow there are loggers available both for MxNet and FastAI.\u003c/p\u003e\n\u003cp\u003eIt takes only a few lines to incorporate Aim to your code.\u003c/p\u003e\n\u003cp\u003eHere is how it looks for MXNet in your code. More¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-mxnet\"\u003eMXNet integration details here\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.mxnet import AimLoggingHandler\n\naim_log_handler = AimLoggingHandler(repo='.', experiment_name='mxnet_example',\n                                    log_interval=1, metrics=[train_acc, train_loss, val_acc])\n\nest.fit(train_data=train_data_loader, val_data=val_data_loader,\n        epochs=num_epochs, event_handlers=[aim_log_handler])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the code for fastai Aim integrations.¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\"\u003eMore details here.\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.fastai import AimCallback\n\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_example'))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eNew tooltip positioning modes\u003c/h2\u003e\n\u003cp\u003eWe have added three buttons to the Explorers tooltip on click.\u003c/p\u003e\n\u003cp\u003eThese buttons allow to fix where you‚Äôd want to see these tooltips appear when hovering over the metrics.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71V5hEnGIOg2-QIi01ok1A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this case we have chosen to place the tooltip at the top of the page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vDsrHdRa42jxpTcG66nkg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eAbility to edit tags on the runs table\u003c/h1\u003e\n\u003cp\u003eTags have become one of the widely used features lately and we are iterating so you can make the best out of them.\u003c/p\u003e\n\u003cp\u003eNow you can add tags to your runs from within the Runs Explorer.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vk0DJyFCGdw9A_TPzGDMRg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e¬†üôå\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful ‚≠êÔ∏è.\u003c/p\u003e"},"_id":"posts/aim-3-14-‚Äî-brand-new-home-page.md","_raw":{"sourceFilePath":"posts/aim-3-14-‚Äî-brand-new-home-page.md","sourceFileName":"aim-3-14-‚Äî-brand-new-home-page.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-14-‚Äî-brand-new-home-page"},"type":"Post"},{"title":"Aim 3.15 ‚Äî Callbacks, logging and notifications, line chart legends, experiment page, Audios Explorer","date":"2022-12-06T21:40:50.164Z","author":"Gor Arakelyan","description":"Aim 3.15 is out! Callbacks, Logging \u0026 notifications, Line chart legends, Audios Explorer, Experiment page, Robust locking mechanism, PaddlePaddle \u0026 Optuna integrations.","slug":"aim-3-15-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer","image":"https://miro.medium.com/max/1400/1*wQWwnwDeCSVQ-YbZapbHlQ.webp","draft":false,"categories":["New Releases"],"body":{"raw":"## Hey team, get ready for a major upgrade!\n\nWe are thrilled to announce the release of Aim v3.15, packed with new features and improvements.\n\n\u003e *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Without your help, Aim would not be where it is today.*\n\u003e\n\u003e *Congratulations to¬†[jangop](https://github.com/jangop),¬†[hjoonjang](https://github.com/hjoonjang)¬†and¬†[kumarshreshtha](https://github.com/kumarshreshtha)*¬†*for their first contributions.*\n\n**TL;DR**¬†The new version includes lots of significant and highly requested enhancements, as well as key improvements. Here are the highlights:\n\n* **Callbacks**\n* **Logging and notifications**\n* **Line chart legends**\n* **Audios Explorer**\n* **Experiment page**\n* **Robust locking mechanism**\n* **Integrations with PaddlePaddle and Optuna**\n\n# Aim SDK\n\n## **Callbacks**\n\nMany things can go wrong during ML training that could result in wasted GPUs and time. The Aim callbacks API helps to define custom callbacks to be executed at any point during ML training.\n\nFind API docs and examples¬†[here](https://aimstack.readthedocs.io/en/latest/using/callbacks.html).\n\nAn end-to-end toy script below:\n\n*Send a notification and exits the process, when the loss explodes.*\n\n```\nimport random\nimport time\n\nfrom aim import Run\nfrom aim import TrainingFlow\nfrom aim.sdk.callbacks import events\n\n\n# Define the callbacks\nclass MyCallbacks:\n    @events.on.training_metrics_collected\n    def check_loss_explosion(self, metrics, run, **kwargs):\n        if metrics.get('loss') \u003e 1:\n            # Log warning message and send the corresponding notification\n            run.log_warning(f'loss exploded: {metrics[\"loss\"]}')\n            # Exit the process\n            exit()\n\n\n# Initialize Aim run and register callbacks\nrun = Run()\ntraining_flow = TrainingFlow(run=run)\ntraining_flow.register(MyCallbacks())\n\n\n# Training simulation\nfor _ in range(100):\n    loss = random.random() * 2\n    print(loss)\n    run.track(loss, name='loss', context={'subset': 'train'})\n    training_flow.training_metrics_collected(\n        metrics={'loss': loss},\n        run=run\n    )\n    time.sleep(1)\n```\n\n## Logging and notifications\n\nDuring the training many key events happen. Use new Aim logging API to log any text message and send the corresponding notification to slack or workplace. It is as easy as calling¬†`aim_run.log_error(message)`.\n\nIt perfectly fits with callbacks API and enables programmatically defining training heuristics and including it as part of the code. A simple example is ‚Äúlet me know when the training run starts to overfit‚Äù.\n\nRead more in the¬†[docs](https://aimstack.readthedocs.io/en/latest/using/logging.html).\n\n# Aim UI\n\n## Chart legends\n\nSince the early days of Aim‚Äôs launch, one of the most requested features has been displaying chart legends. And finally it is here! The legends section makes it easy to learn metrics corresponding fields and hyper-params at a glance.\n\n![](https://miro.medium.com/max/1400/1*TFNCZjjwXFg7JZv1F1qSpg.webp)\n\n## **Audios Explorer**\n\nTo track the performance of ML models that process speech data, it is important to track audio objects.\n\nThe new Audios Explorer makes it super easy to query and play tracked audio objects across the runs. Flexible grouping options enable arranging them into rows and columns to make exploration even easier.\n\n![](https://miro.medium.com/max/1400/1*mnP4qbmlQuJE-nGqe-36lw.webp)\n\n\n\n## Experiment page\n\nThe experiment page provides an holistic view of the results of the trainings in order to identify trends and patterns that can help inform further experimentation and optimization.\n\nExperiment page consists from the following tabs:\n\n* üè†¬†**Overview**¬†‚Äî Experiment overview, number of active/finished runs, timeline of the runs.\n* üèÉ¬†**Runs**¬†‚ÄîHolistic view of the experiment runs.\n* üìì¬†**Notes**¬†‚Äî Experiment notes and insights with notion-like rich editor.\n* ‚öôÔ∏è¬†**Settings**¬†‚Äî Experiment settings, such as name and description.\n\n![](https://miro.medium.com/max/1400/1*h-DLE3XnBM5uGKBDRcu_iA.webp)\n\n# Integrations\n\n## Integration with PaddlePaddle\n\nWith Aim v3.15 it takes just two steps to integrate Aim into your PaddlePaddle training script.\n\n```\n# Step 1: Explicitly import the AimCallback for tracking training metadata\nfrom aim.paddle import AimCallback\n\n...\n\n# Step 2: Pass the callback to callbacks list upon initiating your training\ncallback = AimCallback(repo='.', experiment='paddle_test')\nmodel.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)\n\n```\n\nSee docs¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-paddlepaddle).\n\n## Integration with Optuna\n\nSmoothly integrate Aim into your Optuna project.\n\nHere are the¬†[docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-optuna)¬†and a short code snippet.\n\n```\nfrom aim.optuna import AimCallback\n\n...\naim_callback = AimCallback(experiment_name=\"optuna_single_run\")\nstudy.optimize(objective, n_trials=10, callbacks=[aim_callback])\n```\n\n# *Key optimizations and fixes*\n\n\n\n## Runs locking mechanism and finished runs detection\n\nNumber of issues were reported w.r.t runs locking. More robust locking mechanism is applied starting from Aim v3.15.\n\n## Remote server scaling\n\nNow it is possible to spin up several workers to achieve higher processing speed for remote tracking server. Just run the server passing ‚Äúworkers‚Äù argument:¬†`aim server --workers=N`\n\n# Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)¬†üôå\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://discord.gg/F5TpzUeY), share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful ‚≠êÔ∏è","html":"\u003ch2\u003eHey team, get ready for a major upgrade!\u003c/h2\u003e\n\u003cp\u003eWe are thrilled to announce the release of Aim v3.15, packed with new features and improvements.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Without your help, Aim would not be where it is today.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eCongratulations to¬†\u003ca href=\"https://github.com/jangop\"\u003ejangop\u003c/a\u003e,¬†\u003ca href=\"https://github.com/hjoonjang\"\u003ehjoonjang\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/kumarshreshtha\"\u003ekumarshreshtha\u003c/a\u003e\u003c/em\u003e¬†\u003cem\u003efor their first contributions.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e¬†The new version includes lots of significant and highly requested enhancements, as well as key improvements. Here are the highlights:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCallbacks\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLogging and notifications\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLine chart legends\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAudios Explorer\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExperiment page\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust locking mechanism\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntegrations with PaddlePaddle and Optuna\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eAim SDK\u003c/h1\u003e\n\u003ch2\u003e\u003cstrong\u003eCallbacks\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eMany things can go wrong during ML training that could result in wasted GPUs and time. The Aim callbacks API helps to define custom callbacks to be executed at any point during ML training.\u003c/p\u003e\n\u003cp\u003eFind API docs and examples¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/callbacks.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAn end-to-end toy script below:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eSend a notification and exits the process, when the loss explodes.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport random\nimport time\n\nfrom aim import Run\nfrom aim import TrainingFlow\nfrom aim.sdk.callbacks import events\n\n\n# Define the callbacks\nclass MyCallbacks:\n    @events.on.training_metrics_collected\n    def check_loss_explosion(self, metrics, run, **kwargs):\n        if metrics.get('loss') \u003e 1:\n            # Log warning message and send the corresponding notification\n            run.log_warning(f'loss exploded: {metrics[\"loss\"]}')\n            # Exit the process\n            exit()\n\n\n# Initialize Aim run and register callbacks\nrun = Run()\ntraining_flow = TrainingFlow(run=run)\ntraining_flow.register(MyCallbacks())\n\n\n# Training simulation\nfor _ in range(100):\n    loss = random.random() * 2\n    print(loss)\n    run.track(loss, name='loss', context={'subset': 'train'})\n    training_flow.training_metrics_collected(\n        metrics={'loss': loss},\n        run=run\n    )\n    time.sleep(1)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLogging and notifications\u003c/h2\u003e\n\u003cp\u003eDuring the training many key events happen. Use new Aim logging API to log any text message and send the corresponding notification to slack or workplace. It is as easy as calling¬†\u003ccode\u003eaim_run.log_error(message)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt perfectly fits with callbacks API and enables programmatically defining training heuristics and including it as part of the code. A simple example is ‚Äúlet me know when the training run starts to overfit‚Äù.\u003c/p\u003e\n\u003cp\u003eRead more in the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\"\u003edocs\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eAim UI\u003c/h1\u003e\n\u003ch2\u003eChart legends\u003c/h2\u003e\n\u003cp\u003eSince the early days of Aim‚Äôs launch, one of the most requested features has been displaying chart legends. And finally it is here! The legends section makes it easy to learn metrics corresponding fields and hyper-params at a glance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TFNCZjjwXFg7JZv1F1qSpg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAudios Explorer\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eTo track the performance of ML models that process speech data, it is important to track audio objects.\u003c/p\u003e\n\u003cp\u003eThe new Audios Explorer makes it super easy to query and play tracked audio objects across the runs. Flexible grouping options enable arranging them into rows and columns to make exploration even easier.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*mnP4qbmlQuJE-nGqe-36lw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eExperiment page\u003c/h2\u003e\n\u003cp\u003eThe experiment page provides an holistic view of the results of the trainings in order to identify trends and patterns that can help inform further experimentation and optimization.\u003c/p\u003e\n\u003cp\u003eExperiment page consists from the following tabs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eüè†¬†\u003cstrong\u003eOverview\u003c/strong\u003e¬†‚Äî Experiment overview, number of active/finished runs, timeline of the runs.\u003c/li\u003e\n\u003cli\u003eüèÉ¬†\u003cstrong\u003eRuns\u003c/strong\u003e¬†‚ÄîHolistic view of the experiment runs.\u003c/li\u003e\n\u003cli\u003eüìì¬†\u003cstrong\u003eNotes\u003c/strong\u003e¬†‚Äî Experiment notes and insights with notion-like rich editor.\u003c/li\u003e\n\u003cli\u003e‚öôÔ∏è¬†\u003cstrong\u003eSettings\u003c/strong\u003e¬†‚Äî Experiment settings, such as name and description.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*h-DLE3XnBM5uGKBDRcu_iA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eIntegrations\u003c/h1\u003e\n\u003ch2\u003eIntegration with PaddlePaddle\u003c/h2\u003e\n\u003cp\u003eWith Aim v3.15 it takes just two steps to integrate Aim into your PaddlePaddle training script.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Step 1: Explicitly import the AimCallback for tracking training metadata\nfrom aim.paddle import AimCallback\n\n...\n\n# Step 2: Pass the callback to callbacks list upon initiating your training\ncallback = AimCallback(repo='.', experiment='paddle_test')\nmodel.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee docs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-paddlepaddle\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eIntegration with Optuna\u003c/h2\u003e\n\u003cp\u003eSmoothly integrate Aim into your Optuna project.\u003c/p\u003e\n\u003cp\u003eHere are the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-optuna\"\u003edocs\u003c/a\u003e¬†and a short code snippet.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.optuna import AimCallback\n\n...\naim_callback = AimCallback(experiment_name=\"optuna_single_run\")\nstudy.optimize(objective, n_trials=10, callbacks=[aim_callback])\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\u003cem\u003eKey optimizations and fixes\u003c/em\u003e\u003c/h1\u003e\n\u003ch2\u003eRuns locking mechanism and finished runs detection\u003c/h2\u003e\n\u003cp\u003eNumber of issues were reported w.r.t runs locking. More robust locking mechanism is applied starting from Aim v3.15.\u003c/p\u003e\n\u003ch2\u003eRemote server scaling\u003c/h2\u003e\n\u003cp\u003eNow it is possible to spin up several workers to achieve higher processing speed for remote tracking server. Just run the server passing ‚Äúworkers‚Äù argument:¬†\u003ccode\u003eaim server --workers=N\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://discord.gg/F5TpzUeY\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful ‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/aim-3-15-‚Äî-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","_raw":{"sourceFilePath":"posts/aim-3-15-‚Äî-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","sourceFileName":"aim-3-15-‚Äî-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-15-‚Äî-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer"},"type":"Post"},{"title":"Aim 3.19 -- New HTTP-based tracking server, Support of Artifacts logging","date":"2024-03-14T19:09:56.893Z","author":"Tatyana Manaseryan","description":"Hey community, Aim 3.19 is out!! It's packed with new enhancements, such as Artifacts support, New HTTP-based tracking server.","slug":"aim-3-19-new-http-based-tracking-server-support-of-artifacts-logging","image":"/images/dynamic/twitter1-2-.png","draft":false,"categories":["NewReleases"],"body":{"raw":"Hey community, Aim 3.19 is out!! In this blogpost I will share the latest improvements.\n\n\n\nStay tuned on the¬†[AimStack twitter ](https://twitter.com/aimstackio)where we will share more use-cases.\n\n### Aim is evolving!! üöÄ\n\n\u003e We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\u003e\n\u003e Thanks to¬†[dushyantbehl](https://github.com/dushyantbehl), [martenlienen](https://github.com/martenlienen) and¬†[angusmf1](https://github.com/angusmf1) for their first contributions!! üî•\n\n## Here are the key highlights:\n\n\\- Support of Artifacts Logging to S3\n\n\\- New HTTP-based remote tracking server\n\n\\- Allow HF callback to initialize Aim run at init end\n\n## Support of Artifacts logging\n\n\n\nThe ability to log files/artifacts is a key feature for experiment trackers. Examples of such artifacts are the model checkpoints, training run configuration files, etc.\n\nStarting from the version 3.19, Aim provides the logging API for artifacts, and the UI for showing artifact's metadata.\n\n![Run Artifacts](/images/dynamic/run-overview-artifacts.png \"Run Artifacts\")\n\n## Artifacts logging SDK\n\nThere are only two steps for logging artifacts with Aim:\n\n1. Set the artifacts storage URI for the Aim Run:\n\n```\nimport aim\nrun = aim.Run()\n# Use S3 as artifacts storage\nrun.set_artifacts_uri('s3://aim/artifacts/')\n```\n\nAim will create directory with the name of \\`run.hash\\` and store all artifacts there.\n\nNote that setting artifacts storage URI is required only once per run.\n\n2. Log artifact object:\n\n```\n# Log run configuration files as artifacts\nrun.log_artifact('config.yaml', name='run-config')\n```\n\n## Storage backends\n\nCurrent implementation logs the artifacts to S3 as we found out together withe the community that it‚Äôs the most practical use-case for many teams.\n\nOnce logged, the artifact metadata will appear in the Aim UI Run details page.\n\n## New HTTP-based tracking server\n\nThe first and still experimental implementation of the Aim remote tracking server have been GRPC-based and had lots of issues around it. \n\nThe new Aim tracking server facilitates real-time logging of experiments from remote locations. This  command launches the server with specified configurations, including host, port, and associated repository.\n\nLike the UI, the server can also run in production or development mode (better debugging options).\n\n## Allow HF callback to initialize Aim run at init end\n\n\n\n### Previously\n\nImplementation of Hugging Face callback in Aim initializes the¬†\\`Run\\`¬†object at training begin time. This means if the users wanted to track any additional metrics they had to track before training finished while using¬†\\`HFTrainer\\`¬†object.\n\n### Now\n\nThis modification allows \\`HFTrainer\\` users to add more custom metrics by accessing the run object via \\`callback.experiment\\`. \n\nYou can now effortlessly track additional metrics or parameters in conjunction with the default metrics produced by HF, enriching data collection and analysis capabilities.\n\nSee docs [here.](https://aimstack.readthedocs.io/en/latest/)[](https://aimstack.readthedocs.io/en/latest/)\n\nSee the Release Notes [here.](https://github.com/aimhubio/aim/releases/tag/v3.19.0)\n\nWe're developing new features and improvements that we can't wait to show you!!\n\nThanks for all love and support. Stay tuned! üöÄ\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)¬†\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the [Aim Community](https://community.aimstack.io/)[](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful ‚≠êÔ∏è","html":"\u003cp\u003eHey community, Aim 3.19 is out!! In this blogpost I will share the latest improvements.\u003c/p\u003e\n\u003cp\u003eStay tuned on the¬†\u003ca href=\"https://twitter.com/aimstackio\"\u003eAimStack twitter \u003c/a\u003ewhere we will share more use-cases.\u003c/p\u003e\n\u003ch3\u003eAim is evolving!! üöÄ\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eThanks to¬†\u003ca href=\"https://github.com/dushyantbehl\"\u003edushyantbehl\u003c/a\u003e, \u003ca href=\"https://github.com/martenlienen\"\u003emartenlienen\u003c/a\u003e and¬†\u003ca href=\"https://github.com/angusmf1\"\u003eangusmf1\u003c/a\u003e for their first contributions!! üî•\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eHere are the key highlights:\u003c/h2\u003e\n\u003cp\u003e- Support of Artifacts Logging to S3\u003c/p\u003e\n\u003cp\u003e- New HTTP-based remote tracking server\u003c/p\u003e\n\u003cp\u003e- Allow HF callback to initialize Aim run at init end\u003c/p\u003e\n\u003ch2\u003eSupport of Artifacts logging\u003c/h2\u003e\n\u003cp\u003eThe ability to log files/artifacts is a key feature for experiment trackers. Examples of such artifacts are the model checkpoints, training run configuration files, etc.\u003c/p\u003e\n\u003cp\u003eStarting from the version 3.19, Aim provides the logging API for artifacts, and the UI for showing artifact's metadata.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/run-overview-artifacts.png\" alt=\"Run Artifacts\" title=\"Run Artifacts\"\u003e\u003c/p\u003e\n\u003ch2\u003eArtifacts logging SDK\u003c/h2\u003e\n\u003cp\u003eThere are only two steps for logging artifacts with Aim:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSet the artifacts storage URI for the Aim Run:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eimport aim\nrun = aim.Run()\n# Use S3 as artifacts storage\nrun.set_artifacts_uri('s3://aim/artifacts/')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAim will create directory with the name of `run.hash` and store all artifacts there.\u003c/p\u003e\n\u003cp\u003eNote that setting artifacts storage URI is required only once per run.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eLog artifact object:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e# Log run configuration files as artifacts\nrun.log_artifact('config.yaml', name='run-config')\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eStorage backends\u003c/h2\u003e\n\u003cp\u003eCurrent implementation logs the artifacts to S3 as we found out together withe the community that it‚Äôs the most practical use-case for many teams.\u003c/p\u003e\n\u003cp\u003eOnce logged, the artifact metadata will appear in the Aim UI Run details page.\u003c/p\u003e\n\u003ch2\u003eNew HTTP-based tracking server\u003c/h2\u003e\n\u003cp\u003eThe first and still experimental implementation of the Aim remote tracking server have been GRPC-based and had lots of issues around it.\u003c/p\u003e\n\u003cp\u003eThe new Aim tracking server facilitates real-time logging of experiments from remote locations. This  command launches the server with specified configurations, including host, port, and associated repository.\u003c/p\u003e\n\u003cp\u003eLike the UI, the server can also run in production or development mode (better debugging options).\u003c/p\u003e\n\u003ch2\u003eAllow HF callback to initialize Aim run at init end\u003c/h2\u003e\n\u003ch3\u003ePreviously\u003c/h3\u003e\n\u003cp\u003eImplementation of Hugging Face callback in Aim initializes the¬†`Run`¬†object at training begin time. This means if the users wanted to track any additional metrics they had to track before training finished while using¬†`HFTrainer`¬†object.\u003c/p\u003e\n\u003ch3\u003eNow\u003c/h3\u003e\n\u003cp\u003eThis modification allows `HFTrainer` users to add more custom metrics by accessing the run object via `callback.experiment`.\u003c/p\u003e\n\u003cp\u003eYou can now effortlessly track additional metrics or parameters in conjunction with the default metrics produced by HF, enriching data collection and analysis capabilities.\u003c/p\u003e\n\u003cp\u003eSee docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003ehere.\u003c/a\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSee the Release Notes \u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.19.0\"\u003ehere.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe're developing new features and improvements that we can't wait to show you!!\u003c/p\u003e\n\u003cp\u003eThanks for all love and support. Stay tuned! üöÄ\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e¬†\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim Community\u003c/a\u003e\u003ca href=\"https://community.aimstack.io/\"\u003e\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful ‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/aim-3-19-new-http-based-tracking-server-support-of-artifacts-logging.md","_raw":{"sourceFilePath":"posts/aim-3-19-new-http-based-tracking-server-support-of-artifacts-logging.md","sourceFileName":"aim-3-19-new-http-based-tracking-server-support-of-artifacts-logging.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-19-new-http-based-tracking-server-support-of-artifacts-logging"},"type":"Post"},{"title":"Aim 3.2 ‚Äî Jupyter Notebook integration \u0026 Histograms","date":"2021-12-08T15:09:19.264Z","author":"Gev Soghomonian","description":"Aim 3.2 featuring Jupyter notebook integration is now available! üòä We are on a mission to democratize AI dev tools. ","slug":"aim-3-2-jupyter-notebook-integration-histograms","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQCHweFLuEHppnFsQOaUnw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.2 featuring Jupyter notebook integration is now available!¬†\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWithin a week after the¬†[v3.1](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)¬†we are releasing the v3.2 with a number of key highly requested features in:\n\n* Aim Jupyter Notebook extension (Colab is coming soon‚Ä¶)\n* Histogram tracking and visualization\n* Full-size images, images resize and render modes\n* Search autofill and suggest on the UI\n\n## Aim \u0026 Jupyter notebook\n\n\n\nNow you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with the¬†`aim`¬†package.\n\nYou don‚Äôt need to install anything other than Aim. Here are the¬†[docs](https://aimstack.readthedocs.io/en/latest/index.html).\\\nA quick demo of integrating Aim \u0026 Jupyter Notebook::\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*7Xqp7esswOPWZpRoDqSawg.gif \"It‚Äôs that easy to use Aim on your Jupyter notebook!\")\n\n## Histogram tracking and Visualization\n\nStarting from 3.2 you can track Distributions and visualize them as histograms on the run page.\n\nThis is how to track the weights distribution in your training code:\n\n```\nfrom aim import Distribution\n\nfor step in range(1000):\n    my_run.track(\n        Distribution(tensor), # Pass distribution\n        name='gradients', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'type': 'weights',\n        },\n    )\n```\n\nThe¬†[demo](http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51),¬†[docs](https://aimstack.readthedocs.io/en/latest/index.html)¬†and¬†[examples](https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py)¬†\n\nAnd this is how it looks on the UI.¬†Aim lets you visualize the distributions every layer at every tracked step.¬†Super powerful!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Il1FjcQVpYHyuKHUVAtS0A.gif \"Visualize every layer and every tracked step.\")\n\n## Full-size images, images resize and render modes\n\nNow you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.\n\nHere is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*zYuodUJ1ykDhrFD9rdYC_w.gif \"Easily resize the queried images\")\n\nWe have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*pTG1Z34twqOOo7x6yoo1YQ.gif \"Toggle between pixelated and smooth images render modes\")\n\nYou can also view full-size images by clicking on the top right corner magnifier on each images.\n\nBTW¬†the smooth / pixelated config also translates to the full-size images.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qvhjgoBWWCaT0Hm_6eXzOw.gif \"full-size images: easy!\")\n\n## Query Language autosuggest editor\n\nAnother highly requested feature.\n\nNow there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.\n\nHopefully¬†this makes it much easier to get started with Aim.Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*eHTlgK8CdxpUsWC_edw1rw.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eAim 3.2 featuring Jupyter notebook integration is now available!¬†\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\u003c/p\u003e\n\u003cp\u003eWithin a week after the¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003ev3.1\u003c/a\u003e¬†we are releasing the v3.2 with a number of key highly requested features in:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim Jupyter Notebook extension (Colab is coming soon‚Ä¶)\u003c/li\u003e\n\u003cli\u003eHistogram tracking and visualization\u003c/li\u003e\n\u003cli\u003eFull-size images, images resize and render modes\u003c/li\u003e\n\u003cli\u003eSearch autofill and suggest on the UI\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAim \u0026#x26; Jupyter notebook\u003c/h2\u003e\n\u003cp\u003eNow you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with the¬†\u003ccode\u003eaim\u003c/code\u003e¬†package.\u003c/p\u003e\n\u003cp\u003eYou don‚Äôt need to install anything other than Aim. Here are the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/index.html\"\u003edocs\u003c/a\u003e.\u003cbr\u003e\nA quick demo of integrating Aim \u0026#x26; Jupyter Notebook::\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*7Xqp7esswOPWZpRoDqSawg.gif\" alt=\"\" title=\"It‚Äôs that easy to use Aim on your Jupyter notebook!\"\u003e\u003c/p\u003e\n\u003ch2\u003eHistogram tracking and Visualization\u003c/h2\u003e\n\u003cp\u003eStarting from 3.2 you can track Distributions and visualize them as histograms on the run page.\u003c/p\u003e\n\u003cp\u003eThis is how to track the weights distribution in your training code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Distribution\n\nfor step in range(1000):\n    my_run.track(\n        Distribution(tensor), # Pass distribution\n        name='gradients', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'type': 'weights',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe¬†\u003ca href=\"http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51\"\u003edemo\u003c/a\u003e,¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/index.html\"\u003edocs\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py\"\u003eexamples\u003c/a\u003e¬†\u003c/p\u003e\n\u003cp\u003eAnd this is how it looks on the UI.¬†Aim lets you visualize the distributions every layer at every tracked step.¬†Super powerful!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Il1FjcQVpYHyuKHUVAtS0A.gif\" alt=\"\" title=\"Visualize every layer and every tracked step.\"\u003e\u003c/p\u003e\n\u003ch2\u003eFull-size images, images resize and render modes\u003c/h2\u003e\n\u003cp\u003eNow you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.\u003c/p\u003e\n\u003cp\u003eHere is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*zYuodUJ1ykDhrFD9rdYC_w.gif\" alt=\"\" title=\"Easily resize the queried images\"\u003e\u003c/p\u003e\n\u003cp\u003eWe have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*pTG1Z34twqOOo7x6yoo1YQ.gif\" alt=\"\" title=\"Toggle between pixelated and smooth images render modes\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can also view full-size images by clicking on the top right corner magnifier on each images.\u003c/p\u003e\n\u003cp\u003eBTW¬†the smooth / pixelated config also translates to the full-size images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qvhjgoBWWCaT0Hm_6eXzOw.gif\" alt=\"\" title=\"full-size images: easy!\"\u003e\u003c/p\u003e\n\u003ch2\u003eQuery Language autosuggest editor\u003c/h2\u003e\n\u003cp\u003eAnother highly requested feature.\u003c/p\u003e\n\u003cp\u003eNow there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.\u003c/p\u003e\n\u003cp\u003eHopefully¬†this makes it much easier to get started with Aim.Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*eHTlgK8CdxpUsWC_edw1rw.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community,\u003c/a\u003e share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-2-‚Äî-jupyter-notebook-integration-histograms.md","_raw":{"sourceFilePath":"posts/aim-3-2-‚Äî-jupyter-notebook-integration-histograms.md","sourceFileName":"aim-3-2-‚Äî-jupyter-notebook-integration-histograms.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-2-‚Äî-jupyter-notebook-integration-histograms"},"type":"Post"},{"title":"Aim 3.3 ‚Äî Audio \u0026 Text tracking, Plotly \u0026 Colab integrations","date":"2022-01-13T15:33:59.865Z","author":"Gev Soghomonian","description":"Introducing Aim 3.3: Audio \u0026 Text Tracking, Plotly \u0026 Colab Integration, Images visualization on run details page. Democratizing AI dev tools with new features. Explore the enhancements now!","slug":"aim-3-3-audio-text-tracking-plotly-colab-integrations","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*300ZXHcgylKBit_O2KDeMA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, Aim 3.3 is now available! üòä\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.\n\nAim 3.3 is full of highly requested features. Lots of items scratched from the public Aim¬†[roadmap](https://github.com/aimhubio/aim#roadmap).\n\n* Audio tracking and exploration\n* Text tracking and visualization\n* Scatter plot explorer\n* Colab integration\n* Plotly integration\n* Images visualization on run details page\n\nNew Demo:¬†[play.aimstack.io:10004](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a)\n\nDemo Code:¬†[github.com/osoblanco/FastSpeech2](https://github.com/osoblanco/FastSpeech2/blob/master/train.py)\n\nSpecial thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.\n\n## Audio tracking and exploration\n\nNow you can track audio files with Aim during your speech-to-text or other audio-involving experiments.\n\nIt will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.\n\nJust like tracking the metrics, we have enabled a simple API to track the audio.\n\n```\nfrom aim import Audio\n\nfor step in range(1000):\n    my_run.track(\n        Audio(arr), # Pass audio file or numpy array\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nHere is how it looks like on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*LkeFigHPnQM1drADXJ3yZQ.gif)\n\n## **Text tracking and visualization**\n\nUse this to compare text inputs and outputs during training\n\nSimilarly, you can also track text with Aim during your NLP experiments.\n\nHere is how the code looks like:\n\n```\nfrom aim import Text\n\nfor step in range(1000):\n    my_run.track(\n        Text(string), # Pass a string you want to track\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nThis is the end result on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*nkqUz9DecxaMmJoCUe_8Rw.gif \"Training info tracked as a text\")\n\n## Colab integration\n\nAfter we have integrated¬†[Aim to Jupyter](https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms), there were many requests to enable Aim on Colab too. Now it has arrived!¬†\n\nWith fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!\n\nHere is an example¬†[colab to get started with](https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing).\n\n\u003e Note: Please make sure to run all the cells to be able to use the UI as well\n\nSo this is how it looks on your browser:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*SjYF_Kq4WzWOodVzsjemvA.gif)\n\n## Plotly integration\n\nNow you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.\n\nThis is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.\n\n```\nfrom aim import Figure\n\nfor step in range(1000):\n    my_run.track(\n        Figure(fig_obj), # Pass any plotly figure\n        name='plotly_bars', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nThe end-result on the Aim Web UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*8xn3PH_Hzk6fotyZQqy-AQ.gif)\n\n## Images visualization on run details page\n\n\n\nAs we had launched the images tracking and visualization in[¬†3.1](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer), we haven‚Äôt enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.\n\nHere is how it looks on the UI\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*jnUogPgbzwX6GOmDaXHKMg.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs.\n\nAnd don‚Äôt forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eHey community, Aim 3.3 is now available! üòä\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.\u003c/p\u003e\n\u003cp\u003eAim 3.3 is full of highly requested features. Lots of items scratched from the public Aim¬†\u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eroadmap\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAudio tracking and exploration\u003c/li\u003e\n\u003cli\u003eText tracking and visualization\u003c/li\u003e\n\u003cli\u003eScatter plot explorer\u003c/li\u003e\n\u003cli\u003eColab integration\u003c/li\u003e\n\u003cli\u003ePlotly integration\u003c/li\u003e\n\u003cli\u003eImages visualization on run details page\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNew Demo:¬†\u003ca href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a\"\u003eplay.aimstack.io:10004\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDemo Code:¬†\u003ca href=\"https://github.com/osoblanco/FastSpeech2/blob/master/train.py\"\u003egithub.com/osoblanco/FastSpeech2\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSpecial thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.\u003c/p\u003e\n\u003ch2\u003eAudio tracking and exploration\u003c/h2\u003e\n\u003cp\u003eNow you can track audio files with Aim during your speech-to-text or other audio-involving experiments.\u003c/p\u003e\n\u003cp\u003eIt will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.\u003c/p\u003e\n\u003cp\u003eJust like tracking the metrics, we have enabled a simple API to track the audio.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Audio\n\nfor step in range(1000):\n    my_run.track(\n        Audio(arr), # Pass audio file or numpy array\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is how it looks like on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LkeFigHPnQM1drADXJ3yZQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eText tracking and visualization\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eUse this to compare text inputs and outputs during training\u003c/p\u003e\n\u003cp\u003eSimilarly, you can also track text with Aim during your NLP experiments.\u003c/p\u003e\n\u003cp\u003eHere is how the code looks like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Text\n\nfor step in range(1000):\n    my_run.track(\n        Text(string), # Pass a string you want to track\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the end result on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*nkqUz9DecxaMmJoCUe_8Rw.gif\" alt=\"\" title=\"Training info tracked as a text\"\u003e\u003c/p\u003e\n\u003ch2\u003eColab integration\u003c/h2\u003e\n\u003cp\u003eAfter we have integrated¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms\"\u003eAim to Jupyter\u003c/a\u003e, there were many requests to enable Aim on Colab too. Now it has arrived!¬†\u003c/p\u003e\n\u003cp\u003eWith fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!\u003c/p\u003e\n\u003cp\u003eHere is an example¬†\u003ca href=\"https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing\"\u003ecolab to get started with\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: Please make sure to run all the cells to be able to use the UI as well\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSo this is how it looks on your browser:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*SjYF_Kq4WzWOodVzsjemvA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePlotly integration\u003c/h2\u003e\n\u003cp\u003eNow you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.\u003c/p\u003e\n\u003cp\u003eThis is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Figure\n\nfor step in range(1000):\n    my_run.track(\n        Figure(fig_obj), # Pass any plotly figure\n        name='plotly_bars', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe end-result on the Aim Web UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*8xn3PH_Hzk6fotyZQqy-AQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eImages visualization on run details page\u003c/h2\u003e\n\u003cp\u003eAs we had launched the images tracking and visualization in\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003e¬†3.1\u003c/a\u003e, we haven‚Äôt enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.\u003c/p\u003e\n\u003cp\u003eHere is how it looks on the UI\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*jnUogPgbzwX6GOmDaXHKMg.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-3-‚Äî-audio-text-tracking-plotly-colab-integrations.md","_raw":{"sourceFilePath":"posts/aim-3-3-‚Äî-audio-text-tracking-plotly-colab-integrations.md","sourceFileName":"aim-3-3-‚Äî-audio-text-tracking-plotly-colab-integrations.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-3-‚Äî-audio-text-tracking-plotly-colab-integrations"},"type":"Post"},{"title":"Aim 3.4 ‚Äì Remote Tracking Alpha, Sorting \u0026 deleting runs","date":"2022-01-24T16:06:51.383Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.4  featuring remote-tracking is now available! üòä Aim is an open source AI experiment tracking tool.","slug":"aim-3-4-remote-tracking-alpha-sorting-deleting-runs","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-Ubp6reeVEAahicQEz-SQ.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.4¬† featuring remote-tracking is now available!¬†\n\nAim is an open source AI experiment tracking tool. We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\nThis is a milestone release with lots of anticipated new features and one that‚Äôs super-anticipated (remote-tracking!). Brace yourselves, the collaborative Aim is here one commit at a time!!!\n\n## Here is what‚Äôs new:\n\n* Remote tracking \\[experimental]\n* Run delete and archive: batch and single\n* Ability to stack images on the¬†[Images Explorer](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)\n* Text filtering via regexp\n* Trendline on scatterplots\n* More images features: display images by original size, align by width, images reordering\n\nSpecial thanks to Geoffrey Chen, gormat, sjakkampudi, osoblanco, Sennevs, gloryVine, krstp and others for continuous feedback and help.\n\n## Remote tracking \\[experimental]\n\nThis is a milestone release with lots of anticipated new features and one that‚Äôs super-anticipated. Brace yourselves, the collaborative Aim is here one commit at a time!!!\n\nAim Remote Tracking is very simple and easy to get started with¬†\n\nHere are the steps to make it work. For more details pls check out¬†[the end-to-end guide on Aim docs](https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html).\n\n````\n### Ensure Aim version 3.4.x\n```sh\n$ pip install \"aim\u003e=3.4.0\"\n```\n\n### Initialize the remote server\n```sh\n$ aim init # (optional)\n$ aim server --repo \u003cREPO_PATH\u003e\n```\nOutput:\n```\n\u003e Server is mounted on 0.0.0.0:53800\n\u003e Press Ctrl+C to exit\n```\n### Start the Aim UI\n```\n$ aim up --repo \u003cREPO_PATH\u003e\n```\n\n### Change only 1 line in your training code\nPoint your repo to the remote location\n```py\naim_run = Run(repo='aim://172.3.66.145:53800')# replace example IP with your tracking server IP/hostname\n```\n````\n\n## Run, delete and archive: Batch and single\n\nOne of the very highly anticipated features by the Aim community. Now you can archive or hard delete your premature, interrupted runs from all the explorer pages as well as Run Details Settings tab.\n\nIt‚Äôs really easy to use. just select the run and the¬†`Delete`¬†/¬†`Archive`¬†buttons will appear.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qu9172RudbYfvFaVXVvNg.png)\n\n## Ability to stack images on the Images Explorer\n\nThis feature allows to not only view the tracked images sequentially, but also stack them by the last grouped param.\n\nOnce stacked, a slider appears that will allow you to navigate between the layers of stacked images. This feature will allow to closely observe the evolution of output images (GANs, wave outputs as img etc)\n\nHere is a quick demo¬†[link](http://play.aimstack.io:10002/images?grouping=7sKkRob88jEDScwcb7RLVkxZMAGNj8QPqfbeDBN11Hau1HzW4sr7FGSESSWwWH7qPYX4NvhSCtkgWXMTkHXEixu6soDSnPEYZ6CCa5fqUhbpWZ17EGMYDCknV4JErUVLaxyDtt2VjKEVSgEfGdWghLKLL5euM3YAF34MwNEtRVJxuXGAreiKEtXYbwbMHrs5JJ5kZ\u0026select=gTSaqzN7Mb8GdSizQfqNj9qCzd61EVTqUzAWmhAMFt1NVE2ABWM3PcYXbvEoHXrHBBwG51ytmWtG2AXtTVbUr8PVynYCpDzJqg4ZFEGauDS7PFw91DQ7nnxPzstQYFwbxWBQTR8x2eiUdScMgXh5XvkpPpn1HQpTbKz7ZgU1GXuezCrjcVXN9KGatx2S5iJAwNS4pLWpuE6HthgD9ww8sNzQzDBJuDHHHUSgkXDV2CoJqzAEE6RKf8gsdrWwP9GLovkHk43oMqPzwcmXCnUG4TFG7oL2HGaqXk2MSAN44RAMDbhwBH9nwV4KAyCHZD6kcrvdyBdadH92iPJMsW4cDQD26XaMsRxN3mWAqkvZPvAznnppWaVNFHv4V8q4kmemmoTNRi5jCD2r4b1ZgpuVN639cNsniPdQxPHm38bw56\u0026images=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)¬†to try out. Here is a glimpse of the UI:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdJPM1ggTMtdcri1htvZdg.png)\n\n## Text Search\n\nNow a comprehensive text searching is available to be able to outline the desired text. Regular Expressions are available among others.\n\nHere is a link for a quick¬†[demo](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/texts). And a glimpse of the UI:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt8sjFzpbhomdqRDwKb-Mw.png)\n\n## Trendline on scatterplots\n\nNow you can render a trendline over your scatterplots on Scatters Explorer. We have enabled linear and Loess trendline.\n\nThanks to¬†[sjakkampudi](https://github.com/aimhubio/aim/issues/1160)¬†for the help on this one.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pndAq3SIevuTckBZ20wwqg.png)\n\n## More Images Features\n\nWe have added a number of new tools to the Images Explorer\n\n## Display Images by original size\n\nNow you can view \u0026 compare the tracked images by their original size. This has been a key missing ingredient for effective images comparison.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXgyBHiVPAV_8l6-im_P3A.png)\n\n## Align by width\n\nYou can also align images by their width. This is especially handy when long images are tracked, such as sound spectograms (40px / 1000px). This feature will allow to effectively compare such images.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHHMux9rYd1sLRX4OjLCeQ.png)\n\n## Images Sorting\n\nYou can also apply sorting of images by any tracked available parameter. In this case I have enabled sorting by step in descending order to see the later generated images in my GAN experiments. A lot smoother than the ones above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4gA2EO7b0AI39YjcpLwIA.png)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eHey team, Aim 3.4¬† featuring remote-tracking is now available!¬†\u003c/p\u003e\n\u003cp\u003eAim is an open source AI experiment tracking tool. We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eThis is a milestone release with lots of anticipated new features and one that‚Äôs super-anticipated (remote-tracking!). Brace yourselves, the collaborative Aim is here one commit at a time!!!\u003c/p\u003e\n\u003ch2\u003eHere is what‚Äôs new:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemote tracking [experimental]\u003c/li\u003e\n\u003cli\u003eRun delete and archive: batch and single\u003c/li\u003e\n\u003cli\u003eAbility to stack images on the¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003eImages Explorer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eText filtering via regexp\u003c/li\u003e\n\u003cli\u003eTrendline on scatterplots\u003c/li\u003e\n\u003cli\u003eMore images features: display images by original size, align by width, images reordering\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpecial thanks to Geoffrey Chen, gormat, sjakkampudi, osoblanco, Sennevs, gloryVine, krstp and others for continuous feedback and help.\u003c/p\u003e\n\u003ch2\u003eRemote tracking [experimental]\u003c/h2\u003e\n\u003cp\u003eThis is a milestone release with lots of anticipated new features and one that‚Äôs super-anticipated. Brace yourselves, the collaborative Aim is here one commit at a time!!!\u003c/p\u003e\n\u003cp\u003eAim Remote Tracking is very simple and easy to get started with¬†\u003c/p\u003e\n\u003cp\u003eHere are the steps to make it work. For more details pls check out¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html\"\u003ethe end-to-end guide on Aim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e### Ensure Aim version 3.4.x\n```sh\n$ pip install \"aim\u003e=3.4.0\"\n```\n\n### Initialize the remote server\n```sh\n$ aim init # (optional)\n$ aim server --repo \u0026#x3C;REPO_PATH\u003e\n```\nOutput:\n```\n\u003e Server is mounted on 0.0.0.0:53800\n\u003e Press Ctrl+C to exit\n```\n### Start the Aim UI\n```\n$ aim up --repo \u0026#x3C;REPO_PATH\u003e\n```\n\n### Change only 1 line in your training code\nPoint your repo to the remote location\n```py\naim_run = Run(repo='aim://172.3.66.145:53800')# replace example IP with your tracking server IP/hostname\n```\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eRun, delete and archive: Batch and single\u003c/h2\u003e\n\u003cp\u003eOne of the very highly anticipated features by the Aim community. Now you can archive or hard delete your premature, interrupted runs from all the explorer pages as well as Run Details Settings tab.\u003c/p\u003e\n\u003cp\u003eIt‚Äôs really easy to use. just select the run and the¬†\u003ccode\u003eDelete\u003c/code\u003e¬†/¬†\u003ccode\u003eArchive\u003c/code\u003e¬†buttons will appear.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qu9172RudbYfvFaVXVvNg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAbility to stack images on the Images Explorer\u003c/h2\u003e\n\u003cp\u003eThis feature allows to not only view the tracked images sequentially, but also stack them by the last grouped param.\u003c/p\u003e\n\u003cp\u003eOnce stacked, a slider appears that will allow you to navigate between the layers of stacked images. This feature will allow to closely observe the evolution of output images (GANs, wave outputs as img etc)\u003c/p\u003e\n\u003cp\u003eHere is a quick demo¬†\u003ca href=\"http://play.aimstack.io:10002/images?grouping=7sKkRob88jEDScwcb7RLVkxZMAGNj8QPqfbeDBN11Hau1HzW4sr7FGSESSWwWH7qPYX4NvhSCtkgWXMTkHXEixu6soDSnPEYZ6CCa5fqUhbpWZ17EGMYDCknV4JErUVLaxyDtt2VjKEVSgEfGdWghLKLL5euM3YAF34MwNEtRVJxuXGAreiKEtXYbwbMHrs5JJ5kZ\u0026#x26;select=gTSaqzN7Mb8GdSizQfqNj9qCzd61EVTqUzAWmhAMFt1NVE2ABWM3PcYXbvEoHXrHBBwG51ytmWtG2AXtTVbUr8PVynYCpDzJqg4ZFEGauDS7PFw91DQ7nnxPzstQYFwbxWBQTR8x2eiUdScMgXh5XvkpPpn1HQpTbKz7ZgU1GXuezCrjcVXN9KGatx2S5iJAwNS4pLWpuE6HthgD9ww8sNzQzDBJuDHHHUSgkXDV2CoJqzAEE6RKf8gsdrWwP9GLovkHk43oMqPzwcmXCnUG4TFG7oL2HGaqXk2MSAN44RAMDbhwBH9nwV4KAyCHZD6kcrvdyBdadH92iPJMsW4cDQD26XaMsRxN3mWAqkvZPvAznnppWaVNFHv4V8q4kmemmoTNRi5jCD2r4b1ZgpuVN639cNsniPdQxPHm38bw56\u0026#x26;images=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\"\u003elink\u003c/a\u003e¬†to try out. Here is a glimpse of the UI:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdJPM1ggTMtdcri1htvZdg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eText Search\u003c/h2\u003e\n\u003cp\u003eNow a comprehensive text searching is available to be able to outline the desired text. Regular Expressions are available among others.\u003c/p\u003e\n\u003cp\u003eHere is a link for a quick¬†\u003ca href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/texts\"\u003edemo\u003c/a\u003e. And a glimpse of the UI:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt8sjFzpbhomdqRDwKb-Mw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eTrendline on scatterplots\u003c/h2\u003e\n\u003cp\u003eNow you can render a trendline over your scatterplots on Scatters Explorer. We have enabled linear and Loess trendline.\u003c/p\u003e\n\u003cp\u003eThanks to¬†\u003ca href=\"https://github.com/aimhubio/aim/issues/1160\"\u003esjakkampudi\u003c/a\u003e¬†for the help on this one.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pndAq3SIevuTckBZ20wwqg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eMore Images Features\u003c/h2\u003e\n\u003cp\u003eWe have added a number of new tools to the Images Explorer\u003c/p\u003e\n\u003ch2\u003eDisplay Images by original size\u003c/h2\u003e\n\u003cp\u003eNow you can view \u0026#x26; compare the tracked images by their original size. This has been a key missing ingredient for effective images comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXgyBHiVPAV_8l6-im_P3A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAlign by width\u003c/h2\u003e\n\u003cp\u003eYou can also align images by their width. This is especially handy when long images are tracked, such as sound spectograms (40px / 1000px). This feature will allow to effectively compare such images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHHMux9rYd1sLRX4OjLCeQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eImages Sorting\u003c/h2\u003e\n\u003cp\u003eYou can also apply sorting of images by any tracked available parameter. In this case I have enabled sorting by step in descending order to see the later generated images in my GAN experiments. A lot smoother than the ones above.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4gA2EO7b0AI39YjcpLwIA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-4-‚Äì-remote-tracking-alpha-sorting-deleting-runs.md","_raw":{"sourceFilePath":"posts/aim-3-4-‚Äì-remote-tracking-alpha-sorting-deleting-runs.md","sourceFileName":"aim-3-4-‚Äì-remote-tracking-alpha-sorting-deleting-runs.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-4-‚Äì-remote-tracking-alpha-sorting-deleting-runs"},"type":"Post"},{"title":"Aim 3.5 ‚Äî TensorBoard logs support, Matplotlib integration \u0026 System Params logging","date":"2022-02-10T16:15:32.942Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.5 featuring TensorBoard logs support is now available!! We are on a mission to democratize MLOps tools. ","slug":"aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xQnlTBk3BmsoMgsaE_0oQ.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.5 featuring TensorBoard logs support is now available!!¬†We are on a mission to democratize MLOps tools.¬†Thanks to the awesome Aim community for the help and contributions.\n\nHere is what‚Äôs new:\n\n* Aim supports TensorBoard logs\n* Track system params, CLI ,Env, Executable, Git, Installed packages\n* Matplotlib figure tracking and visualization\n* Ability move runs between Aim repos\n\n\u003e Special thanks to¬†[gloryVine](https://github.com/gloryVine),¬†[hughperkins](https://github.com/hughperkins),¬†[Ssamdav](https://github.com/SSamDav),¬†[ptaejoon](https://github.com/ptaejoon), mahnerak, gormat and¬†[Mike](https://github.com/mikel-brostrom)l for feedback, issues and help.\n\n## TensorBoard logs support\n\nIt‚Äôs been one of the most highly requested features by the Aim community. This feature was available for Aim 2.x and folks used to love this it. However, we had to drop it due to the backend changes.\n\nNow its back! Better than it was before and this is how it works:\n\n```\n$ cd /path/to/.aim\n$ aim convert tf --logdir ~/tensorflow/logdir\n```\n\nThis command will¬† scan then convert the¬†`scalar`¬†and¬†`image`¬†type logs from your directory into Aim runs.\n\nRead more about how it works¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-tensorboard-logs-in-aim).\n\n## Tracking Env info, git info with Aim\n\nTracking your ENV variables, CLI argument, git info, etc could be a lot of details to care about.\n\nNow there is a way to enable Aim to track the environment info automatically and they will be available as params.\n\nIt takes a small tweak to enable that:\n\n```\nrun = Run(log_system_params=True)\n```\n\nThen once tracked, you can search experiments based on these values too:\n\n```\nrun.__system_params.git_info.branch == 'feature/testing'\n```\n\nMore on this feature find out¬†[here](https://aimstack.readthedocs.io/en/latest/using/configure_runs.html#how-to-enable-system-params-automatic-logging).\n\n\u003e A special UI for these tracked data is to be shipped with the next version.\n\n## Tracking Matplotlib figures\n\nStarting Aim 3.5 you can also track¬†[Matplotlib](https://matplotlib.org/)¬†figures with Aim. During research (especially with Jupyter Notebooks) Matplotlib is very helpful in rendering intermediate images for analysis.\n\nNow you can track all such figures on Aim (both as Matplotlib figure and as an image). When tracking as an image, you can query and compare them too at scale on the¬†[Images Explorer.](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYdPka8XdPLAOxLdITESAw.png)\n\n[Here are the docs for more details](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#figure-tracking-with-aim).\n\n## Moving Runs between Aim repos\n\nWe have added a CLI command to move runs between folders.\n\nThis will allow to easily move your best runs from a draft scratch project to your main one with one command. Here is how it works:\n\n```\nim mv --destination /new/path/to/.aim \u003cmy_run_hash_1\u003e ...\n```\n\n\\\nFor more info, check the runs subcommand docs¬†[here](https://aimstack.readthedocs.io/en/latest/refs/cli.html#runs).\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling¬† and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"\u003cp\u003eHey team, Aim 3.5 featuring TensorBoard logs support is now available!!¬†We are on a mission to democratize MLOps tools.¬†Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what‚Äôs new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim supports TensorBoard logs\u003c/li\u003e\n\u003cli\u003eTrack system params, CLI ,Env, Executable, Git, Installed packages\u003c/li\u003e\n\u003cli\u003eMatplotlib figure tracking and visualization\u003c/li\u003e\n\u003cli\u003eAbility move runs between Aim repos\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to¬†\u003ca href=\"https://github.com/gloryVine\"\u003egloryVine\u003c/a\u003e,¬†\u003ca href=\"https://github.com/hughperkins\"\u003ehughperkins\u003c/a\u003e,¬†\u003ca href=\"https://github.com/SSamDav\"\u003eSsamdav\u003c/a\u003e,¬†\u003ca href=\"https://github.com/ptaejoon\"\u003eptaejoon\u003c/a\u003e, mahnerak, gormat and¬†\u003ca href=\"https://github.com/mikel-brostrom\"\u003eMike\u003c/a\u003el for feedback, issues and help.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eTensorBoard logs support\u003c/h2\u003e\n\u003cp\u003eIt‚Äôs been one of the most highly requested features by the Aim community. This feature was available for Aim 2.x and folks used to love this it. However, we had to drop it due to the backend changes.\u003c/p\u003e\n\u003cp\u003eNow its back! Better than it was before and this is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd /path/to/.aim\n$ aim convert tf --logdir ~/tensorflow/logdir\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis command will¬† scan then convert the¬†\u003ccode\u003escalar\u003c/code\u003e¬†and¬†\u003ccode\u003eimage\u003c/code\u003e¬†type logs from your directory into Aim runs.\u003c/p\u003e\n\u003cp\u003eRead more about how it works¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-tensorboard-logs-in-aim\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eTracking Env info, git info with Aim\u003c/h2\u003e\n\u003cp\u003eTracking your ENV variables, CLI argument, git info, etc could be a lot of details to care about.\u003c/p\u003e\n\u003cp\u003eNow there is a way to enable Aim to track the environment info automatically and they will be available as params.\u003c/p\u003e\n\u003cp\u003eIt takes a small tweak to enable that:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun = Run(log_system_params=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen once tracked, you can search experiments based on these values too:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.__system_params.git_info.branch == 'feature/testing'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMore on this feature find out¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/configure_runs.html#how-to-enable-system-params-automatic-logging\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA special UI for these tracked data is to be shipped with the next version.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eTracking Matplotlib figures\u003c/h2\u003e\n\u003cp\u003eStarting Aim 3.5 you can also track¬†\u003ca href=\"https://matplotlib.org/\"\u003eMatplotlib\u003c/a\u003e¬†figures with Aim. During research (especially with Jupyter Notebooks) Matplotlib is very helpful in rendering intermediate images for analysis.\u003c/p\u003e\n\u003cp\u003eNow you can track all such figures on Aim (both as Matplotlib figure and as an image). When tracking as an image, you can query and compare them too at scale on the¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003eImages Explorer.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYdPka8XdPLAOxLdITESAw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#figure-tracking-with-aim\"\u003eHere are the docs for more details\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eMoving Runs between Aim repos\u003c/h2\u003e\n\u003cp\u003eWe have added a CLI command to move runs between folders.\u003c/p\u003e\n\u003cp\u003eThis will allow to easily move your best runs from a draft scratch project to your main one with one command. Here is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eim mv --destination /new/path/to/.aim \u0026#x3C;my_run_hash_1\u003e ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cbr\u003e\nFor more info, check the runs subcommand docs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/refs/cli.html#runs\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling¬† and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-5-‚Äî-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","_raw":{"sourceFilePath":"posts/aim-3-5-‚Äî-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","sourceFileName":"aim-3-5-‚Äî-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-5-‚Äî-tensorboard-logs-support-matplotlib-integration-system-params-logging"},"type":"Post"},{"title":"Aim 3.6 ‚Äî Chart export, PyTorch Ignite \u0026 Activeloop Hub integrations","date":"2022-02-24T16:27:07.254Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite \u0026 Activeloop Hub integrations is now available!","slug":"aim-3-6-chart-export-pytorch-ignite-activeloop-hub-integrations","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-4kxMg5kF4ntVhW5889Xw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite \u0026 Activeloop Hub integrations is now available!\n\nWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is what‚Äôs new in addition to¬†[Aim 3.5](https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging).\n\n* Export chart as image\n* MLflow to Aim logs converter\n* Pytorch Ignite integration\n* Activeloop Hub integration\n* Wildcard support for¬†*aim runs*¬†subcommands\n\n*Special thanks to¬†[krstp](https://github.com/krstp),¬†[ptaejoon](https://github.com/ptaejoon),¬†[farizrahman4u](https://github.com/farizrahman4u)¬†from Activeloop,¬†[vfdev-5](https://github.com/vfdev-5)¬†from PyTorch Ignite,¬†[hughperkins](https://github.com/hughperkins),¬†[Ssamdav](https://github.com/ssamdav)¬†and¬†[mahnerak](https://github.com/mahnerak)¬†for feedback, reviews and help.*\n\n## Export chart as image\n\nHey awesome Aim community, sorry for shipping¬†[this feature](https://github.com/aimhubio/aim/issues/339)¬†so late. We know it has been so highly requested, just couldn‚Äôt get our hands on it over and over again.\n\nNow the images export is here! You can export charts as JPEG, PNG and SVG so you can include the awesome Aim visuals in your reports and research papers.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0-dHK5OoYjUAWhyKfBWA.png)\n\nYou can control the width, height, the format of the downloadable image.\n\n\u003e The ability to add / edit legends will be added in the coming versions.\n\nWould still love to hear feedback on how would you like us to tune this feature.\n\n## MLflow to Aim logs converter\n\nIts been also¬†[requested](https://github.com/aimhubio/aim/issues/409)¬†across the board by many users. Now you can convert your MLflow logs to Aim so you can compare all of them regardless on where it was tracked.\n\nHere is how it works:\n\n```\n$ aim init\n$ aim convert mlflow --tracking_uri 'file:///Users/aim_user/mlruns'\n```\n\nBesides the metrics, You can convert Images, Texts and Sound/Audios. More on that in¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-mlflow-logs-in-aim).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idkzKK4Ua8N3ATiokIx_rQ.png)\n\n## Pytorch Ignite integration\n\nFor all the Aim users who use PyTorch Ignite, there is now a native PI callback available so you can track your basic metrics automatically!\n\nHere is a code snippet on how to use the PI integration:\n\n```\n# call aim sdk designed for pytorch ignite\nfrom aim.pytorch_ignite import AimLogger\n\n# track experimential data by using Aim\naim_logger = AimLogger(\n    experiment='aim_on_pt_ignite',\n    train_metric_prefix='train_',\n    val_metric_prefix='val_',\n    test_metric_prefix='test_',\n)\n\n# track experimential data by using Aim\naim_logger.attach_output_handler(\n    train_evaluator,\n    event_name=Events.EPOCH_COMPLETED,\n    tag=\"train\",\n    metric_names=[\"nll\", \"accuracy\"],\n    global_step_transform=global_step_from_engine(trainer),\n)\n```\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKznHJq6W2XxmzyYpF17og.png)\n\n## Activeloop Hub integration\n\nExcited to share that thanks to¬†[an integration with Activeloop](https://activeloop.ai/), starting Aim 3.6 you can add your¬†[Hub dataset](https://github.com/activeloopai/Hub)¬†info as a run param and use them to search and compare your runs across all Explorers.\n\nHere is a code snippet to demonstrate the usage:\n\n```\nimport hub\n\nfrom aim.sdk.objects.plugins.hub_dataset import HubDataset\nfrom aim.sdk import Run\n\n# create dataset object\nds = hub.dataset('hub://activeloop/cifar100-test')\n\n# log dataset metadata\nrun = Run(system_tracking_interval=None)\nrun['hub_ds'] = HubDataset(ds)\n```\n\nThe following information becomes available among others:\n\n```\nrun.hub_ds.version\nrun.hub_ds.num_samples\nrun.hub_ds.tensors.name\nrun.hub_ds.tensors.compression_type\n```\n\nHub is an awesome tool to build, manage, query \u0026 visualize datasets for deep learning, as well as stream data real-time to PyTorch/TensorFlow \u0026 version-control it. Check out the¬†[Hub docs](https://docs.activeloop.ai/).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XlJrxPe3g9RYdqkvMaVUg.png)\n\nWildcard support for the \\`aim runs\\` command\n\nOn¬†[Aim 3.5](https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging)¬†we made available the¬†`run mv`¬†command which allows to move single runs between folders.\n\nNow we have also added the wildcard to move the whole folder altogether\n\nHere is the CLI interface:\n\n```\n$ aim runs --repo \u003csource_repo/.aim\u003e mv * --destination \u003cdest_repo/.aim\u003e\n```\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](http://github.com/aimhubio/aim)¬†a star on GitHub for support üôå.","html":"\u003cp\u003eHey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite \u0026#x26; Activeloop Hub integrations is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what‚Äôs new in addition to¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging\"\u003eAim 3.5\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExport chart as image\u003c/li\u003e\n\u003cli\u003eMLflow to Aim logs converter\u003c/li\u003e\n\u003cli\u003ePytorch Ignite integration\u003c/li\u003e\n\u003cli\u003eActiveloop Hub integration\u003c/li\u003e\n\u003cli\u003eWildcard support for¬†\u003cem\u003eaim runs\u003c/em\u003e¬†subcommands\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eSpecial thanks to¬†\u003ca href=\"https://github.com/krstp\"\u003ekrstp\u003c/a\u003e,¬†\u003ca href=\"https://github.com/ptaejoon\"\u003eptaejoon\u003c/a\u003e,¬†\u003ca href=\"https://github.com/farizrahman4u\"\u003efarizrahman4u\u003c/a\u003e¬†from Activeloop,¬†\u003ca href=\"https://github.com/vfdev-5\"\u003evfdev-5\u003c/a\u003e¬†from PyTorch Ignite,¬†\u003ca href=\"https://github.com/hughperkins\"\u003ehughperkins\u003c/a\u003e,¬†\u003ca href=\"https://github.com/ssamdav\"\u003eSsamdav\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/mahnerak\"\u003emahnerak\u003c/a\u003e¬†for feedback, reviews and help.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003eExport chart as image\u003c/h2\u003e\n\u003cp\u003eHey awesome Aim community, sorry for shipping¬†\u003ca href=\"https://github.com/aimhubio/aim/issues/339\"\u003ethis feature\u003c/a\u003e¬†so late. We know it has been so highly requested, just couldn‚Äôt get our hands on it over and over again.\u003c/p\u003e\n\u003cp\u003eNow the images export is here! You can export charts as JPEG, PNG and SVG so you can include the awesome Aim visuals in your reports and research papers.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0-dHK5OoYjUAWhyKfBWA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can control the width, height, the format of the downloadable image.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe ability to add / edit legends will be added in the coming versions.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWould still love to hear feedback on how would you like us to tune this feature.\u003c/p\u003e\n\u003ch2\u003eMLflow to Aim logs converter\u003c/h2\u003e\n\u003cp\u003eIts been also¬†\u003ca href=\"https://github.com/aimhubio/aim/issues/409\"\u003erequested\u003c/a\u003e¬†across the board by many users. Now you can convert your MLflow logs to Aim so you can compare all of them regardless on where it was tracked.\u003c/p\u003e\n\u003cp\u003eHere is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n$ aim convert mlflow --tracking_uri 'file:///Users/aim_user/mlruns'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBesides the metrics, You can convert Images, Texts and Sound/Audios. More on that in¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-mlflow-logs-in-aim\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idkzKK4Ua8N3ATiokIx_rQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePytorch Ignite integration\u003c/h2\u003e\n\u003cp\u003eFor all the Aim users who use PyTorch Ignite, there is now a native PI callback available so you can track your basic metrics automatically!\u003c/p\u003e\n\u003cp\u003eHere is a code snippet on how to use the PI integration:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# call aim sdk designed for pytorch ignite\nfrom aim.pytorch_ignite import AimLogger\n\n# track experimential data by using Aim\naim_logger = AimLogger(\n    experiment='aim_on_pt_ignite',\n    train_metric_prefix='train_',\n    val_metric_prefix='val_',\n    test_metric_prefix='test_',\n)\n\n# track experimential data by using Aim\naim_logger.attach_output_handler(\n    train_evaluator,\n    event_name=Events.EPOCH_COMPLETED,\n    tag=\"train\",\n    metric_names=[\"nll\", \"accuracy\"],\n    global_step_transform=global_step_from_engine(trainer),\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKznHJq6W2XxmzyYpF17og.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eActiveloop Hub integration\u003c/h2\u003e\n\u003cp\u003eExcited to share that thanks to¬†\u003ca href=\"https://activeloop.ai/\"\u003ean integration with Activeloop\u003c/a\u003e, starting Aim 3.6 you can add your¬†\u003ca href=\"https://github.com/activeloopai/Hub\"\u003eHub dataset\u003c/a\u003e¬†info as a run param and use them to search and compare your runs across all Explorers.\u003c/p\u003e\n\u003cp\u003eHere is a code snippet to demonstrate the usage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport hub\n\nfrom aim.sdk.objects.plugins.hub_dataset import HubDataset\nfrom aim.sdk import Run\n\n# create dataset object\nds = hub.dataset('hub://activeloop/cifar100-test')\n\n# log dataset metadata\nrun = Run(system_tracking_interval=None)\nrun['hub_ds'] = HubDataset(ds)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following information becomes available among others:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.hub_ds.version\nrun.hub_ds.num_samples\nrun.hub_ds.tensors.name\nrun.hub_ds.tensors.compression_type\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHub is an awesome tool to build, manage, query \u0026#x26; visualize datasets for deep learning, as well as stream data real-time to PyTorch/TensorFlow \u0026#x26; version-control it. Check out the¬†\u003ca href=\"https://docs.activeloop.ai/\"\u003eHub docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XlJrxPe3g9RYdqkvMaVUg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWildcard support for the `aim runs` command\u003c/p\u003e\n\u003cp\u003eOn¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging\"\u003eAim 3.5\u003c/a\u003e¬†we made available the¬†\u003ccode\u003erun mv\u003c/code\u003e¬†command which allows to move single runs between folders.\u003c/p\u003e\n\u003cp\u003eNow we have also added the wildcard to move the whole folder altogether\u003c/p\u003e\n\u003cp\u003eHere is the CLI interface:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim runs --repo \u0026#x3C;source_repo/.aim\u003e mv * --destination \u0026#x3C;dest_repo/.aim\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"http://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support üôå.\u003c/p\u003e"},"_id":"posts/aim-3-6-‚Äî-chart-export-pytorch-ignite-activeloop-hub-integrations.md","_raw":{"sourceFilePath":"posts/aim-3-6-‚Äî-chart-export-pytorch-ignite-activeloop-hub-integrations.md","sourceFileName":"aim-3-6-‚Äî-chart-export-pytorch-ignite-activeloop-hub-integrations.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-6-‚Äî-chart-export-pytorch-ignite-activeloop-hub-integrations"},"type":"Post"},{"title":"Aim 3.7 ‚Äî Revamped Run Single Page and Aim Docker Image","date":"2022-03-16T16:59:53.238Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.7 featuring Aim Docker Image is now available! We are on a mission to democratize MLOps tools.","slug":"aim-3-7-revamped-run-single-page-and-aim-docker-image","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INjZ3YZMiIgIc3EhLos_sw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.7 featuring Aim Docker Image is now available!\n\nWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\n[Aim](https://github.com/aimhubio/aim)¬†is an open-source, self-hosted ML experiment tracking tool. It‚Äôs good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\n\nYou can use not only the great Aim UI but also its SDK to query your runs‚Äô metadata programmatically. That‚Äôs especially useful for automations and additional analysis on a Jupyter Notebook.\n\nHere is what‚Äôs new in the V3.7 release:\n\n* Completely revamped Run Single Page\n* TF/Keras adaptors refactoring\n* Aim Docker Image\n\n\u003e Special thanks to osoblanco for the help, insights and feedback.\n\n## **Run Single Page Overview**\n\nWe have created an overview section on the Run Single page to quickly observe the relevant info about your before getting deep into it.\n\n* The last values of metrics\n* Flattened list of key params\n* Overview information about your run\n* Tracked reproducibility fields (env and git info, etc)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQXh2J_Clq2SE1EDq_ooQ.png)\n\nAim Docker Image\n\nNow you can pull the Aim docker image from the¬†[docker hub](https://hub.docker.com/r/aimstack/aim)!\n\nFor the users who are trying to set up Aim on their clusters or server infrastructure via docker, now you can use the native container ‚Äî released with each release.\n\nThe Aim docker container is pre-packaged Aim that can be mounted on¬†`.aim`¬†directory and UI could be ran.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjSOM1o9Dk4WNHgDEcE64w.png \"Aimstack docker image\")\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAlso check the recent blog post on¬†[Aim 3.6](https://aimstack.io/blog/new-releases/aim-3-6-chart-export-pytorch-ignite-activeloop-hub-integrations)¬†release featuring Mlflow logs converter \u0026 more!","html":"\u003cp\u003eHey team, Aim 3.7 featuring Aim Docker Image is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†is an open-source, self-hosted ML experiment tracking tool. It‚Äôs good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\u003c/p\u003e\n\u003cp\u003eYou can use not only the great Aim UI but also its SDK to query your runs‚Äô metadata programmatically. That‚Äôs especially useful for automations and additional analysis on a Jupyter Notebook.\u003c/p\u003e\n\u003cp\u003eHere is what‚Äôs new in the V3.7 release:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompletely revamped Run Single Page\u003c/li\u003e\n\u003cli\u003eTF/Keras adaptors refactoring\u003c/li\u003e\n\u003cli\u003eAim Docker Image\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to osoblanco for the help, insights and feedback.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\u003cstrong\u003eRun Single Page Overview\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eWe have created an overview section on the Run Single page to quickly observe the relevant info about your before getting deep into it.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe last values of metrics\u003c/li\u003e\n\u003cli\u003eFlattened list of key params\u003c/li\u003e\n\u003cli\u003eOverview information about your run\u003c/li\u003e\n\u003cli\u003eTracked reproducibility fields (env and git info, etc)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQXh2J_Clq2SE1EDq_ooQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim Docker Image\u003c/p\u003e\n\u003cp\u003eNow you can pull the Aim docker image from the¬†\u003ca href=\"https://hub.docker.com/r/aimstack/aim\"\u003edocker hub\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eFor the users who are trying to set up Aim on their clusters or server infrastructure via docker, now you can use the native container ‚Äî released with each release.\u003c/p\u003e\n\u003cp\u003eThe Aim docker container is pre-packaged Aim that can be mounted on¬†\u003ccode\u003e.aim\u003c/code\u003e¬†directory and UI could be ran.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjSOM1o9Dk4WNHgDEcE64w.png\" alt=\"\" title=\"Aimstack docker image\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAlso check the recent blog post on¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-6-chart-export-pytorch-ignite-activeloop-hub-integrations\"\u003eAim 3.6\u003c/a\u003e¬†release featuring Mlflow logs converter \u0026#x26; more!\u003c/p\u003e"},"_id":"posts/aim-3-7-‚Äî-revamped-run-single-page-and-aim-docker-image.md","_raw":{"sourceFilePath":"posts/aim-3-7-‚Äî-revamped-run-single-page-and-aim-docker-image.md","sourceFileName":"aim-3-7-‚Äî-revamped-run-single-page-and-aim-docker-image.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-7-‚Äî-revamped-run-single-page-and-aim-docker-image"},"type":"Post"},{"title":"Aim 3.8 ‚Äî DVC integration \u0026 Extensible HuggingFace callbacks","date":"2022-04-04T20:27:53.206Z","author":"Gev Soghomonian","description":"Aim 3.8 featuring extensible HuggingFace trainer callbacks is out ! We are on a mission to democratize AI dev tools. ","slug":"aim-3-8-dvc-integration-extensible-huggingface-callbacks","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIfbNRqB72W36j8gooKi4Q.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.8 featuring extensible HuggingFace trainer callbacks is out !\n\nWe are on a mission to[¬†democratize AI dev tools](https://aimstack.io/blog/tutorials/aim-from-zero-to-hero). Thanks to the awesome Aim community for the help and contributions.\n\nHere is what‚Äôs new:\n\n* Color scale of numeric values\n* DVC integration\n* Extensible HuggingFace and XGBoost callbacks\n\n\u003e Special thanks to osoblanco, ashutoshsaboo and mohamadelgaar for the help and feedback\n\n\\\nColor scale of numeric values\n\nDue to popular demand an Aim 2.x feature is back!\n\nNow you can color-code the aggregated metric values on the Runs Explorer. The highest values will be in green and the lowest in yellow. The color changes from yellow to green at the median.\n\nWe are surely going to iterate over this feature. Further feedbacks are welcome!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOgBqVnIeq3fc_EduNAAfw.png \"Aimstack: Color scale of numeric values\")\n\n## DVC integration\n\nNow you can track the info about your DVC tracked files and datasets on Aim. With this we are iterating towards easy connection between Aim and DVC so you can connect your tracked artifacts with experiment tracking.\n\nShare more feedback with us. How would you like to see this evolve?\n\nThe code looks as simple as this:\n\n```\nfrom aim.sdk import Run\nfrom aim.sdk.objects.plugins.dvc_metadata import DvcData\n\nrun = Run(system_tracking_interval=None)\n\npath_to_dvc_repo = '.'\nrun['dvc_info'] = DvcData(path_to_dvc_repo)\n```\n\n## Extensible HuggingFace and XGBoost callbacks\n\nWhen using Aim with your favorite frameworks, the metadata is logged through¬†`AimCallback`¬†which is limited as it allows only specific group of logged metrics per framework.\n\nNow you can extend the AimCallback and log any other metadata made available by the framework.¬†[Detailed docs here.](https://aimstack.readthedocs.io/en/latest/using/integration_guides.html)\n\nHere is an example on how to extend the HuggingFace callback to track texts with Aim:\n\n```\nfrom aim.hugging_face import AimCallback\nfrom aim import Text\n\n\nclass CustomCallback(AimCallback):\n    def on_log(self, args, state, control,\n               model=None, logs=None, **kwargs):\n        super().on_log(args, state, control, model, logs, **kwargs)\n\n        context = {\n            'subset': self._current_shift,\n        }\n        for log_name, log_value in logs.items():\n            if isinstance(log_value, str):\n                self.experiment.track(Text(log_value), name=log_name, context=context)\n```\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eAim 3.8 featuring extensible HuggingFace trainer callbacks is out !\u003c/p\u003e\n\u003cp\u003eWe are on a mission to\u003ca href=\"https://aimstack.io/blog/tutorials/aim-from-zero-to-hero\"\u003e¬†democratize AI dev tools\u003c/a\u003e. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what‚Äôs new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eColor scale of numeric values\u003c/li\u003e\n\u003cli\u003eDVC integration\u003c/li\u003e\n\u003cli\u003eExtensible HuggingFace and XGBoost callbacks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to osoblanco, ashutoshsaboo and mohamadelgaar for the help and feedback\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cbr\u003e\nColor scale of numeric values\u003c/p\u003e\n\u003cp\u003eDue to popular demand an Aim 2.x feature is back!\u003c/p\u003e\n\u003cp\u003eNow you can color-code the aggregated metric values on the Runs Explorer. The highest values will be in green and the lowest in yellow. The color changes from yellow to green at the median.\u003c/p\u003e\n\u003cp\u003eWe are surely going to iterate over this feature. Further feedbacks are welcome!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOgBqVnIeq3fc_EduNAAfw.png\" alt=\"\" title=\"Aimstack: Color scale of numeric values\"\u003e\u003c/p\u003e\n\u003ch2\u003eDVC integration\u003c/h2\u003e\n\u003cp\u003eNow you can track the info about your DVC tracked files and datasets on Aim. With this we are iterating towards easy connection between Aim and DVC so you can connect your tracked artifacts with experiment tracking.\u003c/p\u003e\n\u003cp\u003eShare more feedback with us. How would you like to see this evolve?\u003c/p\u003e\n\u003cp\u003eThe code looks as simple as this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.sdk import Run\nfrom aim.sdk.objects.plugins.dvc_metadata import DvcData\n\nrun = Run(system_tracking_interval=None)\n\npath_to_dvc_repo = '.'\nrun['dvc_info'] = DvcData(path_to_dvc_repo)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eExtensible HuggingFace and XGBoost callbacks\u003c/h2\u003e\n\u003cp\u003eWhen using Aim with your favorite frameworks, the metadata is logged through¬†\u003ccode\u003eAimCallback\u003c/code\u003e¬†which is limited as it allows only specific group of logged metrics per framework.\u003c/p\u003e\n\u003cp\u003eNow you can extend the AimCallback and log any other metadata made available by the framework.¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/integration_guides.html\"\u003eDetailed docs here.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example on how to extend the HuggingFace callback to track texts with Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hugging_face import AimCallback\nfrom aim import Text\n\n\nclass CustomCallback(AimCallback):\n    def on_log(self, args, state, control,\n               model=None, logs=None, **kwargs):\n        super().on_log(args, state, control, model, logs, **kwargs)\n\n        context = {\n            'subset': self._current_shift,\n        }\n        for log_name, log_value in logs.items():\n            if isinstance(log_value, str):\n                self.experiment.track(Text(log_value), name=log_name, context=context)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-8-‚Äî-dvc-integration-extensible-huggingface-callbacks.md","_raw":{"sourceFilePath":"posts/aim-3-8-‚Äî-dvc-integration-extensible-huggingface-callbacks.md","sourceFileName":"aim-3-8-‚Äî-dvc-integration-extensible-huggingface-callbacks.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-8-‚Äî-dvc-integration-extensible-huggingface-callbacks"},"type":"Post"},{"title":"Aim 3.9 ‚Äî Notes on training runs and upgrade to Pytorch Lightning 1.6 Pytorch Lightning","date":"2022-05-08T20:34:11.966Z","author":"Gev Soghomonian","description":"Discover Aim 3.9's latest features! Add notes on Single Run page, enhance Table component usability, and more. Read the article for full details!","slug":"aim-3-9-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoVjVDwfOytJ-FyRqp_n0Q.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.9 is now available!\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is what‚Äôs new:\n\n* Notes on training runs\n* Upgrade to Pytorch Lightning 1.6\n\n\u003e Special thanks to ashutoshsaboo, haritsahm and Justin Burton for the help and feedback.\n\n## Notes on the training runs\n\nNow there is an ability to add rich metadata edit (similar to notion) over the runs. It‚Äôs been a highly requested feature to be able to leave relevant code, result-related notes, next steps etc on the runs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghbd3OrbVatbwSwnwpIm-A.png)\n\n## Upgrade to Pytorch Lightning 1.6\n\nWe have upgraded the Aim Logger for Pytorch Lightning 1.6 updates.\n\nLearn more in the following thread:¬†\u003chttps://github.com/aimhubio/aim/issues/1619\u003e\n\nMeanwhile, check out an earlier¬†[tutorial](https://aimstack.io/blog/tutorials/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim)¬†on how to tune hyperparams with fixed seeds using PyTorch Lightning and Aim.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave Aim a star on GitHub for support","html":"\u003cp\u003eHey team, Aim 3.9 is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what‚Äôs new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNotes on training runs\u003c/li\u003e\n\u003cli\u003eUpgrade to Pytorch Lightning 1.6\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to ashutoshsaboo, haritsahm and Justin Burton for the help and feedback.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eNotes on the training runs\u003c/h2\u003e\n\u003cp\u003eNow there is an ability to add rich metadata edit (similar to notion) over the runs. It‚Äôs been a highly requested feature to be able to leave relevant code, result-related notes, next steps etc on the runs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghbd3OrbVatbwSwnwpIm-A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eUpgrade to Pytorch Lightning 1.6\u003c/h2\u003e\n\u003cp\u003eWe have upgraded the Aim Logger for Pytorch Lightning 1.6 updates.\u003c/p\u003e\n\u003cp\u003eLearn more in the following thread:¬†\u003ca href=\"https://github.com/aimhubio/aim/issues/1619\"\u003ehttps://github.com/aimhubio/aim/issues/1619\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMeanwhile, check out an earlier¬†\u003ca href=\"https://aimstack.io/blog/tutorials/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim\"\u003etutorial\u003c/a\u003e¬†on how to tune hyperparams with fixed seeds using PyTorch Lightning and Aim.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave Aim a star on GitHub for support\u003c/p\u003e"},"_id":"posts/aim-3-9-‚Äî-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","_raw":{"sourceFilePath":"posts/aim-3-9-‚Äî-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","sourceFileName":"aim-3-9-‚Äî-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-9-‚Äî-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning"},"type":"Post"},{"title":"AimOS: Open-source modular observability for AI Systems","date":"2023-10-12T20:09:25.239Z","author":"Gev Sogomonian, Gor Arakelyan, Ben Lorica","description":"Discover AimOS: Open-source modular observability for AI systems. Easily log, connect and observe any parts of your AI Systems from experiments to production to prompts to AI system monitoring","slug":"aim-4-0-open-source-modular-observability-for-ai-systems","image":"/images/dynamic/medium8.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The software lifecycle is broken\n\n\n\nAI is changing how software is built and deployed. AI Systems are not deterministic, so observability is a key layer in the AI Systems development stack.\n\nChanging the code is no longer enough to change the behavior of the AI system - a kind of systems that are inherently unreliable and hard to iterate on.\n\nWith the rise of the LLMs and Generative AI, there are many more use-cases that are built with AI. This means more software needs to be monitored and observed.\n\n\u003e The mainstream ideas of software logging and observability may not work anymore as the use-cases and needs have increased dramatically.\n\n## Software systems need to be logged\n\nLogging is inseparable from software all along - from code versioning to production monitoring to AI experiment tracking to agent tracing to full observability.\n\n![](/images/dynamic/1111-1.png)\n\n**With AI systems, we now have to deal with many more dimensions. Developing an AI System is as demanding as deploying it to production and integrating it with other software components.**\n\nWe are now faced with problems of AI alignment and safety. AI regulation conversations are moving at full speed. The AI system governance and lineage are key when tackling any of these problems.\n\nFor this, teams are trying to log as much as they can at every possible dimension.\n\nTo satisfy these needs, we must be able to:\n\n* Log anything from every part of the AI Infra\n* Observe and interpret the logged data at scale in a flexible fashion\n* Add layers and layers of automations around the logged data\n\nAltogether these form the observability and logging layer for AI Systems.\n\n\u003e The observability layer for AI Systems is inherently complex and needs to encompass end-to-end development + production and beyond to allow to fully connect the dots.\n\n## What do developers need from the AI Systems observability layer?\n\nWe need to log and observe at every step of building AI Systems.\n\n![Basic AI system observability](/images/dynamic/222-1.png \"Basic AI system observability\")\n\nOver the last three years, we have collaborated with hundreds of teams to enhance our understanding of observability requirements in AI systems. The capability to track diverse data sources stands out, given that logs are invaluable for discerning system health and facilitating infrastructure evolution.\n\nIn addition, the ability to customize log visualizations increases the depth of analysis, enabling teams to discern intricate relationships within their setups. The use of automation ensures not only optimal performance but also increased reliability for AI systems.\n\nThe lineage of an AI System is also vitally important, covering a wide range of considerations from governance to regulatory to best practices. The need for robust governance over AI Systems grows as compliance needs increase. In the end, we aim for an integrated approach to governance that provides detailed lineage insights and unwavering traceability.\n\n![Key requirements: AI Observability Layer](/images/dynamic/444-1.png \"Key requirements: AI Observability Layer\")\n\n### We need well-developed open-source tools for observability use-cases\n\nLogs are now essential for overcoming the fundamental challenges of AI adoption. It is crucial for organizations to maintain ownership of their logs and ensure that they remain in an open format without third-party control. As AI systems evolve, the tools used for observability must also mature.\n\n\u003e Ideally, these tools should be easy to use but powerful enough to meet next-stage infrastructure needs.\n\nUnfortunately, current mainstream observability tools are centralized, lack interoperability, and mostly run on closed-source platforms. These tools are not inherently scalable and often only address a fraction of the overall observability requirements.\n\n## Introducing AimOS - Operating system for logs\n\nAim began as a tool for tracking experiments, and it ran on a single node\n\n* Aim 2.0 - AI experiment logger for metrics and hyperparameters.\n* [Aim 3.0 ](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative)- AI experiment logger for any kind of Python objects.\n* **AimOS - Operating system for logs.** üí´\n\nAimOS runs logging applications that can easily be composed and extended to larger ones encompassing both the main and long-tail use-cases.\n\n![Modular observability for AI Systems](/images/dynamic/aimstack2.png \"Modular observability for AI Systems\")\n\nWithin the context of AimOS, logging applications are Python packages that AimOS can run.\n\n## There are many logging apps that need to exist\n\n![AimOS logging app structure](/images/dynamic/aim-logging-app-structure.png \"AimOS logging app structure\")\n\nThere are many logging uses-cases for AI systems, which necessitates collaboration across the community. Many of these use-cases are at the intersection of different tools and frameworks used in development and production.\n\nHere are some examples of apps that need to be built:\n\n* data visualization apps (+ automations)\n* model performance apps (+ automations),\n* model explainability apps + automations,\n* workflow visualization apps + automations,\n* model monitoring apps + automations,\n* specific regulation-apps + automations,\n* connector and integration apps + automations,\n* lineage apps + automations.\n\n## AimOS is simple first, then powerful\n\nUsers don‚Äôt have to know AimOS in-depth to be able to use it. You don‚Äôt even need to know if it‚Äôs an operating system - it takes a few commands and built-in packages to start with.\n\n![AimOS default apps](/images/dynamic/666-1-.png \"AimOS default apps\")\n\nUsing AimOS means installing AimOS and running an AimOS logging app. It comes with a number of built- apps that can go a long way:\n\n* Base app,\n* Experiment tracking app,\n* AI Systems tracing app.\n\nIt takes only a couple of commands to start with AimOS.\n\nWe have published high-level roadmap about the next steps. \n\n\u003e Checkout the AimOS documentation [here](https://aimos.readthedocs.io/en/latest/#).\n\nThe contributors to core Aim and package creators are very welcome to help us build out the future of logging infrastructure for AI Systems and beyond.\n\n## **Learn more**\n\n[AimOS is on a mission to democratize AI Systems logging tools.](https://aimos.readthedocs.io/en/latest/apps/overview.html)¬†üôå\n\nTry out¬†[AimOS](https://github.com/aimhubio/aimos), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aimos/tree/main)¬†if you think AimOS is useful, and here is the link to [Aim,](https://github.com/aimhubio/aim) an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003ch2\u003eThe software lifecycle is broken\u003c/h2\u003e\n\u003cp\u003eAI is changing how software is built and deployed. AI Systems are not deterministic, so observability is a key layer in the AI Systems development stack.\u003c/p\u003e\n\u003cp\u003eChanging the code is no longer enough to change the behavior of the AI system - a kind of systems that are inherently unreliable and hard to iterate on.\u003c/p\u003e\n\u003cp\u003eWith the rise of the LLMs and Generative AI, there are many more use-cases that are built with AI. This means more software needs to be monitored and observed.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe mainstream ideas of software logging and observability may not work anymore as the use-cases and needs have increased dramatically.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eSoftware systems need to be logged\u003c/h2\u003e\n\u003cp\u003eLogging is inseparable from software all along - from code versioning to production monitoring to AI experiment tracking to agent tracing to full observability.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1111-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith AI systems, we now have to deal with many more dimensions. Developing an AI System is as demanding as deploying it to production and integrating it with other software components.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe are now faced with problems of AI alignment and safety. AI regulation conversations are moving at full speed. The AI system governance and lineage are key when tackling any of these problems.\u003c/p\u003e\n\u003cp\u003eFor this, teams are trying to log as much as they can at every possible dimension.\u003c/p\u003e\n\u003cp\u003eTo satisfy these needs, we must be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLog anything from every part of the AI Infra\u003c/li\u003e\n\u003cli\u003eObserve and interpret the logged data at scale in a flexible fashion\u003c/li\u003e\n\u003cli\u003eAdd layers and layers of automations around the logged data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAltogether these form the observability and logging layer for AI Systems.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe observability layer for AI Systems is inherently complex and needs to encompass end-to-end development + production and beyond to allow to fully connect the dots.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eWhat do developers need from the AI Systems observability layer?\u003c/h2\u003e\n\u003cp\u003eWe need to log and observe at every step of building AI Systems.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/222-1.png\" alt=\"Basic AI system observability\" title=\"Basic AI system observability\"\u003e\u003c/p\u003e\n\u003cp\u003eOver the last three years, we have collaborated with hundreds of teams to enhance our understanding of observability requirements in AI systems. The capability to track diverse data sources stands out, given that logs are invaluable for discerning system health and facilitating infrastructure evolution.\u003c/p\u003e\n\u003cp\u003eIn addition, the ability to customize log visualizations increases the depth of analysis, enabling teams to discern intricate relationships within their setups. The use of automation ensures not only optimal performance but also increased reliability for AI systems.\u003c/p\u003e\n\u003cp\u003eThe lineage of an AI System is also vitally important, covering a wide range of considerations from governance to regulatory to best practices. The need for robust governance over AI Systems grows as compliance needs increase. In the end, we aim for an integrated approach to governance that provides detailed lineage insights and unwavering traceability.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/444-1.png\" alt=\"Key requirements: AI Observability Layer\" title=\"Key requirements: AI Observability Layer\"\u003e\u003c/p\u003e\n\u003ch3\u003eWe need well-developed open-source tools for observability use-cases\u003c/h3\u003e\n\u003cp\u003eLogs are now essential for overcoming the fundamental challenges of AI adoption. It is crucial for organizations to maintain ownership of their logs and ensure that they remain in an open format without third-party control. As AI systems evolve, the tools used for observability must also mature.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIdeally, these tools should be easy to use but powerful enough to meet next-stage infrastructure needs.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eUnfortunately, current mainstream observability tools are centralized, lack interoperability, and mostly run on closed-source platforms. These tools are not inherently scalable and often only address a fraction of the overall observability requirements.\u003c/p\u003e\n\u003ch2\u003eIntroducing AimOS - Operating system for logs\u003c/h2\u003e\n\u003cp\u003eAim began as a tool for tracking experiments, and it ran on a single node\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim 2.0 - AI experiment logger for metrics and hyperparameters.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003eAim 3.0 \u003c/a\u003e- AI experiment logger for any kind of Python objects.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAimOS - Operating system for logs.\u003c/strong\u003e üí´\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAimOS runs logging applications that can easily be composed and extended to larger ones encompassing both the main and long-tail use-cases.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/aimstack2.png\" alt=\"Modular observability for AI Systems\" title=\"Modular observability for AI Systems\"\u003e\u003c/p\u003e\n\u003cp\u003eWithin the context of AimOS, logging applications are Python packages that AimOS can run.\u003c/p\u003e\n\u003ch2\u003eThere are many logging apps that need to exist\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/aim-logging-app-structure.png\" alt=\"AimOS logging app structure\" title=\"AimOS logging app structure\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are many logging uses-cases for AI systems, which necessitates collaboration across the community. Many of these use-cases are at the intersection of different tools and frameworks used in development and production.\u003c/p\u003e\n\u003cp\u003eHere are some examples of apps that need to be built:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edata visualization apps (+ automations)\u003c/li\u003e\n\u003cli\u003emodel performance apps (+ automations),\u003c/li\u003e\n\u003cli\u003emodel explainability apps + automations,\u003c/li\u003e\n\u003cli\u003eworkflow visualization apps + automations,\u003c/li\u003e\n\u003cli\u003emodel monitoring apps + automations,\u003c/li\u003e\n\u003cli\u003especific regulation-apps + automations,\u003c/li\u003e\n\u003cli\u003econnector and integration apps + automations,\u003c/li\u003e\n\u003cli\u003elineage apps + automations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAimOS is simple first, then powerful\u003c/h2\u003e\n\u003cp\u003eUsers don‚Äôt have to know AimOS in-depth to be able to use it. You don‚Äôt even need to know if it‚Äôs an operating system - it takes a few commands and built-in packages to start with.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/666-1-.png\" alt=\"AimOS default apps\" title=\"AimOS default apps\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing AimOS means installing AimOS and running an AimOS logging app. It comes with a number of built- apps that can go a long way:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBase app,\u003c/li\u003e\n\u003cli\u003eExperiment tracking app,\u003c/li\u003e\n\u003cli\u003eAI Systems tracing app.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt takes only a couple of commands to start with AimOS.\u003c/p\u003e\n\u003cp\u003eWe have published high-level roadmap about the next steps.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCheckout the AimOS documentation \u003ca href=\"https://aimos.readthedocs.io/en/latest/#\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe contributors to core Aim and package creators are very welcome to help us build out the future of logging infrastructure for AI Systems and beyond.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eLearn more\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools.\u003c/a\u003e¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aimos/tree/main\"\u003eGitHub\u003c/a\u003e¬†if you think AimOS is useful, and here is the link to \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim,\u003c/a\u003e an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/aim-4-0-open-source-modular-observability-for-ai-systems.md","_raw":{"sourceFilePath":"posts/aim-4-0-open-source-modular-observability-for-ai-systems.md","sourceFileName":"aim-4-0-open-source-modular-observability-for-ai-systems.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-4-0-open-source-modular-observability-for-ai-systems"},"type":"Post"},{"title":"Aim 4.0 moved to a new repo AimOS. Here is why?","date":"2023-10-13T20:01:12.714Z","author":"Gev Sogomonian","description":"We have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.","slug":"aim-4-0-to-a-new-repo-aimos-here-is-why","image":"/images/dynamic/aimos-banner-1-.png","draft":false,"categories":["New Releases"],"body":{"raw":"We have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.\n\nRight now you can access the original Aim Experiment Tracker as is through the Aim repo. You can also access the Aim 4.0 as AimOS - an end-to-end modular observability tool for AI systems.\n\n## Why the move?\n\n\n\nThe [AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems) mission is to enable a modular end-to-end observability for the AI Systems.\n\n\n\nIn the first week of the release and user feedback, we admit that Aim 4.0 and [Aim 3.0](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative) are two different products and that the users want both. So we oblige.\n\nThe Aim 3.0 is used by many teams and we are sorry for the disruption.\n\nGoing forward we are going to maintain both projects. \n\nAim 3.0 roadmap has also been updated. Stay tuned for that.\n\n## Learn more\n\n\n\n[AimOS is on a mission to democratize AI Systems logging tools.](https://aimos.readthedocs.io/en/latest/apps/overview.html) üôå\n\nTry out[ AimOS](https://github.com/aimhubio/aimos), join the [Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on [GitHub](https://github.com/aimhubio/aimos/tree/main) if you think AimOS is useful, and here is the link to[ Aim](https://github.com/aimhubio/aim), an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003cp\u003eWe have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.\u003c/p\u003e\n\u003cp\u003eRight now you can access the original Aim Experiment Tracker as is through the Aim repo. You can also access the Aim 4.0 as AimOS - an end-to-end modular observability tool for AI systems.\u003c/p\u003e\n\u003ch2\u003eWhy the move?\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e mission is to enable a modular end-to-end observability for the AI Systems.\u003c/p\u003e\n\u003cp\u003eIn the first week of the release and user feedback, we admit that Aim 4.0 and \u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003eAim 3.0\u003c/a\u003e are two different products and that the users want both. So we oblige.\u003c/p\u003e\n\u003cp\u003eThe Aim 3.0 is used by many teams and we are sorry for the disruption.\u003c/p\u003e\n\u003cp\u003eGoing forward we are going to maintain both projects.\u003c/p\u003e\n\u003cp\u003eAim 3.0 roadmap has also been updated. Stay tuned for that.\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools.\u003c/a\u003e üôå\u003c/p\u003e\n\u003cp\u003eTry out\u003ca href=\"https://github.com/aimhubio/aimos\"\u003e AimOS\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community,\u003c/a\u003e share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aimos/tree/main\"\u003eGitHub\u003c/a\u003e if you think AimOS is useful, and here is the link to\u003ca href=\"https://github.com/aimhubio/aim\"\u003e Aim\u003c/a\u003e, an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/aim-4-0-to-a-new-repo-aimos-here-is-why.md","_raw":{"sourceFilePath":"posts/aim-4-0-to-a-new-repo-aimos-here-is-why.md","sourceFileName":"aim-4-0-to-a-new-repo-aimos-here-is-why.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-4-0-to-a-new-repo-aimos-here-is-why"},"type":"Post"},{"title":"Aim and MLflow ‚Äî Choosing Experiment Tracker for Zero-Shot Cross-Lingual Transfer","date":"2023-02-14T06:41:14.310Z","author":"Hovhannes Tamoyan","description":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user...","slug":"aim-and-mlflow-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v64PbdBn6kBvsH3t5bkv8w.png","draft":false,"categories":["Tutorials"],"body":{"raw":"The release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.\n\nThe question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.\n\nIn this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.\n\n# Task Setup\n\nThe task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.\n\nParticularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.\n\nZero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.\n\nWe will explore two techniques in our experimentation:\n\n* Fine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.\n* Feature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.\n\nIn both techniques, we will undertake training utilizing the¬†`en`¬†subset of the¬†`XNLI`¬†dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from the¬†`XNLI`¬†dataset, including English(`en`), German(`de`), French(`fr`), Spanish(`es`), Chinese(`zh`), and Arabic(`ar`).\n\n# The Datasets\n\nWe will utilize the¬†`XNLI`¬†(cross-lingual¬†`NLI`) dataset, which is a selection of a few thousand examples from the¬†`MNLI`¬†(multi¬†`NLI`) dataset, translated into 14 different languages, including some with limited resources.\n\nThe template of the¬†`NLI`¬†task is as follows. Given a pair of sentences, a¬†`premise`¬†and a¬†`hypothesis`¬†need to determine whether a¬†`hypothesis`¬†is true (entailment), false (contradiction), or undetermined (neutral) given a¬†`premise`.\n\nLet‚Äôs take a look at a few samples to get hang of it. Say the given hypothesis is¬†`‚ÄúIssues in Data Synthesis.‚Äù`¬†and the premise is¬†`‚ÄúProblems in data synthesis.‚Äù`. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.\n\nAnother example this time of a neutral pair of sentences is the following: the hypothesis is¬†`‚ÄúShe was so happy she couldn't stop smiling.‚Äù`¬†and the premise is¬†`‚ÄúShe smiled back.‚Äù`. The first sentence doesn‚Äôt imply the second one, however, it doesn‚Äôt contradict it as well. Thus they are neutral.\n\nAn instance of contradiction, the hypothesis is¬†`‚ÄúThe analysis proves that there is no link between PM and bronchitis.‚Äù`¬†and the premise is¬†`‚ÄúThis analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.‚Äù`. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.\n\nFor more examples please explore the HuggingFace Datasets page powered by Streamlit:¬†\u003chttps://huggingface.co/datasets/viewer/\u003e.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png)\n\n# The Models\n\n\n\nIn our experiments, we will utilize the following set of pre-trained multilingual language models:\n\n![](/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png)\n\nWe will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.\n\n# Setting up Training Environment\n\nBefore beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:\n\n\n\n```\naim-and-mlflow-usecase\n‚îú‚îÄ‚îÄ logs\n‚îÇ   ‚îú‚îÄ‚îÄ aim_callback\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ aimlflow\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ checkpoints\n‚îÇ   ‚îî‚îÄ‚îÄ mlruns\n‚îî‚îÄ‚îÄ main.py\n```\n\nLet‚Äôs start off by creating the main directory, we named it¬†`aim-and-mlflow-usecase`, you can simply name anything you want. After which we need to download the¬†`main.py`¬†from the following source:¬†\u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\u003e. The code explanation and sample usage can be found in the¬†`README.md`¬†file of the directory. We will be using this script to run our experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png)\n\nThe¬†`logs`¬†directory as the name suggests stores the logs. In the¬†`checkpoints`¬†folder, all the model states will be saved. The¬†`mlruns`¬†is the repository for MLflow experiments. The¬†`aim_callback`¬†will store the repository of Aim runs tracked using Aim‚Äôs built-in callback for Hugging Face Transformers, meanwhile, the¬†`aimlflow`¬†will store the runs converted from MLflow using the aimlflow tool.\n\n\u003e *It is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.*\n\nIn order to start the training process, we will be using the following command. But first, let‚Äôs navigate to the directory where our script is located (`aim-and-mlflow-usecase`¬†in our case).\n\n```\npython main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nWhere¬†`{PATH_TO}`¬†is the absolute path of the¬†`aim-and-mlflow-usecase`¬†directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:\n\n```\npython main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n```\n\nGo grab some snacks, trainings take a while üç´ ‚ò∫Ô∏è.\n\n# Using aimlflow\n\nMeanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use the¬†`aimlflow`¬†to simply convert and view our experiments live on Aim. Let‚Äôs first show how this can be done after which tackle the question.\n\nInstal¬†`aimlflow`¬†on your machine via¬†`pip`, if it is not already installed:\n\n```\n$ pip install aimlflow\n```\n\n```\n$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n```\n\nThis command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.\n\nMore on how the¬†`aimlflow`¬†can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:\n\n* **[Exploring MLflow experiments with a powerful UI](https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e)**\n* **[How to integrate aimlflow with your remote MLflow](https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf)**\n\nThis is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aim‚Äôs built-in callback,¬†`aim.hugging_face.AimCallback`, which is specifically designed for t`ransformers.Trainer`¬†functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.\n\n# Unlocking the Power of Data Analysis\n\n\n\nOnce the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, let‚Äôs launch both tools‚Äô UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:\n\n```\n$ mlflow ui\n```\n\n```\n$ aim up\n```\n\n\n\n\u003e *Note that for the best possible experience, we will be using the*¬†`aim_callback/.aim`¬†*repository in this demonstration, as it has deeper integration with the*¬†`Trainer`*.*\n\nThe UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visiting¬†`http://127.0.0.1:5000`¬†for MLflow and¬†`http://127.0.0.1:43800`¬†for Aim.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png \"The user interface of MLflow on first look\")\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png \"The user interface of Aim on first look\")\n\nIn order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit the¬†`Compare Runs`¬†page:\n\nBy selecting all the experiments, navigate to the comparison page by clicking the¬†`Compare`¬†button.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png)\n\nThe¬†`Run details`¬†section presents the run metadata, including the start and end time and duration of the run. The¬†`Parameters`¬†section displays the hyperparameters used for the run, such as the optimizer and architecture. The¬†`Metrics`¬†section showcases the latest values for each metric.\n\nHaving access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.\n\nTo organize the parameters into meaningful groups for our experiment, simply go to Aim‚Äôs¬†`Metrics Explorer`¬†page and follow a few straightforward steps:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif)\n\n\n\n\n\n# Gaining insights\n\n\n\nLet‚Äôs examine the charts more closely and uncover valuable insights from our experiments.\n\nA quick examination of the charts reveals the following observations:\n\n* The fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.\n* The graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.\n\n\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png)\n\n\n\n* Even though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only by¬†`train_batch_size`. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png)\n\n\n\n* The charts unmistakably show that the¬†`bert-base-multilingual-cased`¬†model achieved the best accuracy results, with the highest score observed for the¬†`en`¬†subset, as the model was trained on that subset. Subsequently,¬†`es`,¬†`fr`,¬†`de`,¬†`zh`, and¬†`ar`¬†followed. Unsurprisingly the scores for the¬†`zh`¬†and¬†`ar`¬†datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png)\n\n\n\n* Let us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.\n* ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png)\n\n\n\n  One can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. The¬†`Parameter Explorer`¬†will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how the¬†`Parameter Explorer`¬†works, let‚Äôs pick the following parameters:¬†`train_batch_size`,¬†`learning_rate`,`_name_or_path`,¬†`loss`, and the accuracies of¬†`sub_dataset`s. The following chart will be observed after clicking the¬†`Search`¬†button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses the¬†`bert-base-multilingual-cased`¬†model, with 5¬∑10‚Åª‚Åµ¬†`learning_rate`¬†and the¬†`batch_size`¬†is 8.\n\n  Taking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.\n\n  # Conclusion\n\n  This blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aim‚Äôs powerful user interface to obtain valuable insights from the experiments.\n\n  We also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.","html":"\u003cp\u003eThe release of aimlflow sparked user curiosity, a tool that facilitates seamless integration of a powerful experiment tracking user interface of Aim with MLflow logs.\u003c/p\u003e\n\u003cp\u003eThe question arises as to why we need aimlflow or why we need to view MLflow tracked logs on Aim. The answer is that Aim provides a highly effective user interface that reveals the potential for gaining valuable insights.\u003c/p\u003e\n\u003cp\u003eIn this blog post, we will address the zero-shot cross-lingual transfer task in NLP, and subsequently, monitor the metadata using both Aim and MLflow. Finally, we will attempt to obtain insightful observations from our experiments through the utilization of their respective user interfaces.\u003c/p\u003e\n\u003ch1\u003eTask Setup\u003c/h1\u003e\n\u003cp\u003eThe task that we will be tackling in this scope is the zero-shot cross-lingual transfer. Zero-shot cross-lingual transfer refers to a machine learning technique where a model is trained in one language but is able to perform well in another language without any additional fine-tuning. This means that the model has the ability to generalize its understanding of the task across different languages without the need for additional training data.\u003c/p\u003e\n\u003cp\u003eParticularly, we will train a model for the natural language inference task (NLI) on the English dataset and then classify the label of a given sample written in a different language, without additional training. This approach is useful in situations where labeled data is scarce in some languages and plentiful in others.\u003c/p\u003e\n\u003cp\u003eZero-shot cross-lingual transfer can be achieved through various methods, including cross-lingual word embeddings and shared multilingual representations learned in a common space (multilingual language model), the latter is widely used.\u003c/p\u003e\n\u003cp\u003eWe will explore two techniques in our experimentation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFine-tuning the entire pre-trained multilingual language model. Adjusting the weights of the network to solve the given classification task, resuming training from the last state of a pre-trained model.\u003c/li\u003e\n\u003cli\u003eFeature extraction which refers to attaching a classification head on top of the pre-trained language model and only training that portion of the network.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn both techniques, we will undertake training utilizing the¬†\u003ccode\u003een\u003c/code\u003e¬†subset of the¬†\u003ccode\u003eXNLI\u003c/code\u003e¬†dataset. Following this, we will conduct evaluations on our evaluation set, consisting of six language subsets from the¬†\u003ccode\u003eXNLI\u003c/code\u003e¬†dataset, including English(\u003ccode\u003een\u003c/code\u003e), German(\u003ccode\u003ede\u003c/code\u003e), French(\u003ccode\u003efr\u003c/code\u003e), Spanish(\u003ccode\u003ees\u003c/code\u003e), Chinese(\u003ccode\u003ezh\u003c/code\u003e), and Arabic(\u003ccode\u003ear\u003c/code\u003e).\u003c/p\u003e\n\u003ch1\u003eThe Datasets\u003c/h1\u003e\n\u003cp\u003eWe will utilize the¬†\u003ccode\u003eXNLI\u003c/code\u003e¬†(cross-lingual¬†\u003ccode\u003eNLI\u003c/code\u003e) dataset, which is a selection of a few thousand examples from the¬†\u003ccode\u003eMNLI\u003c/code\u003e¬†(multi¬†\u003ccode\u003eNLI\u003c/code\u003e) dataset, translated into 14 different languages, including some with limited resources.\u003c/p\u003e\n\u003cp\u003eThe template of the¬†\u003ccode\u003eNLI\u003c/code\u003e¬†task is as follows. Given a pair of sentences, a¬†\u003ccode\u003epremise\u003c/code\u003e¬†and a¬†\u003ccode\u003ehypothesis\u003c/code\u003e¬†need to determine whether a¬†\u003ccode\u003ehypothesis\u003c/code\u003e¬†is true (entailment), false (contradiction), or undetermined (neutral) given a¬†\u003ccode\u003epremise\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eLet‚Äôs take a look at a few samples to get hang of it. Say the given hypothesis is¬†\u003ccode\u003e‚ÄúIssues in Data Synthesis.‚Äù\u003c/code\u003e¬†and the premise is¬†\u003ccode\u003e‚ÄúProblems in data synthesis.‚Äù\u003c/code\u003e. Now there are 3 options whether this hypothesis entails the premise, contradicts, or is neutral. In this pair of sentences, it is obvious that the answer is entailment because the words issues and problems are synonyms and the term data synthesis remains the same.\u003c/p\u003e\n\u003cp\u003eAnother example this time of a neutral pair of sentences is the following: the hypothesis is¬†\u003ccode\u003e‚ÄúShe was so happy she couldn't stop smiling.‚Äù\u003c/code\u003e¬†and the premise is¬†\u003ccode\u003e‚ÄúShe smiled back.‚Äù\u003c/code\u003e. The first sentence doesn‚Äôt imply the second one, however, it doesn‚Äôt contradict it as well. Thus they are neutral.\u003c/p\u003e\n\u003cp\u003eAn instance of contradiction, the hypothesis is¬†\u003ccode\u003e‚ÄúThe analysis proves that there is no link between PM and bronchitis.‚Äù\u003c/code\u003e¬†and the premise is¬†\u003ccode\u003e‚ÄúThis analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.‚Äù\u003c/code\u003e. In the hypothesis, it is stated that the analysis shows that there is no link between two biological terms. Meanwhile, the premise states the opposite, that the analysis combined two studies to show that there is a connection between the two terms.\u003c/p\u003e\n\u003cp\u003eFor more examples please explore the HuggingFace Datasets page powered by Streamlit:¬†\u003ca href=\"https://huggingface.co/datasets/viewer/\"\u003ehttps://huggingface.co/datasets/viewer/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAfkZJ1RS2ulEYmJFIOlMA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eThe Models\u003c/h1\u003e\n\u003cp\u003eIn our experiments, we will utilize the following set of pre-trained multilingual language models:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2023-03-27-at-10.46.58.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe will load each model with its last state weights and continue training the entire network (fine-tuning) or the classification head only (feature extraction) from that state. All of the mentioned models are trained with the Masked Language Modeling (MLM) objective.\u003c/p\u003e\n\u003ch1\u003eSetting up Training Environment\u003c/h1\u003e\n\u003cp\u003eBefore beginning, it is important to keep in mind that the following is what the ultimate structure of our directory will resemble:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim-and-mlflow-usecase\n‚îú‚îÄ‚îÄ logs\n‚îÇ   ‚îú‚îÄ‚îÄ aim_callback\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ aimlflow\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .aim\n‚îÇ   ‚îú‚îÄ‚îÄ checkpoints\n‚îÇ   ‚îî‚îÄ‚îÄ mlruns\n‚îî‚îÄ‚îÄ main.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet‚Äôs start off by creating the main directory, we named it¬†\u003ccode\u003eaim-and-mlflow-usecase\u003c/code\u003e, you can simply name anything you want. After which we need to download the¬†\u003ccode\u003emain.py\u003c/code\u003e¬†from the following source:¬†\u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/cross-lingual-transfer\u003c/a\u003e. The code explanation and sample usage can be found in the¬†\u003ccode\u003eREADME.md\u003c/code\u003e¬†file of the directory. We will be using this script to run our experiments.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l7ZT-XYl8fKjm4zDRvEUg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe¬†\u003ccode\u003elogs\u003c/code\u003e¬†directory as the name suggests stores the logs. In the¬†\u003ccode\u003echeckpoints\u003c/code\u003e¬†folder, all the model states will be saved. The¬†\u003ccode\u003emlruns\u003c/code\u003e¬†is the repository for MLflow experiments. The¬†\u003ccode\u003eaim_callback\u003c/code\u003e¬†will store the repository of Aim runs tracked using Aim‚Äôs built-in callback for Hugging Face Transformers, meanwhile, the¬†\u003ccode\u003eaimlflow\u003c/code\u003e¬†will store the runs converted from MLflow using the aimlflow tool.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eIt is important to keep in mind that due to limited computational resources, we have chosen to use only 15,000 samples of the training dataset and will be training for a mere 3 epochs with a batch size of 8. Consequently, the obtained results may not be optimal. Nevertheless, the aim of this use case is not to achieve the best possible results, but rather to showcase the advantages of using both Aim and MLflow.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn order to start the training process, we will be using the following command. But first, let‚Äôs navigate to the directory where our script is located (\u003ccode\u003eaim-and-mlflow-usecase\u003c/code\u003e¬†in our case).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython main.py feature-extraction \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere¬†\u003ccode\u003e{PATH_TO}\u003c/code\u003e¬†is the absolute path of the¬†\u003ccode\u003eaim-and-mlflow-usecase\u003c/code\u003e¬†directory. In this particular command, we use the feature-extraction technique for 4 pre-trained models and validate on 6 languages of the XNLI dataset. In parallel or after the first process completion we can run the same command this time using the fine-tuning technique:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython main.py fine-tune \\\n    --model-names bert-base-multilingual-cased bert-base-multilingual-uncased xlm-roberta-base distilbert-base-multilingual-cased \\\n    --eval-datasets-names en de fr es ar zh \\\n    --output-dir {PATH_TO}/logs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGo grab some snacks, trainings take a while üç´ ‚ò∫Ô∏è.\u003c/p\u003e\n\u003ch1\u003eUsing aimlflow\u003c/h1\u003e\n\u003cp\u003eMeanwhile, one might wonder why we are tracking the experiment results using MLflow and Aim. We could simply track the metrics via MLflow and use the¬†\u003ccode\u003eaimlflow\u003c/code\u003e¬†to simply convert and view our experiments live on Aim. Let‚Äôs first show how this can be done after which tackle the question.\u003c/p\u003e\n\u003cp\u003eInstal¬†\u003ccode\u003eaimlflow\u003c/code\u003e¬†on your machine via¬†\u003ccode\u003epip\u003c/code\u003e, if it is not already installed:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ pip install aimlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e$ aimlflow sync --mlflow-tracking-uri=logs/mlruns --aim-repo=logs/aimlflow/.aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis command will start converting all of the experiment hyperparameters, metrics, and artifacts from MLflow to Aim, and continuously update the database with new runs every 10 seconds.\u003c/p\u003e\n\u003cp\u003eMore on how the¬†\u003ccode\u003eaimlflow\u003c/code\u003e¬†can be set up for local and remote MLflow experiments can be found in these two blog posts respectively:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\"\u003eExploring MLflow experiments with a powerful UI\u003c/a\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://medium.com/aimstack/how-to-integrate-aimlflow-with-your-remote-mlflow-3e9ace826eaf\"\u003eHow to integrate aimlflow with your remote MLflow\u003c/a\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is one approach to follow, but for a more improved user interface experience, we suggest utilizing Aim‚Äôs built-in callback,¬†\u003ccode\u003eaim.hugging_face.AimCallback\u003c/code\u003e, which is specifically designed for t\u003ccode\u003eransformers.Trainer\u003c/code\u003e¬†functionality. It tracks a vast array of information, including environment details, packages and their versions, CPU, and GPU usage, and much more.\u003c/p\u003e\n\u003ch1\u003eUnlocking the Power of Data Analysis\u003c/h1\u003e\n\u003cp\u003eOnce the training completes some steps, we can start exploring the experiments and comparing the MLflow and Aim user interfaces. First, let‚Äôs launch both tools‚Äô UIs. To do this, we need to navigate to the logs directory and run the MLflow UI using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ mlflow ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote that for the best possible experience, we will be using the\u003c/em\u003e¬†\u003ccode\u003eaim_callback/.aim\u003c/code\u003e¬†\u003cem\u003erepository in this demonstration, as it has deeper integration with the\u003c/em\u003e¬†\u003ccode\u003eTrainer\u003c/code\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe UIs of both MLflow and Aim will be available by default on ports 5000 and 43800 respectively. You can access the homepages of each tool by visiting¬†\u003ccode\u003ehttp://127.0.0.1:5000\u003c/code\u003e¬†for MLflow and¬†\u003ccode\u003ehttp://127.0.0.1:43800\u003c/code\u003e¬†for Aim.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3ThRGvGGEvPFvLQ43VW1w.png\" alt=\"\" title=\"The user interface of MLflow on first look\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXR2nt6w0WMo50aQ4TFJDA.png\" alt=\"\" title=\"The user interface of Aim on first look\"\u003e\u003c/p\u003e\n\u003cp\u003eIn order to gain valuable insights from the experiment results, it is imperative to navigate to the proper pages in both user interfaces. To do so, in MLflow, we can visit the¬†\u003ccode\u003eCompare Runs\u003c/code\u003e¬†page:\u003c/p\u003e\n\u003cp\u003eBy selecting all the experiments, navigate to the comparison page by clicking the¬†\u003ccode\u003eCompare\u003c/code\u003e¬†button.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fk7qu58g2Qwyx-OFtu0mHg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe¬†\u003ccode\u003eRun details\u003c/code\u003e¬†section presents the run metadata, including the start and end time and duration of the run. The¬†\u003ccode\u003eParameters\u003c/code\u003e¬†section displays the hyperparameters used for the run, such as the optimizer and architecture. The¬†\u003ccode\u003eMetrics\u003c/code\u003e¬†section showcases the latest values for each metric.\u003c/p\u003e\n\u003cp\u003eHaving access to all this information is great, but what if we want to explore the evolution of the metrics over time to gain insights into our training and evaluation processes? Unfortunately, MLflow does not offer this functionality. However, Aim does provide it, to our advantage.\u003c/p\u003e\n\u003cp\u003eTo organize the parameters into meaningful groups for our experiment, simply go to Aim‚Äôs¬†\u003ccode\u003eMetrics Explorer\u003c/code\u003e¬†page and follow a few straightforward steps:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qwyGgzlWQHR7nIW-rTD8Iw.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eGaining insights\u003c/h1\u003e\n\u003cp\u003eLet‚Äôs examine the charts more closely and uncover valuable insights from our experiments.\u003c/p\u003e\n\u003cp\u003eA quick examination of the charts reveals the following observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe fine-tuning technique is clearly superior to feature extraction in terms of accuracy and loss, as evidenced by a comparison of the maximum and minimum results in charts 1 and 3, and charts 2 and 4, respectively.\u003c/li\u003e\n\u003cli\u003eThe graphs for feature extraction show significant fluctuations across languages, whereas the results for fine-tuning vary greatly with changes to the model. To determine the extent of the variation, we can consolidate the metrics by removing either the grouping by language (color) or model (stroke), respectively. In this case, we will maintain the grouping by model name to examine the variation of each model for a given technique.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKJ2k0_oy8218EBIfhxn6Q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEven though we have only trained a single model with a larger batch size (16, instead of the default 8), it is still valuable to examine the trend. To accomplish this, we will eliminate the grouping by model name and group only by¬†\u003ccode\u003etrain_batch_size\u003c/code\u003e. As we can observe, after only 2500 steps, there is a trend of decreasing loss and increasing accuracy at a quicker pace. Thus, it is worth considering training with larger batch sizes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7FN0NfWgWALylCCTwy3EQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe charts unmistakably show that the¬†\u003ccode\u003ebert-base-multilingual-cased\u003c/code\u003e¬†model achieved the best accuracy results, with the highest score observed for the¬†\u003ccode\u003een\u003c/code\u003e¬†subset, as the model was trained on that subset. Subsequently,¬†\u003ccode\u003ees\u003c/code\u003e,¬†\u003ccode\u003efr\u003c/code\u003e,¬†\u003ccode\u003ede\u003c/code\u003e,¬†\u003ccode\u003ezh\u003c/code\u003e, and¬†\u003ccode\u003ear\u003c/code\u003e¬†followed. Unsurprisingly the scores for the¬†\u003ccode\u003ezh\u003c/code\u003e¬†and¬†\u003ccode\u003ear\u003c/code\u003e¬†datasets were lower, given that they belong to distinct linguistic families and possess unique syntax.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJoo9JlhjXfknqIVoFLGkw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eLet us examine the training times and efficiencies. By setting the x-axis to align with relative time rather than default steps, we observe that the final tracking point of the fine-tuning technique took almost 25% more time to complete compared to the feature-extraction technique.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNlhMHrCtUtc72s4FCQLaQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOne can continue analysis further after training with bigger batch sizes and more variations of the learning rate, models, etc. The¬†\u003ccode\u003eParameter Explorer\u003c/code\u003e¬†will then lend its aid in presenting intricate, multi-layered data in a visually appealing, multi-dimensional format. To demonstrate how the¬†\u003ccode\u003eParameter Explorer\u003c/code\u003e¬†works, let‚Äôs pick the following parameters:¬†\u003ccode\u003etrain_batch_size\u003c/code\u003e,¬†\u003ccode\u003elearning_rate\u003c/code\u003e,\u003ccode\u003e_name_or_path\u003c/code\u003e,¬†\u003ccode\u003eloss\u003c/code\u003e, and the accuracies of¬†\u003ccode\u003esub_dataset\u003c/code\u003es. The following chart will be observed after clicking the¬†\u003ccode\u003eSearch\u003c/code\u003e¬†button. From here we can see that the run which resulted in the highest accuracies for all subsets has a final loss value equal to 0.6, uses the¬†\u003ccode\u003ebert-base-multilingual-cased\u003c/code\u003e¬†model, with 5¬∑10‚Åª‚Åµ¬†\u003ccode\u003elearning_rate\u003c/code\u003e¬†and the¬†\u003ccode\u003ebatch_size\u003c/code\u003e¬†is 8.\u003c/p\u003e\n\u003cp\u003eTaking into account the aforementioned insights, we can move forward with future experiments. It is worth noting that while fine-tuning results improved accuracy scores, it requires a slightly longer training time. Increasing the batch size and training for longer steps/epochs is expected to further enhance the results. Furthermore, fine-tuning other hyperparameters such as the learning rate, weight decay, and dropout will make the experiments set more diverse and may lead to even better outcomes.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eThis blog post demonstrates how to solve an NLP task, namely zero-shot cross-lingual transfer while tracking your run metrics and hyperparameters with MLflow and then utilizing Aim‚Äôs powerful user interface to obtain valuable insights from the experiments.\u003c/p\u003e\n\u003cp\u003eWe also showcased how to use the aimlfow which has piqued user interest as a tool that seamlessly integrates the experiment tracking user interface of Aim with MLflow logs.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","_raw":{"sourceFilePath":"posts/aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileName":"aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-and-mlflow-‚Äî-choosing-experiment-tracker-for-zero-shot-cross-lingual-transfer"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Discover how to leverage Aim's context and subplot features for effortless comparison of validation and test metrics, enhancing your ML experiment analysis.","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets ‚Äî train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that on¬†[Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\nUse the¬†[aim.track](https://github.com/aimhubio/aim#track)¬†context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n```\nimport aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n```\n\nOnce the training is ran, execute¬†`aim up`¬†in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n\u003e **\\*Note:**¬†The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLet‚Äôs go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.**¬†Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.**¬†Add the¬†`bleu`¬†metric to the Select input ‚Äî query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.**¬†Search by¬†`context.subset`¬†to show both¬†`test`¬†and¬†`val`¬†`loss`¬†and¬†`bleu`¬†metrics. Divide into subplots further by¬†`context.subset`¬†too so Aim UI shows¬†`test`¬†and¬†`val`¬†metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot it‚Äôs easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\n\n\nHere is a full summary [video](https://youtu.be/DGI8S7SUfEk) on how to do it on the UI.\n\n# Learn More\n\nIf you find Aim useful, support us and¬†[star the project](https://github.com/aimhubio/aim)¬†on GitHub. Join the¬†[Aim community](https://slack.aimstack.io/)¬†and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003cp\u003eResearchers divide datasets into three subsets ‚Äî train, validation and test so they can test their model performance at different levels.\u003c/p\u003e\n\u003cp\u003eThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\u003c/p\u003e\n\u003cp\u003eThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\u003c/p\u003e\n\u003cp\u003eModels are usually ran on validation subset after each epoch.\u003cbr\u003e\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\u003c/p\u003e\n\u003cp\u003eThere is a need to collect and effectively compare all these metrics.\u003c/p\u003e\n\u003cp\u003eHere is how to do that on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003eUsing context to track for different subsets?\u003c/h1\u003e\n\u003cp\u003eUse the¬†\u003ca href=\"https://github.com/aimhubio/aim#track\"\u003eaim.track\u003c/a\u003e¬†context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport aim\n\n# train loop\nfor epoch in range(num_epochs):\n  for i, (images, labels) in enumerate(train_loader):\n    if i % 30 == 0:\n      aim.track(loss.item(), name='loss', epoch=epoch, subset='train')\n      aim.track(acc.item(), name='accuracy', epoch=epoch, subset='train')\n    \n  # calculate validation metrics at the end of each epoch\n  # ...\n  aim.track(loss.item(), name='loss', epoch=epoch, subset='val')\n  aim.track(acc.item(), name='acc', epoch=epoch, subset='val')\n  # ...\n  \n  # calculate test metrics \n  # ...\n  aim.track(loss.item(), name='loss', subset='test')\n  aim.track(acc.item(), name='loss', subset='test')\n   \n  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the training is ran, execute¬†\u003ccode\u003eaim up\u003c/code\u003e¬†in your terminal and start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eUsing subplots to compare test, val loss and bleu metrics\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e*Note:\u003c/strong\u003e¬†The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet‚Äôs go step-by-step on how to break down lots of experiments using subplots.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1.\u003c/strong\u003e¬†Explore the runs, the context table, play with the query language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2.\u003c/strong\u003e¬†Add the¬†\u003ccode\u003ebleu\u003c/code\u003e¬†metric to the Select input ‚Äî query both metrics at the same time. Divide into subplots by metric.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3.\u003c/strong\u003e¬†Search by¬†\u003ccode\u003econtext.subset\u003c/code\u003e¬†to show both¬†\u003ccode\u003etest\u003c/code\u003e¬†and¬†\u003ccode\u003eval\u003c/code\u003e¬†\u003ccode\u003eloss\u003c/code\u003e¬†and¬†\u003ccode\u003ebleu\u003c/code\u003e¬†metrics. Divide into subplots further by¬†\u003ccode\u003econtext.subset\u003c/code\u003e¬†too so Aim UI shows¬†\u003ccode\u003etest\u003c/code\u003e¬†and¬†\u003ccode\u003eval\u003c/code\u003e¬†metrics on different subplots for better comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"\u003e\u003c/p\u003e\n\u003cp\u003eNot it‚Äôs easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\u003c/p\u003e\n\u003ch1\u003eSummary\u003c/h1\u003e\n\u003cp\u003eHere is a full summary \u003ca href=\"https://youtu.be/DGI8S7SUfEk\"\u003evideo\u003c/a\u003e on how to do it on the UI.\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you find Aim useful, support us and¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e¬†on GitHub. Join the¬†\u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e¬†and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim Community is Moving to Discord!! üéâ","date":"2022-12-21T21:45:47.434Z","author":"Gev Soghomonian","description":"Aim community is officially moving to Discord!! üéâ Let's rock and build great things together!! The slack workspace is going to become deprecated by the end of 2022.","slug":"aim-community-is-moving-to-discord-","image":"https://miro.medium.com/max/1400/1*c_ovadkKTTkZOokR3WiUTg.webp","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)¬†community is officially moving to Discord!! üéâ\n\n[Join us on Discord](https://community.aimstack.io/), let‚Äôs rock and build great things together!!\n\nIn this new space we are going to\n\n* be very proactive in sharing next steps, areas of focus and plans\n* share weekly roadmap / commitments\n* share more community-specific content\n* of course continue answering the user questions\n* work with the users to build the best open-source experiment tracking tools.\n\n***The slack workspace is going to become deprecated by the end of 2022.***\n\nI also asked a friend on the technical reasons for moving to Discord. Here is what he responded. \n\n![](https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp)\n\nAim High!! üöÄ","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†community is officially moving to Discord!! üéâ\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://community.aimstack.io/\"\u003eJoin us on Discord\u003c/a\u003e, let‚Äôs rock and build great things together!!\u003c/p\u003e\n\u003cp\u003eIn this new space we are going to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebe very proactive in sharing next steps, areas of focus and plans\u003c/li\u003e\n\u003cli\u003eshare weekly roadmap / commitments\u003c/li\u003e\n\u003cli\u003eshare more community-specific content\u003c/li\u003e\n\u003cli\u003eof course continue answering the user questions\u003c/li\u003e\n\u003cli\u003ework with the users to build the best open-source experiment tracking tools.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eThe slack workspace is going to become deprecated by the end of 2022.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI also asked a friend on the technical reasons for moving to Discord. Here is what he responded.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim High!! üöÄ\u003c/p\u003e"},"_id":"posts/aim-community-is-moving-to-discord-üéâ.md","_raw":{"sourceFilePath":"posts/aim-community-is-moving-to-discord-üéâ.md","sourceFileName":"aim-community-is-moving-to-discord-üéâ.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-community-is-moving-to-discord-üéâ"},"type":"Post"},{"title":"Aim from Zero to Hero","date":"2022-03-16T16:39:52.780Z","author":"Gev Soghomonian","description":"In this blog post, we show how to use Aim‚Äôs basic to highly advanced functionality in order to track your machine learning experiments with various","slug":"aim-from-zero-to-hero","image":"/images/dynamic/image.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"In this blog post, we show how to use Aim‚Äôs basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\n\n## *Starting from basics: how Aim tracks machine learning experiments*\n\nMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with¬†`matplotlib`. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\n\nTo track the losses, simply create an experiment run and add a tracking common like this\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n```\n\nYou are going to end up with a visualization of this kind.\n\n![](/images/dynamic/image-2-.png)\n\nA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\n\n```\naim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n```\n\nAfter grouping, we end up with a visualization akin to this.\n\n![](/images/dynamic/image.jpeg)\n\nAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the¬†[complete documentation](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html)¬†or simply play around in our¬†[interactive Demo](http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026chart=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\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n- - -\n\n## *Rising beyond watching losses*\n\nAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic¬†dict¬†or dictionary-like object, i.e.\n\n```\naim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n```\n\nIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can¬†[group with respect to the parameters/hyperparameters](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter)¬†that you saved with aim.\n\nViewing the¬†[standalone parameters](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page)¬†is fast for each separate Single Run\n\n![](/images/dynamic/image.png)\n\n[Filtering/grouping your runs using the selected trackable](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter)¬†can be accessed and looked through in our¬†[interactive tutorial](http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM\u0026chart=7DTSXL8yGkKjwNDtEcYjw4opymsQFTsrFJVani1UKjeQnySYAJU8THRU8HtWL3VdRyvJz4M6JQ4i5aQhRdAY2eMcNhsWdc3GBGfJ557AxjDiYoBKS4mAFvEyFP7iiBKcQcNZmTYCtF2Kc5v5sYaxoxb1nrG2LHfngen5ec8K96qQFnmgBbmhx6avZ2KFV5SAoY4iJ6Yu9Y5Bs6VENDzHWKCGVTsRsoBaY94TLJmzpxM6CfCdL5W4jEV1rnVmruGM7YwobY4vPBq49DyvMdYkcfQnKudBEUYLcrEavGiiXgXBdNbQeri7D2ErVTDKVzRPf6qiQdYBXfFceHnRaYNRvAuge3mY4ixbYz24rKKtVnaWCkEX9nTUdfYzPzbgDdgs5m8k4QPcQqs4AiHju2RDtrinvYcj9F1GPdepQvrkvqSfxZS1hMSuS68Yxn769UzjCPD7WwjEpobY6C32U5J4DazxZkzdVce3maPQj4osX3BFmP9j9xTR98JPuU3ApP5pUYZqbBpaY9QkFELgLoVR1eixABK8bXyYnu3mDomMUHCmZqi34ttmX2hKVRV2i84zFQ82fq6SAMpHADdSGagp9u2H5CGCWRrBvQ3RmbXzBJ2R6qn41CERpCv2rmBXT9qs71fGQ73a51K7Bft3mBUi83rXdt1wRMHMMi9LSnEeiba4JdaapTrBbZV4tS3m5p9wi9vxeWDtVSTtZjJRqHN96x3FtsAG1xSpcgiWGutBAxw7yS5Ng9Pcd5ZfUCtVhgQdeTqJGZ4ThmJaNNFv2AMo2Vxiu1BdUAmTB3bQ35WJNauQhybmJX61rE83F3s7Ahd47ASLu7e6LEP2JL9XkiCWNG7rLGPv18oHKWkj2RtKcqUbstP8afoRdFhfibnResF7Jnb2Cqa4yET3iPboGmj6APutdy7YEy6itShcuRRx3c5h7ZXMfxU1atMDxK2bEBksu7UYrCzszGDFjuj9PU58TX2Y4m3izdEm9hRALYw4Tp49SdxdE6XMqjRu3XDpwSjFAGZXPZ4i58CL9gLydZmsJVjyuA43i8F6UGz34KhcbGw61hXCKuHmkzTftKyK7MHz7ZvYwa1hthDDdXTayD8BcZsT4VNWYPeHXUiVbYufWapjRDTVkUFVYNFKa7oDVY3uP373sKCZuBL2kVxwbRKniwjLYMndmAWmzF179PaqjtdkvDFqwW8xWDBDJSV2uMxet7Eu5K5qbRUAqZJiHad1yyWpVcmCJgiMzrfLLtMm9jXAEy1P7w5XnhZ1vJbPx5wzzb7p6MNS1EwTPugu1X8GvXeuPuBq2jsd3LuGcbHC3nhPfCJeX8PVxNyEpEKdCMojYThfGnE5UrztsPqoCet9Py1jzEoayNnyD3NVec5RsGFvmRrusHy8GprvAfDrFTmF5ryhM2GXpSELvwHMMcKqXuaUrT4rbEM7Yirh2de7fnncziZjWSv3YV1FjCsSPvwEAnL7jjnm1sn6vig7JCpqD4bJRchSkMQmENcxLfthcd2ZXMga2uHhPAK5vxDLwrdX7J8Npxgur7jbYvAUmvynhWXC5bGCG9uY46ej8Rxwa8iu5LNP2BqpJ1YDWMgbHxoTQnbtokEZbTkwfrRDoCe5wvoGJuC2StnXEfGMjbqFHL7q5kKWeQv4C16cgCv2YE3hEsvc9Q2LLY5HXA2NcGxHSZHWLi4f13nsGzVX6YEiVcYKyDwBcQcv4ZmiV9G2QxUfwhUK3w7ZpATFKq3mjaGZJNTRiBp1RKieBZ2Yd7S9arWjg5cHKeW9STtc5ZeuQjzGuZwwhiwygfUW4PWiTbtJpMjkVuxQLA6zAXGxzmkgrH9weaP4pQHjdTj2gj282RYa6H9p48dtscE4c8fqhkkG71WRrE3ZKpC5hrg8fhbaCyrBFFcwCZAJubz4JGBik7w8vxEbuCJ5MMj7NSChvXHTqnvwaALhqFiVWh2qqwr1z8cXJJ9Z6pZmhvdArAXPPjAr15gVddxFb5yBKeREqb6rM4pf8zSY6fhDnX52TvBP6Jv3cnHbtxsrDmPgVCumRsgenWsDmqfGLKHoJNwmZ9viuzqwa7ZpWpT5wj8xX1BL8ZDBUiEj1yQgxKKj52s5gT8kjHXa2CfKW2VjDEV4qi4Ww67YFAcS5y1qLZGhmyUDsau9yX5AWwo9CbBgukTGxQFSGAiqT61AEkUBQSukH3U2GJkxdtFjRLsCaQRysmna44x9Uxs4TELbh9PSjmdQXC5KegQU6MhqKRq3aWmwTJ8Vr2tuyAaWVLPaBG8Ptme2F9Ru8r3D7ksPDn5hXRhZMHLFRAGkqCoVnTwv1zfGB9b6mJgFCVHbZerUshFPaWNj1quoY8673eNLiDB3c7mJrci6wBxkdXrcTvdKzdhMCMiUthoRjy2dpXu5QuU1puDfMw8W8SsGJfwVxeTYQixkEo1e7URCECqHc31eM4S3fhY7MxmxrSdXpNegTSQwcvEPoWu17PWHFo4k5EeFaWnAsHb3FxYEYeBHWhPB2i8uj3oR4tAr77ZSWxPxu8dZTywFpK8y5wVxnn7kU3kzrXjZA2M7x532AA4kVaHhYwq4UAzSdYqNxYPJgJVjj31Uq7mEVjQdVCvcMQsWKmMsj8Mf7dpRhry28iEUnTMnMfVDoMHcX9KeGU8kuQ7SCu6QJfXJnva3G5P54AACpDCoAKTH4B5omFSU5KqERKTk5gnEYXjPeB3vUPQyRchdDK5CBeAnxNp3JwfiMCeDuZ4WY3Se2BuxLpW2R54rsMjrRW33yAWhH86y8P77UTfnsQUR2ZCyhpyzhUHDwYeqN3XoegykHwmTkug1APed7n5BUBMRiGEDySh1V8uYSutFLuT3fp3KwExSGh46ihqsxTz3GuKmD7i63s6ZgEnM17tYd4mGGTpnn1bdjAqPyUs59V6c9Fsv94QCzfhx5PgRUazG81TuQ6zd169ZS73TFzAi4eeJP9FXTu41yRkYzEfHAWtov4WvmtcTEiogi8ryPETwD2ELh7gYpRRpY3nBDx9CNuGx4rrpzWhycLXn5qK9Epsd9xDBqD3K8DDDQSZo6pNxd6cvBwo274bH2L4fRrofmcGyHzNT51tXxqDSiokaHTDN6q39A1UwmjATiQK5ZmXoi7mJSZ4PGyAUXZFxGq4GRYao7HKHMWcEArbhQkJgJm2aJo4SH6X5GQNGxiskrJAebNem4a3mLvGP8LwPwTjQfT9QZa47hgm6JTVQnXbQ2BqYfXxqjpomizqCwiYpEQrcKs6vkeCtKdh8Mu824BeomKjV3HouBa4ecopqV8zKudHNSpUcmUPahxQpqe7KKaX6gWnDk7h6UNCwZvrQ28qNFky8s5bniXHxxhk7jiEvqzUS7A4jndMjCAaLTbr6LrUNdNaUybDzi7ZpXomY8o6bJxDjoAgo6WNVaKjQHAMgcRcUptazCqey4jNaAi9gv8jAXAHr8h7iRRqsjuriwp8zfZnkDFvSbbiahYK4FhhYRdimp6eWjuXTbtDsrdS2tcCfCEpKwEf3Q1Vqs5CZrMWwqsGF6nGYGPyNmS8GzaK5ZYQ3u4PF7nVgRkeBwyFpEWXG8fEUm6LXgYSejbEaxBCq6nnheLEWT69FoE7zcbdmRsMrtuKLxmRxUm13W7DkAsktXJHMu2EaeRCkmtqJT8okNWzAkc9aNm9rWaYevhDitrsiZhhGdwQqmLjWMtrHAUtEBh6W7XbGgRSmV9XxXb3fc6RaQTdEwqT76RT8GHNLgTnS5Rs2Ek9oWZkVuk7VQbBHo8JP11z49my3DNL1yu9ec5XpwGyd6SUaQW2GdJ4rprexUDAPzEUUzeFJB1ucNDnrswMCHEtVeqp6cjCTZQ3sLzJCv8JTDRYs6UgnRsnSkVMTtSLa4GkZmcTAZqSitxmWEbPtGgUkAqfugMRHqsAqCMBrkCkJ8fh6f7ffFYMyAGv3y6eY3DAQwSqUqr3k6oHVhMSN28A6rn91r146vNSsSa9NTTCdLGhuS2uxEJuKHhhoVvbzq1DQtup6spCYDFJ8Upxdhdypq7SXXaS99KtH8bW5p8i6DcpAQ4YBzJ6F5E1tSZQQhzb1abByA51fnQRqfZqhXZjWh9ky6A1hmDLhjLfrL9jthecYGbWq6bbnac4DrmrHggWN5KfUPJVX3ix4brpppdDrrZQZ5HUo3e6ps67hyTEd8n86ofcWwFqq44aPabCVD8yRFZysUyccKTpVaCgthAysW9zCefN6wFPsurVZA6bVbzdivXHB9CtEUjwexM439i7faF4VmZFhXjENh7TdGCghePGnKNnTkhM3FjPUAV2Udfq19jFvY21cYbTNE7KCKXbb7QD8nr9KXMKv71k1XsrL4QDz7owCmu7JXYvQpwzxZxh2Ac4RWtoP1qG2f9EcAVpiyNv6WPM8gxQgMfYK2EwnZAiHQQbM3qT7YL8QTNKAXkvuvew4uSbNPgQtcGYYvAnDz6LBvBVabV6aAEzU7RvfNJQMEczxjy2gkAbY8rEfqgFFXDBWPFs5WS4wjAqrhTMcDqyQQhMX3wd8K8G9qqoXTUBsW4vg74ZAUMmiSTdJ4wJZhSVunr9dYtJntPGZvDNzoLxgJfxyzPLuF5v8Xscs41LPifQWa3uCCLvqWPfeYW1FDELfsSskFGYmcs8r11t1Yu5GGRsbxxrpi5PFTr5dgdAUGqb7eUvX87H4PfAhDT6vWcAeemifpbQjBGGPY3idrDBn1tb9MqxsGXNrRuBMhBd1q3SyVARZ1Jcu4stfmfRLSaXYr7FMvZGyjUAtErby22J87JAKUZY4D3LQAA1bagYNQDuwmx525W14CCWG996KDMzG8gm1Ruvn1Y72HCJ8t9zqwuuPTkoMFPb13Mn4M8mR4xnCwvoh21MKvjHizhTmGoUQ86UVYg3gjqFuLxxhyZTDvw517qdzmGqcPdd1EhTRJFHc1yfq5YxaQ2291XgFXHZfaghmRFzn2JhoD8gwVtR72pA9bgPYprJLQZvDzAz5EPmthHbcQNPM9B3V3Y6Ds6Wj9kcJesqAUohi4wmf46vJrN1CxqkKq8rVW6E2kWUDYm8WKUYUvYKWrkQMgjcccQwsaGcNNKYUaumrQeA1BcCpCkm9Z9JMdWCHJhd1Z7GHZotRFyHigJ7YR4fMWpoVQ6eFfqZFW11UUgpRGUYJqHBb1z9mybkmgCFKjC6XKB5qocLZkVhd589cHgLXz6797eTmZtzcRFeFz45GpECtzSGLXQ95BUE1h1k5xYrmNAYeKxLojL6ugcu4hYuTiEihGfzRyLhNoY5Jh63Akdnkx6mLjoSaSo3UpjjTcNYsCCDjFWRJgPnzxVdA11WGfYomVy74vS3asBw7ropcGvjAsPF754tsdbpTgqNs5bacd8s3HUvxYRo7jVWQAhb62qsXqwRMNUWakvAQioXmPbLAwP4Vq8v3wiCH1D4gFKWs7AVnaBGf1WCA1adnhbYT12tGGnMhYqsPZTtUAqrR55TM2VDaSiDW3e5S17kzb3weNtjRVc93FAeLF7vm65kNxnDYchB8RcVEhpSn1y4bWscb8jCAoU7DcatK2ksaYjvEE6XxhC21yjcSzLH9283WGh2rFa8u5WBSneqZmYFH97V2Sv3XWdoeUkGb8ouqFKTCtK4BurMLhVFhosjYaMyyuJW9Gz1oGQyhzH5Ff56qYriu8Q4t69v2qyin5B8dgAfZzAM9qme4uhDNCLUH2v4GV5zYnh36YLau8iSDB6sN1XeuzCA9rDTq25FVbGVHvRGDRr8RkQusVcCnRHPx3YhmrSJymp91fiBxDRif7TDaaukad8hNodnkMVJpDN2gYrrJVYraoJcjJ5b6KfZzSJfP8x9ChVMhq8DcrzezNeXpDtJBCnAX99ayKvSXEEstQMJfAYHYip1d9MUqgCnJeA8B8byDt9HgY7gdq24s4MCP1nC5pmtDWeSp2VHCXUCp5bf4zJJ1RGUMcJDfxvu1SzT8ApwGLLFNz7BWPgHkibU4mPXAzHhBj8Ms9BgBZf8StsVXLDewaqbgDjWkoTNZEeJVgNbJsG3uzXbuCDxbCLahkJSPRkvtLgh5KkK15jR3epzrQzHfMDivm6aPbj1A6vJwBLsqjHqkapnXZVfcZYWzSRQ2ZYGQjw6Ph4isyFA4p8mkJafUqtZjvBUWcgDRyjQNA4PLE7B6syMY3g9HwJbkfsh6hqZ1TZhFfr1UnA3cx2Auk2vEVkPH5wjquk6zS3HjAQTDutBkGWrLcEr32QMNZyMGP11njWoj3EeWnR1Gw8v1THCZaFqq9Cd7q9XMRqXGtSLoXatPPD4GJwfeHnBKyQTt8xt6QeytBRgYNhS16GZ3DyWP78zvLdvDcdfYkrQ19yoNwh1Ne6BTQajTr3ci1p6UDxFkab4LCQkJq9Li49XxwsWzmCLpz9p4GnXbzq3y2XF1Zr9kHpv9Nq3D2JnjLGRUu41wiDwGqoqFKKyy2D3hRWYS7MzsPgCEKCGv34iipoq8yNVc7twSmUvyQtH6q\u0026select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb).\n\n![](/images/dynamic/image-1-.png)\n\nFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language¬†[AimQL](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2), by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\n\n![](/images/dynamic/image-4-.png)\n\n## ***What if numbers are simply not enough?***\n\nVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\n\nAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\n\n```\naim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n```\n\nYou can find the complete guide to tracking in the¬†[official documentation](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#).\n\nAll the grouping/filtering/aggregation functional presented above is also available for Images.\n\n![](/images/dynamic/image-2-.jpeg)\n\n![](/images/dynamic/image-3-.jpeg)\n\n![](/images/dynamic/image-5-.png)\n\nThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\n\n![](/images/dynamic/image.gif)\n\nThe grouping is rather flexible with a¬†[multitude of options](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer).\n\nAnother interesting thing that one can¬†[track is distributions](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7). For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\n\n```\nfrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n```\n\nDistributions can be accessed from the Single Run Page as well.\n\n![](/images/dynamic/image-6-.png)\n\nYou can play around with the image and single run explorers and see all the trackables across our numerous Demos ([FS2](http://play.aimstack.io:10004/),¬†[Spleen Segmentation](http://play.aimstack.io:10005/),¬†[Lightweight GAN](http://play.aimstack.io:10002/),¬†[Machine Translation](http://play.aimstack.io:10001/)).\n\n***One Run to rule them all***\n\nThere are times when a researcher would need to focus upon only a¬†[single run of the experiment](https://aimstack.io/blog/new-releases/aim-3-7-revamped-run-single-page-and-aim-docker-image), where he can iterate through a complete list of all the things he tracked. Aim has a dedicated¬†[Single Run Page](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page)¬†for this very purpose.\n\nYou can view all the trackables in the following Tabs:\n\n* Parameters/ Hyperparametrs ‚Äì Everything tracked regarding the experiment\n* Metrics ‚Äì All the Tracked Metrics/Losses etc.\n* System ‚Äì All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\n* Distributions ‚Äì All the distributions tracked (i.e. Flowing gradients and weights)\n* Images ‚Äì All the Image objects saved\n* Audios ‚Äì All the Audio objects saved\n* Texts ‚Äì All the raw text tracked during experimentation\n* Figures ‚Äì All the¬†`matplotlin`/`Plotly`¬†etc. Figures\n* Settings ‚Äì Settings that runs share\n\nLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\n\n![](/images/dynamic/image-7-.png)\n\n![](/images/dynamic/image-8-.png)\n\n![](/images/dynamic/image-9-.png)\n\nYou can look around the individual runs in one of our interactive Demos ([FS2](http://play.aimstack.io:10004/),¬†[Spleen Segmentation](http://play.aimstack.io:10005/),¬†[Lightweight GAN](http://play.aimstack.io:10002/),¬†[Machine Translation](http://play.aimstack.io:10001/)).\n\n- - -\n\n## ***No More localhost. Track experiments remotely***\n\nAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\n\n```\naim init\n# Tringgering aim server\naim server --repo \u003cREPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo \u003cREPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n```\n\nIntegrating this newly created¬†[remote tracking server](https://aimstack.io/blog/new-releases/aim-3-4-remote-tracking-alpha-sorting-deleting-runs)¬†within your experimentation is even easier.\n\n```\n# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n```\n\nAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\n\n![](/images/dynamic/image-4-.jpeg)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling¬† and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"\u003cp\u003eIn this blog post, we show how to use Aim‚Äôs basic to highly advanced functionality in order to track your machine learning experiments with various levels of granularity and detail. We are going to run through basic tracking that every researcher/engineer would need throughout his development cycles, into a complete end-to-end experiment tracker that slices through the task across various dimensions.\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003eStarting from basics: how Aim tracks machine learning experiments\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eMost of the people working within the machine learning landscape have in some capacity tried to optimize a certain type of metric/Loss w.r.t. the formulation of their task. Gone are the days when you need to simply look at the numbers within the logs or plot the losses post-training with¬†\u003ccode\u003ematplotlib\u003c/code\u003e. Aim allows for simple tracking of such losses (as we will see throughout the post, the ease of integration and use is an inherent theme)\u003c/p\u003e\n\u003cp\u003eTo track the losses, simply create an experiment run and add a tracking common like this\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss = some_loss(...)\n    aim_run.track(loss , name='loss_name', context={'type':'loss_type'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou are going to end up with a visualization of this kind.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-2-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA natural question would be, what if we track numerous machine learning experiments with a variety of losses and metrics. Aim has you covered here with the ease of tracking and grouping.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n# Some Preprocessing\n...\n# Training Pipleine\nfor batch in batches:\n    ...\n    loss_1 = some_loss(...)\n    loss_2 = some_loss(...)\n    metric_1 = some_metric(...)\n    aim_run.track(loss_1 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(loss_2 , name='loss_name', context={'type':'loss_type'})\n    aim_run.track(metric_1 , name='metric_name', context={'type':'metric_type'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter grouping, we end up with a visualization akin to this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregating, grouping, decoupling and customizing the way you want to visualize your experimental metrics is rather intuitive. You can view the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003ecomplete documentation\u003c/a\u003e¬†or simply play around in our¬†\u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=sSkH8CfEjL2N2PMMpyeXDLXkKeqDKJ1MtZvyAT5wgZYPhmo7bFixi1ruiqmZtZGQHFR71J6X6pUtj28ZvTcDJ1NWH4geU9TQg2iR7zCqZjH8w7ibFZh7r9JQZmDzGAeEFJ2g3YuHhkdALqh5SyvBQkQ4C25K8gEeqAeyWUuGt8MvgMRwh3pnPLTUTKn9oTTPZpkPJy7zxorCDuDaxyTs5jEcYbf5FNyfebhPfqNSN1Xy7mEzCXFQ6jZwDna66mgMstQwsyJuzzcE23V3uDMcjgoxyc6nM8u9e6tKsSe6RoTthTb6EPtybXY7wkZK4FiKh3QUdG1RQfdYnEbMPmuTkpfT\u0026#x26;chart=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\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive Demo\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\u003cem\u003eRising beyond watching losses\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAt one point across the development/research cycle, we would like to be able to track model hyper-parameters and attributes regarding the architecture, experimental setting, pre/post-processing steps etc. This is particularly useful when we try to gain insights into When and Why our model succeeded or failed. You can easily add such information into an aim run using any pythonic¬†dict¬†or dictionary-like object, i.e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\naim_run['cli_args'] = args\n...\naim_run['traing_config'] = train_conf\n...\naim_run['optimizer_metadata'] = optimizer_metadata\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the previous section, we already mentioned that it is possible to use various customizable groupings and aggregations for your visualizations, furthermore, you can¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\"\u003egroup with respect to the parameters/hyperparameters\u003c/a\u003e¬†that you saved with aim.\u003c/p\u003e\n\u003cp\u003eViewing the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\"\u003estandalone parameters\u003c/a\u003e¬†is fast for each separate Single Run\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#group-by-any-parameter\"\u003eFiltering/grouping your runs using the selected trackable\u003c/a\u003e¬†can be accessed and looked through in our¬†\u003ca href=\"http://play.aimstack.io:10005/metrics?grouping=9AhrZ4Jr2S7DYd155V7XSWY2yErtjsBc2TEDN9bcFnSZcVChZefLkK6YimoWnWxEBw6Gw2hvoTk5QdGoWSkZshXWtEgpdx5m9JJin28jPrpy5js5SZk2dvgTvbL3ruGrYwjZrksVB1jpaYo48DJLpyP2VjPBbk9f9cBd264d5qzas5PkTZvwzK1nKTUBqYJtvA5PgmU9dXBvH45pLjTvuTMzLMYbZzVFvuSaLPSf4Y7AZLS94ehLxz6nrvMCKnpb1N9HatKhydec2jDSxV11joTPQaC51NyxmnDNyDtUKyuj3JrSh4919HBweyXJNBteLF7TMnREJyPaLqZ3ieXzs1i2WCDRtMi8AcY5mkgmmGuhUdiHf4cqBmj2gTyfEuzxcLBVePQstYYZ4KjS4tizFybQiM\u0026#x26;chart=7DTSXL8yGkKjwNDtEcYjw4opymsQFTsrFJVani1UKjeQnySYAJU8THRU8HtWL3VdRyvJz4M6JQ4i5aQhRdAY2eMcNhsWdc3GBGfJ557AxjDiYoBKS4mAFvEyFP7iiBKcQcNZmTYCtF2Kc5v5sYaxoxb1nrG2LHfngen5ec8K96qQFnmgBbmhx6avZ2KFV5SAoY4iJ6Yu9Y5Bs6VENDzHWKCGVTsRsoBaY94TLJmzpxM6CfCdL5W4jEV1rnVmruGM7YwobY4vPBq49DyvMdYkcfQnKudBEUYLcrEavGiiXgXBdNbQeri7D2ErVTDKVzRPf6qiQdYBXfFceHnRaYNRvAuge3mY4ixbYz24rKKtVnaWCkEX9nTUdfYzPzbgDdgs5m8k4QPcQqs4AiHju2RDtrinvYcj9F1GPdepQvrkvqSfxZS1hMSuS68Yxn769UzjCPD7WwjEpobY6C32U5J4DazxZkzdVce3maPQj4osX3BFmP9j9xTR98JPuU3ApP5pUYZqbBpaY9QkFELgLoVR1eixABK8bXyYnu3mDomMUHCmZqi34ttmX2hKVRV2i84zFQ82fq6SAMpHADdSGagp9u2H5CGCWRrBvQ3RmbXzBJ2R6qn41CERpCv2rmBXT9qs71fGQ73a51K7Bft3mBUi83rXdt1wRMHMMi9LSnEeiba4JdaapTrBbZV4tS3m5p9wi9vxeWDtVSTtZjJRqHN96x3FtsAG1xSpcgiWGutBAxw7yS5Ng9Pcd5ZfUCtVhgQdeTqJGZ4ThmJaNNFv2AMo2Vxiu1BdUAmTB3bQ35WJNauQhybmJX61rE83F3s7Ahd47ASLu7e6LEP2JL9XkiCWNG7rLGPv18oHKWkj2RtKcqUbstP8afoRdFhfibnResF7Jnb2Cqa4yET3iPboGmj6APutdy7YEy6itShcuRRx3c5h7ZXMfxU1atMDxK2bEBksu7UYrCzszGDFjuj9PU58TX2Y4m3izdEm9hRALYw4Tp49SdxdE6XMqjRu3XDpwSjFAGZXPZ4i58CL9gLydZmsJVjyuA43i8F6UGz34KhcbGw61hXCKuHmkzTftKyK7MHz7ZvYwa1hthDDdXTayD8BcZsT4VNWYPeHXUiVbYufWapjRDTVkUFVYNFKa7oDVY3uP373sKCZuBL2kVxwbRKniwjLYMndmAWmzF179PaqjtdkvDFqwW8xWDBDJSV2uMxet7Eu5K5qbRUAqZJiHad1yyWpVcmCJgiMzrfLLtMm9jXAEy1P7w5XnhZ1vJbPx5wzzb7p6MNS1EwTPugu1X8GvXeuPuBq2jsd3LuGcbHC3nhPfCJeX8PVxNyEpEKdCMojYThfGnE5UrztsPqoCet9Py1jzEoayNnyD3NVec5RsGFvmRrusHy8GprvAfDrFTmF5ryhM2GXpSELvwHMMcKqXuaUrT4rbEM7Yirh2de7fnncziZjWSv3YV1FjCsSPvwEAnL7jjnm1sn6vig7JCpqD4bJRchSkMQmENcxLfthcd2ZXMga2uHhPAK5vxDLwrdX7J8Npxgur7jbYvAUmvynhWXC5bGCG9uY46ej8Rxwa8iu5LNP2BqpJ1YDWMgbHxoTQnbtokEZbTkwfrRDoCe5wvoGJuC2StnXEfGMjbqFHL7q5kKWeQv4C16cgCv2YE3hEsvc9Q2LLY5HXA2NcGxHSZHWLi4f13nsGzVX6YEiVcYKyDwBcQcv4ZmiV9G2QxUfwhUK3w7ZpATFKq3mjaGZJNTRiBp1RKieBZ2Yd7S9arWjg5cHKeW9STtc5ZeuQjzGuZwwhiwygfUW4PWiTbtJpMjkVuxQLA6zAXGxzmkgrH9weaP4pQHjdTj2gj282RYa6H9p48dtscE4c8fqhkkG71WRrE3ZKpC5hrg8fhbaCyrBFFcwCZAJubz4JGBik7w8vxEbuCJ5MMj7NSChvXHTqnvwaALhqFiVWh2qqwr1z8cXJJ9Z6pZmhvdArAXPPjAr15gVddxFb5yBKeREqb6rM4pf8zSY6fhDnX52TvBP6Jv3cnHbtxsrDmPgVCumRsgenWsDmqfGLKHoJNwmZ9viuzqwa7ZpWpT5wj8xX1BL8ZDBUiEj1yQgxKKj52s5gT8kjHXa2CfKW2VjDEV4qi4Ww67YFAcS5y1qLZGhmyUDsau9yX5AWwo9CbBgukTGxQFSGAiqT61AEkUBQSukH3U2GJkxdtFjRLsCaQRysmna44x9Uxs4TELbh9PSjmdQXC5KegQU6MhqKRq3aWmwTJ8Vr2tuyAaWVLPaBG8Ptme2F9Ru8r3D7ksPDn5hXRhZMHLFRAGkqCoVnTwv1zfGB9b6mJgFCVHbZerUshFPaWNj1quoY8673eNLiDB3c7mJrci6wBxkdXrcTvdKzdhMCMiUthoRjy2dpXu5QuU1puDfMw8W8SsGJfwVxeTYQixkEo1e7URCECqHc31eM4S3fhY7MxmxrSdXpNegTSQwcvEPoWu17PWHFo4k5EeFaWnAsHb3FxYEYeBHWhPB2i8uj3oR4tAr77ZSWxPxu8dZTywFpK8y5wVxnn7kU3kzrXjZA2M7x532AA4kVaHhYwq4UAzSdYqNxYPJgJVjj31Uq7mEVjQdVCvcMQsWKmMsj8Mf7dpRhry28iEUnTMnMfVDoMHcX9KeGU8kuQ7SCu6QJfXJnva3G5P54AACpDCoAKTH4B5omFSU5KqERKTk5gnEYXjPeB3vUPQyRchdDK5CBeAnxNp3JwfiMCeDuZ4WY3Se2BuxLpW2R54rsMjrRW33yAWhH86y8P77UTfnsQUR2ZCyhpyzhUHDwYeqN3XoegykHwmTkug1APed7n5BUBMRiGEDySh1V8uYSutFLuT3fp3KwExSGh46ihqsxTz3GuKmD7i63s6ZgEnM17tYd4mGGTpnn1bdjAqPyUs59V6c9Fsv94QCzfhx5PgRUazG81TuQ6zd169ZS73TFzAi4eeJP9FXTu41yRkYzEfHAWtov4WvmtcTEiogi8ryPETwD2ELh7gYpRRpY3nBDx9CNuGx4rrpzWhycLXn5qK9Epsd9xDBqD3K8DDDQSZo6pNxd6cvBwo274bH2L4fRrofmcGyHzNT51tXxqDSiokaHTDN6q39A1UwmjATiQK5ZmXoi7mJSZ4PGyAUXZFxGq4GRYao7HKHMWcEArbhQkJgJm2aJo4SH6X5GQNGxiskrJAebNem4a3mLvGP8LwPwTjQfT9QZa47hgm6JTVQnXbQ2BqYfXxqjpomizqCwiYpEQrcKs6vkeCtKdh8Mu824BeomKjV3HouBa4ecopqV8zKudHNSpUcmUPahxQpqe7KKaX6gWnDk7h6UNCwZvrQ28qNFky8s5bniXHxxhk7jiEvqzUS7A4jndMjCAaLTbr6LrUNdNaUybDzi7ZpXomY8o6bJxDjoAgo6WNVaKjQHAMgcRcUptazCqey4jNaAi9gv8jAXAHr8h7iRRqsjuriwp8zfZnkDFvSbbiahYK4FhhYRdimp6eWjuXTbtDsrdS2tcCfCEpKwEf3Q1Vqs5CZrMWwqsGF6nGYGPyNmS8GzaK5ZYQ3u4PF7nVgRkeBwyFpEWXG8fEUm6LXgYSejbEaxBCq6nnheLEWT69FoE7zcbdmRsMrtuKLxmRxUm13W7DkAsktXJHMu2EaeRCkmtqJT8okNWzAkc9aNm9rWaYevhDitrsiZhhGdwQqmLjWMtrHAUtEBh6W7XbGgRSmV9XxXb3fc6RaQTdEwqT76RT8GHNLgTnS5Rs2Ek9oWZkVuk7VQbBHo8JP11z49my3DNL1yu9ec5XpwGyd6SUaQW2GdJ4rprexUDAPzEUUzeFJB1ucNDnrswMCHEtVeqp6cjCTZQ3sLzJCv8JTDRYs6UgnRsnSkVMTtSLa4GkZmcTAZqSitxmWEbPtGgUkAqfugMRHqsAqCMBrkCkJ8fh6f7ffFYMyAGv3y6eY3DAQwSqUqr3k6oHVhMSN28A6rn91r146vNSsSa9NTTCdLGhuS2uxEJuKHhhoVvbzq1DQtup6spCYDFJ8Upxdhdypq7SXXaS99KtH8bW5p8i6DcpAQ4YBzJ6F5E1tSZQQhzb1abByA51fnQRqfZqhXZjWh9ky6A1hmDLhjLfrL9jthecYGbWq6bbnac4DrmrHggWN5KfUPJVX3ix4brpppdDrrZQZ5HUo3e6ps67hyTEd8n86ofcWwFqq44aPabCVD8yRFZysUyccKTpVaCgthAysW9zCefN6wFPsurVZA6bVbzdivXHB9CtEUjwexM439i7faF4VmZFhXjENh7TdGCghePGnKNnTkhM3FjPUAV2Udfq19jFvY21cYbTNE7KCKXbb7QD8nr9KXMKv71k1XsrL4QDz7owCmu7JXYvQpwzxZxh2Ac4RWtoP1qG2f9EcAVpiyNv6WPM8gxQgMfYK2EwnZAiHQQbM3qT7YL8QTNKAXkvuvew4uSbNPgQtcGYYvAnDz6LBvBVabV6aAEzU7RvfNJQMEczxjy2gkAbY8rEfqgFFXDBWPFs5WS4wjAqrhTMcDqyQQhMX3wd8K8G9qqoXTUBsW4vg74ZAUMmiSTdJ4wJZhSVunr9dYtJntPGZvDNzoLxgJfxyzPLuF5v8Xscs41LPifQWa3uCCLvqWPfeYW1FDELfsSskFGYmcs8r11t1Yu5GGRsbxxrpi5PFTr5dgdAUGqb7eUvX87H4PfAhDT6vWcAeemifpbQjBGGPY3idrDBn1tb9MqxsGXNrRuBMhBd1q3SyVARZ1Jcu4stfmfRLSaXYr7FMvZGyjUAtErby22J87JAKUZY4D3LQAA1bagYNQDuwmx525W14CCWG996KDMzG8gm1Ruvn1Y72HCJ8t9zqwuuPTkoMFPb13Mn4M8mR4xnCwvoh21MKvjHizhTmGoUQ86UVYg3gjqFuLxxhyZTDvw517qdzmGqcPdd1EhTRJFHc1yfq5YxaQ2291XgFXHZfaghmRFzn2JhoD8gwVtR72pA9bgPYprJLQZvDzAz5EPmthHbcQNPM9B3V3Y6Ds6Wj9kcJesqAUohi4wmf46vJrN1CxqkKq8rVW6E2kWUDYm8WKUYUvYKWrkQMgjcccQwsaGcNNKYUaumrQeA1BcCpCkm9Z9JMdWCHJhd1Z7GHZotRFyHigJ7YR4fMWpoVQ6eFfqZFW11UUgpRGUYJqHBb1z9mybkmgCFKjC6XKB5qocLZkVhd589cHgLXz6797eTmZtzcRFeFz45GpECtzSGLXQ95BUE1h1k5xYrmNAYeKxLojL6ugcu4hYuTiEihGfzRyLhNoY5Jh63Akdnkx6mLjoSaSo3UpjjTcNYsCCDjFWRJgPnzxVdA11WGfYomVy74vS3asBw7ropcGvjAsPF754tsdbpTgqNs5bacd8s3HUvxYRo7jVWQAhb62qsXqwRMNUWakvAQioXmPbLAwP4Vq8v3wiCH1D4gFKWs7AVnaBGf1WCA1adnhbYT12tGGnMhYqsPZTtUAqrR55TM2VDaSiDW3e5S17kzb3weNtjRVc93FAeLF7vm65kNxnDYchB8RcVEhpSn1y4bWscb8jCAoU7DcatK2ksaYjvEE6XxhC21yjcSzLH9283WGh2rFa8u5WBSneqZmYFH97V2Sv3XWdoeUkGb8ouqFKTCtK4BurMLhVFhosjYaMyyuJW9Gz1oGQyhzH5Ff56qYriu8Q4t69v2qyin5B8dgAfZzAM9qme4uhDNCLUH2v4GV5zYnh36YLau8iSDB6sN1XeuzCA9rDTq25FVbGVHvRGDRr8RkQusVcCnRHPx3YhmrSJymp91fiBxDRif7TDaaukad8hNodnkMVJpDN2gYrrJVYraoJcjJ5b6KfZzSJfP8x9ChVMhq8DcrzezNeXpDtJBCnAX99ayKvSXEEstQMJfAYHYip1d9MUqgCnJeA8B8byDt9HgY7gdq24s4MCP1nC5pmtDWeSp2VHCXUCp5bf4zJJ1RGUMcJDfxvu1SzT8ApwGLLFNz7BWPgHkibU4mPXAzHhBj8Ms9BgBZf8StsVXLDewaqbgDjWkoTNZEeJVgNbJsG3uzXbuCDxbCLahkJSPRkvtLgh5KkK15jR3epzrQzHfMDivm6aPbj1A6vJwBLsqjHqkapnXZVfcZYWzSRQ2ZYGQjw6Ph4isyFA4p8mkJafUqtZjvBUWcgDRyjQNA4PLE7B6syMY3g9HwJbkfsh6hqZ1TZhFfr1UnA3cx2Auk2vEVkPH5wjquk6zS3HjAQTDutBkGWrLcEr32QMNZyMGP11njWoj3EeWnR1Gw8v1THCZaFqq9Cd7q9XMRqXGtSLoXatPPD4GJwfeHnBKyQTt8xt6QeytBRgYNhS16GZ3DyWP78zvLdvDcdfYkrQ19yoNwh1Ne6BTQajTr3ci1p6UDxFkab4LCQkJq9Li49XxwsWzmCLpz9p4GnXbzq3y2XF1Zr9kHpv9Nq3D2JnjLGRUu41wiDwGqoqFKKyy2D3hRWYS7MzsPgCEKCGv34iipoq8yNVc7twSmUvyQtH6q\u0026#x26;select=nRQXLnzJjzDBzMLB3gt1tALq8LJuSijBuau3LyWmSS6Vs1q1nAq4EnJj8B9RDW4NfkmHXeWB4bTG7j4V3cTmta8mai2rg8qPSakFJPPAv2P6E1x743h8tgv1w3jM6YezL5fxoFob5jRmZCny2eSx8zom9Qn4pzxghL3QhXKtoXP9rYkVZjAX4ygouGixW1zptzZL4sWJpB7XCm5T6iofmEa982TuLyChnTJEJVhSaYnpKPpJerJF9fzAPdpUgiGLGuw14fLfhd72ZbXjqMSvE2YG2YQc96yUMfo2YUtjfaAeez7D89DhqBRrCaK4Yj4Mr4TAxahAo7YT2j1cSU52L1h2KdaSDaXx5kWMFSPxWHLMswAdZUznB1nFx9YjLnsyZiqNDcE76zC9AZzfNYjfnnG2MLKHAKMQ7c4tbfXczLBWWVs3gPsz7wNzywaeQ4N1audqH4MGVKBUeeAFSiX2FbEXuzK57EkmaLg3rAC4WrDMi8WC6t3b75o3XkdkZrwoR6eDHVrhUaa4fr5CeMuFSSTzzPUm25gSUGwWHiXxV4So7FxJ8UDqCfm46DGDQLpKgAHAvRSFeHxT5bvCkXgpUJykrJLNmtsGxijMqi6Dgidd4VcUgkjd6iE8k7UzuHWCv4JSD6RyspgAei1p6YK5rdKdtQwhFm1YBRqSuaKBzn2GhRztAfC23jb\"\u003einteractive tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-1-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFor a more in-depth/hands-on filtering of metadata you track in machine learning experiments, you can use our very own pythonic search query language¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#id2\"\u003eAimQL\u003c/a\u003e, by simply typing a pythonic query on the search bar. Note that AimQL support auto-completion can access all the tracked parameters and hyperparameters.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-4-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eWhat if numbers are simply not enough?\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eVarious machine learning tasks can involve looking through intermediate results that the model produced in order to understand the generalization capacity, rate of convergence or prevent overfitting among many other uses. Such tasks involve but are not limited to semantic segmentation, object detection, speech synthesis etc.\u003c/p\u003e\n\u003cp\u003eAim supports tracking objects such as images (PIL, numpy, tf.tensors, torch Tensors etc.), audios (Any kind of wav or wav-like), Figures and animations (matplotlib, Plotly, etc.). Tracking of these objects is completely reminiscent of the loss tracking:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = aim.Run(experiment='some experiment name')\n...\naim_run.track(\n\taim.Image(image_blob, caption='Image Caption'), \n\tname='validation',\n\tcontext={'context_key': 'context_value'}\n)\n...\naim_run.track(\n\taim.Figure(figure), \n\tname='some_name', \n\tcontext={'context_key':'context_value'}\n)\n...\n# You can also track a set of images/figures/audios\naim_run.track(\n\t[\n\t\taim.Audio(audio_1, format='wav', caption='Audio 1 Caption'),\n\t\taim.Audio(audio_2, format='wav', caption = 'Audio 2 Caption')\n\t],\n  name='some_name', context={'context_key': 'context_value'}\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can find the complete guide to tracking in the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#\"\u003eofficial documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAll the grouping/filtering/aggregation functional presented above is also available for Images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-2-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-3-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-5-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Figures that one tracked during experimentation can be accessed through the Single Run Page which we will present in the next section.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe grouping is rather flexible with a¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html#images-explorer\"\u003emultitude of options\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAnother interesting thing that one can¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#id7\"\u003etrack is distributions\u003c/a\u003e. For example, there are cases where you need a complete hands-on dive into how the weights and gradients flow within your model across training iterations. Aim has integrated support for this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.pytorch import track_params_dists, track_gradients_dists .... track_gradients_dists(model, aim_run)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDistributions can be accessed from the Single Run Page as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-6-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can play around with the image and single run explorers and see all the trackables across our numerous Demos (\u003ca href=\"http://play.aimstack.io:10004/\"\u003eFS2\u003c/a\u003e,¬†\u003ca href=\"http://play.aimstack.io:10005/\"\u003eSpleen Segmentation\u003c/a\u003e,¬†\u003ca href=\"http://play.aimstack.io:10002/\"\u003eLightweight GAN\u003c/a\u003e,¬†\u003ca href=\"http://play.aimstack.io:10001/\"\u003eMachine Translation\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eOne Run to rule them all\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThere are times when a researcher would need to focus upon only a¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-7-revamped-run-single-page-and-aim-docker-image\"\u003esingle run of the experiment\u003c/a\u003e, where he can iterate through a complete list of all the things he tracked. Aim has a dedicated¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#single-run-page\"\u003eSingle Run Page\u003c/a\u003e¬†for this very purpose.\u003c/p\u003e\n\u003cp\u003eYou can view all the trackables in the following Tabs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eParameters/ Hyperparametrs ‚Äì Everything tracked regarding the experiment\u003c/li\u003e\n\u003cli\u003eMetrics ‚Äì All the Tracked Metrics/Losses etc.\u003c/li\u003e\n\u003cli\u003eSystem ‚Äì All the system information tracked within the experimentation (CPU/GPU temperature, % used, Disk info etc.)\u003c/li\u003e\n\u003cli\u003eDistributions ‚Äì All the distributions tracked (i.e. Flowing gradients and weights)\u003c/li\u003e\n\u003cli\u003eImages ‚Äì All the Image objects saved\u003c/li\u003e\n\u003cli\u003eAudios ‚Äì All the Audio objects saved\u003c/li\u003e\n\u003cli\u003eTexts ‚Äì All the raw text tracked during experimentation\u003c/li\u003e\n\u003cli\u003eFigures ‚Äì All the¬†\u003ccode\u003ematplotlin\u003c/code\u003e/\u003ccode\u003ePlotly\u003c/code\u003e¬†etc. Figures\u003c/li\u003e\n\u003cli\u003eSettings ‚Äì Settings that runs share\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking through all these insights iteratively can allow for an in-depth granular assessment of your experimentation. Visually it has the following form\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-7-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-8-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-9-.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can look around the individual runs in one of our interactive Demos (\u003ca href=\"http://play.aimstack.io:10004/\"\u003eFS2\u003c/a\u003e,¬†\u003ca href=\"http://play.aimstack.io:10005/\"\u003eSpleen Segmentation\u003c/a\u003e,¬†\u003ca href=\"http://play.aimstack.io:10002/\"\u003eLightweight GAN\u003c/a\u003e,¬†\u003ca href=\"http://play.aimstack.io:10001/\"\u003eMachine Translation\u003c/a\u003e).\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\u003cem\u003e\u003cstrong\u003eNo More localhost. Track experiments remotely\u003c/strong\u003e\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003eAim provides an opportunity for users to track their experiments within a secluded server outside of our local environment. Setting up within a server can be easily completed within command line commands\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim init\n# Tringgering aim server\naim server --repo \u0026#x3C;REPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n# Tringgering aim UI frontend\naim up --repo \u0026#x3C;REPO_PATH\u003e --host 0.0.0.0 --port some_open_port\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIntegrating this newly created¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-4-remote-tracking-alpha-sorting-deleting-runs\"\u003eremote tracking server\u003c/a\u003e¬†within your experimentation is even easier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# This is an example aim://ip:port\nremote_tracking_server = 'aim://17.116.226.20:12345'\n# Initialize Aim Run\naim_run = aim.Run(\n\texperiment='experiment_name', \n\trepo=remote_tracking_server\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter this, you can view your experiments tracked live from the ip:port where you hosted Aim UI. Our very own Demos are a clear example of this.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/image-4-.jpeg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling¬† and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community,\u003c/a\u003e share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-from-zero-to-hero.md","_raw":{"sourceFilePath":"posts/aim-from-zero-to-hero.md","sourceFileName":"aim-from-zero-to-hero.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-from-zero-to-hero"},"type":"Post"},{"title":"Aim tutorial for Weights and Biases users","date":"2022-09-07T21:13:32.862Z","author":"Hovhannes Tamoyan","description":"Using Aim‚Äôs Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026 Biases runs into the native format \u0026 explore them via Aim UI.","slug":"aim-tutorial-for-weights-and-biases-users","image":"https://miro.medium.com/max/1400/1*PBF_k6VevuUrquadJ86vfA.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"I am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\n\nI have been using¬†[Aim](https://github.com/aimhubio/aim)¬†for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\n\nThere are lots of converters already available to migrate or use along with other experiment trackers:¬†[Aim Converters](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html).\n\nUsing Aim‚Äôs Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026 Biases (wandb) runs into the native format and explore them via¬†[Aim UI](https://aimstack.readthedocs.io/en/latest/ui/overview.html).\n\nIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\n\n# Example Project Setup\n\nLet‚Äôs take a look at a concrete example. We will be using¬†`keras-tuner`¬†to train a¬†`CNN`¬†on¬†`Cifar10`¬†dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be¬†`sample-team`:\n\n```\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n```\n\nModel creation, data loading and other parts can be done as:\n\n```\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n```\n\nThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let‚Äôs convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\n\n\u003e *Note: During this project I noticed that the*¬†`wandb.keras.WandbCallback`¬†*was tracking the metrics on epoch end. For better experience we recommend using the*¬†`aim.keras_tuner.AimCallback`*: to track all the metrics on batch end.*\n\n# Converting runs from Wandb to Aim\n\nTo be able to explore Weights \u0026 Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\n\nPick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\n\n```\n$ aim init\n```\n\nTo start converting wandb experiments from entity/team:¬†`sample-team`¬†and project:¬†`my-awesome-project`¬†run:\n\n```\n$ aim convert wandb --entity sample-team --project my-awesome-project\n```\n\nThe converter will iterate over all the experiments in the project¬†`my-awesome-project`¬†and create a distinct Aim run for each experiment.\n\nFor more please see the¬†[‚ÄúShow Weights and Biases logs in Aim‚Äù](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim)¬†section in¬†[Aim docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nNow that we have our Aim repository initialized, we simply need to do:\n\n```\n$ aim up\n```\n\nto start the Aim UI.\n\n# Exploring the differences in Aim and wandb UIs\n\nTo see the configs, metadata and more on wandb we need to navigate to the ‚ÄúOverview‚Äù page:\n\n![](https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp)\n\nUnder the Config section, our logged configurations are shown. On the Summary page the metrics‚Äô latest results.\n\nThe wandb run ‚ÄúOverview‚Äù page counterpart on Aim UI is the individual run page, which can be accessed from the ‚ÄúRuns‚Äù page:\n\n![](https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp)\n\nMore information can be found when opening an individual run‚Äôs overview page.\n\n![](https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp)\n\nOn this page we can see the params, even the¬†`wandb_run_id`¬†and the¬†`wandb_run_name`¬†which are extracted from wandb runs during the conversion process.\n\nMore detailed information about the run can be found on each of the tabs, e.g. ‚ÄúRun Params‚Äù, ‚ÄúMetrics‚Äù, ‚ÄúSystem‚Äù etc.\n\n# Filtering and Grouping the Metrics\n\nOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\n\nTo see the tracked metrics and compare them with other runs we need to open the ‚ÄúWorkspace‚Äù page on wandb:\n\n![](https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp)\n\nTo see the metrics on Aim you need to navigate to the ‚ÄúMetrics‚Äù page. By default you will see an empty page with ‚Äúno results‚Äù message (which I hope they make this experience better soon), this is because we haven‚Äôt selected any metrics to show. To do that we can use the ‚Äú+ Metrics‚Äù button to open up the list of available metrics and select the desired metrics, e.g. ‚Äúloss‚Äù and ‚Äúaccuracy‚Äù:\n\n![](https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp)\n\nThis is the view we will get after selecting to show the loss and accuracy:\n\n![](https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp)\n\nNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan‚Äôt to see the plots by metric name, so each plot will represent a metric‚Äôs values.\n\n![](https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp)\n\nWe simply need to access the¬†`metric.name`¬†.\n\n![](https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp)\n\nTo have unique coloring for each run, we can use the ‚ÄúGroup by color‚Äù functionality, and provide the run.hash. Aim assigns a unique hash to each run.\n\n\u003e *Note: in this example the Aim UI automatically assigns individual colors for each run.*\n\n\n\nNote: in this example the Aim UI automatically assigns individual colors for each run.)\n\n\\\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the ‚Äúadvanced search mode‚Äù click on the pen button under the search button:\n\n![](https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp)\n\nthis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\n\n```\n((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n```\n\nThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\n\n```\nmetric.name in [\"accuracy\", \"loss\"]\n```\n\nFor more please see the ‚Äú[Search Runs](https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs)‚Äù page on docs.\n\n# Hyperparameter Explorer\n\nTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the ‚ÄúAdd Panel‚Äù button, and add the parameters and the metrics we need, e.g.:\n\n![](https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp)\n\nAim counterpart parameters explorer is on the page of ‚ÄúParams‚Äù. Where by using the ‚Äú+ Run Params‚Äù button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\n\n![](https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp)\n\n## Plot Operations\n\nThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‚Äòreadable‚Äô and explicit. On wandb one needs to click on ‚ÄúEdit panel‚Äù sign to see this view:\n\n![](https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp)\n\n\\\nOn Aim you can find the operations on the right sidebar of the Metrics page:\n\n![](https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp)\n\n# Conclusion\n\nAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I‚Äôve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please¬†[read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\nAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.","html":"\u003cp\u003eI am Hovhannes - ML researcher at YerevaNN. Experiment trackers are key for any researchers day-to-day and I have had my fair share of trials and turbulations with them.\u003c/p\u003e\n\u003cp\u003eI have been using¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†for my projects for the past 6 months and I am addicted to it! So I have put together this basic tutorial on Aim for Wandb users (for my friends!).\u003c/p\u003e\n\u003cp\u003eThere are lots of converters already available to migrate or use along with other experiment trackers:¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html\"\u003eAim Converters\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUsing Aim‚Äôs Explorers along with your Wandb dashboard is easy. Aim allows you to convert Weights \u0026#x26; Biases (wandb) runs into the native format and explore them via¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\"\u003eAim UI\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn this blog/post we will go over the steps required to migrate wandb logs to Aim. For that, we will use a sample project.\u003c/p\u003e\n\u003ch1\u003eExample Project Setup\u003c/h1\u003e\n\u003cp\u003eLet‚Äôs take a look at a concrete example. We will be using¬†\u003ccode\u003ekeras-tuner\u003c/code\u003e¬†to train a¬†\u003ccode\u003eCNN\u003c/code\u003e¬†on¬†\u003ccode\u003eCifar10\u003c/code\u003e¬†dataset. Create a project and track the experiments using wandb. We will set the team name/entity to be¬†\u003ccode\u003esample-team\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport wandb\nfrom wandb.keras import WandbCallback\nwandb.init(project=\"my-awesome-project\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eModel creation, data loading and other parts can be done as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = inputs\n    for i in range(hp.Int(\"conv_blocks\", 3, 5, default=3)):\n        filters = hp.Int(\"filters_\" + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.ReLU()(x)\n        if hp.Choice(\"pooling_\" + str(i), [\"avg\", \"max\"]) == \"max\":\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dense(hp.Int(\"hidden_size\", 30, 100, step=10, default=50), activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(\"dropout\", 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n\ndata = tfds.load(\"cifar10\")\ntrain_ds, test_ds = data[\"train\"], data[\"test\"]\n\n\ndef standardize_record(record):\n    return tf.cast(record[\"image\"], tf.float32) / 255.0, record[\"label\"]\n\n\ntrain_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\ntest_ds = test_ds.map(standardize_record).cache().batch(64)\n\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[WandbCallback(tuner=tuner)],\n)\n\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis code snippet should train multiple CNNs, by varying their parameters. After the parameter search and the training is succeeded let‚Äôs convert these runs to Aim format, and store it in an Aim repo after which we will explore the same experiment on both UIs.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote: During this project I noticed that the\u003c/em\u003e¬†\u003ccode\u003ewandb.keras.WandbCallback\u003c/code\u003e¬†\u003cem\u003ewas tracking the metrics on epoch end. For better experience we recommend using the\u003c/em\u003e¬†\u003ccode\u003eaim.keras_tuner.AimCallback\u003c/code\u003e\u003cem\u003e: to track all the metrics on batch end.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eConverting runs from Wandb to Aim\u003c/h1\u003e\n\u003cp\u003eTo be able to explore Weights \u0026#x26; Biases (wandb) runs with Aim, please run the wandb to Aim converter. All the metrics, tags, config, artifacts, and experiment descriptions will be converted and stored in the .aim repo.\u003c/p\u003e\n\u003cp\u003ePick a directory where our .aim repo will be initialized and navigate to it, after which init an empty aim repo by simply doing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo start converting wandb experiments from entity/team:¬†\u003ccode\u003esample-team\u003c/code\u003e¬†and project:¬†\u003ccode\u003emy-awesome-project\u003c/code\u003e¬†run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim convert wandb --entity sample-team --project my-awesome-project\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe converter will iterate over all the experiments in the project¬†\u003ccode\u003emy-awesome-project\u003c/code\u003e¬†and create a distinct Aim run for each experiment.\u003c/p\u003e\n\u003cp\u003eFor more please see the¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-weights-and-biases-logs-in-aim\"\u003e‚ÄúShow Weights and Biases logs in Aim‚Äù\u003c/a\u003e¬†section in¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNow that we have our Aim repository initialized, we simply need to do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eExploring the differences in Aim and wandb UIs\u003c/h1\u003e\n\u003cp\u003eTo see the configs, metadata and more on wandb we need to navigate to the ‚ÄúOverview‚Äù page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*N46ps6lpOp8-HOEN4MzROw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eUnder the Config section, our logged configurations are shown. On the Summary page the metrics‚Äô latest results.\u003c/p\u003e\n\u003cp\u003eThe wandb run ‚ÄúOverview‚Äù page counterpart on Aim UI is the individual run page, which can be accessed from the ‚ÄúRuns‚Äù page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*QBZdJ6L2eiC7t_LYeGoR_A.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eMore information can be found when opening an individual run‚Äôs overview page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Uy3ditnEHQ_Zw4i5kzn9Bg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOn this page we can see the params, even the¬†\u003ccode\u003ewandb_run_id\u003c/code\u003e¬†and the¬†\u003ccode\u003ewandb_run_name\u003c/code\u003e¬†which are extracted from wandb runs during the conversion process.\u003c/p\u003e\n\u003cp\u003eMore detailed information about the run can be found on each of the tabs, e.g. ‚ÄúRun Params‚Äù, ‚ÄúMetrics‚Äù, ‚ÄúSystem‚Äù etc.\u003c/p\u003e\n\u003ch1\u003eFiltering and Grouping the Metrics\u003c/h1\u003e\n\u003cp\u003eOne of the main difference between wandb and aim UIs is that wandb by default presents each metric of each run on a separate plot. Meanwhile aim does exactly the opposite. The metrics are grouped by default and one needs to regroup them into smaller logical parts by their needs.\u003c/p\u003e\n\u003cp\u003eTo see the tracked metrics and compare them with other runs we need to open the ‚ÄúWorkspace‚Äù page on wandb:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*7l3GifR4JRJsh1Ezib-tZQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo see the metrics on Aim you need to navigate to the ‚ÄúMetrics‚Äù page. By default you will see an empty page with ‚Äúno results‚Äù message (which I hope they make this experience better soon), this is because we haven‚Äôt selected any metrics to show. To do that we can use the ‚Äú+ Metrics‚Äù button to open up the list of available metrics and select the desired metrics, e.g. ‚Äúloss‚Äù and ‚Äúaccuracy‚Äù:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*cMw_urZ5rtGwvZhMP1rn6A.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the view we will get after selecting to show the loss and accuracy:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*HzjqORKIIhn9_ArX0d18UQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow as mentioned before aim groups metrics by default, to split them we need to select some criterions. Say we wan‚Äôt to see the plots by metric name, so each plot will represent a metric‚Äôs values.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*rjCuyj66TRfcgwV8ncpBqA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe simply need to access the¬†\u003ccode\u003emetric.name\u003c/code\u003e¬†.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*V-Pzr7-GK1Figl_oiZdfcA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo have unique coloring for each run, we can use the ‚ÄúGroup by color‚Äù functionality, and provide the run.hash. Aim assigns a unique hash to each run.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eNote: in this example the Aim UI automatically assigns individual colors for each run.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNote: in this example the Aim UI automatically assigns individual colors for each run.)\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nTo select runs with non-trivial comparisons we can use a custom query instead of just adding the metrics. To enable the ‚Äúadvanced search mode‚Äù click on the pen button under the search button:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*lAfBed9fb1D_IqXzVCGiPA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003ethis operation will automatically convert the selections into the form of a query. In our case the query should look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e((metric.name == \"accuracy\") or (metric.name == \"loss\"))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe syntax of Aim query is Pythonic: we can access the properties of objects by dot operation make comparisons and even more. Meanwhile on wandb the operations are limited. For example we can transform our current query into more compact format by just doing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emetric.name in [\"accuracy\", \"loss\"]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more please see the ‚Äú\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/run_management.html#search-runs\"\u003eSearch Runs\u003c/a\u003e‚Äù page on docs.\u003c/p\u003e\n\u003ch1\u003eHyperparameter Explorer\u003c/h1\u003e\n\u003cp\u003eTo explore the hyperparameters and their importance on wandb we need to create a new panel. To do that we need to click on the ‚ÄúAdd Panel‚Äù button, and add the parameters and the metrics we need, e.g.:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*DfyPTNmUZJd8OOWmc3pVrg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim counterpart parameters explorer is on the page of ‚ÄúParams‚Äù. Where by using the ‚Äú+ Run Params‚Äù button we can add the parameters and the metrics we need to compare. It works super fast, feel free to add as many parameters as you want! :)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TvIf8CIeMVMbypLZrqqmyQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePlot Operations\u003c/h2\u003e\n\u003cp\u003eThe operations with plots such as scale change, ignoring outliers, smoothing and more are one of the necessary things to make the graphs more ‚Äòreadable‚Äô and explicit. On wandb one needs to click on ‚ÄúEdit panel‚Äù sign to see this view:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*wrXfeQaHLeNrMAjyxN1jcw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nOn Aim you can find the operations on the right sidebar of the Metrics page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*DG0LGNShSWbS-9d_PTb3wA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eAs much as both of the experiment tracking tools might look similar, they both have individual goals and functionalities. In this simple guid I‚Äôve tried to help a user to transition from wandb to aim or use both. We drawn parallels between their UIs major functionalities. For more please¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eread the docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAim is an open-source rapidly growing experiment tracking tool, new features are added periodically and the existing ones are made even faster and better. The maintainers are open to contributions and are super helpful along with the overall community.\u003c/p\u003e"},"_id":"posts/aim-tutorial-for-weights-and-biases-users.md","_raw":{"sourceFilePath":"posts/aim-tutorial-for-weights-and-biases-users.md","sourceFileName":"aim-tutorial-for-weights-and-biases-users.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-tutorial-for-weights-and-biases-users"},"type":"Post"},{"title":"Aim v2.2.0 ‚Äî Hugging Face integration","date":"2021-03-24T13:20:24.937Z","author":"Gev Soghomonian","description":"Aim 2.2.0 featuring Hugging Face integration is out! Hugging Face integration has been one of the most requested features and we are excited to finally ship it.","slug":"aim-v2-2-0-hugging-face-integration","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MqbllMOE307ZvLLuH0MhvA.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)¬†2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\n\nThanks to¬†[siddk](https://github.com/siddk),¬†[TommasoBendinelli](https://github.com/TommasoBendinelli)¬†and¬†[Khazhak](https://github.com/Khazhak)¬†for their contribution to this release.\n\n\u003e **Note on the Aim versioning:**¬†The previous two release posts:¬†[Aim 1.3.5](https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment)¬†and¬†[Aim 1.3.8](https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring)¬†had used the version number of¬†[AimUI](https://aimstack.readthedocs.io/en/latest/ui/overview.html)¬†as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.¬†[Check out the Aim CHANGELOG](https://github.com/aimhubio/aim/blob/main/CHANGELOG.md).\n\nWe have also added¬†[milestones](https://github.com/aimhubio/aim/milestones)¬†for each version. As well as the¬†[Roadmap](https://github.com/aimhubio/aim#roadmap).\n\nCheck out the new features at¬†[play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\n## Hugging Face integration\n\nHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\n\n```\nfrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n```\n\nHere is how it works:¬†`aim_callback`¬†will automatically open an Aim¬†`Session`¬†and close it when the trainer is done. When¬†`trainer.train()`,¬†`trainer.evaluate()`¬†or¬†`trainer.predict()`¬†is called,¬†`aim_callback`¬†will automatically log the hyper-params and respective metrics.\n\nFind a full example¬†[here](https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py).\n\n## Metric Visibility Control\n\nWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\n\nA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*yZAWw55lUWVCa35zptdCrA.gif \"Hide individual metrics as well as collectively\")\n\n## Column resize\n\nWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.¬†Resize is available both on Explore and Dashboard tables.Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*KFUs6pmpqOCVhfdWMO6CVg.gif \"Drag column edges back-and-forth to resize\")\n\n## Hide columns with similar values\n\nMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\n\nThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*klN4dR6T8nSHLqyvqIUjOg.gif \"Leave only different columns with a button click, if needed customize afterwards\")\n\n## Logscale\n\nOne of the must-have features that we hadn‚Äôt had a chance to work on. Finally it‚Äôs available!!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cin-2u7a14bj9fB46WmZJw.gif \"log-scale on Aim\")\n\n## New methods for aggregation\n\nNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*xfp0OtVsz6s4vhIdQ1nLIw.gif)\n\n## Learn More\n\n[Aim is on the mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support¬†üôå.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\u003c/p\u003e\n\u003cp\u003eThanks to¬†\u003ca href=\"https://github.com/siddk\"\u003esiddk\u003c/a\u003e,¬†\u003ca href=\"https://github.com/TommasoBendinelli\"\u003eTommasoBendinelli\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/Khazhak\"\u003eKhazhak\u003c/a\u003e¬†for their contribution to this release.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote on the Aim versioning:\u003c/strong\u003e¬†The previous two release posts:¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment\"\u003eAim 1.3.5\u003c/a\u003e¬†and¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring\"\u003eAim 1.3.8\u003c/a\u003e¬†had used the version number of¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\"\u003eAimUI\u003c/a\u003e¬†as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion.¬†\u003ca href=\"https://github.com/aimhubio/aim/blob/main/CHANGELOG.md\"\u003eCheck out the Aim CHANGELOG\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe have also added¬†\u003ca href=\"https://github.com/aimhubio/aim/milestones\"\u003emilestones\u003c/a\u003e¬†for each version. As well as the¬†\u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eRoadmap\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCheck out the new features at¬†\u003ca href=\"http://play.aimstack.io:43900/dashboard\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eHugging Face integration\u003c/h2\u003e\n\u003cp\u003eHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is how it works:¬†\u003ccode\u003eaim_callback\u003c/code\u003e¬†will automatically open an Aim¬†\u003ccode\u003eSession\u003c/code\u003e¬†and close it when the trainer is done. When¬†\u003ccode\u003etrainer.train()\u003c/code\u003e,¬†\u003ccode\u003etrainer.evaluate()\u003c/code\u003e¬†or¬†\u003ccode\u003etrainer.predict()\u003c/code\u003e¬†is called,¬†\u003ccode\u003eaim_callback\u003c/code\u003e¬†will automatically log the hyper-params and respective metrics.\u003c/p\u003e\n\u003cp\u003eFind a full example¬†\u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eMetric Visibility Control\u003c/h2\u003e\n\u003cp\u003eWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\u003c/p\u003e\n\u003cp\u003eA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*yZAWw55lUWVCa35zptdCrA.gif\" alt=\"\" title=\"Hide individual metrics as well as collectively\"\u003e\u003c/p\u003e\n\u003ch2\u003eColumn resize\u003c/h2\u003e\n\u003cp\u003eWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table.¬†Resize is available both on Explore and Dashboard tables.Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*KFUs6pmpqOCVhfdWMO6CVg.gif\" alt=\"\" title=\"Drag column edges back-and-forth to resize\"\u003e\u003c/p\u003e\n\u003ch2\u003eHide columns with similar values\u003c/h2\u003e\n\u003cp\u003eMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\u003c/p\u003e\n\u003cp\u003eThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*klN4dR6T8nSHLqyvqIUjOg.gif\" alt=\"\" title=\"Leave only different columns with a button click, if needed customize afterwards\"\u003e\u003c/p\u003e\n\u003ch2\u003eLogscale\u003c/h2\u003e\n\u003cp\u003eOne of the must-have features that we hadn‚Äôt had a chance to work on. Finally it‚Äôs available!!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*cin-2u7a14bj9fB46WmZJw.gif\" alt=\"\" title=\"log-scale on Aim\"\u003e\u003c/p\u003e\n\u003ch2\u003eNew methods for aggregation\u003c/h2\u003e\n\u003cp\u003eNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*xfp0OtVsz6s4vhIdQ1nLIw.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on the mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support¬†üôå.\u003c/p\u003e"},"_id":"posts/aim-v2-2-0-‚Äî-hugging-face-integration.md","_raw":{"sourceFilePath":"posts/aim-v2-2-0-‚Äî-hugging-face-integration.md","sourceFileName":"aim-v2-2-0-‚Äî-hugging-face-integration.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-2-0-‚Äî-hugging-face-integration"},"type":"Post"},{"title":"Aim v2.3.0 ‚Äî System Resource Usage and Reverse Grouping","date":"2021-04-20T13:39:39.516Z","author":"Gev Soghomonian","description":"Aim 2.3.0 allowing System Resource Usage optimization is out! Some highlights: Reverse grouping, Line Chart Smoothing, New aggregation modes. Check out! ","slug":"aim-v2-3-0-system-resource-usage-and-reverse-grouping","image":"https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim)¬†2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\n\nCheck out the updated Aim at¬†[play.aimstack.io](http://play.aimstack.io:10001/).\n\nBelow are the highlight features of the update. Find the full list of changes¬†[here](https://github.com/aimhubio/aim/milestone/3?closed=1).\n\n## System Resource Usage\n\nNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with¬†`system_tracking_interval==0`.\n\nOnce you run the training, you will see the following. Here is a demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*pOMxDdOpUDIOedrNIM53pQ.gif \"Aim automatic system resource tracking demo\")\n\nOf course you can also¬†query all those new metrics¬†in the Explore page and compare, group, aggregate with the rest of the metrics!\n\n## Reverse grouping (‚Äúagainst‚Äù the param)\n\nAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a¬†Reverse Grouping. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif \"Aim Reverse grouping demo\")\n\nUse the toggle next to switch between grouping modes.\n\n## Line Chart Smoothing\n\nNow you can apply smoothing on metrics. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*MgNuAaH6gzm7u-H2QUh15g.gif \"Aim metric smoothing\")\n\nThere are two type of smoothing options available:¬†`Exponential Moving Average`¬†and¬†`Central Moving Average`. Further info of¬†[how](https://en.wikipedia.org/wiki/Moving_average)¬†it‚Äôs calculated.\n\n## New aggregation modes\n\nWe have additionally added two more aggregation modes:¬†`standard error`¬†and¬†`standard deviation`¬†. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*LS17WlLey5aJfMyJbtgtBg.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim at¬†\u003ca href=\"http://play.aimstack.io:10001/\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow are the highlight features of the update. Find the full list of changes¬†\u003ca href=\"https://github.com/aimhubio/aim/milestone/3?closed=1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSystem Resource Usage\u003c/h2\u003e\n\u003cp\u003eNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with¬†\u003ccode\u003esystem_tracking_interval==0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce you run the training, you will see the following. Here is a demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*pOMxDdOpUDIOedrNIM53pQ.gif\" alt=\"\" title=\"Aim automatic system resource tracking demo\"\u003e\u003c/p\u003e\n\u003cp\u003eOf course you can also¬†query all those new metrics¬†in the Explore page and compare, group, aggregate with the rest of the metrics!\u003c/p\u003e\n\u003ch2\u003eReverse grouping (‚Äúagainst‚Äù the param)\u003c/h2\u003e\n\u003cp\u003eAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a¬†Reverse Grouping. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif\" alt=\"\" title=\"Aim Reverse grouping demo\"\u003e\u003c/p\u003e\n\u003cp\u003eUse the toggle next to switch between grouping modes.\u003c/p\u003e\n\u003ch2\u003eLine Chart Smoothing\u003c/h2\u003e\n\u003cp\u003eNow you can apply smoothing on metrics. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*MgNuAaH6gzm7u-H2QUh15g.gif\" alt=\"\" title=\"Aim metric smoothing\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are two type of smoothing options available:¬†\u003ccode\u003eExponential Moving Average\u003c/code\u003e¬†and¬†\u003ccode\u003eCentral Moving Average\u003c/code\u003e. Further info of¬†\u003ca href=\"https://en.wikipedia.org/wiki/Moving_average\"\u003ehow\u003c/a\u003e¬†it‚Äôs calculated.\u003c/p\u003e\n\u003ch2\u003eNew aggregation modes\u003c/h2\u003e\n\u003cp\u003eWe have additionally added two more aggregation modes:¬†\u003ccode\u003estandard error\u003c/code\u003e¬†and¬†\u003ccode\u003estandard deviation\u003c/code\u003e¬†. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LS17WlLey5aJfMyJbtgtBg.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-v2-3-0-‚Äî-system-resource-usage-and-reverse-grouping.md","_raw":{"sourceFilePath":"posts/aim-v2-3-0-‚Äî-system-resource-usage-and-reverse-grouping.md","sourceFileName":"aim-v2-3-0-‚Äî-system-resource-usage-and-reverse-grouping.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-3-0-‚Äî-system-resource-usage-and-reverse-grouping"},"type":"Post"},{"title":"Aim v2.6.0‚Ää‚Äî‚ÄäDocker requirement removed and Metric as x-axis","date":"2021-06-24T14:25:34.071Z","author":"Gev Soghomonian","description":"Explore Aim v2.6.0: No More Docker Requirement, Metric as x-axis. Enhance your analysis with alternate axes, improving metric correlations. Learn how!","slug":"aim-v2-6-0-docker-requirement-removed-and-metric-as-x-axis","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8Nw162Czv6_lzfQeh2m9A.png","draft":false,"categories":["New Releases"],"body":{"raw":"## Metric as an alternative x-axis\n\nSometimes when comparing the metrics, it‚Äôs not enough to just put them into perspective via subplots.\n\nFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared ‚Äî¬†`loss`¬†and`bleu`. One of the main indicators of a performant model in NMT, is when higher values of¬†`bleu`¬†correlates with the lower values of¬†`loss`.\n\nJust plotting them besides each other sometimes isn‚Äôt enough to see how they correlate. In this case we use¬†`loss`¬†as the x-axis to directly see the correlation between the¬†`loss`and the`bleu`¬†metrics for two groups of runs (¬†`max_k==3`and¬†`max_k == 1`).\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Y0wAL9nZ7IhgLOUr-cCRRQ.gif \"Demo for metric as alternative x-axis\")\n\nUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\n\n\u003e **Note**: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support.","html":"\u003ch2\u003eMetric as an alternative x-axis\u003c/h2\u003e\n\u003cp\u003eSometimes when comparing the metrics, it‚Äôs not enough to just put them into perspective via subplots.\u003c/p\u003e\n\u003cp\u003eFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared ‚Äî¬†\u003ccode\u003eloss\u003c/code\u003e¬†and\u003ccode\u003ebleu\u003c/code\u003e. One of the main indicators of a performant model in NMT, is when higher values of¬†\u003ccode\u003ebleu\u003c/code\u003e¬†correlates with the lower values of¬†\u003ccode\u003eloss\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eJust plotting them besides each other sometimes isn‚Äôt enough to see how they correlate. In this case we use¬†\u003ccode\u003eloss\u003c/code\u003e¬†as the x-axis to directly see the correlation between the¬†\u003ccode\u003eloss\u003c/code\u003eand the\u003ccode\u003ebleu\u003c/code\u003e¬†metrics for two groups of runs (¬†\u003ccode\u003emax_k==3\u003c/code\u003eand¬†\u003ccode\u003emax_k == 1\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Y0wAL9nZ7IhgLOUr-cCRRQ.gif\" alt=\"\" title=\"Demo for metric as alternative x-axis\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-v2-6-0‚Ää‚Äî‚Äädocker-requirement-removed-and-metric-as-x-axis.md","_raw":{"sourceFilePath":"posts/aim-v2-6-0‚Ää‚Äî‚Äädocker-requirement-removed-and-metric-as-x-axis.md","sourceFileName":"aim-v2-6-0‚Ää‚Äî‚Äädocker-requirement-removed-and-metric-as-x-axis.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-6-0‚Ää‚Äî‚Äädocker-requirement-removed-and-metric-as-x-axis"},"type":"Post"},{"title":"Aim v2.7.1 ‚Äî Table export and Explore view bookmarks","date":"2021-07-15T14:31:32.317Z","author":"Gev Soghomonian","description":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback! Check out the updated","slug":"aim-v2-7-1-table-export-and-explore-view-bookmarks","image":"https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif","draft":false,"categories":["New Releases"],"body":{"raw":"Aim v2.7.1¬†is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\n\nCheck out the updated Aim demo at¬†[play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\nBelow are the two main highlights of¬†Aim v2.7.1.\n\n## Aim Table CSV export\n\nNow you can export both tables from¬†Explore¬†and¬†Dashboard¬†to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with¬†[Pandas](https://pandas.pydata.org/)) for your MLOps needs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif \"Exporting CSV from the Explore table\")\n\n## Bookmark/Save the Explore State \\[experimental]\n\nThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you‚Äôd like to revisit.\n\nIn the future¬†[releases](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative), we will build on top of this feature to potentially add notes, fully manage and share them.¬†*For now, this feature is in experimental mode*. Please feel free to share more feedback and how we can improve its experience.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*U5a36lDx7-xgSONKh1-D0A.gif \"Use bookmarks to save important explore states\")\n\n## Learn More\n\n[Aim is on a mission to democratize MLOps tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don‚Äôt forget to leave¬†[Aim](https://github.com/aimhubio/aim)¬†a star on GitHub for support¬†.","html":"\u003cp\u003eAim v2.7.1¬†is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim demo at¬†\u003ca href=\"http://play.aimstack.io:43900/dashboard\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow are the two main highlights of¬†Aim v2.7.1.\u003c/p\u003e\n\u003ch2\u003eAim Table CSV export\u003c/h2\u003e\n\u003cp\u003eNow you can export both tables from¬†Explore¬†and¬†Dashboard¬†to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with¬†\u003ca href=\"https://pandas.pydata.org/\"\u003ePandas\u003c/a\u003e) for your MLOps needs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif\" alt=\"\" title=\"Exporting CSV from the Explore table\"\u003e\u003c/p\u003e\n\u003ch2\u003eBookmark/Save the Explore State [experimental]\u003c/h2\u003e\n\u003cp\u003eThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you‚Äôd like to revisit.\u003c/p\u003e\n\u003cp\u003eIn the future¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003ereleases\u003c/a\u003e, we will build on top of this feature to potentially add notes, fully manage and share them.¬†\u003cem\u003eFor now, this feature is in experimental mode\u003c/em\u003e. Please feel free to share more feedback and how we can improve its experience.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*U5a36lDx7-xgSONKh1-D0A.gif\" alt=\"\" title=\"Use bookmarks to save important explore states\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize MLOps tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don‚Äôt forget to leave¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†a star on GitHub for support¬†.\u003c/p\u003e"},"_id":"posts/aim-v2-7-1-‚Äî-table-export-and-explore-view-bookmarks.md","_raw":{"sourceFilePath":"posts/aim-v2-7-1-‚Äî-table-export-and-explore-view-bookmarks.md","sourceFileName":"aim-v2-7-1-‚Äî-table-export-and-explore-view-bookmarks.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-7-1-‚Äî-table-export-and-explore-view-bookmarks"},"type":"Post"},{"title":"Aim v3.16 ‚Äî Run messages in UI, TensorBoard real-time sync, integration with Hugging Face Datasets","date":"2023-02-15T06:56:43.141Z","author":"Gor Arakelyan","description":"üöÄ Aim v3.16 is out! Users will be able to view all Run messages right in the UI. Integration of Aim with TensorBoard, Hugging Face Datasets, Acme, Stable-Baselines3. ","slug":"aim-v3-16-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-AzW2_QT8nvsgLUQH9JeA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, excited to announce Aim v3.16 is out! üöÄ It is packed with new integrations and key enhancements.\n\n\u003e *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Every question and every issue makes Aim better!*\n\u003e\n\u003e *Congratulations to¬†[timokau,](https://github.com/timokau)¬†[dsblank](https://github.com/dsblank)¬†and¬†[grigoryan-davit](https://github.com/grigoryan-davit)¬†for their first contributions. üôå*\n\n# Key highlights\n\n## **Run messages tab in UI**\n\nStarting from version 3.15, Aim has enabled an ability to¬†[log training messages](https://aimstack.readthedocs.io/en/latest/using/logging.html)¬†and even¬†[send notifications](https://aimstack.readthedocs.io/en/latest/using/notifications.html)¬†to Slack or Workplace.\n\nWith the version 3.16, you‚Äôll be able to view all your¬†[run messages](https://aimstack.readthedocs.io/en/latest/using/logging.html)¬†right in the UI. All the information you need is just a click away! üí´\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxqbmJJcqYaH1tLmcg59Xw.png)\n\n## **Real-time sync with TensorBoard logs**\n\n\n\nAim smoothly integrates with experiment tracking tools. If you‚Äôre part of a team that‚Äôs already deeply integrated TensorBoard into your projects and pipelines, you will love this enhancement.\n\nWith just one simple line of code, you can integrate Aim with your existing TensorBoard projects. This means that all your logs will be automatically converted to Aim format in real-time. üî•üî•üî•\n\n```\nfrom aim.ext.tensorboard_tracker import Run\n\naim_run = Run(\n    sync_tensorboard_log_dir='TB_LOG_DIR_TO_SYNC_RUNS'\n)\n\n```\n\n## **Support for Python 3.11**\n\nThe Python community has received fantastic news towards the end of 2022. The stable Python 3.11 has been officially released!\n\nWith Aim 3.16 release, you can install and use Aim in your python 3.11 projects. üéâ\n\n```\npip3.11 install aim\n```\n\n## **Dropped support for Python 3.6**\n\nTime to say goodbye: Python 3.6 has reached the end-of-life. üíÄ\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpHtGUIfWc5RXtDk0TNKTQ.png)\n\nThis change allows us to take advantage of the latest advancements in the Python language and provide you with a more robust and reliable library.\n\nPlease note that if you have been using Aim in your Python 3.6 projects, you will need to upgrade to newer Python versions in order to continue using Aim.\n\n**We apologize for any inconvenience this may cause, but rest assured that the improved stability of Aim will make it worth the transition.**\n\n# New integrations\n\nAim 3.16 is packed with new integrations with favorite ML tools!\n\n## **Aim + Hugging Face Datasets = ‚ù§**\n\nHappy to share now you can easily track and store dataset metadata in Aim run and explore it on the UI.\n\n```\nfrom datasets import load_dataset\n\nfrom aim import Run\nfrom aim.hf_dataset import HFDataset\n\n# Load the dataset\ndataset = load_dataset('rotten_tomatoes')\n\n# Store the dataset metadata\nrun = Run()\nrun['datasets_info'] = HFDataset(dataset)\n```\n\nSee the docs¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\n\n## **Aim + Acme = ‚ù§**\n\nAim‚Äôs been added a built-in support for tracking¬†[Acme](https://dm-acme.readthedocs.io/en/latest/)¬†trainings. It takes few simple steps to integrate Aim into your training script.\n\n1. Explicitly import the¬†`AimCallback`¬†and¬†`AimWriter`¬†for tracking training metadata:\n\n```\nfrom aim.sdk.acme import AimCallback, AimWriter\n```\n\n2. Initialize an Aim Run via¬†`AimCallback`, and create a log factory:\n\n```\naim_run = AimCallback(repo=\".\", experiment_name=\"acme_test\")\n\ndef logger_factory(\n    name: str,\n    steps_key: Optional[str] = None,\n    task_id: Optional[int] = None,\n) -\u003e loggers.Logger:\n    return AimWriter(aim_run, name, steps_key, task_id)\n```\n\n3. Pass the logger factory to¬†`logger_factory`¬†upon initiating your training:\n\n```\nexperiment_config = experiments.ExperimentConfig(\n    builder=d4pg_builder,\n    environment_factory=make_environment,\n    network_factory=network_factory,\n    logger_factory=logger_factory,\n    seed=0,\n    max_num_actor_steps=5000)\n```\n\n\n\nSee the docs¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-acme).\n\n## **Aim + Stable-Baselines3 = ‚ù§**\n\nNow you can easily track¬†[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)¬†trainings with Aim. It takes two steps to integrate Aim into your training script.\n\n1. Explicitly import the¬†`AimCallback`¬†for tracking training metadata\n\n   ```\n   from aim.sb3 import AimCallback\n   ```\n\n   2. Pass the callback to¬†`callback`¬†upon initiating your training:\n\n   ```\n   model.learn(total_timesteps=10_000, callback=AimCallback(repo='.', experiment_name='sb3_test'))\n   ```\n\n   See the docs¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-stable-baselines3).\n\n   # Learn more\n\n   [Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)¬†üôå\n\n   Try out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n   Don‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful. ‚≠êÔ∏è","html":"\u003cp\u003eHey community, excited to announce Aim v3.16 is out! üöÄ It is packed with new integrations and key enhancements.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Every question and every issue makes Aim better!\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eCongratulations to¬†\u003ca href=\"https://github.com/timokau\"\u003etimokau,\u003c/a\u003e¬†\u003ca href=\"https://github.com/dsblank\"\u003edsblank\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/grigoryan-davit\"\u003egrigoryan-davit\u003c/a\u003e¬†for their first contributions. üôå\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eKey highlights\u003c/h1\u003e\n\u003ch2\u003e\u003cstrong\u003eRun messages tab in UI\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eStarting from version 3.15, Aim has enabled an ability to¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\"\u003elog training messages\u003c/a\u003e¬†and even¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/notifications.html\"\u003esend notifications\u003c/a\u003e¬†to Slack or Workplace.\u003c/p\u003e\n\u003cp\u003eWith the version 3.16, you‚Äôll be able to view all your¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\"\u003erun messages\u003c/a\u003e¬†right in the UI. All the information you need is just a click away! üí´\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxqbmJJcqYaH1tLmcg59Xw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eReal-time sync with TensorBoard logs\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAim smoothly integrates with experiment tracking tools. If you‚Äôre part of a team that‚Äôs already deeply integrated TensorBoard into your projects and pipelines, you will love this enhancement.\u003c/p\u003e\n\u003cp\u003eWith just one simple line of code, you can integrate Aim with your existing TensorBoard projects. This means that all your logs will be automatically converted to Aim format in real-time. üî•üî•üî•\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.ext.tensorboard_tracker import Run\n\naim_run = Run(\n    sync_tensorboard_log_dir='TB_LOG_DIR_TO_SYNC_RUNS'\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cstrong\u003eSupport for Python 3.11\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThe Python community has received fantastic news towards the end of 2022. The stable Python 3.11 has been officially released!\u003c/p\u003e\n\u003cp\u003eWith Aim 3.16 release, you can install and use Aim in your python 3.11 projects. üéâ\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3.11 install aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cstrong\u003eDropped support for Python 3.6\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eTime to say goodbye: Python 3.6 has reached the end-of-life. üíÄ\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpHtGUIfWc5RXtDk0TNKTQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis change allows us to take advantage of the latest advancements in the Python language and provide you with a more robust and reliable library.\u003c/p\u003e\n\u003cp\u003ePlease note that if you have been using Aim in your Python 3.6 projects, you will need to upgrade to newer Python versions in order to continue using Aim.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWe apologize for any inconvenience this may cause, but rest assured that the improved stability of Aim will make it worth the transition.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eNew integrations\u003c/h1\u003e\n\u003cp\u003eAim 3.16 is packed with new integrations with favorite ML tools!\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAim + Hugging Face Datasets = ‚ù§\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eHappy to share now you can easily track and store dataset metadata in Aim run and explore it on the UI.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom datasets import load_dataset\n\nfrom aim import Run\nfrom aim.hf_dataset import HFDataset\n\n# Load the dataset\ndataset = load_dataset('rotten_tomatoes')\n\n# Store the dataset metadata\nrun = Run()\nrun['datasets_info'] = HFDataset(dataset)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the docs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAim + Acme = ‚ù§\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAim‚Äôs been added a built-in support for tracking¬†\u003ca href=\"https://dm-acme.readthedocs.io/en/latest/\"\u003eAcme\u003c/a\u003e¬†trainings. It takes few simple steps to integrate Aim into your training script.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExplicitly import the¬†\u003ccode\u003eAimCallback\u003c/code\u003e¬†and¬†\u003ccode\u003eAimWriter\u003c/code\u003e¬†for tracking training metadata:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.sdk.acme import AimCallback, AimWriter\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eInitialize an Aim Run via¬†\u003ccode\u003eAimCallback\u003c/code\u003e, and create a log factory:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = AimCallback(repo=\".\", experiment_name=\"acme_test\")\n\ndef logger_factory(\n    name: str,\n    steps_key: Optional[str] = None,\n    task_id: Optional[int] = None,\n) -\u003e loggers.Logger:\n    return AimWriter(aim_run, name, steps_key, task_id)\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003ePass the logger factory to¬†\u003ccode\u003elogger_factory\u003c/code\u003e¬†upon initiating your training:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eexperiment_config = experiments.ExperimentConfig(\n    builder=d4pg_builder,\n    environment_factory=make_environment,\n    network_factory=network_factory,\n    logger_factory=logger_factory,\n    seed=0,\n    max_num_actor_steps=5000)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the docs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-acme\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAim + Stable-Baselines3 = ‚ù§\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNow you can easily track¬†\u003ca href=\"https://stable-baselines3.readthedocs.io/en/master/\"\u003eStable-Baselines3\u003c/a\u003e¬†trainings with Aim. It takes two steps to integrate Aim into your training script.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eExplicitly import the¬†\u003ccode\u003eAimCallback\u003c/code\u003e¬†for tracking training metadata\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.sb3 import AimCallback\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ePass the callback to¬†\u003ccode\u003ecallback\u003c/code\u003e¬†upon initiating your training:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emodel.learn(total_timesteps=10_000, callback=AimCallback(repo='.', experiment_name='sb3_test'))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the docs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-stable-baselines3\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful. ‚≠êÔ∏è\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e"},"_id":"posts/aim-v3-16-‚Äî-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","_raw":{"sourceFilePath":"posts/aim-v3-16-‚Äî-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","sourceFileName":"aim-v3-16-‚Äî-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v3-16-‚Äî-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets"},"type":"Post"},{"title":"Aim v3.18 ‚Äî Metric min/max/first values support in Runs Explorer, SQLAlchemy 2.0 Support","date":"2024-02-07T16:19:10.950Z","author":"Tatyana Manaseryan","description":"Hey team, Aim 3.18 is now available! Discover what's new: Metric min/max/first values support in Runs Explorer, SQLAlchemy 2.0 Support ","slug":"aim-v3-18-metric-min-max-first-values-support-in-runs-explorer-sqlalchemy-2-0-support","image":"/images/dynamic/twitter.png","draft":false,"categories":["NewReleases"],"body":{"raw":"Hey community, excited to announce Aim v3.18.1 is out! üöÄ It is packed with new enhancements, aimed at improving performance analysis and user experience.\n\n\n\n\u003e *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community.*\n\u003e\n\u003e\n\u003e\n\u003e *Every question and every issue makes Aim better! Congratulations to[¬†inc0](https://github.com/inc0)¬†and [ChanderG](https://github.com/ChanderG)¬†for their first contributions. üôå* \n\n\n\nWe have been busy working behind the scenes. Excited to keep sharing updates for Aim more frequently going forward.\n\n# Key Highlights\n\n\n\n## Min/max/first values tracking for metrics in UI\n\nThe new update lets you track the minimum, maximum, and first values of metrics, giving you deeper insights into your model's performance. This is great for optimizing your model, making it easier to see when and how your model performs best or needs improvement.\n\n\n\nWhether it's identifying the peak accuracy of a classification model or the lowest error rate of a predictive model, this metric tracking simplifies the task of fine-tuning models for optimal performance.\n\n![Runs Explorer](/images/dynamic/screen-shot-2024-02-07-at-19.10.05.png \"Runs Explorer\")\n\n## SQLAlchemy 2.0 Support\n\n\n\nWe've added support for SQLAlchemy 2.0 to ensure you have the latest database management capabilities at your fingertips.\n\n\n\n## Improvements in Run Locking and Compatibility\n\n\n\nRun locking issues are fixed and we made sure everything works smoothly with older versions of SQLAlchemy.\n\nWe're developing new features and improvements that we can't wait to show you :))\n\n\n\nThanks for all love and support. Stay tuned! üöÄ¬†\n\n# Learn more\n\n[Aim is on a mission to democratize AI dev tools](https://aimstack.readthedocs.io/en/latest/overview.html).¬†üôå\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful. ‚≠êÔ∏è","html":"\u003cp\u003eHey community, excited to announce Aim v3.18.1 is out! üöÄ It is packed with new enhancements, aimed at improving performance analysis and user experience.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eEvery question and every issue makes Aim better! Congratulations to\u003ca href=\"https://github.com/inc0\"\u003e¬†inc0\u003c/a\u003e¬†and \u003ca href=\"https://github.com/ChanderG\"\u003eChanderG\u003c/a\u003e¬†for their first contributions. üôå\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe have been busy working behind the scenes. Excited to keep sharing updates for Aim more frequently going forward.\u003c/p\u003e\n\u003ch1\u003eKey Highlights\u003c/h1\u003e\n\u003ch2\u003eMin/max/first values tracking for metrics in UI\u003c/h2\u003e\n\u003cp\u003eThe new update lets you track the minimum, maximum, and first values of metrics, giving you deeper insights into your model's performance. This is great for optimizing your model, making it easier to see when and how your model performs best or needs improvement.\u003c/p\u003e\n\u003cp\u003eWhether it's identifying the peak accuracy of a classification model or the lowest error rate of a predictive model, this metric tracking simplifies the task of fine-tuning models for optimal performance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screen-shot-2024-02-07-at-19.10.05.png\" alt=\"Runs Explorer\" title=\"Runs Explorer\"\u003e\u003c/p\u003e\n\u003ch2\u003eSQLAlchemy 2.0 Support\u003c/h2\u003e\n\u003cp\u003eWe've added support for SQLAlchemy 2.0 to ensure you have the latest database management capabilities at your fingertips.\u003c/p\u003e\n\u003ch2\u003eImprovements in Run Locking and Compatibility\u003c/h2\u003e\n\u003cp\u003eRun locking issues are fixed and we made sure everything works smoothly with older versions of SQLAlchemy.\u003c/p\u003e\n\u003cp\u003eWe're developing new features and improvements that we can't wait to show you :))\u003c/p\u003e\n\u003cp\u003eThanks for all love and support. Stay tuned! üöÄ¬†\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools\u003c/a\u003e.¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful. ‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/aim-v3-18-‚Äî-metric-min-max-first-values-support-in-runs-explorer-sqlalchemy-2-0-support.md","_raw":{"sourceFilePath":"posts/aim-v3-18-‚Äî-metric-min-max-first-values-support-in-runs-explorer-sqlalchemy-2-0-support.md","sourceFileName":"aim-v3-18-‚Äî-metric-min-max-first-values-support-in-runs-explorer-sqlalchemy-2-0-support.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v3-18-‚Äî-metric-min-max-first-values-support-in-runs-explorer-sqlalchemy-2-0-support"},"type":"Post"},{"title":"Aim‚Äôs foundations \u0026 why we‚Äôre building a Tensorboard alternative","date":"2021-10-22T14:48:48.452Z","author":"Gev Soghomonian","description":"The origins of Aim. In the fateful summer of 2020, our friend mahnerak ‚Äì a researcher at a non-profit lab was hitting the limits of Tensorboard. He wasn‚Äôt","slug":"aims-foundations-why-were-building-a-tensorboard-alternative","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd5FbXnYnB3fbELmRLTPwA.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The origins of Aim\n\nIn the fateful summer of 2020, our friend¬†[mahnerak](https://twitter.com/mahnerak)¬†‚Äì a researcher at¬†[a non-profit lab](https://yerevann.com/)¬†was hitting the limits of Tensorboard. He wasn‚Äôt going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That‚Äôs how we decided to build a Tensorboard alternative.\n\n[Gor](https://github.com/gorarakelyan)¬†and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\n\nBy fall 2020,¬†[Aim 2.0](https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration)¬†launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even¬†[r/MachineLearning loved it](https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/).\n\n By spring 2021, mahnerak co-authoerd a paper [WARP](https://aclanthology.org/2021.acl-long.381/)¬†([code](https://github.com/yerevann/warp)): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\n\n## A scale problem\n\nBut Aim‚Äôs power users ‚Äî who often do 5K+ runs ‚Äîwere hitting issues.\n\nAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests.¬†*Live updates, image tracking, distribution tracking*‚Ä¶ and Aim 2.0 was hitting the limits of Aim 1.0‚Äôs design.\n\nIn order to support the future, we had to make changes to the foundation now.\n\n## Launching Aim 3.0.0\n\nAn additional¬†[317 pull requests](https://github.com/aimhubio/aim/milestone/13?closed=1)¬†later, we are excited to launch¬†Aim v3.0.0¬†!!!\n\nAs a result, the most important changes include:\n\n**A completely revamped UI**\n\n* Home page and run detail page\n* Runs, metrics and params explorers\n* Bookmarks and Tags\n\n**A completely revamped Aim Python SDK**\n\n* New and much more intuitive (but still quite vanilla) API to track your training runs\n* New and 10x faster embedded storage based on¬†[Rocksdb](http://rocksdb.org/). This will allow us to store virtually any type of AI metadata. On the contrary,¬†[AimRecords](https://github.com/aimhubio/aimrecords)¬†was designed for metrics and hyperparams only.\n\nEnjoy the changes!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*xbg148dzileheVdOALBMBg.gif)\n\n## Performance improvements\n\n* Average run query execution time on ~2000 runs: 0.784s.\n* Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\n* New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (*for comparison, v2 was performant with limitation for only 100 metrics*).\n\n## Comparisons to familiar tools\n\n### Tensorboard\n\n**Training run comparison**\n\n* The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\n* With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params.¬†After all,¬†TensorBoard doesn‚Äôt have features to group, aggregate the metrics\n\n**Scalability**\n\n* Aim can handle 1000s of training runs both on the backend and on the UI.\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\n\nAim will have the beloved TB visualizations\n\n* Embedding projector.\n* Neural network visualization.\n\n### [](https://github.com/aimhubio/aim#mlflow)MLFlow\n\nMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\n\n**Run comparison**\n\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\n* MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\n\n**UI Scalability**\n\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\n\n### [](https://github.com/aimhubio/aim#weights-and-biases)Weights and Biases\n\n**Hosted vs self-hosted**\n\n* Weights and Biases is a hosted closed-source MLOps platform.\n* Aim is self-hosted, free and open-source experiment tracking tool.\n\n## Aim Roadmap[](https://github.com/aimhubio/aim#tensorboard)\n\nWith this version we are also publishing the Aim¬†[roadmap](https://github.com/aimhubio/aim#roadmap)¬†for the next 3 months.\n\nThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\n\nWe are also¬†[inviting community contributors](https://github.com/aimhubio/aim#community)¬†to help us get there faster!\n\n## Why are we building Aim?\n\nWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\n\nExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\n\nHowever we see lots of improvements to be made. Especially around issues like:\n\n* ability to handle 1000s of large-scale experiments\n* actionable, beautiful and performant visualizations\n* extensibility ‚Äî how easy are the apis for extension/democratization?\n\nWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\n\nOur mission‚Ä¶\n\nAim‚Äôs mission is to democratize AI dev tools. We believe that the best AI tools need to be:\n\n* open-source, open-data-format, community-driven\n* have great UI/UX, CLI and other interfaces for automation\n* performant both on UI and data\n* extensible ‚Äî enable ways to build around for so many use-cases\n\n## Thanks to\n\n[Ruben Karapetyan](https://twitter.com/roubkar)¬†for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\n\n[Mahnerak](https://twitter.com/mahnerak)¬†for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\n\nAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\n\nThe contributors who have been relentlessly iterating over the course of the summer.\n\nOn to the next generation of ML tools!!\n\n## Join Us!\n\n\n\nJoin the¬†[Aim community](https://community.aimstack.io/), test Aim out, ask questions, help us build the future of AI tooling!\n\nIf you find Aim useful, drop by and star the repo ‚≠ê","html":"\u003ch2\u003eThe origins of Aim\u003c/h2\u003e\n\u003cp\u003eIn the fateful summer of 2020, our friend¬†\u003ca href=\"https://twitter.com/mahnerak\"\u003emahnerak\u003c/a\u003e¬†‚Äì a researcher at¬†\u003ca href=\"https://yerevann.com/\"\u003ea non-profit lab\u003c/a\u003e¬†was hitting the limits of Tensorboard. He wasn‚Äôt going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That‚Äôs how we decided to build a Tensorboard alternative.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/gorarakelyan\"\u003eGor\u003c/a\u003e¬†and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\u003c/p\u003e\n\u003cp\u003eBy fall 2020,¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration\"\u003eAim 2.0\u003c/a\u003e¬†launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even¬†\u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/\"\u003er/MachineLearning loved it\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBy spring 2021, mahnerak co-authoerd a paper \u003ca href=\"https://aclanthology.org/2021.acl-long.381/\"\u003eWARP\u003c/a\u003e¬†(\u003ca href=\"https://github.com/yerevann/warp\"\u003ecode\u003c/a\u003e): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\u003c/p\u003e\n\u003ch2\u003eA scale problem\u003c/h2\u003e\n\u003cp\u003eBut Aim‚Äôs power users ‚Äî who often do 5K+ runs ‚Äîwere hitting issues.\u003c/p\u003e\n\u003cp\u003eAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests.¬†\u003cem\u003eLive updates, image tracking, distribution tracking\u003c/em\u003e‚Ä¶ and Aim 2.0 was hitting the limits of Aim 1.0‚Äôs design.\u003c/p\u003e\n\u003cp\u003eIn order to support the future, we had to make changes to the foundation now.\u003c/p\u003e\n\u003ch2\u003eLaunching Aim 3.0.0\u003c/h2\u003e\n\u003cp\u003eAn additional¬†\u003ca href=\"https://github.com/aimhubio/aim/milestone/13?closed=1\"\u003e317 pull requests\u003c/a\u003e¬†later, we are excited to launch¬†Aim v3.0.0¬†!!!\u003c/p\u003e\n\u003cp\u003eAs a result, the most important changes include:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA completely revamped UI\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHome page and run detail page\u003c/li\u003e\n\u003cli\u003eRuns, metrics and params explorers\u003c/li\u003e\n\u003cli\u003eBookmarks and Tags\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eA completely revamped Aim Python SDK\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNew and much more intuitive (but still quite vanilla) API to track your training runs\u003c/li\u003e\n\u003cli\u003eNew and 10x faster embedded storage based on¬†\u003ca href=\"http://rocksdb.org/\"\u003eRocksdb\u003c/a\u003e. This will allow us to store virtually any type of AI metadata. On the contrary,¬†\u003ca href=\"https://github.com/aimhubio/aimrecords\"\u003eAimRecords\u003c/a\u003e¬†was designed for metrics and hyperparams only.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEnjoy the changes!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*xbg148dzileheVdOALBMBg.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePerformance improvements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAverage run query execution time on ~2000 runs: 0.784s.\u003c/li\u003e\n\u003cli\u003eAverage metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\u003c/li\u003e\n\u003cli\u003eNew UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (\u003cem\u003efor comparison, v2 was performant with limitation for only 100 metrics\u003c/em\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eComparisons to familiar tools\u003c/h2\u003e\n\u003ch3\u003eTensorboard\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTraining run comparison\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\u003c/li\u003e\n\u003cli\u003eWith Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params.¬†After all,¬†TensorBoard doesn‚Äôt have features to group, aggregate the metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eScalability\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim can handle 1000s of training runs both on the backend and on the UI.\u003c/li\u003e\n\u003cli\u003eTensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAim will have the beloved TB visualizations\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEmbedding projector.\u003c/li\u003e\n\u003cli\u003eNeural network visualization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003ca href=\"https://github.com/aimhubio/aim#mlflow\"\u003e\u003c/a\u003eMLFlow\u003c/h3\u003e\n\u003cp\u003eMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRun comparison\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\u003c/li\u003e\n\u003cli\u003eMLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eUI Scalability\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\u003c/li\u003e\n\u003cli\u003eMLflow UI becomes slow to use when there are a few hundreds of runs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003ca href=\"https://github.com/aimhubio/aim#weights-and-biases\"\u003e\u003c/a\u003eWeights and Biases\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHosted vs self-hosted\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWeights and Biases is a hosted closed-source MLOps platform.\u003c/li\u003e\n\u003cli\u003eAim is self-hosted, free and open-source experiment tracking tool.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAim Roadmap\u003ca href=\"https://github.com/aimhubio/aim#tensorboard\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWith this version we are also publishing the Aim¬†\u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eroadmap\u003c/a\u003e¬†for the next 3 months.\u003c/p\u003e\n\u003cp\u003eThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\u003c/p\u003e\n\u003cp\u003eWe are also¬†\u003ca href=\"https://github.com/aimhubio/aim#community\"\u003einviting community contributors\u003c/a\u003e¬†to help us get there faster!\u003c/p\u003e\n\u003ch2\u003eWhy are we building Aim?\u003c/h2\u003e\n\u003cp\u003eWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\u003c/p\u003e\n\u003cp\u003eExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\u003c/p\u003e\n\u003cp\u003eHowever we see lots of improvements to be made. Especially around issues like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eability to handle 1000s of large-scale experiments\u003c/li\u003e\n\u003cli\u003eactionable, beautiful and performant visualizations\u003c/li\u003e\n\u003cli\u003eextensibility ‚Äî how easy are the apis for extension/democratization?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\u003c/p\u003e\n\u003cp\u003eOur mission‚Ä¶\u003c/p\u003e\n\u003cp\u003eAim‚Äôs mission is to democratize AI dev tools. We believe that the best AI tools need to be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen-source, open-data-format, community-driven\u003c/li\u003e\n\u003cli\u003ehave great UI/UX, CLI and other interfaces for automation\u003c/li\u003e\n\u003cli\u003eperformant both on UI and data\u003c/li\u003e\n\u003cli\u003eextensible ‚Äî enable ways to build around for so many use-cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThanks to\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/roubkar\"\u003eRuben Karapetyan\u003c/a\u003e¬†for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/mahnerak\"\u003eMahnerak\u003c/a\u003e¬†for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\u003c/p\u003e\n\u003cp\u003eAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\u003c/p\u003e\n\u003cp\u003eThe contributors who have been relentlessly iterating over the course of the summer.\u003c/p\u003e\n\u003cp\u003eOn to the next generation of ML tools!!\u003c/p\u003e\n\u003ch2\u003eJoin Us!\u003c/h2\u003e\n\u003cp\u003eJoin the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, test Aim out, ask questions, help us build the future of AI tooling!\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, drop by and star the repo ‚≠ê\u003c/p\u003e"},"_id":"posts/aim‚Äôs-foundations-why-we‚Äôre-building-a-tensorboard-alternative.md","_raw":{"sourceFilePath":"posts/aim‚Äôs-foundations-why-we‚Äôre-building-a-tensorboard-alternative.md","sourceFileName":"aim‚Äôs-foundations-why-we‚Äôre-building-a-tensorboard-alternative.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim‚Äôs-foundations-why-we‚Äôre-building-a-tensorboard-alternative"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":"Integration: Aim Logger with XGBoost Library. Learn how to seamlessly use Aim's capabilities in XGBoost experiments. Enhance analysis and insights today! ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\n\n[Aim](https://github.com/aimhubio/aim)¬†is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n## What is XGBoost?\n\n[XGBoost](https://github.com/dmlc/xgboost)¬†is an optimized gradient boosting library with highly¬†¬†*efficient*,¬†¬†*flexible,*¬† and¬†¬†*portable*¬†design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\n## How to use Aim with XGBoost?\n\nCheck out end-to-end¬†[Aim integration](https://aimstack.io/blog/new-releases/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements)¬†examples with multiple frameworks¬†[here](https://github.com/aimhubio/aim/tree/main). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\n\n```\n# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n```\n\nAs you can see on line 49,¬†AimCallback¬†is imported from¬†`aim.xgboost`¬†and passed to¬†`xgb.train`¬†as one of the callbacks. Aim session can open and close by the AimCallback and the¬†metrics and hparamsstore by XGBoost. In addition to that, thesystem measures¬†pass to Aim as well.\n\n## What it looks like?\n\nAfter you run the experiment and the¬†`aim up`¬†command in the¬†`aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png \"Aim UI dashboard page\")\n\nTo explore the run, we should:\n\n* Choose the¬†`xgboost_test`experiment.\n* Select the metrics to explore.\n* Divide into charts by metrics.\n\nFor example, the gif below illustrates the steps above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif)\n\n\\\n So this is what the final result looks like.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png)\n\nAs easy as that, we can analyze the runs and the system usage.\n\n## Learn More\n\n\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nIf you find Aim useful, support us and star¬†[the project](https://github.com/aimhubio/aim)¬†on GitHub. Also, join [Aim Community ](https://community.aimstack.io/)and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is Aim?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\u003c/p\u003e\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dmlc/xgboost\"\u003eXGBoost\u003c/a\u003e¬†is an optimized gradient boosting library with highly¬†¬†\u003cem\u003eefficient\u003c/em\u003e,¬†¬†\u003cem\u003eflexible,\u003c/em\u003e¬† and¬†¬†\u003cem\u003eportable\u003c/em\u003e¬†design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\u003c/p\u003e\n\u003ch2\u003eHow to use Aim with XGBoost?\u003c/h2\u003e\n\u003cp\u003eCheck out end-to-end¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements\"\u003eAim integration\u003c/a\u003e¬†examples with multiple frameworks¬†\u003ca href=\"https://github.com/aimhubio/aim/tree/main\"\u003ehere\u003c/a\u003e. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs you can see on line 49,¬†AimCallback¬†is imported from¬†\u003ccode\u003eaim.xgboost\u003c/code\u003e¬†and passed to¬†\u003ccode\u003exgb.train\u003c/code\u003e¬†as one of the callbacks. Aim session can open and close by the AimCallback and the¬†metrics and hparamsstore by XGBoost. In addition to that, thesystem measures¬†pass to Aim as well.\u003c/p\u003e\n\u003ch2\u003eWhat it looks like?\u003c/h2\u003e\n\u003cp\u003eAfter you run the experiment and the¬†\u003ccode\u003eaim up\u003c/code\u003e¬†command in the¬†\u003ccode\u003eaim_logs\u003c/code\u003edirectory, Aim UI will be running. When first opened, the dashboard page will come up.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Sli50uagUeSumna_K0Aow.png\" alt=\"\" title=\"Aim UI dashboard page\"\u003e\u003c/p\u003e\n\u003cp\u003eTo explore the run, we should:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoose the¬†\u003ccode\u003exgboost_test\u003c/code\u003eexperiment.\u003c/li\u003e\n\u003cli\u003eSelect the metrics to explore.\u003c/li\u003e\n\u003cli\u003eDivide into charts by metrics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, the gif below illustrates the steps above.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*l1b8Fz49JItkl0aSHzhQfA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nSo this is what the final result looks like.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVUZa089vNTRvpiZ5okWag.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs easy as that, we can analyze the runs and the system usage.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, support us and star¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003ethe project\u003c/a\u003e¬†on GitHub. Also, join \u003ca href=\"https://community.aimstack.io/\"\u003eAim Community \u003c/a\u003eand share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"Exploring MLflow experiments with a powerful UI","date":"2023-01-17T21:52:43.983Z","author":"Gor Arakelyan","description":"We are excited to announce the release of aimlflow, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! üéâ","slug":"exploring-mlflow-experiments-with-a-powerful-ui","image":"https://miro.medium.com/max/1400/1*YMlI66d-QPxI3fcQCoPSlw.webp","draft":false,"categories":["Tutorials"],"body":{"raw":"\n\nWe are excited to announce the release of¬†[aimlflow](https://github.com/aimhubio/aimlflow), an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! üéâ\n\n[MLflow](https://github.com/mlflow/mlflow)¬†is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments.¬†[Aim](https://github.com/aimhubio/aim)¬†is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\n\n![](https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif)\n\n**With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.**\n\n![](https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp)\n\nIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let‚Äôs dive in and see how to make it happen.\n\n\u003e Aim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\n\u003e\n\u003e View more on GitHub:¬†\u003chttps://github.com/aimhubio/aim\u003e\n\n# Project overview\n\nWe use a simple project that trains a CNN using¬†[PyTorch](https://github.com/pytorch/pytorch)¬†and¬†[Ray Tune](https://github.com/ray-project/ray/tree/master/python/ray/tune)¬†on the¬†[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)¬†dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\n\n* **PyTorch**¬†for building and training the model\n* **Ray Tune**¬†for hyper-parameters tuning\n* **MLflow**¬†for experiment tracking\n\nFind the full project code on GitHub:¬†\u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003e\n\n![](https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp)\n\nRun the trainings by downloading and executing¬†`tune.py`¬†python file:\n\n```\npython tune.py\n```\n\nYou should see a similar output, meaning the trainings are successfully initiated:\n\n![](https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp)\n\n## Getting started with aimlflow\n\nAfter the hyper-parameter tuning is ran, let‚Äôs see how aimlflow can help us to explore the tracked experiments via UI.\n\nTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a¬†`.aim`¬†repo located on the file system.\n\n**This means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn‚Äôt it amazing? ü§©**\n\n## 1. Install aimlflow on your machine\n\nIt is super easy to install aimlflow, simply run the following command:\n\n```\npip3 install aim-mlflow\n```\n\n## 2. Sync MLflow logs with Aim\n\nPick any directory on your file system and initialize a¬†`.aim`¬†repo:\n\n```\naim init\n```\n\nRun the¬†`aimlflow sync`¬†command to sync MLflow experiments with the Aim repo:\n\n```\naimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n```\n\n## 3. Run Aim\n\nNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\n\n```\naim up --repo=AIM_REPO_PATH\n```\n\nYou will see the following message on the terminal output:\n\n![](https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp)\n\nCongratulations! Now you can explore the training logs with Aim. üéâ\n\n# Quick tour of Aim for MLflow users\n\nIn this section, we will take a quick tour of Aim‚Äôs features, including:\n\n* Exploring hyper-parameters tuning results\n* Comparing tracked metrics\n* As well as, taking a look at the other capabilities Aim provides\n\n## Exploring MLflow experiments\n\nNow then Aim is set up and running, we navigate to the project overview page at¬†`127.0.0.1:43800`, where the summary of the project is displayed:\n\n* The number of tracked training runs and experiments\n* Statistics on the amount of tracked metadata\n* A list of experiments and tags, with the ability to quickly explore selected items\n* A calendar and feed of contributions\n* A table of in-progress trainings\n\n![](https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp \"Project overview\")\n\n\\\nTo view the results of the trainings, let‚Äôs navigate to the runs dashboard at¬†`127.0.0.1:43800/runs`. Here, you can see hyper-parameters and metrics results all of the trainings.\n\n![](https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp \" Runs dashboard\")\n\nWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\n\n![](https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp \"Run page\")\n\nOn this page, we can view the tracked hparams, including the¬†`mlflow_run_id`¬†and the¬†`mlflow_run_name`¬†which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\n\n## Comparing metrics\n\nComparing metrics across several runs is super easy with Aim:\n\n* Open the metrics page from the left sidebar\n* Select desired metrics by clicking on¬†`+ Metrics`¬†button\n* Pressing¬†`Search`¬†button on the top right corner\n\nWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\n\n![](https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp)\n\nAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\n\n* **by color**¬†‚Äî each group of metrics will be filled in with its unique color\n* **by stroke style**¬†‚Äî each group will have a unique stroke style (solid, dashed, etc)\n* **by facet**¬†‚Äî each group of metrics will be displayed in a separate subplot\n\nTo learn which set of trials performed the best, let‚Äôs apply several groupings:\n\n* Group by¬†`run.hparams.l1`¬†hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\n* Group by¬†`metric.name`¬†to divide losses and accuracies into separate subplots (this grouping is applied by default)\n\n![](https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp)\n\n**Aim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.**\n\nFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\n\n```\nrun.metrics['training_iteration'].last \u003e 1\n```\n\n\n\n\u003e Read more about Aim query language capabilities in the docs:¬†\u003chttps://aimstack.readthedocs.io/en/latest/using/search.html\u003e\n\nThis will result in querying and displaying 9 matched runs:\n\n![](https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp)\n\nLet‚Äôs aggregate the groups to see which one performed the best:\n\n![](https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp)\n\nFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. üéâ\n\n\u003e For more please see Aim official docs here:¬†\u003chttps://aimstack.readthedocs.io/en/latest/\u003e\n\n## Last, but not least: a closer look at Aim‚Äôs key features\n\n* Use powerful pythonic search to select the runs you want to analyze:\n\n![](https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp)\n\nGroup metrics by hyperparameters to analyze hyperparameters‚Äô influence on run performance:\n\n![](https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp)\n\nSelect multiple metrics and analyze them side by side:\n\n![](https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp)\n\nAggregate metrics by std.dev, std.err, conf.interval:\n\n* ![](https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp)\n\n  Align x axis by any other metric:\n\n  * ![](https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp)\n\n  Scatter plots to learn correlations and trends:\n\n  ![](https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp)\n\n  High dimensional data visualization via parallel coordinate plot:\n\n  ![](https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp)\n\n  Explore media metadata, such as images and audio objects via Aim Explorers:\n\n  * ![](https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp \"Audio Explorer\")\n\n  ![](https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp \"Images Explorer\")\n\n  # Conclusion\n\n  In conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\n\n  In this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\n\n  We covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please¬†[read the docs](https://aimstack.readthedocs.io/en/latest/overview.html).\n\n  # Learn more\n\n  If you have any questions join¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. üôå\n\n  Show some love by dropping a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"\u003cp\u003eWe are excited to announce the release of¬†\u003ca href=\"https://github.com/aimhubio/aimlflow\"\u003eaimlflow\u003c/a\u003e, an integration that helps to seamlessly run a powerful experiment tracking UI on MLflow logs! üéâ\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mlflow/mlflow\"\u003eMLflow\u003c/a\u003e¬†is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. While MLflow provides a great foundation for managing machine learning projects, it can be challenging to effectively explore and understand the results of tracked experiments.¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†is a tool that addresses this challenge by providing a variety of features for deeply exploring and learning tracked experiments insights and understanding results via UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*WNn0RhcPt_UCDhwRUnNj4A.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*xXGWEV5bJFEOwpjtDZOoHw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this article, we will guide you through the process of running a several CNN trainings, setting up aimlfow and exploring the results via UI. Let‚Äôs dive in and see how to make it happen.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAim is an easy-to-use open-source experiment tracking tool supercharged with abilities to compare 1000s of runs in a few clicks. Aim enables a beautiful UI to compare and explore them.\u003c/p\u003e\n\u003cp\u003eView more on GitHub:¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eWe use a simple project that trains a CNN using¬†\u003ca href=\"https://github.com/pytorch/pytorch\"\u003ePyTorch\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/ray-project/ray/tree/master/python/ray/tune\"\u003eRay Tune\u003c/a\u003e¬†on the¬†\u003ca href=\"https://www.cs.toronto.edu/~kriz/cifar.html\"\u003eCIFAR-10\u003c/a\u003e¬†dataset. We will train multiple CNNs by adjusting the learning rate and the number of neurons in two of the network layers using the following stack:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePyTorch\u003c/strong\u003e¬†for building and training the model\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRay Tune\u003c/strong\u003e¬†for hyper-parameters tuning\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMLflow\u003c/strong\u003e¬†for experiment tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFind the full project code on GitHub:¬†\u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*47TN9ur7psCbJZHslWNvFA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eRun the trainings by downloading and executing¬†\u003ccode\u003etune.py\u003c/code\u003e¬†python file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython tune.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should see a similar output, meaning the trainings are successfully initiated:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*p8K-B7Cu-IT9D0dRUBnqfA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eGetting started with aimlflow\u003c/h2\u003e\n\u003cp\u003eAfter the hyper-parameter tuning is ran, let‚Äôs see how aimlflow can help us to explore the tracked experiments via UI.\u003c/p\u003e\n\u003cp\u003eTo be able to explore MLflow logs with Aim, we will need to convert MLflow experiments to Aim format. All the metrics, tags, config, artifacts, and experiment descriptions will be stored and live-synced in a¬†\u003ccode\u003e.aim\u003c/code\u003e¬†repo located on the file system.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis means that you can run your training script, and without modifying a single line of code, live-time view the logs on the beautiful UI of Aim. Isn‚Äôt it amazing? ü§©\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e1. Install aimlflow on your machine\u003c/h2\u003e\n\u003cp\u003eIt is super easy to install aimlflow, simply run the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install aim-mlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e2. Sync MLflow logs with Aim\u003c/h2\u003e\n\u003cp\u003ePick any directory on your file system and initialize a¬†\u003ccode\u003e.aim\u003c/code\u003e¬†repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the¬†\u003ccode\u003eaimlflow sync\u003c/code\u003e¬†command to sync MLflow experiments with the Aim repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimlflow sync --mlflow-tracking-uri=MLFLOW_URI --aim-repo=AIM_REPO_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e3. Run Aim\u003c/h2\u003e\n\u003cp\u003eNow that we have synced MLflow logs and we have some trainings logged, all we need to do is to run Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up --repo=AIM_REPO_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will see the following message on the terminal output:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*H8dvLDWAF-EE-MgO2MjSWg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eCongratulations! Now you can explore the training logs with Aim. üéâ\u003c/p\u003e\n\u003ch1\u003eQuick tour of Aim for MLflow users\u003c/h1\u003e\n\u003cp\u003eIn this section, we will take a quick tour of Aim‚Äôs features, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExploring hyper-parameters tuning results\u003c/li\u003e\n\u003cli\u003eComparing tracked metrics\u003c/li\u003e\n\u003cli\u003eAs well as, taking a look at the other capabilities Aim provides\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExploring MLflow experiments\u003c/h2\u003e\n\u003cp\u003eNow then Aim is set up and running, we navigate to the project overview page at¬†\u003ccode\u003e127.0.0.1:43800\u003c/code\u003e, where the summary of the project is displayed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe number of tracked training runs and experiments\u003c/li\u003e\n\u003cli\u003eStatistics on the amount of tracked metadata\u003c/li\u003e\n\u003cli\u003eA list of experiments and tags, with the ability to quickly explore selected items\u003c/li\u003e\n\u003cli\u003eA calendar and feed of contributions\u003c/li\u003e\n\u003cli\u003eA table of in-progress trainings\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TjHqr4lK-aFPJPPh5rGAqQ.webp\" alt=\"\" title=\"Project overview\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nTo view the results of the trainings, let‚Äôs navigate to the runs dashboard at¬†\u003ccode\u003e127.0.0.1:43800/runs\u003c/code\u003e. Here, you can see hyper-parameters and metrics results all of the trainings.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*hryGi06eJll6wy2L3zWzyg.webp\" alt=\"\" title=\" Runs dashboard\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can deeply explore the results and tracked metadata for a specific run by clicking on its name on the dashboard.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*0IfU15byWb7Fx2kPf-d5Jg.webp\" alt=\"\" title=\"Run page\"\u003e\u003c/p\u003e\n\u003cp\u003eOn this page, we can view the tracked hparams, including the¬†\u003ccode\u003emlflow_run_id\u003c/code\u003e¬†and the¬†\u003ccode\u003emlflow_run_name\u003c/code\u003e¬†which are extracted from MLflow runs during the conversion process. Additionally, detailed information about the run can be found on each of the tabs, such as tracked hparams, metrics, notes, output logs, system resource usage, etc.\u003c/p\u003e\n\u003ch2\u003eComparing metrics\u003c/h2\u003e\n\u003cp\u003eComparing metrics across several runs is super easy with Aim:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen the metrics page from the left sidebar\u003c/li\u003e\n\u003cli\u003eSelect desired metrics by clicking on¬†\u003ccode\u003e+ Metrics\u003c/code\u003e¬†button\u003c/li\u003e\n\u003cli\u003ePressing¬†\u003ccode\u003eSearch\u003c/code\u003e¬†button on the top right corner\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe will select losses and accuracies to compare them over all the trials. The following view of metrics will appear:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*aMzdqTNogK0dONh5nzb_LQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim comes with powerful grouping capabilities. Grouping enables a way to divide metrics into subgroups based on some criteria and apply the corresponding style. Aim supports 3 grouping ways for metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eby color\u003c/strong\u003e¬†‚Äî each group of metrics will be filled in with its unique color\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eby stroke style\u003c/strong\u003e¬†‚Äî each group will have a unique stroke style (solid, dashed, etc)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eby facet\u003c/strong\u003e¬†‚Äî each group of metrics will be displayed in a separate subplot\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo learn which set of trials performed the best, let‚Äôs apply several groupings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGroup by¬†\u003ccode\u003erun.hparams.l1\u003c/code\u003e¬†hyper-parameter to color the picked metrics based on the number of outputs of the first fully connected layer\u003c/li\u003e\n\u003cli\u003eGroup by¬†\u003ccode\u003emetric.name\u003c/code\u003e¬†to divide losses and accuracies into separate subplots (this grouping is applied by default)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TAOwGrp9b7X3eJgzsLE3tg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAim also provides a way to select runs programmatically. It enables applying a custom query instead of just picking metrics of all the runs. Aim query supports python syntax, allowing us to access the properties of objects by dot operation, make comparisons, and perform more advanced python operations.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFor example, in order to display only runs that were trained for more than one iteration, we will run the following query:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.metrics['training_iteration'].last \u003e 1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRead more about Aim query language capabilities in the docs:¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/search.html\"\u003ehttps://aimstack.readthedocs.io/en/latest/using/search.html\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis will result in querying and displaying 9 matched runs:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Rcby1yyJPMzTdMgcNvaqlw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eLet‚Äôs aggregate the groups to see which one performed the best:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*gh_Ep2PVX2WsQQbPL2NyhA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the visualizations, it is obvious that the purple group achieved better performance compared to the rest. This means that the trials that had 64 outputs in the first fully connected layer achieved the best performance. üéâ\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFor more please see Aim official docs here:¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003ehttps://aimstack.readthedocs.io/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eLast, but not least: a closer look at Aim‚Äôs key features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUse powerful pythonic search to select the runs you want to analyze:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*UnfUbRh5yLa5PceBi2a0HQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eGroup metrics by hyperparameters to analyze hyperparameters‚Äô influence on run performance:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*1M8Z_CS6j9_nBQXifp-JBw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSelect multiple metrics and analyze them side by side:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*j6K9X_-LL4aHb9ZhCDxahQ.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAggregate metrics by std.dev, std.err, conf.interval:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*8hXsPXkgqiDm-GqM1PqJ_w.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAlign x axis by any other metric:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*ulpsYaFXGiTPSh5oM9XYwQ.webp\" alt=\"\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eScatter plots to learn correlations and trends:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*4uxIpoDd9r7DWa96lR-imw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHigh dimensional data visualization via parallel coordinate plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*Z37L2fKGl0mA-x7tOdq9IA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eExplore media metadata, such as images and audio objects via Aim Explorers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*2E8ybu8TxejHzjgRT6sReg.webp\" alt=\"\" title=\"Audio Explorer\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/0*GdFVthJ1ee2AeinV.webp\" alt=\"\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eIn conclusion, Aim enables a completely new level of open-source experiment tracking and aimlflow makes it available for MLflow users with just a few commands and zero code change!\u003c/p\u003e\n\u003cp\u003eIn this guide, we demonstrated how MLflow experiments can be explored with Aim. When it comes to exploring experiments, Aim augments MLflow capabilities by enabling rich visualizations and manipulations.\u003c/p\u003e\n\u003cp\u003eWe covered the basics of Aim, it has much more to offer, with super fast query systems and a nice visualization interface. For more please¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eread the docs\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. üôå\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ‚≠êÔ∏è on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you think Aim is useful.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","_raw":{"sourceFilePath":"posts/exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileName":"exploring-mlflow-experiments-with-a-powerful-ui.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-mlflow-experiments-with-a-powerful-ui"},"type":"Post"},{"title":"Exploring your PyTorch Lightning experiments with AimOS","date":"2024-01-11T13:41:33.464Z","author":"Hovhannes Tamoyan","description":"Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS.","slug":"exploring-your-pytorch-lightning-experiments-with-aimos","image":"/images/dynamic/banner1.png","draft":false,"categories":["Integrations"],"body":{"raw":"Discover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.\n\n## Introduction\n\n\n\n[AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems), through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let's dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.\n\n\u003e AimOS üîç ‚Äî An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.\n\n## Running a Sample Training with PyTorch Lightning: A Step-by-Step Guide\n\n\n\nLet's take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.\n\n\n\n### Setting the Stage\n\n\n\nBefore diving into the script, ensure that AimOS is installed. If not, simply run: \\`pip3 install aimos\\`\n\n### 1. Importing Required Modules\n\n\n\nStart by importing the necessary modules.\n\n```\nimport os\n\nimport lightning.pytorch as pl\nimport torch\nfrom aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger\nfrom torch import nn, optim, utils\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n```\n\n### 2. Preparing the Dataset and the DataLoaders\n\nDownload the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.\n\n```\ndataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)\nval_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)\ntest_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)\n```\n\n### 3. Instantiate the Model\n\nLet's create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.\n\n```\nclass LitModule(pl.LightningModule):\n    def __init__(self, hidden_size=64, lr=5e-3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lr = lr\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n```\n\nTo log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.\n\n### 4. Initialize AimOS Callback\n\n\n\nInitialize the AimOS callback by providing the experiment name.\n\n```\naim_logger = AimLogger(\n    experiment_name=\"example_experiment\"\n)\n```\n\nMake sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.\n\n```\naimos server\n```\n\n### 5. Training\n\nInstantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. üöÄ\n\n```\nmodel = LitModule(lr=lr)\naim_logger.experiment.set(\"lr\", lr, strict=False)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=aim_logger)\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\ntrainer.test(dataloaders=test_loader)\n```\n\nLet‚Äôs use these learning rates to have more understanding about the learning curve of the task: `[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]`.\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the `.aim `repository is initialized and execute:\n\n```\naimos ui\n```\n\nThis is the view you'll see after following the provided URL:\n\n![AimOS apps](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png \"AimOS apps\")\n\n## Interpreting Experiment Results\n\nNow let‚Äôs dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment's performance.\n\n\n\n## AimOS Overview Page\n\nHead over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\n\n![Overview](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png \"Overview\")\n\n## AimOS Runs Page\n\nVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\n\n![Runs page](/images/dynamic/dec-21-screenshot-from-aimos.png \"Runs page\")\n\n## Run Overview\n\nBy navigating to each run‚Äôs individual page, you'll have a detailed view of the activities within that run.\n\nNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\n\n![Run page](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png \"Run page\")\n\n## Params Tab\n\nFor a more detailed view of all parameters, navigate to the Params tab.\n\n![Params tab](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png \"Params tan\")\n\n## Metrics Tab\n\nIn the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let's apply color grouping to the runs based on their context.\n\n![Metrics tab](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png \"Metrics tab\")\n\nIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\n\n## Comparing all Runs\n\n\n\nLet's use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the `loss` and `acc `metrics, apply the stroke by `metric.context.subset `, and use a log-scale on the y-axis.\n\n![Metrics explorer](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png \"Metrics explorer\")\n\n## Wrapping up\n\nAimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!\n\n\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on¬†[AimStack blog.](https://aimstack.io/blog)\n\n## Learn more\n\n\n\n[AimOS is on a mission to democratize AI Systems logging tools](https://aimos.readthedocs.io/en/latest/apps/overview.html).¬†üôå\n\nTry out¬†[AimOS,](https://github.com/aimhubio/aimos) join the¬†[Aim community](https://community.aimstack.io/)¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon‚Äôt forget to leave us a star on¬†GitHub if you think [AimOS](https://github.com/aimhubio/aimos) is useful, and here is the repository of¬†[Aim](https://github.com/aimhubio/aim),¬†an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003cp\u003eDiscover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e, through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let's dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAimOS üîç ‚Äî An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eRunning a Sample Training with PyTorch Lightning: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eLet's take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.\u003c/p\u003e\n\u003ch3\u003eSetting the Stage\u003c/h3\u003e\n\u003cp\u003eBefore diving into the script, ensure that AimOS is installed. If not, simply run: `pip3 install aimos`\u003c/p\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eStart by importing the necessary modules.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\n\nimport lightning.pytorch as pl\nimport torch\nfrom aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger\nfrom torch import nn, optim, utils\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Preparing the Dataset and the DataLoaders\u003c/h3\u003e\n\u003cp\u003eDownload the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)\nval_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)\ntest_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Instantiate the Model\u003c/h3\u003e\n\u003cp\u003eLet's create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass LitModule(pl.LightningModule):\n    def __init__(self, hidden_size=64, lr=5e-3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lr = lr\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.\u003c/p\u003e\n\u003ch3\u003e4. Initialize AimOS Callback\u003c/h3\u003e\n\u003cp\u003eInitialize the AimOS callback by providing the experiment name.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_logger = AimLogger(\n    experiment_name=\"example_experiment\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Training\u003c/h3\u003e\n\u003cp\u003eInstantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. üöÄ\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = LitModule(lr=lr)\naim_logger.experiment.set(\"lr\", lr, strict=False)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=aim_logger)\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\ntrainer.test(dataloaders=test_loader)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet‚Äôs use these learning rates to have more understanding about the learning curve of the task: \u003ccode\u003e[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the \u003ccode\u003e.aim \u003c/code\u003erepository is initialized and execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the view you'll see after following the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png\" alt=\"AimOS apps\" title=\"AimOS apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting Experiment Results\u003c/h2\u003e\n\u003cp\u003eNow let‚Äôs dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment's performance.\u003c/p\u003e\n\u003ch2\u003eAimOS Overview Page\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eAimOS Runs Page\u003c/h2\u003e\n\u003cp\u003eVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/dec-21-screenshot-from-aimos.png\" alt=\"Runs page\" title=\"Runs page\"\u003e\u003c/p\u003e\n\u003ch2\u003eRun Overview\u003c/h2\u003e\n\u003cp\u003eBy navigating to each run‚Äôs individual page, you'll have a detailed view of the activities within that run.\u003c/p\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png\" alt=\"Run page\" title=\"Run page\"\u003e\u003c/p\u003e\n\u003ch2\u003eParams Tab\u003c/h2\u003e\n\u003cp\u003eFor a more detailed view of all parameters, navigate to the Params tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png\" alt=\"Params tab\" title=\"Params tan\"\u003e\u003c/p\u003e\n\u003ch2\u003eMetrics Tab\u003c/h2\u003e\n\u003cp\u003eIn the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let's apply color grouping to the runs based on their context.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png\" alt=\"Metrics tab\" title=\"Metrics tab\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\u003c/p\u003e\n\u003ch2\u003eComparing all Runs\u003c/h2\u003e\n\u003cp\u003eLet's use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003eacc \u003c/code\u003emetrics, apply the stroke by \u003ccode\u003emetric.context.subset \u003c/code\u003e, and use a log-scale on the y-axis.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png\" alt=\"Metrics explorer\" title=\"Metrics explorer\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eAimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on¬†\u003ca href=\"https://aimstack.io/blog\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools\u003c/a\u003e.¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS,\u003c/a\u003e join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†GitHub if you think \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e is useful, and here is the repository of¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e,¬†an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/exploring-your-pytorch-lightning-experiments-with-aimos.md","_raw":{"sourceFilePath":"posts/exploring-your-pytorch-lightning-experiments-with-aimos.md","sourceFileName":"exploring-your-pytorch-lightning-experiments-with-aimos.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-your-pytorch-lightning-experiments-with-aimos"},"type":"Post"},{"title":"How to integrate aimlflow with your remote MLflow","date":"2023-01-30T06:25:58.163Z","author":"Hovhannes Tamoyan","description":"We are thrilled to unveil aimlflow, a tool that allows for a smooth integration of a robust experiment tracking UI with MLflow logs! üöÄ","slug":"how-to-integrate-aimlflow-with-your-remote-mlflow","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSCbDO5InRlBXTxzD3NVwQ.png","draft":false,"categories":["Tutorials"],"body":{"raw":"With aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.\n\nWe have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:\n\nRunning Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here:¬†\u003chttps://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\u003e\n\nIn this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.\n\n# Project overview\n\nWe will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.\n\nWe will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.\n\nFind the full project code on GitHub:¬†\u003chttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003e\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png)\n\n# Server-side/Remote Configuration\n\nLet‚Äôs create a separate directory for the demo and name it¬†`mlflow-demo-remote`. After which download and run the¬†`tune.py`¬†python script from the Github repo to conduct the training sessions:\n\n```\n$ python tune.py\n```\n\nRay Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png)\n\nOnce started, mlflow will commence recording the results in the¬†`mlruns`¬†directory. Our remote directory will have the following structure:\n\n```\nmlflow-demo-remote\n‚îú‚îÄ‚îÄ tune.py\n‚îî‚îÄ‚îÄ mlruns\n    ‚îú‚îÄ‚îÄ ...\n```\n\nLet‚Äôs open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from the¬†`mlflow-demo-remote`¬†directory:\n\n```\n$ mlflow ui --host 0.0.0.0\n```\n\nBy default, the¬†`--host`¬†is set to¬†`127.0.0.1`, limiting access to the service to the local machine only. To expose it to external machines, set the host to¬†`0.0.0.0`.\n\nBy default, the system listens on port¬†`5000`.\n\nOne can set¬†`--backend-store-uri`¬†param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path of¬†`mlruns`¬†directory.\n\nUpon navigating to¬†`http://127.0.0.1:5000`, you will be presented with a page that looks similar to this:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png)\n\n# Synchronising MLflow Runs with Aim\n\n\n\nAfter successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.\n\nFirst, let‚Äôs move forward with the installation process of aimlflow. It‚Äôs incredibly easy to set up on your device, just execute the following command:\n\n```\n$ pip install aim-mlflow\n```\n\nAfter successfully installing aimlflow on your machine let‚Äôs create a directory named¬†`mlflow-demo-local`¬†where the¬†`.aim`repository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:\n\n```\n$ aim init\n```\n\nThis will establish an Aim repository in the present directory and it will be named¬†`.aim`.\n\nThis is how our local system directory will look like:\n\n```\nmlflow-demo-local\n‚îî‚îÄ‚îÄ .aim\n    ‚îú‚îÄ‚îÄ ...\n```\n\nIn order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into the¬†`.aim`¬†repository.\n\nTo begin the process of converting MLflow experiments from the the hosted url¬†`YOUR_REMOTE_IP:5000`¬†into the Aim repository¬†`.aim`, execute the following command from our local¬†`mlflow-demo-local`¬†directory:\n\n```\n$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n```\n\nThe converter will go through all experiments within the project and create a unique¬†`Aim`¬†run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.\n\nThis means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? ‚ò∫Ô∏è\n\nNow that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:\n\n```\n$ aim up\n```\n\nto open the user interface and explore our metrics and other information.\n\nFor further reading please referee to¬†[Aim documentation](https://aimstack.readthedocs.io/en/latest/)¬†where you will learn more about the superpowers of Aim.\n\n# Conclusion\n\n\n\nTo sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.\n\nIn this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.\n\nThe added value of Aim makes installing aimlflow and enabling the additional capability well worth it.\n\n# Learn more\n\nIf you have any questions join¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. üôå\n\nShow some love by dropping a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you think Aim is useful.","html":"\u003cp\u003eWith aimlflow, MLflow users can now seamlessly view and explore their MLflow experiments using Aim‚Äôs powerful features, leading to deeper understanding and more effective decision-making.\u003c/p\u003e\n\u003cp\u003eWe have created a dedicated post on the setup of aimlflow on local environment. For further information and guidance, please refer to the following link:\u003c/p\u003e\n\u003cp\u003eRunning Aim on the local environment is pretty similar to running it on the remote. See the guide on running multiple trainings using Airflow and exploring results through the UI here:¬†\u003ca href=\"https://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\"\u003ehttps://medium.com/aimstack/exploring-mlflow-experiments-with-a-powerful-ui-238fa2acf89e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn this tutorial, we will showcase the steps required to successfully use aimlflow to track experiments on a remote server.\u003c/p\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eWe will use PyTorch and Ray Tune to train a simple convolutional neural network (CNN) on the Cifar10 dataset. We will be experimenting with different sizes for the last layers of the network and varying the learning rate to observe the impact on network performance.\u003c/p\u003e\n\u003cp\u003eWe will use PyTorch to construct and train the network, leverage Ray Tune to fine-tune the hyperparameters, and utilize MLflow to meticulously log the training metrics throughout the process.\u003c/p\u003e\n\u003cp\u003eFind the full project code on GitHub:¬†\u003ca href=\"https://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\"\u003ehttps://github.com/aimhubio/aimlflow/tree/main/examples/hparam-tuning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XY23jEW68KnwcC3tBKvOw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eServer-side/Remote Configuration\u003c/h1\u003e\n\u003cp\u003eLet‚Äôs create a separate directory for the demo and name it¬†\u003ccode\u003emlflow-demo-remote\u003c/code\u003e. After which download and run the¬†\u003ccode\u003etune.py\u003c/code\u003e¬†python script from the Github repo to conduct the training sessions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ python tune.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRay Tune will start multiple trials of trainings with different combinations of the hyperparameters, and yield a similar output on the terminal:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0lMlACoiaj1U8A3QB1C7Q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOnce started, mlflow will commence recording the results in the¬†\u003ccode\u003emlruns\u003c/code\u003e¬†directory. Our remote directory will have the following structure:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emlflow-demo-remote\n‚îú‚îÄ‚îÄ tune.py\n‚îî‚îÄ‚îÄ mlruns\n    ‚îú‚îÄ‚îÄ ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet‚Äôs open up the mlfow UI to explore the runs. To launch the mlflow user interface, we simply need to execute the following command from the¬†\u003ccode\u003emlflow-demo-remote\u003c/code\u003e¬†directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ mlflow ui --host 0.0.0.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default, the¬†\u003ccode\u003e--host\u003c/code\u003e¬†is set to¬†\u003ccode\u003e127.0.0.1\u003c/code\u003e, limiting access to the service to the local machine only. To expose it to external machines, set the host to¬†\u003ccode\u003e0.0.0.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eBy default, the system listens on port¬†\u003ccode\u003e5000\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOne can set¬†\u003ccode\u003e--backend-store-uri\u003c/code\u003e¬†param to specify the URI from which the source will be red, wether its an SQLAlchemy-compatible database connection or a local filesystem URI, by default its the path of¬†\u003ccode\u003emlruns\u003c/code\u003e¬†directory.\u003c/p\u003e\n\u003cp\u003eUpon navigating to¬†\u003ccode\u003ehttp://127.0.0.1:5000\u003c/code\u003e, you will be presented with a page that looks similar to this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8NMyKkAIhdMdFwyIghuVA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eSynchronising MLflow Runs with Aim\u003c/h1\u003e\n\u003cp\u003eAfter successfully initiating our training on the remote server and hosting the user interface, we can begin converting mlflow runs from the remote to our local Aim repository.\u003c/p\u003e\n\u003cp\u003eFirst, let‚Äôs move forward with the installation process of aimlflow. It‚Äôs incredibly easy to set up on your device, just execute the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ pip install aim-mlflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter successfully installing aimlflow on your machine let‚Äôs create a directory named¬†\u003ccode\u003emlflow-demo-local\u003c/code\u003e¬†where the¬†\u003ccode\u003e.aim\u003c/code\u003erepository will be initialized and navigate to it. Then, initialize an empty aim repository by executing the following simple command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will establish an Aim repository in the present directory and it will be named¬†\u003ccode\u003e.aim\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis is how our local system directory will look like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emlflow-demo-local\n‚îî‚îÄ‚îÄ .aim\n    ‚îú‚îÄ‚îÄ ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn order to navigate and explore MLflow runs using Aim, the aimlflow synchroniser must be run. This will convert and store all metrics, tags, configurations, artifacts, and experiment descriptions from the remote into the¬†\u003ccode\u003e.aim\u003c/code\u003e¬†repository.\u003c/p\u003e\n\u003cp\u003eTo begin the process of converting MLflow experiments from the the hosted url¬†\u003ccode\u003eYOUR_REMOTE_IP:5000\u003c/code\u003e¬†into the Aim repository¬†\u003ccode\u003e.aim\u003c/code\u003e, execute the following command from our local¬†\u003ccode\u003emlflow-demo-local\u003c/code\u003e¬†directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aimlflow sync --mlflow-tracking-uri='http://YOUR_REMOTE_IP:5000' --aim-repo=.aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe converter will go through all experiments within the project and create a unique¬†\u003ccode\u003eAim\u003c/code\u003e¬†run for each experiment with corresponding hyperparameters, tracked metrics and the logged artifacts. This command will periodically check for updates from the remote server every 10 seconds, and keep the data syncronized between the remote and the local databases.\u003c/p\u003e\n\u003cp\u003eThis means that you can run your training script on your remote server without any changes, and at the same time, you can view the real-time logs on the visually appealing UI of Aim on your local machine. How great is that? ‚ò∫Ô∏è\u003c/p\u003e\n\u003cp\u003eNow that we have initialized the Aim repository and have logged some parameters, we simply need to run the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto open the user interface and explore our metrics and other information.\u003c/p\u003e\n\u003cp\u003eFor further reading please referee to¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/\"\u003eAim documentation\u003c/a\u003e¬†where you will learn more about the superpowers of Aim.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eTo sum up, Aim brings a revolutionary level of open-source experiment tracking to the table and aimlflow makes it easily accessible for MLflow users with minimal effort. The added capabilities of Aim allow for a deeper exploration of remote runs, making it a valuable addition to any MLflow setup.\u003c/p\u003e\n\u003cp\u003eIn this guide, we demonstrated how the MLflow remote runs can be explored and analyzed using the Aim. While both tools share some basic functionalities, the Aim UI provides a more in-depth exploration of the runs.\u003c/p\u003e\n\u003cp\u003eThe added value of Aim makes installing aimlflow and enabling the additional capability well worth it.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. üôå\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ‚≠êÔ∏è on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you think Aim is useful.\u003c/p\u003e"},"_id":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","_raw":{"sourceFilePath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileName":"how-to-integrate-aimlflow-with-your-remote-mlflow.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-integrate-aimlflow-with-your-remote-mlflow"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\n\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\n\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\n\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.¬†One thing you do not need when tuning hyper-parameters is variance.\n\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\n\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using¬†[Aim](https://github.com/aimhubio/aim).\n\n## **How to fix the seed in PyTorch Lightning**\n\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\n\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use¬†`pl.seed_everything(seed)`¬†. See the docs¬†[here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\n\n\u003e Note: in other libraries you would use something like:¬†`np.random.seed()`¬†or¬†`torch.manual_seed()`¬†\n\n## **Implementation**\n\nFind the full code for this and other tutorials¬†[here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\n\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n```\n\n## Analyzing the Training Runs\n\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after¬†[Aim 1.3.5 release](https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment)) and have multiple charts of different metrics on the same screen.\n\nDo the following steps to see the different effects of the optimizers\n\n* Go to dashboard, explore by experiment\n* Add loss to¬†`SELECT`¬†and divide into subplots by metric\n* Group by experiment to make all metrics of similar color\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\n\nHere is how it looks on Aim:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png)\n\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\n\nIf you apply the same settings to the learning rate, this is the result:\n\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\n\n* Remove both previous groupings\n* Group by color by learning rate\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif)\n\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\n\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\n\nOn top of this, if we also add grouping by style by optimizer:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png)\n\nNow, it is obvious that the the runs with SGD optimizer and¬†`lr=0.01`¬†(green, broken lines) are the best choices for all the seeds we have tried.\n\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\n\n## Learn More\n\nIf you find Aim useful, support us and¬†[star the project](https://github.com/aimhubio/aim)¬†on GitHub. Join the¬†[Aim community](https://community.aimstack.io/)¬†and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is a random seed and how is it important?\u003c/h2\u003e\n\u003cp\u003eThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\u003c/li\u003e\n\u003cli\u003eDropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\u003c/li\u003e\n\u003cli\u003eAugmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance.¬†One thing you do not need when tuning hyper-parameters is variance.\u003c/p\u003e\n\u003cp\u003eThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\u003c/p\u003e\n\u003cp\u003eLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eHow to fix the seed in PyTorch Lightning\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python\u003ccode\u003erandom.seed(seed)\u003c/code\u003e, but it is not enough for PL.\u003c/p\u003e\n\u003cp\u003ePytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use¬†\u003ccode\u003epl.seed_everything(seed)\u003c/code\u003e¬†. See the docs¬†\u003ca href=\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: in other libraries you would use something like:¬†\u003ccode\u003enp.random.seed()\u003c/code\u003e¬†or¬†\u003ccode\u003etorch.manual_seed()\u003c/code\u003e¬†\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFind the full code for this and other tutorials¬†\u003ca href=\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAnalyzing the Training Runs\u003c/h2\u003e\n\u003cp\u003eAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after¬†\u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment\"\u003eAim 1.3.5 release\u003c/a\u003e) and have multiple charts of different metrics on the same screen.\u003c/p\u003e\n\u003cp\u003eDo the following steps to see the different effects of the optimizers\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGo to dashboard, explore by experiment\u003c/li\u003e\n\u003cli\u003eAdd loss to¬†\u003ccode\u003eSELECT\u003c/code\u003e¬†and divide into subplots by metric\u003c/li\u003e\n\u003cli\u003eGroup by experiment to make all metrics of similar color\u003c/li\u003e\n\u003cli\u003eGroup by style by optimizer to see different optimizers on loss and accuracy and its effects\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere is how it looks on Aim:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*cKgeMrtWMPyz4tm801DQXQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPKt9pkOD5weJVlHQ8j1sA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\u003c/p\u003e\n\u003cp\u003eIf you apply the same settings to the learning rate, this is the result:\u003c/p\u003e\n\u003ch2\u003eFor the next step to analyze how learning rate affects the experiments, do the following steps:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemove both previous groupings\u003c/li\u003e\n\u003cli\u003eGroup by color by learning rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*RmwmBReVr378QfA-VHx0yQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\u003c/p\u003e\n\u003cp\u003eWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\u003c/p\u003e\n\u003cp\u003eOn top of this, if we also add grouping by style by optimizer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Nrc-Z1aq71y9IO0bh-480Q.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjdL4A9rAdF2X6C6tEhScw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, it is obvious that the the runs with SGD optimizer and¬†\u003ccode\u003elr=0.01\u003c/code\u003e¬†(green, broken lines) are the best choices for all the seeds we have tried.\u003c/p\u003e\n\u003cp\u003eFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eIf you find Aim useful, support us and¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e¬†on GitHub. Join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e¬†and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"},{"title":"How YerevaNN researchers use Aim to compare 100s of models","date":"2022-05-31T16:29:36.844Z","author":"Gev Soghomonian","description":"Explore YerevaNN's use of Aim: Comparing 100s of models for cross-lingual transfer in NLP. Reproducing studies, model selection, and insights revealed. Learn more!","slug":"how-yerevann-researchers-use-aim-to-compare-100s-of-models","image":"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*lnmBXL_nhBGxOvjNO56-Rw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"*Thanks to¬†[Knarik Mheryan](https://medium.com/u/309b97b32187?source=post_page-----1a34ced2ced0--------------------------------)¬†and¬†[Hrant Khachatrian](https://medium.com/u/83126d4b3c1b?source=post_page-----1a34ced2ced0--------------------------------)for co-writing this blogpost.*\n\n[YerevaNN](https://yerevann.com/) is a non-profit computer science and mathematics research lab based in Yerevan, Armenia. The lab‚Äôs main research interests are focused around machine learning algorithms and machine learning for biomedical data. YerevaNN also spends significant effort on supervision of student projects in order to support the up and coming Armenian scholars.\n\n[YerevaNN¬†](https://yerevann.com/)collaborates with University of Southern California ‚Äî focused on low-resource learning problems, ENSAE / ParisTech on theoretical statistics problems, biologists and chemists from around the world on ML applications in various biomedical problems. YerevaNN has publications at ACL, CVPR, WMT, Annals of Statistics and Nature Scientific Data among others.\n\nWe traditionally onboard new teammates with a task to reproduce a well-known paper. This allows us to both introduce them to our internal systems, codebase, resources as well as it‚Äôs a great leeway into the problems they are going to focus on as part of their research at YerevaNN. This is a story about how Knarik Mheryan has gotten started at YerevaNN and how¬†[Aim](https://github.com/aimhubio/aim/)¬†has helped her in the process.\n\nThe Challenges\n\n## Model selection for cross-lingual transfer\n\nDeep learning has enabled high-accuracy solutions to a wide variety of NLP tasks.. As these models are trained primarily on English corpora, they aren‚Äôt readily applicable or available for other human languages (due to the lack of labeled data).\n\nOne approach to solve this problem is to pre-train a model on an unlabeled multilingual dataset in an unsupervised fashion, then fine-tune it only on English labeled data for the given NLP task. It turns out the model‚Äôs performance on the target languages is strong, even though the model is fine-tuned only on English data.\n\nModel selection is a crucial step in this approach. When fine-tuning on English data with different hyperparams and seeds we are left with a set of models. The challenge is to select the model with the highest performance (e.g. F1 score) in the target language.\n\n**In 2021, Yang Chen and Alan Ritter, have published a¬†[paper in EMNLP](https://arxiv.org/abs/2010.06127)¬†that tries to address this problem. Knarik Mheryan‚Äôs first task at YerevaNN has been to reproduce this work as part of the¬†[ML Reproducibility Challenge 2021](https://paperswithcode.com/rc2021).**\n\n## Reproducing the study\n\nA common tactic has been to select models that perform well on the English data. This paper shows that the accuracy of models on English data does not always correlate well with the target language performance. To reproduce this, we have fine-tuned 240 models on a Named Entity Recognition task.\n\nThe dimensionality of these experiments is high. So analyzing several 100 models is not a trivial task and it can take hours when comparing them with traditional tools. At YerevaNN we have been using Aim as it allows us to compare 1000s of high-dimensional (all metrics, hyperparams and other metadata) experiments in a few clicks.\n\n# The Solution\n\n## Using Params Explorer to compare models\n\nParams explorer allows us to directly compare the models with one view and with a few clicks. We have tracked the F1 scores for all models, languages. Using Aim‚Äôs¬†[context](https://aimstack.readthedocs.io/en/latest/understanding/concepts.html?highlight=context#sequence-context)¬†makes it easy to add additional metadata to the metrics (such as lang=‚Äùfr‚Äù) so there is no need to bake that data into the name.\n\nWe have selected two hyperparameters of interest and F1 scores of 5 languages of interest. This one view shows all the models (each curve is a model) at once and we can compare them relative to selected hyperparams and metrics. (see Figure 1)\n\nMore importantly, the plot below shows that the best performing model on English data (F1 lang=‚Äùen‚Äù) certainly doesn‚Äôt perform well on German, Spanish, Dutch and Chinese.¬†**The red bold curve represents the best model on the English dataset.**\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMiTQD1jlsycSLCgPV0hfw.png)\n\nFigure 1: Aim‚Äôs Params plot for hundreds of fine-tuned models. Each curve corresponds to a single model with some hyperparameters and random seeds. Each vertical axis shows the model‚Äôs hyperparameters or the F1 scores for some language (German, Spanish, Dutch, Chinese, English).\n\n\\\nIt took us only a few clicks to see this key insight.\n\n**And this makes a couple of hours of work into something that‚Äôs less than a minute.**\n\n# Qualitative metrics comparison with\n\n# [Metrics Explorer](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)\n\n## Catastrophic Forgetting on the best Eng-based model\n\nIn our pursuit of understanding why models behave that way, we have tracked F1 scores across 60 epochs for the model that performs best on the English dataset. The Figure 2 below shows the behavior of F1 scores for 5 different languages (German, Spanish, Dutch, Chinese and English).\n\nThe red line on Figure 2 represents the evaluation on English data. There is a clear difference in performance between the languages. Effectively Aim shows that the model overfits on English and what the ‚ÄòCatastrophic forgetting‚Äô looks like.\n\nNotably, the Chinese (orange) gets worse faster than the other languages. Possibly because there are major linguistic differences between Chinese and the rest of the languages.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTYunp3onfPgzAorAUed9A.png)\n\nFigure 2: Visualization of the Catastrophic forgetting phenomenon. Evaluations on 5 languages development datasets for 60 epochs after fine-tuning the model on the English training data. The languages are English (red), Dutch (blue), German (green), Spanish (violet), and Chinese (orange).\n\n## Proposed model selection algorithm analysis\n\nWe reproduce the paper in two phases.\n\n1. First, we have created candidate models by fine-tuning them on English labeled data and checked their performance on the proposed 5 target languages.\n2. Second, we are going to build the algorithm for model selection ‚Äî proposed in the paper and verify if it works better than the conventional methods.\n\n**Analyzing the Learning Rates**\n\nAim‚Äôs Metric Explorer has allowed us to analyze the fine-tuning process through all the runs. In Figure 3, each line is a run with a specific target language, hyperparameter and random seed. Here we group runs by learning rate giving them the same color.\n\nWith Aim we frequently group runs into different charts. In this case we have grouped the metrics into charts by language . This allows us to visualize the fine-tuning with different learning rates per language.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab3o77BJ0R8hbEezh7NrcA.png)\n\nFigure 3: Visualization of the fine-tuning process with different learning rates and random seeds for Spanish, German, Chinese, Dutch and English languages.\n\nWith one more click, we aggregate runs of the same group (grouped by learning rate through all seeds). The aggregation helps us to see the trends of the F1 metrics collected across epochs.\n\nFrom Figure 4 it is clear that all models with very small learning rates (red) barely learn anything in 20 epochs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiam-hSc66ISC3deKgMoUg.png)\n\nFigure 4: Visualization of the fine-tuning process aggregated by learning rates through different random seeds. Lines through highlighted intervals are medians from runs with different seeds. Each chart plots the fine-tuning process for a particular language (Spanish, German, Chinese, Dutch and English)\n\nIn this paper the authors suggest an alternative strategy for model selection named Learned Model Selection (LMS). LMS is a neural network based model that learns to score the compatibility between a fine-tuned model and a target language. According to the paper, this method consistently selects better models than English development data across the 4 target languages.\n\nFor this final step we have also calculated the LMS scores for each model and have used Aim Params Explorer to compare the new and old methods for their efficiency. By just selecting the params and metrics we have produced the Figure 5 on Aim. Each highlighted curve in Figure 5 corresponds to one of the model candidates. With the old method (with the English axis) the best candidate F1 on the English axis is 0.699. On the other hand, if we select by the new algorithm (with LMS score axis), the F1 score would be 0.71, which is indeed higher than with the English candidate model.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7Jpwdraw_XlSji_GTk2yA.png)\n\nFigure 5: Visualization of the final process of model selection. The most left axis is the model number (like id) from 0 to 59. ‚ÄòF1 lang=‚ÄùEnglish‚Äù‚Äò is the F1 score on the English development data. ‚ÄòF1 lang=‚ÄùGerman‚Äù‚Äò is the F1 score on the German test data. ‚ÄòLMS score‚Äô is the ranking score provided by LMS, the algorithm proposed in the paper, the higher the better.\n\nWith Aim our experiments analysis at YerevaNN has become order of magnitude faster due to intuitive, beautiful and fast UI and the Aim explorers. The team at YerevaNN uses Aim on a daily basis to analyze their experiments.\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nThis article was originally published on¬†[Aim Blog](https://aimstack.io/blog). Find more in depth guides and details of the newest releases there.","html":"\u003cp\u003e\u003cem\u003eThanks to¬†\u003ca href=\"https://medium.com/u/309b97b32187?source=post_page-----1a34ced2ced0--------------------------------\"\u003eKnarik Mheryan\u003c/a\u003e¬†and¬†\u003ca href=\"https://medium.com/u/83126d4b3c1b?source=post_page-----1a34ced2ced0--------------------------------\"\u003eHrant Khachatrian\u003c/a\u003efor co-writing this blogpost.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://yerevann.com/\"\u003eYerevaNN\u003c/a\u003e is a non-profit computer science and mathematics research lab based in Yerevan, Armenia. The lab‚Äôs main research interests are focused around machine learning algorithms and machine learning for biomedical data. YerevaNN also spends significant effort on supervision of student projects in order to support the up and coming Armenian scholars.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://yerevann.com/\"\u003eYerevaNN¬†\u003c/a\u003ecollaborates with University of Southern California ‚Äî focused on low-resource learning problems, ENSAE / ParisTech on theoretical statistics problems, biologists and chemists from around the world on ML applications in various biomedical problems. YerevaNN has publications at ACL, CVPR, WMT, Annals of Statistics and Nature Scientific Data among others.\u003c/p\u003e\n\u003cp\u003eWe traditionally onboard new teammates with a task to reproduce a well-known paper. This allows us to both introduce them to our internal systems, codebase, resources as well as it‚Äôs a great leeway into the problems they are going to focus on as part of their research at YerevaNN. This is a story about how Knarik Mheryan has gotten started at YerevaNN and how¬†\u003ca href=\"https://github.com/aimhubio/aim/\"\u003eAim\u003c/a\u003e¬†has helped her in the process.\u003c/p\u003e\n\u003cp\u003eThe Challenges\u003c/p\u003e\n\u003ch2\u003eModel selection for cross-lingual transfer\u003c/h2\u003e\n\u003cp\u003eDeep learning has enabled high-accuracy solutions to a wide variety of NLP tasks.. As these models are trained primarily on English corpora, they aren‚Äôt readily applicable or available for other human languages (due to the lack of labeled data).\u003c/p\u003e\n\u003cp\u003eOne approach to solve this problem is to pre-train a model on an unlabeled multilingual dataset in an unsupervised fashion, then fine-tune it only on English labeled data for the given NLP task. It turns out the model‚Äôs performance on the target languages is strong, even though the model is fine-tuned only on English data.\u003c/p\u003e\n\u003cp\u003eModel selection is a crucial step in this approach. When fine-tuning on English data with different hyperparams and seeds we are left with a set of models. The challenge is to select the model with the highest performance (e.g. F1 score) in the target language.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIn 2021, Yang Chen and Alan Ritter, have published a¬†\u003ca href=\"https://arxiv.org/abs/2010.06127\"\u003epaper in EMNLP\u003c/a\u003e¬†that tries to address this problem. Knarik Mheryan‚Äôs first task at YerevaNN has been to reproduce this work as part of the¬†\u003ca href=\"https://paperswithcode.com/rc2021\"\u003eML Reproducibility Challenge 2021\u003c/a\u003e.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003eReproducing the study\u003c/h2\u003e\n\u003cp\u003eA common tactic has been to select models that perform well on the English data. This paper shows that the accuracy of models on English data does not always correlate well with the target language performance. To reproduce this, we have fine-tuned 240 models on a Named Entity Recognition task.\u003c/p\u003e\n\u003cp\u003eThe dimensionality of these experiments is high. So analyzing several 100 models is not a trivial task and it can take hours when comparing them with traditional tools. At YerevaNN we have been using Aim as it allows us to compare 1000s of high-dimensional (all metrics, hyperparams and other metadata) experiments in a few clicks.\u003c/p\u003e\n\u003ch1\u003eThe Solution\u003c/h1\u003e\n\u003ch2\u003eUsing Params Explorer to compare models\u003c/h2\u003e\n\u003cp\u003eParams explorer allows us to directly compare the models with one view and with a few clicks. We have tracked the F1 scores for all models, languages. Using Aim‚Äôs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/understanding/concepts.html?highlight=context#sequence-context\"\u003econtext\u003c/a\u003e¬†makes it easy to add additional metadata to the metrics (such as lang=‚Äùfr‚Äù) so there is no need to bake that data into the name.\u003c/p\u003e\n\u003cp\u003eWe have selected two hyperparameters of interest and F1 scores of 5 languages of interest. This one view shows all the models (each curve is a model) at once and we can compare them relative to selected hyperparams and metrics. (see Figure 1)\u003c/p\u003e\n\u003cp\u003eMore importantly, the plot below shows that the best performing model on English data (F1 lang=‚Äùen‚Äù) certainly doesn‚Äôt perform well on German, Spanish, Dutch and Chinese.¬†\u003cstrong\u003eThe red bold curve represents the best model on the English dataset.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMiTQD1jlsycSLCgPV0hfw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 1: Aim‚Äôs Params plot for hundreds of fine-tuned models. Each curve corresponds to a single model with some hyperparameters and random seeds. Each vertical axis shows the model‚Äôs hyperparameters or the F1 scores for some language (German, Spanish, Dutch, Chinese, English).\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nIt took us only a few clicks to see this key insight.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnd this makes a couple of hours of work into something that‚Äôs less than a minute.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eQualitative metrics comparison with\u003c/h1\u003e\n\u003ch1\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003eMetrics Explorer\u003c/a\u003e\u003c/h1\u003e\n\u003ch2\u003eCatastrophic Forgetting on the best Eng-based model\u003c/h2\u003e\n\u003cp\u003eIn our pursuit of understanding why models behave that way, we have tracked F1 scores across 60 epochs for the model that performs best on the English dataset. The Figure 2 below shows the behavior of F1 scores for 5 different languages (German, Spanish, Dutch, Chinese and English).\u003c/p\u003e\n\u003cp\u003eThe red line on Figure 2 represents the evaluation on English data. There is a clear difference in performance between the languages. Effectively Aim shows that the model overfits on English and what the ‚ÄòCatastrophic forgetting‚Äô looks like.\u003c/p\u003e\n\u003cp\u003eNotably, the Chinese (orange) gets worse faster than the other languages. Possibly because there are major linguistic differences between Chinese and the rest of the languages.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTYunp3onfPgzAorAUed9A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 2: Visualization of the Catastrophic forgetting phenomenon. Evaluations on 5 languages development datasets for 60 epochs after fine-tuning the model on the English training data. The languages are English (red), Dutch (blue), German (green), Spanish (violet), and Chinese (orange).\u003c/p\u003e\n\u003ch2\u003eProposed model selection algorithm analysis\u003c/h2\u003e\n\u003cp\u003eWe reproduce the paper in two phases.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFirst, we have created candidate models by fine-tuning them on English labeled data and checked their performance on the proposed 5 target languages.\u003c/li\u003e\n\u003cli\u003eSecond, we are going to build the algorithm for model selection ‚Äî proposed in the paper and verify if it works better than the conventional methods.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eAnalyzing the Learning Rates\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim‚Äôs Metric Explorer has allowed us to analyze the fine-tuning process through all the runs. In Figure 3, each line is a run with a specific target language, hyperparameter and random seed. Here we group runs by learning rate giving them the same color.\u003c/p\u003e\n\u003cp\u003eWith Aim we frequently group runs into different charts. In this case we have grouped the metrics into charts by language . This allows us to visualize the fine-tuning with different learning rates per language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab3o77BJ0R8hbEezh7NrcA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 3: Visualization of the fine-tuning process with different learning rates and random seeds for Spanish, German, Chinese, Dutch and English languages.\u003c/p\u003e\n\u003cp\u003eWith one more click, we aggregate runs of the same group (grouped by learning rate through all seeds). The aggregation helps us to see the trends of the F1 metrics collected across epochs.\u003c/p\u003e\n\u003cp\u003eFrom Figure 4 it is clear that all models with very small learning rates (red) barely learn anything in 20 epochs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiam-hSc66ISC3deKgMoUg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 4: Visualization of the fine-tuning process aggregated by learning rates through different random seeds. Lines through highlighted intervals are medians from runs with different seeds. Each chart plots the fine-tuning process for a particular language (Spanish, German, Chinese, Dutch and English)\u003c/p\u003e\n\u003cp\u003eIn this paper the authors suggest an alternative strategy for model selection named Learned Model Selection (LMS). LMS is a neural network based model that learns to score the compatibility between a fine-tuned model and a target language. According to the paper, this method consistently selects better models than English development data across the 4 target languages.\u003c/p\u003e\n\u003cp\u003eFor this final step we have also calculated the LMS scores for each model and have used Aim Params Explorer to compare the new and old methods for their efficiency. By just selecting the params and metrics we have produced the Figure 5 on Aim. Each highlighted curve in Figure 5 corresponds to one of the model candidates. With the old method (with the English axis) the best candidate F1 on the English axis is 0.699. On the other hand, if we select by the new algorithm (with LMS score axis), the F1 score would be 0.71, which is indeed higher than with the English candidate model.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7Jpwdraw_XlSji_GTk2yA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 5: Visualization of the final process of model selection. The most left axis is the model number (like id) from 0 to 59. ‚ÄòF1 lang=‚ÄùEnglish‚Äù‚Äò is the F1 score on the English development data. ‚ÄòF1 lang=‚ÄùGerman‚Äù‚Äò is the F1 score on the German test data. ‚ÄòLMS score‚Äô is the ranking score provided by LMS, the algorithm proposed in the paper, the higher the better.\u003c/p\u003e\n\u003cp\u003eWith Aim our experiments analysis at YerevaNN has become order of magnitude faster due to intuitive, beautiful and fast UI and the Aim explorers. The team at YerevaNN uses Aim on a daily basis to analyze their experiments.\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eThis article was originally published on¬†\u003ca href=\"https://aimstack.io/blog\"\u003eAim Blog\u003c/a\u003e. Find more in depth guides and details of the newest releases there.\u003c/p\u003e"},"_id":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","_raw":{"sourceFilePath":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","sourceFileName":"how-yerevann-researchers-use-aim-to-compare-100s-of-models.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-yerevann-researchers-use-aim-to-compare-100s-of-models"},"type":"Post"},{"title":"Hugging the Chaos: Connecting Datasets to Trainings with Hugging Face and Aim","date":"2023-02-17T07:05:28.709Z","author":"Gor Arakelyan","description":"Combining Hugging Face and Aim to make machine learning experiments traceable, reproducible and easier to compare.","slug":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-luoNtZBpA1SGkTsHzCrkA.jpeg","draft":false,"categories":["Tutorials"],"body":{"raw":"# The cost of neglecting experiments management\n\n\n\nWorking with large and frequently changing datasets is hard!\\\nYou can easily end up in a mess if you don‚Äôt have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.\n\nIn this article, we will explore how you can combine¬†[Hugging Face Datasets](https://github.com/huggingface/datasets)¬†and¬†[Aim](https://github.com/aimhubio/aim)¬†to make machine learning experiments traceable, reproducible and easier to compare.\n\nLet‚Äôs dive in and get started!\n\n# Hugging Face Datasets + Aim = ‚ù§\n\n[Hugging Face Datasets](https://github.com/huggingface/datasets)¬†is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, you‚Äôll never have to worry about manually loading and versioning your data again.\n\n[Aim](https://github.com/aimhubio/aim), on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png)\n\nHugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.\n\n# Project overview\n\nLet‚Äôs go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.\n\nHere‚Äôs what we‚Äôll be using:\n\n* [Hugging Face Datasets](https://github.com/huggingface/datasets)¬†to load and manage the dataset.\n* [Hugging Face Hub](http://huggingface.co/)¬†to host the dataset.\n* [PyTorch](https://github.com/pytorch/pytorch)¬†to build and train the model.\n* [Aim](https://github.com/aimhubio/aim)¬†to keep track of all the model and dataset metadata.\n\nOur dataset is going to be called ‚ÄúA-MNIST‚Äù ‚Äî a version of the ‚ÄúMNIST‚Äù dataset with extra samples added. We‚Äôll start with the original ‚ÄúMNIST‚Äù dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.\n\n# Dataset preparation\n\n## Uploading the dataset to Hub\n\n\n\nLet‚Äôs head over to Hugging Face Hub and create a new dataset repository called¬†[‚ÄúA-MNIST‚Äù](https://huggingface.co/datasets/gorar/A-MNIST)¬†to store our dataset.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png)\n\nCurrently, the repository is empty, so let‚Äôs upload the initial version of the dataset.\n\nWe‚Äôll use the original MNIST dataset, as the first version v1.0.0. To do this, we‚Äôll need to upload the dataset files along with the dataset loading script¬†`A-MNIST.py`. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:\n\n```\nclass AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n```\n\n*See the full script here:¬†\u003chttps://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\u003e*\n\nWe‚Äôll have the following setup, including all the necessary git configurations and the dataset card:\n\n```\n- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n```\n\nLet‚Äôs commit the changes and push the dataset to the Hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png)\n\nAwesome! Now we‚Äôve got the first version of ‚ÄúA-MNIST‚Äù hosted on Hugging Face Hub!\n\n## Augmenting the dataset\n\n\n\nNext up, let‚Äôs add more images to the MNIST dataset using augmentation techniques.\n\nWe will rotate all the train images by 20 degree and append to the dataset:\n\n```\nrotated_images[i] = rotate(images[i], angle=20, reshape=False)\n```\n\nAs well as, update the ‚ÄúA-MNIST‚Äù dataset version to 1.1.0:\n\n```\nBUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n```\n\nAfter preparing the new train dataset, let‚Äôs commit the changes and push the upgraded version to the hub.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png)\n\nPerfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. üéâ\n\n# Training setup\n\nLet‚Äôs load the dataset using datasets python package. It‚Äôs really simple, just one line of code:\n\n```\ndataset = load_dataset(\"gorar/A-MNIST\")\n```\n\nOne of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:\n\n```\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n```\n\nWe‚Äôre ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:\n\n```\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n```\n\n*See the full code here:¬†\u003chttps://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\u003e*\n\nWe‚Äôve got everything ready now. Let‚Äôs kick off the training by running:\n\n```\npython train.py\n```\n\nHurray, the training is underway! üèÉ\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png)\n\nIt would be tough to monitor my training progress on terminal. This is where Aim‚Äôs superpowers come into play.\n\n# Integrating Aim\n\n\n\n## Trainings tracking\n\n\n\nAim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:\n\n```\nimport aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n```\n\nFurthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.\n\nSince v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply import¬†`HFDataset`¬†module from Aim and you can track all of your dataset metadata with just a single line of code!\n\n```\nfrom aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n```\n\nAim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the dataset‚Äôs description, version, features, etc.\n\n# Experimentation\n\n## Conducting trainings\n\n\n\nWe‚Äôll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, we‚Äôll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.\n\nDatasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:\n\n```\n# Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n```\n\nWe will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.\n\n## Exploring training results via Aim\n\n\n\nNow that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aim‚Äôs UI, we need to run the¬†`aim up`¬†command. A following message would be printed on the terminal output meaning the UI is successfully running:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png)\n\nTo access the UI, let‚Äôs navigate to¬†`127.0.0.1:43800`¬†in the browser.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png)\n\nWe can see our trainings on the contributions map and in the activity feed. Let‚Äôs take a closer look at an individual run details by navigating to the run‚Äôs page on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png)\n\nAll of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.\n\nWe can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png)\n\nThe UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.\n\n## Comparing trainings via Aim\n\nLet‚Äôs find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics Explorer¬†`127.0.0.1:43800/metrics`.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png)\n\nAim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.\n\nLet‚Äôs group by¬†`run.dataset.dataset.meta.version`¬†to see how the models performed based on the dataset version they were trained on.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png)\n\n\\\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.\n\nNow, to improve visibility and better evaluate performance, let‚Äôs go ahead and smooth out these lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png)\n\nIt appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, let‚Äôs group metrics by the learning rate to learn its impact on models performance.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png)\n\nIt seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png)\n\nTo sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.\n\nWith just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! üéâ\n\n# Reproducibility\n\nHave you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.\n\nAim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!\n\n# Conclusion\n\nIn this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.\n\nWe compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.\n\nSimplifying our daily work with efficient stack helps us focus on what really matters ‚Äî getting the best results from our machine learning experiments.\n\n# Learn more\n\n\n\nCheck out the Aim + Datasets integration docs¬†[here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\\\nDatasets repo:¬†\u003chttps://github.com/huggingface/datasets\u003e\\\nAim repo:¬†\u003chttps://github.com/aimhubio/aim\u003e\n\nIf you have questions, join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\n\nDrop a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003ch1\u003eThe cost of neglecting experiments management\u003c/h1\u003e\n\u003cp\u003eWorking with large and frequently changing datasets is hard!\u003cbr\u003e\nYou can easily end up in a mess if you don‚Äôt have a system that traces dataset versions and connects them to your experiments. Moreover, the lack of traceability can make it impossible to effectively compare your experiments. Let alone the reproducibility.\u003c/p\u003e\n\u003cp\u003eIn this article, we will explore how you can combine¬†\u003ca href=\"https://github.com/huggingface/datasets\"\u003eHugging Face Datasets\u003c/a\u003e¬†and¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†to make machine learning experiments traceable, reproducible and easier to compare.\u003c/p\u003e\n\u003cp\u003eLet‚Äôs dive in and get started!\u003c/p\u003e\n\u003ch1\u003eHugging Face Datasets + Aim = ‚ù§\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/huggingface/datasets\"\u003eHugging Face Datasets\u003c/a\u003e¬†is a fantastic library that makes it super easy to access and share datasets for audio, computer vision, and NLP tasks. With Datasets, you‚Äôll never have to worry about manually loading and versioning your data again.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, on the other hand, is an easy-to-use and open-source experiment tracker with lots of superpowers. It enables easily logging training runs, comparing them via a beautiful UI, and querying them programmatically.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeZVBvuC_vExlOoZUT84mw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eHugging Face Datasets and Aim combined are a powerful way to track training runs and their respective dataset metadata. So you can compare your experiments based on the dataset version and reproduce them super seamlessly.\u003c/p\u003e\n\u003ch1\u003eProject overview\u003c/h1\u003e\n\u003cp\u003eLet‚Äôs go through an object classification project and show how you can use Datasets to load and version your data, and Aim to keep track of everything along the way.\u003c/p\u003e\n\u003cp\u003eHere‚Äôs what we‚Äôll be using:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/huggingface/datasets\"\u003eHugging Face Datasets\u003c/a\u003e¬†to load and manage the dataset.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://huggingface.co/\"\u003eHugging Face Hub\u003c/a\u003e¬†to host the dataset.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/pytorch/pytorch\"\u003ePyTorch\u003c/a\u003e¬†to build and train the model.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e¬†to keep track of all the model and dataset metadata.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOur dataset is going to be called ‚ÄúA-MNIST‚Äù ‚Äî a version of the ‚ÄúMNIST‚Äù dataset with extra samples added. We‚Äôll start with the original ‚ÄúMNIST‚Äù dataset and then add 60,000 rotated versions of the original training samples to create a new, augmented version. We will run trainings on both dataset versions and see how the models are performing.\u003c/p\u003e\n\u003ch1\u003eDataset preparation\u003c/h1\u003e\n\u003ch2\u003eUploading the dataset to Hub\u003c/h2\u003e\n\u003cp\u003eLet‚Äôs head over to Hugging Face Hub and create a new dataset repository called¬†\u003ca href=\"https://huggingface.co/datasets/gorar/A-MNIST\"\u003e‚ÄúA-MNIST‚Äù\u003c/a\u003e¬†to store our dataset.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lK4w_iokcetDnyOi9HgZbw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eCurrently, the repository is empty, so let‚Äôs upload the initial version of the dataset.\u003c/p\u003e\n\u003cp\u003eWe‚Äôll use the original MNIST dataset, as the first version v1.0.0. To do this, we‚Äôll need to upload the dataset files along with the dataset loading script¬†\u003ccode\u003eA-MNIST.py\u003c/code\u003e. The dataset loading script is a python file that defines the different configurations and splits of the dataset, as well as how to download and process the data:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass AMNIST(datasets.GeneratorBasedBuilder):\n    \"\"\"A-MNIST Data Set\"\"\"\n\n    BUILDER_CONFIGS = [\n            datasets.BuilderConfig(\n                name=\"amnist\",\n                version=datasets.Version(\"1.0.0\"),\n                description=_DESCRIPTION,\n            )\n        ]\n    ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eSee the full script here:¬†\u003ca href=\"https://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\"\u003ehttps://huggingface.co/datasets/gorar/A-MNIST/blob/main/A-MNIST.py\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe‚Äôll have the following setup, including all the necessary git configurations and the dataset card:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- `data` - contains the dataset.\n    - `t10k-images-idx3-ubyte.gz` - test images.\n    - `t10k-labels-idx1-ubyte.gz` - test labels.\n    - `train-images-idx3-ubyte.gz` - train images.\n    - `train-labels-idx1-ubyte.gz` - train labels.\n- `A-MNIST.py` - the dataset loading script.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet‚Äôs commit the changes and push the dataset to the Hub.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzEHq-JVe6AyNm6xUPIWzg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAwesome! Now we‚Äôve got the first version of ‚ÄúA-MNIST‚Äù hosted on Hugging Face Hub!\u003c/p\u003e\n\u003ch2\u003eAugmenting the dataset\u003c/h2\u003e\n\u003cp\u003eNext up, let‚Äôs add more images to the MNIST dataset using augmentation techniques.\u003c/p\u003e\n\u003cp\u003eWe will rotate all the train images by 20 degree and append to the dataset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erotated_images[i] = rotate(images[i], angle=20, reshape=False)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs well as, update the ‚ÄúA-MNIST‚Äù dataset version to 1.1.0:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBUILDER_CONFIGS = [\n    datasets.BuilderConfig(\n        name=\"amnist\",\n        version=datasets.Version(\"1.1.0\"),\n        description=_DESCRIPTION,\n    )\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter preparing the new train dataset, let‚Äôs commit the changes and push the upgraded version to the hub.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoIdeuBE1pB0VMh-ZAY7Rw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003ePerfect, v1.1.0 is now available on the Hub! This means that we can easily access it in the training script using the Hugging Face Datasets. üéâ\u003c/p\u003e\n\u003ch1\u003eTraining setup\u003c/h1\u003e\n\u003cp\u003eLet‚Äôs load the dataset using datasets python package. It‚Äôs really simple, just one line of code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = load_dataset(\"gorar/A-MNIST\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOne of the amazing features of datasets is that is has native support for PyTorch, which allows us to prepare and initialize dataset loaders with ease:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\n# Loading the dataset\ndataset = load_dataset(\"gorar/A-MNIST\")\n\n# Defining train and test splits\ntrain_dataset = dataset['train'].with_format('torch')\ntest_dataset = dataset['test'].with_format('torch')\n\n# Initializing data loaders for train and test split\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe‚Äôre ready to write the training script and build the model. We are using PyTorch to build a simple convolutional neural network with two convolutional layers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eSee the full code here:¬†\u003ca href=\"https://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\"\u003ehttps://gist.github.com/gorarakelyan/936fb7b8fbde4de807500c5617b47ea8\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe‚Äôve got everything ready now. Let‚Äôs kick off the training by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython train.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHurray, the training is underway! üèÉ\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYTjRUMNbr_1e9mFX0tyhw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIt would be tough to monitor my training progress on terminal. This is where Aim‚Äôs superpowers come into play.\u003c/p\u003e\n\u003ch1\u003eIntegrating Aim\u003c/h1\u003e\n\u003ch2\u003eTrainings tracking\u003c/h2\u003e\n\u003cp\u003eAim offers a user-friendly interface to keep track of your training hyper-parameters and metrics:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport aim\n\naim_run = aim.Run()\n\n# Track hyper-parameters\naim_run['hparams'] = {\n    'num_epochs': num_epochs,\n    'num_classes': num_classes,\n    ...\n}\n\n# Track metrics\nfor i, samples in enumerate(train_loader):\n    ...\n    aim_run.track(acc, name='accuracy', epoch=epoch, context={'subset': 'train'})\n    aim_run.track(loss, name='loss', epoch=epoch, context={'subset': 'train'})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFurthermore, the awesome thing about Aim is that it is tightly integrated with ML ecosystem.\u003c/p\u003e\n\u003cp\u003eSince v3.16, Aim has a builtin integration with Hugging Face Datasets. Simply import¬†\u003ccode\u003eHFDataset\u003c/code\u003e¬†module from Aim and you can track all of your dataset metadata with just a single line of code!\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hf_dataset import HFDataset\n\naim_run['dataset'] = HFDataset(dataset)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAim will automagically gather the metadata from the dataset instance, store it within the Aim run, and display it on the Aim UI run page, including details like the dataset‚Äôs description, version, features, etc.\u003c/p\u003e\n\u003ch1\u003eExperimentation\u003c/h1\u003e\n\u003ch2\u003eConducting trainings\u003c/h2\u003e\n\u003cp\u003eWe‚Äôll perform several training runs on both versions of the datasets and adjust learning rates to evaluate their impact on the training process. Specifically, we‚Äôll use 1.0.0 and 1.1.0 versions of our dataset, and experiment with 0.01, 0.03, 0.1, 0.3 learning rates.\u003c/p\u003e\n\u003cp\u003eDatasets provide the ability to choose which dataset version we want to load. To do this, we just need to pass version and git revision of the dataset we want to work with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Loading A-MNIST v1.0.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.0.0', revision='dcc966eb0109e31d23c699199ca44bc19a7b1b47')\n\n# Loading A-MNIST v1.1.0\ndataset = load_dataset(\"gorar/A-MNIST\", version='1.1.0', revision='da9a9d63961462871324d514ca8cdca1e5624c5c')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe will run a simple bash script to execute the trainings. However, for a more comprehensive hyper-parameters tuning approach, tools like Ray Tune or other tuning frameworks can be used.\u003c/p\u003e\n\u003ch2\u003eExploring training results via Aim\u003c/h2\u003e\n\u003cp\u003eNow that we have integrated Aim into our training process, we can explore the results in a more user-friendly way. To launch Aim‚Äôs UI, we need to run the¬†\u003ccode\u003eaim up\u003c/code\u003e¬†command. A following message would be printed on the terminal output meaning the UI is successfully running:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfU9n1gY6pEJg4G2CJ8y9w.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo access the UI, let‚Äôs navigate to¬†\u003ccode\u003e127.0.0.1:43800\u003c/code\u003e¬†in the browser.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHcB19xB7dEj9kGdEEMEZA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see our trainings on the contributions map and in the activity feed. Let‚Äôs take a closer look at an individual run details by navigating to the run‚Äôs page on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TwA3xW06wD98adH2UF3wA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAll of the information is displayed in an organized and easy-to-read manner, allowing us to quickly understand how our training is performing and identify any potential issues.\u003c/p\u003e\n\u003cp\u003eWe can view the tracked hyper-parameters, metadata related to the dataset, and metrics results. Additionally, Aim automatically gathers information like system resource usage and outputs from the terminal.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSYcQgvpzbFvan_E7ygvIw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe UI also provides tools for visualizing and comparing results from multiple runs, making it easy to compare different model architectures, hyper-parameter and dataset settings to determine the best approach for a given problem.\u003c/p\u003e\n\u003ch2\u003eComparing trainings via Aim\u003c/h2\u003e\n\u003cp\u003eLet‚Äôs find out which models performed the best. We can easily compare the metrics of all the runs by opening up the Metrics Explorer¬†\u003ccode\u003e127.0.0.1:43800/metrics\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNQIEWDX5oKKz7CyBY0okw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim is very powerful when it comes to filtering runs and grouping. It provides an ability to group metrics based on any tracked metadata value.\u003c/p\u003e\n\u003cp\u003eLet‚Äôs group by¬†\u003ccode\u003erun.dataset.dataset.meta.version\u003c/code\u003e¬†to see how the models performed based on the dataset version they were trained on.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yg-joaFpEIrfVcbc5LMMg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nAccording to the legends section, the green lines represent models that were trained on the v1.0.0 dataset (the original MNIST dataset), while the blue metrics represent models trained on v1.1.0, an augmented dataset.\u003c/p\u003e\n\u003cp\u003eNow, to improve visibility and better evaluate performance, let‚Äôs go ahead and smooth out these lines.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhgXxth5iZHZ2XHmwuA_Dg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIt appears that both models performed similarly on both dataset versions. Since we also experimented with the learning rate hyper-parameter, let‚Äôs group metrics by the learning rate to learn its impact on models performance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*dd_VOX2BLPShEmVqPN3byw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIt seems that the blue lines, associated with a learning rate of 0.01, demonstrated the best performance!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*dxSto6wosNPeO-P7orw4cg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eTo sum it up, regardless of the dataset version, the models trained with a learning rate of 0.01 came out on top in terms of performance.\u003c/p\u003e\n\u003cp\u003eWith just a few clicks, we were able to compare different runs based on the dataset and learning rate value they were trained on! üéâ\u003c/p\u003e\n\u003ch1\u003eReproducibility\u003c/h1\u003e\n\u003cp\u003eHave you noticed how we achieved out-of-the-box reproducibility with the Hugging Face datasets and Aim integration? By versioning the dataset and keeping track of its revision, we can effortlessly reproduce experiments using the same dataset it was previously trained on.\u003c/p\u003e\n\u003cp\u003eAim stores model hyper-parameters, dataset metadata and other moving pieces, which allows us to quickly recreate and analyze previous experiments!\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eIn this article, we took a closer look at how to get started with uploading and using datasets through the Hugging Face Hub and Datasets. We carried out a series of trainings, and Aim helped us keep track of all the details and made it easier to stay organized.\u003c/p\u003e\n\u003cp\u003eWe compared the results of the trainings conducted on different versions of the dataset. As well as, discovered how helpful the combination of Datasets and Aim can be for reproducing previous runs.\u003c/p\u003e\n\u003cp\u003eSimplifying our daily work with efficient stack helps us focus on what really matters ‚Äî getting the best results from our machine learning experiments.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eCheck out the Aim + Datasets integration docs¬†\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\"\u003ehere\u003c/a\u003e.\u003cbr\u003e\nDatasets repo:¬†\u003ca href=\"https://github.com/huggingface/datasets\"\u003ehttps://github.com/huggingface/datasets\u003c/a\u003e\u003cbr\u003e\nAim repo:¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have questions, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\u003c/p\u003e\n\u003cp\u003eDrop a ‚≠êÔ∏è on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","_raw":{"sourceFilePath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileName":"hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/hugging-the-chaos-connecting-datasets-to-trainings-with-hugging-face-and-aim"},"type":"Post"},{"title":"Keep your model up to date for Question Answering ","date":"2024-02-28T18:24:47.359Z","author":"Hovhannes Tamoyan","description":"In this article, we'll show you how to keep your language model up to date for question answering tasks. We'll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. ","slug":"keep-your-model-up-to-date-for-question-answering","image":"/images/dynamic/medium-1-.png","draft":false,"categories":["Tutorials"],"body":{"raw":"In this article, we'll show you how to keep your language model up to date for question answering tasks. We'll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. We will Aim for tracking our progress and insights.\n\n## Importing Necessary Modules\n\nLet's kick off by importing the necessary modules for our experiments:\n\n```\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    DefaultDataCollator,\n    Trainer,\n    TrainingArguments,\n)\nfrom aim.hugging_face import AimCallback\n\n```\n\n## Preprocessing and Tokenization\n\nTo prepare our data for fine-tuning, we need to preprocess and tokenize it appropriately. We employ the DistilBERT tokenizer and define a preprocessing function to handle this task efficiently:\n\n```\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n    # Tokenize questions and context\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Process answers and calculate start/end positions\n    # ...\n    # Code snippet for answer processing goes here (omitted for brevity)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n```\n\n## Loading and Preparing the Dataset\n\n\n\nWith our preprocessing function ready, we proceed to load the SQuAD dataset and apply the preprocessing function to it. Additionally, we instantiate a classic data collator to be used as an argument for the Trainer:\n\n```\nsquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.train_test_split(test_size=0.2)\n\ntokenized_squad = squad.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=squad[\"train\"].column_names,\n)\n\ndata_collator = DefaultDataCollator()\n\n```\n\n## Defining Training Arguments\n\nNext, we define the training arguments for our fine-tuning process, including parameters such as batch size, learning rate, and number of epochs:\n\n```\nbatch_size = 32\nlr = 4e-5\n\ntraining_args = TrainingArguments(\n    output_dir=f\"qa_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    fp16=True,\n)\n\n```\n\n## Integrating Aim for Experiment Tracking\n\n\n\nIntegrating Aim into our experiments allows us to track metadata effectively. We instantiate the AimCallback, specifying the repository or directory for Aim:\n\n```\naim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:43700\", experiment='squad_huggingface_experiment'\n)\n\n```\n\nSimply provide the \\`aim_callback\\` as a callback in the Trainer arguments, and Aim will handle everything behind the scenes.\n\n## Initializing the Trainer\n\nWe then load the pre-trained DistilBERT model and define our Trainer instance, incorporating the training arguments, dataset, tokenizer, data collator, and AimCallback:\n\n```\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_squad[\"train\"],\n    eval_dataset=tokenized_squad[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[aim_callback],\n)\n\n```\n\n## Training and Evaluation\n\nWith all components set up, we are now ready to commence the training process followed by evaluating our fine-tuned model:\n\n```\ntrainer.train()\ntrainer.evaluate()\n```\n\nBy following these steps, we can effectively fine-tune a pre-trained language model for question answering while leveraging Aim for experiment tracking and metadata analysis. This approach enables us to derive meaningful insights and enhance the performance of our models efficiently.\n\n\n\n## Exploring the runs on Aim\n\n\n\nAfter completing our fine-tuning pipeline, it's super easy to explore and analyze the runs using Aim. We conducted experiments with two batch sizes (16, 32) and three learning rates (0.004, 0.0004, 0.00004) to compare their performance and derive insights in real-time.\n\nTo visualize and interpret the experiment results, we utilize the Aim command-line interface. By running the following command:\n\n```\naim up\n```\n\nWe can access the Aim dashboard via the provided URL, where we'll encounter a comprehensive overview of our experiments and their corresponding metadata. Let's delve into what we'll observe in the Runs Explorer page:\n\n![Runs explorer](/images/dynamic/screenshot-2024-02-20-at-11.34.35‚ÄØpm.png \"Runs explorer\")\n\nEach run's page provides details on hyperparameters, tracked metrics, system metrics, environment variables, and package versions.\n\n![Run overview](/images/dynamic/screenshot-2024-02-20-at-11.35.20‚ÄØpm.png \"Run overview\")\n\nAdditionally, you can review the run status in the Logs tab.\n\n![Logs tab](/images/dynamic/screenshot-2024-02-20-at-11.37.51‚ÄØpm.png \"Logs tab\")\n\n## Experiments Analysis\n\n\n\nThe traditional method for comparing machine learning experiments is through observing learning curves.\n\n![Metrics explorer](/images/dynamic/screenshot-2024-02-20-at-11.40.34‚ÄØpm.png \"Metrics explorer\")\n\nWe plot the loss by differentiating curves based on batch size and learning rate combinations. Evaluation subset curves are shown in solid lines, while training curves are in dashed lines.\n\nMoreover to track hyperparameters and compare them easily, we can utilize the Params Explorer. This involves observing how loss depends on the evaluation subset in relation to batch size and learning rate.\n\n![Param explorer](/images/dynamic/screenshot-2024-02-20-at-11.38.32‚ÄØpm.png \"Params explorer\")\n\nBased on this analysis, we observe that the best performing hyperparameter combination is batch size: 32 and learning rate: 0.00004.\n\n## Answering questions\n\nNow let‚Äôs use the latest checkpoint of the best model to answer some questions and track the questions and the answers:\n\n```\nfrom transformers import pipeline\n\nmodel_ckpt = \"PATH_TO_BEST_MODEL_CKPT\"\nquestion_answerer = pipeline(\"question-answering\", model=model_ckpt)\n\nquestions = [\n    {\n        \"question\": \"How many countries are members of the United Nations?\",\n        \"context\": \"The United Nations has 195 member countries.\",\n    },\n    {\n        \"question\": \"What is the population of China?\",\n        \"context\": \"China has a population of around 1.4 billion.\",\n    },\n    {\n        \"question\": \"Who is the current president of the United States?\",\n        \"context\": \"As of my last update in 2024, Joe Biden is the current President of the United States, succeeding Donald Trump.\",\n    },\n    {\n        \"question\": \"How many planets are there in the solar system?\",\n        \"context\": \"There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\",\n    },\n    {\n        \"question\": \"What is the capital city of Australia?\",\n        \"context\": \"The capital city of Australia is Canberra.\",\n    },\n]\n\nfor question in questions:\n    output = question_answerer(\n        question=question[\"question\"], context=question[\"context\"]\n    )\n\n    aim_callback.experiment.track(\n        Text(question[\"question\"]),\n        name=\"question\",\n        context={\"context\": question[\"context\"]},\n    )\n\n    aim_callback.experiment.track(\n        Text(output[\"answer\"]),\n        name=\"answer\",\n        context={\"context\": question[\"context\"]},\n    )\n```\n\nTo observe the responses we can use the Text Explorer, by grouping by ‚Äútexts.context.context‚Äù in the rows, and ‚Äútexts.name‚Äù in the columns:\n\n![Text explorer](/images/dynamic/screenshot-2024-02-20-at-11.47.57‚ÄØpm.png \"Text explorer\")\n\n## Conclusion\n\nTo sum up, this blog post outlined how to fine-tune a DistilBERT model for question answering with the SQuAD dataset, while leveraging Aim for experiment tracking. We started by preprocessing the data and setting up the Trainer with AimCallback integration. Through systematic experimentation, we discovered the best hyperparameter combinations and monitored our model's performance using Aim's user-friendly interface.\n\n\n\nMoreover, we demonstrated the versatility of exploration beyond mere model training by analyzing text inputs and outputs with Aim. By visualizing learning curves, comparing hyperparameter combinations, and monitoring model responses to questions, we gained valuable insights into our model's behavior.\n\n\n\n# Learn more\n\n\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)¬†üôå\n\n\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aim)¬†if you think Aim is useful. ‚≠êÔ∏è","html":"\u003cp\u003eIn this article, we'll show you how to keep your language model up to date for question answering tasks. We'll tweak a pre-trained DistilBERT model and fine-tune it on the SQuAD question-answering dataset. We will Aim for tracking our progress and insights.\u003c/p\u003e\n\u003ch2\u003eImporting Necessary Modules\u003c/h2\u003e\n\u003cp\u003eLet's kick off by importing the necessary modules for our experiments:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    DefaultDataCollator,\n    Trainer,\n    TrainingArguments,\n)\nfrom aim.hugging_face import AimCallback\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePreprocessing and Tokenization\u003c/h2\u003e\n\u003cp\u003eTo prepare our data for fine-tuning, we need to preprocess and tokenize it appropriately. We employ the DistilBERT tokenizer and define a preprocessing function to handle this task efficiently:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n    # Tokenize questions and context\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Process answers and calculate start/end positions\n    # ...\n    # Code snippet for answer processing goes here (omitted for brevity)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLoading and Preparing the Dataset\u003c/h2\u003e\n\u003cp\u003eWith our preprocessing function ready, we proceed to load the SQuAD dataset and apply the preprocessing function to it. Additionally, we instantiate a classic data collator to be used as an argument for the Trainer:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.train_test_split(test_size=0.2)\n\ntokenized_squad = squad.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=squad[\"train\"].column_names,\n)\n\ndata_collator = DefaultDataCollator()\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDefining Training Arguments\u003c/h2\u003e\n\u003cp\u003eNext, we define the training arguments for our fine-tuning process, including parameters such as batch size, learning rate, and number of epochs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebatch_size = 32\nlr = 4e-5\n\ntraining_args = TrainingArguments(\n    output_dir=f\"qa_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    fp16=True,\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eIntegrating Aim for Experiment Tracking\u003c/h2\u003e\n\u003cp\u003eIntegrating Aim into our experiments allows us to track metadata effectively. We instantiate the AimCallback, specifying the repository or directory for Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:43700\", experiment='squad_huggingface_experiment'\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimply provide the `aim_callback` as a callback in the Trainer arguments, and Aim will handle everything behind the scenes.\u003c/p\u003e\n\u003ch2\u003eInitializing the Trainer\u003c/h2\u003e\n\u003cp\u003eWe then load the pre-trained DistilBERT model and define our Trainer instance, incorporating the training arguments, dataset, tokenizer, data collator, and AimCallback:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_squad[\"train\"],\n    eval_dataset=tokenized_squad[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[aim_callback],\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTraining and Evaluation\u003c/h2\u003e\n\u003cp\u003eWith all components set up, we are now ready to commence the training process followed by evaluating our fine-tuned model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etrainer.train()\ntrainer.evaluate()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy following these steps, we can effectively fine-tune a pre-trained language model for question answering while leveraging Aim for experiment tracking and metadata analysis. This approach enables us to derive meaningful insights and enhance the performance of our models efficiently.\u003c/p\u003e\n\u003ch2\u003eExploring the runs on Aim\u003c/h2\u003e\n\u003cp\u003eAfter completing our fine-tuning pipeline, it's super easy to explore and analyze the runs using Aim. We conducted experiments with two batch sizes (16, 32) and three learning rates (0.004, 0.0004, 0.00004) to compare their performance and derive insights in real-time.\u003c/p\u003e\n\u003cp\u003eTo visualize and interpret the experiment results, we utilize the Aim command-line interface. By running the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can access the Aim dashboard via the provided URL, where we'll encounter a comprehensive overview of our experiments and their corresponding metadata. Let's delve into what we'll observe in the Runs Explorer page:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.34.35%E2%80%AFpm.png\" alt=\"Runs explorer\" title=\"Runs explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eEach run's page provides details on hyperparameters, tracked metrics, system metrics, environment variables, and package versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.35.20%E2%80%AFpm.png\" alt=\"Run overview\" title=\"Run overview\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, you can review the run status in the Logs tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.37.51%E2%80%AFpm.png\" alt=\"Logs tab\" title=\"Logs tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eExperiments Analysis\u003c/h2\u003e\n\u003cp\u003eThe traditional method for comparing machine learning experiments is through observing learning curves.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.40.34%E2%80%AFpm.png\" alt=\"Metrics explorer\" title=\"Metrics explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eWe plot the loss by differentiating curves based on batch size and learning rate combinations. Evaluation subset curves are shown in solid lines, while training curves are in dashed lines.\u003c/p\u003e\n\u003cp\u003eMoreover to track hyperparameters and compare them easily, we can utilize the Params Explorer. This involves observing how loss depends on the evaluation subset in relation to batch size and learning rate.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.38.32%E2%80%AFpm.png\" alt=\"Param explorer\" title=\"Params explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eBased on this analysis, we observe that the best performing hyperparameter combination is batch size: 32 and learning rate: 0.00004.\u003c/p\u003e\n\u003ch2\u003eAnswering questions\u003c/h2\u003e\n\u003cp\u003eNow let‚Äôs use the latest checkpoint of the best model to answer some questions and track the questions and the answers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom transformers import pipeline\n\nmodel_ckpt = \"PATH_TO_BEST_MODEL_CKPT\"\nquestion_answerer = pipeline(\"question-answering\", model=model_ckpt)\n\nquestions = [\n    {\n        \"question\": \"How many countries are members of the United Nations?\",\n        \"context\": \"The United Nations has 195 member countries.\",\n    },\n    {\n        \"question\": \"What is the population of China?\",\n        \"context\": \"China has a population of around 1.4 billion.\",\n    },\n    {\n        \"question\": \"Who is the current president of the United States?\",\n        \"context\": \"As of my last update in 2024, Joe Biden is the current President of the United States, succeeding Donald Trump.\",\n    },\n    {\n        \"question\": \"How many planets are there in the solar system?\",\n        \"context\": \"There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\",\n    },\n    {\n        \"question\": \"What is the capital city of Australia?\",\n        \"context\": \"The capital city of Australia is Canberra.\",\n    },\n]\n\nfor question in questions:\n    output = question_answerer(\n        question=question[\"question\"], context=question[\"context\"]\n    )\n\n    aim_callback.experiment.track(\n        Text(question[\"question\"]),\n        name=\"question\",\n        context={\"context\": question[\"context\"]},\n    )\n\n    aim_callback.experiment.track(\n        Text(output[\"answer\"]),\n        name=\"answer\",\n        context={\"context\": question[\"context\"]},\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo observe the responses we can use the Text Explorer, by grouping by ‚Äútexts.context.context‚Äù in the rows, and ‚Äútexts.name‚Äù in the columns:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-20-at-11.47.57%E2%80%AFpm.png\" alt=\"Text explorer\" title=\"Text explorer\"\u003e\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eTo sum up, this blog post outlined how to fine-tune a DistilBERT model for question answering with the SQuAD dataset, while leveraging Aim for experiment tracking. We started by preprocessing the data and setting up the Trainer with AimCallback integration. Through systematic experimentation, we discovered the best hyperparameter combinations and monitored our model's performance using Aim's user-friendly interface.\u003c/p\u003e\n\u003cp\u003eMoreover, we demonstrated the versatility of exploration beyond mere model training by analyzing text inputs and outputs with Aim. By visualizing learning curves, comparing hyperparameter combinations, and monitoring model responses to questions, we gained valuable insights into our model's behavior.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e¬†üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e¬†if you think Aim is useful. ‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/keep-your-model-up-to-date-for-question-answering.md","_raw":{"sourceFilePath":"posts/keep-your-model-up-to-date-for-question-answering.md","sourceFileName":"keep-your-model-up-to-date-for-question-answering.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/keep-your-model-up-to-date-for-question-answering"},"type":"Post"},{"title":"LangChain + Aim: Building and Debugging AI Systems Made EASY!","date":"2023-04-06T17:51:14.014Z","author":"Gor Arakelyan","description":"As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial. Use Aim to easily trace complex AI systems.","slug":"langchain-aim-building-and-debugging-ai-systems-made-easy","image":"/images/dynamic/langchain_header.jpg","draft":false,"categories":["Integrations"],"body":{"raw":"\n\n# The Rise of Complex AI Systems\n\nWith the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.\n\nBut the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.\n\n**As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.** Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.\n\nIn this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:\n\n* track all inputs and outputs of chains,\n* visualize and explore individual chains,\n* compare several chains side-by-side.\n\n# LangChain: Building AI Systems with LLMs\n\nLangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.\n\nIt provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!\n\n# **Aim:** Upgraded Debugging Experience for AI Systems\n\nMonitoring and debugging AI systems requires more than just scanning output logs on a terminal.\n\n**Introducing Aim!**\n\nAim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.\n\nImportantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.\n\nIn that way, Aim makes debugging, monitoring, comparing different executions a breeze.\n\n**Experience the ultimate control with Aim!**\n\nCheck out Aim on GitHub: **[github.com/aimhubio/aim](http://github.com/aimhubio/aim)**\n\n![](/images/dynamic/explorer.jpg)\n\n# Aim + LangChain = üöÄ\n\nWith the release of LangChain **[v0.0.127](https://github.com/hwchase17/langchain/releases/tag/v0.0.127)**, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! **All you need to do is configure the Aim callback and run your executions as usual.**\n\nAim does the rest for you by tracking tools and LLMs‚Äô inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.\n\n![](/images/dynamic/langchain_aim.jpg)\n\nLet's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.\n\n# Hands-On Example: Building a Multi-Task AI Agent\n\n## Setting up the agent and the Aim callback\n\nLet‚Äôs build an agent equipped with two tools:\n\n* the SerpApi tool to access Google search results,\n* the LLM-math tool to perform required mathematical operations.\n\nIn this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:\n\n```python\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n```\n\nNow that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.\n\n```python\nfrom langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n```\n\n\u003e **Aim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.**\n\nFind the full script and more examples in the official LangChain docs: [](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html)**\u003chttps://python.langchain.com/en/latest/ecosystem/aim_tracking.html\u003e**\n\n## **Executing the agent and running Aim**\n\nBefore executing the agent, ensure that Aim is installed by executing the following command:\n\n```shell\npip install aim\n```\n\nNow, let's run multiple executions and launch the Aim UI to visualize and explore the results:\n\n1. execute the script by running `python example.py`,\n2. then, start the UI with `aim up` command.\n\nWith the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.\n\n## Exploring executions via Aim\n\n### Home page\n\nOn the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.\n\n![](/images/dynamic/home.jpg)\n\n\n\n### Deep dive into a single execution\n\nWhen navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:\n\n* CLI command and arguments,\n* Environment variables,\n* Packages,\n* Git information,\n* System resource usage,\n* and other relevant information about an individual execution.\n\n![](/images/dynamic/individual_exec.jpg)\n\nAim automatically captures terminal outputs during execution. Access these logs in the ‚ÄúLogs‚Äù tab to easily keep track of the progress of your AI system and identify issues.\n\n![](/images/dynamic/logs.jpg)\n\nIn the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.\n\n![](/images/dynamic/text_tab.jpg)\n\nWith Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.\n\n![](/images/dynamic/explorer.jpg)\n\nFor instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.\n\n**With Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.**\n\n# Wrapping Up\n\nIn conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!\n\n# Learn more\n\nCheck out the Aim + LangChain integration docs¬†[here](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html).\n\nLangChain repo:¬†[](https://github.com/hwchase17/langchain)\u003chttps://github.com/hwchase17/langchain\u003e\n\nAim repo:¬†[](https://github.com/aimhubio/aim)\u003chttps://github.com/aimhubio/aim\u003e\n\nIf you have questions, join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\n\nDrop a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003ch1\u003eThe Rise of Complex AI Systems\u003c/h1\u003e\n\u003cp\u003eWith the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.\u003c/p\u003e\n\u003cp\u003eBut the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAs AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.\u003c/strong\u003e Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.\u003c/p\u003e\n\u003cp\u003eIn this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etrack all inputs and outputs of chains,\u003c/li\u003e\n\u003cli\u003evisualize and explore individual chains,\u003c/li\u003e\n\u003cli\u003ecompare several chains side-by-side.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eLangChain: Building AI Systems with LLMs\u003c/h1\u003e\n\u003cp\u003eLangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.\u003c/p\u003e\n\u003cp\u003eIt provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!\u003c/p\u003e\n\u003ch1\u003e\u003cstrong\u003eAim:\u003c/strong\u003e Upgraded Debugging Experience for AI Systems\u003c/h1\u003e\n\u003cp\u003eMonitoring and debugging AI systems requires more than just scanning output logs on a terminal.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroducing Aim!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.\u003c/p\u003e\n\u003cp\u003eImportantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.\u003c/p\u003e\n\u003cp\u003eIn that way, Aim makes debugging, monitoring, comparing different executions a breeze.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExperience the ultimate control with Aim!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCheck out Aim on GitHub: \u003cstrong\u003e\u003ca href=\"http://github.com/aimhubio/aim\"\u003egithub.com/aimhubio/aim\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/explorer.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eAim + LangChain = üöÄ\u003c/h1\u003e\n\u003cp\u003eWith the release of LangChain \u003cstrong\u003e\u003ca href=\"https://github.com/hwchase17/langchain/releases/tag/v0.0.127\"\u003ev0.0.127\u003c/a\u003e\u003c/strong\u003e, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! \u003cstrong\u003eAll you need to do is configure the Aim callback and run your executions as usual.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim does the rest for you by tracking tools and LLMs‚Äô inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/langchain_aim.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.\u003c/p\u003e\n\u003ch1\u003eHands-On Example: Building a Multi-Task AI Agent\u003c/h1\u003e\n\u003ch2\u003eSetting up the agent and the Aim callback\u003c/h2\u003e\n\u003cp\u003eLet‚Äôs build an agent equipped with two tools:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe SerpApi tool to access Google search results,\u003c/li\u003e\n\u003cli\u003ethe LLM-math tool to perform required mathematical operations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003etools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eAim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFind the full script and more examples in the official LangChain docs: \u003ca href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"\u003e\u003c/a\u003e\u003cstrong\u003e\u003ca href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"\u003ehttps://python.langchain.com/en/latest/ecosystem/aim_tracking.html\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eExecuting the agent and running Aim\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eBefore executing the agent, ensure that Aim is installed by executing the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003epip install aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, let's run multiple executions and launch the Aim UI to visualize and explore the results:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eexecute the script by running \u003ccode\u003epython example.py\u003c/code\u003e,\u003c/li\u003e\n\u003cli\u003ethen, start the UI with \u003ccode\u003eaim up\u003c/code\u003e command.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWith the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.\u003c/p\u003e\n\u003ch2\u003eExploring executions via Aim\u003c/h2\u003e\n\u003ch3\u003eHome page\u003c/h3\u003e\n\u003cp\u003eOn the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/home.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eDeep dive into a single execution\u003c/h3\u003e\n\u003cp\u003eWhen navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLI command and arguments,\u003c/li\u003e\n\u003cli\u003eEnvironment variables,\u003c/li\u003e\n\u003cli\u003ePackages,\u003c/li\u003e\n\u003cli\u003eGit information,\u003c/li\u003e\n\u003cli\u003eSystem resource usage,\u003c/li\u003e\n\u003cli\u003eand other relevant information about an individual execution.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/individual_exec.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim automatically captures terminal outputs during execution. Access these logs in the ‚ÄúLogs‚Äù tab to easily keep track of the progress of your AI system and identify issues.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/logs.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/text_tab.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWith Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/explorer.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFor instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eWrapping Up\u003c/h1\u003e\n\u003cp\u003eIn conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eCheck out the Aim + LangChain integration docs¬†\u003ca href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eLangChain repo:¬†\u003ca href=\"https://github.com/hwchase17/langchain\"\u003e\u003c/a\u003e\u003ca href=\"https://github.com/hwchase17/langchain\"\u003ehttps://github.com/hwchase17/langchain\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAim repo:¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003e\u003c/a\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have questions, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\u003c/p\u003e\n\u003cp\u003eDrop a ‚≠êÔ∏è on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","_raw":{"sourceFilePath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileName":"langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy"},"type":"Post"},{"title":"Launching Aim on Hugging Face Spaces","date":"2023-04-19T13:41:34.213Z","author":"Gor Arakelyan","description":"Deploy Aim on Hugging Face Spaces using the Docker template. Aim empowers you to explore logs with interactive visualizations, easily compare training runs at scale and be on top of ML development insights.","slug":"launching-aim-on-hugging-face-spaces","image":"/images/dynamic/hf_space_header.png","draft":false,"categories":["Integrations"],"body":{"raw":"We are excited to announce the launch of Aim on Hugging Face Spaces! üöÄ\n\nWith just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.\n\n![](/images/dynamic/hf_space_me.png)\n\nAim is an open-source, self-hosted AI Metadata tracking tool.\\\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata ‚Äî perfect for automations and Jupyter Notebook analysis.\n\nIn this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Let‚Äôs dive in and get started!\n\nLearn more about Aim on the GitHub repository: **[github.com/aimhubio/aim](https://github.com/aimhubio/aim)**\n\n## Deploy Aim on Hugging Face Spaces within seconds using the Docker template\n\nTo get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the ‚ÄúCreate new Space‚Äù button, or open the page directly by the following link: [](https://huggingface.co/new-space?template=aimstack/aim)**\u003chttps://huggingface.co/new-space?template=aimstack/aim\u003e**\n\n![](/images/dynamic/hf_space_deploy.png)\n\nSet up your Aim Space in no time:\n\n1. Choose a name for your Space.\n2. Adjust Space hardware and the visibility mode.\n3. Submit your Space!\n\nAfter submitting the Space, you'll be able to monitor its progress through the building status:\n\n![](/images/dynamic/hf_space_building.png)\n\nOnce it transitions to ‚ÄúRunning‚Äù, your space is ready to go!\n\n![](/images/dynamic/hf_space_running.png)\n\n**Ta-da! üéâ You're all set to start using Aim on Hugging Face.**\n\nBy pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:\n\n1. Run the following bash command to compress `.aim` directory:\n\n   ```shell\n   tar -czvf aim_repo.tar.gz .aim\n   ```\n2. Commit and push files to your Space.\n\nThat's it! Now open the App section of your Space, and Aim will display your training logs.\n\nUpdating Spaces is incredibly convenient ‚Äì you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. üî•\n\n## See Aim in Action with Existing Demos on the Hub\n\nLet‚Äôs explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.\n\n* Neural machine translation task: [](https://huggingface.co/spaces/aimstack/nmt)\u003chttps://huggingface.co/spaces/aimstack/nmt\u003e\n* Simple handwritten digits recognition task: [](https://huggingface.co/spaces/aimstack/digit-recognition)\u003chttps://huggingface.co/spaces/aimstack/digit-recognition\u003e\n* Image generation task with lightweight GAN implementation: [](https://huggingface.co/spaces/aimstack/image-generation)\u003chttps://huggingface.co/spaces/aimstack/image-generation\u003e\n\n![](/images/dynamic/hf_space_demos.png)\n\nWhen navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.\n\n![Aim homepage](/images/dynamic/hf_space_overview.png \"Aim homepage\")\n\nOpen the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.\n\n![Individual run page](/images/dynamic/hf_space_individual_run.png \"Individual run page\")\n\nTake your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.\n\nMetrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.\n\n![Metrics Explorer](/images/dynamic/hf_space_me.png \"Metrics Explorer\")\n\nExplorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.\n\n![Pythonic search](/images/dynamic/hf_space_pythonic_search.png \"Pythonic search\")\n\nIn addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.\n\n![Images Explorer](/images/dynamic/hf_space_media.png \"Images Explorer\")\n\nUse Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!\n\n## One more thing‚Ä¶ üëÄ\n\nHaving Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.\n\nTo embed your Space, construct the following link based on Space owner and Space name: **`https://owner-space-name.hf.space`**. This link can be used to embed your Space in any website or notebook using the following HTML code:\n\n```html\n%%html\n\u003ciframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n\u003e\n\u003c/iframe\u003e\n```\n\n## Next steps\n\nWe are going to continuously iterate over Aim Space onboarding and usability, including:\n\n* the ability to read logs directly from Hugging Face Hub model repos,\n* automatic conversion of TensorBoard logs to Aim format,\n* Aim HF Space-specific onboarding steps.\n\nMuch more coming soon... stay tuned for the updates!\n\n## Learn more\n\nCheck out Aim Space documentation [here](https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html)\n\nAim repo on GitHub: [github.com/aimhubio/aim](http://github.com/aimhubio/aim)\n\nIf you have questions, join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\n\nDrop a ‚≠êÔ∏è on¬†[GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003cp\u003eWe are excited to announce the launch of Aim on Hugging Face Spaces! üöÄ\u003c/p\u003e\n\u003cp\u003eWith just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_me.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim is an open-source, self-hosted AI Metadata tracking tool.\u003cbr\u003e\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata ‚Äî perfect for automations and Jupyter Notebook analysis.\u003c/p\u003e\n\u003cp\u003eIn this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Let‚Äôs dive in and get started!\u003c/p\u003e\n\u003cp\u003eLearn more about Aim on the GitHub repository: \u003cstrong\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003egithub.com/aimhubio/aim\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003eDeploy Aim on Hugging Face Spaces within seconds using the Docker template\u003c/h2\u003e\n\u003cp\u003eTo get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the ‚ÄúCreate new Space‚Äù button, or open the page directly by the following link: \u003ca href=\"https://huggingface.co/new-space?template=aimstack/aim\"\u003e\u003c/a\u003e\u003cstrong\u003e\u003ca href=\"https://huggingface.co/new-space?template=aimstack/aim\"\u003ehttps://huggingface.co/new-space?template=aimstack/aim\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_deploy.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSet up your Aim Space in no time:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChoose a name for your Space.\u003c/li\u003e\n\u003cli\u003eAdjust Space hardware and the visibility mode.\u003c/li\u003e\n\u003cli\u003eSubmit your Space!\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAfter submitting the Space, you'll be able to monitor its progress through the building status:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_building.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOnce it transitions to ‚ÄúRunning‚Äù, your space is ready to go!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_running.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTa-da! üéâ You're all set to start using Aim on Hugging Face.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBy pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRun the following bash command to compress \u003ccode\u003e.aim\u003c/code\u003e directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003etar -czvf aim_repo.tar.gz .aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCommit and push files to your Space.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThat's it! Now open the App section of your Space, and Aim will display your training logs.\u003c/p\u003e\n\u003cp\u003eUpdating Spaces is incredibly convenient ‚Äì you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. üî•\u003c/p\u003e\n\u003ch2\u003eSee Aim in Action with Existing Demos on the Hub\u003c/h2\u003e\n\u003cp\u003eLet‚Äôs explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeural machine translation task: \u003ca href=\"https://huggingface.co/spaces/aimstack/nmt\"\u003e\u003c/a\u003e\u003ca href=\"https://huggingface.co/spaces/aimstack/nmt\"\u003ehttps://huggingface.co/spaces/aimstack/nmt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSimple handwritten digits recognition task: \u003ca href=\"https://huggingface.co/spaces/aimstack/digit-recognition\"\u003e\u003c/a\u003e\u003ca href=\"https://huggingface.co/spaces/aimstack/digit-recognition\"\u003ehttps://huggingface.co/spaces/aimstack/digit-recognition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eImage generation task with lightweight GAN implementation: \u003ca href=\"https://huggingface.co/spaces/aimstack/image-generation\"\u003e\u003c/a\u003e\u003ca href=\"https://huggingface.co/spaces/aimstack/image-generation\"\u003ehttps://huggingface.co/spaces/aimstack/image-generation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_demos.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_overview.png\" alt=\"Aim homepage\" title=\"Aim homepage\"\u003e\u003c/p\u003e\n\u003cp\u003eOpen the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_individual_run.png\" alt=\"Individual run page\" title=\"Individual run page\"\u003e\u003c/p\u003e\n\u003cp\u003eTake your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.\u003c/p\u003e\n\u003cp\u003eMetrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_me.png\" alt=\"Metrics Explorer\" title=\"Metrics Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eExplorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_pythonic_search.png\" alt=\"Pythonic search\" title=\"Pythonic search\"\u003e\u003c/p\u003e\n\u003cp\u003eIn addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_media.png\" alt=\"Images Explorer\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eUse Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!\u003c/p\u003e\n\u003ch2\u003eOne more thing‚Ä¶ üëÄ\u003c/h2\u003e\n\u003cp\u003eHaving Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.\u003c/p\u003e\n\u003cp\u003eTo embed your Space, construct the following link based on Space owner and Space name: \u003cstrong\u003e\u003ccode\u003ehttps://owner-space-name.hf.space\u003c/code\u003e\u003c/strong\u003e. This link can be used to embed your Space in any website or notebook using the following HTML code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-html\"\u003e%%html\n\u0026#x3C;iframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n\u003e\n\u0026#x3C;/iframe\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eNext steps\u003c/h2\u003e\n\u003cp\u003eWe are going to continuously iterate over Aim Space onboarding and usability, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe ability to read logs directly from Hugging Face Hub model repos,\u003c/li\u003e\n\u003cli\u003eautomatic conversion of TensorBoard logs to Aim format,\u003c/li\u003e\n\u003cli\u003eAim HF Space-specific onboarding steps.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMuch more coming soon... stay tuned for the updates!\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003eCheck out Aim Space documentation \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAim repo on GitHub: \u003ca href=\"http://github.com/aimhubio/aim\"\u003egithub.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have questions, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. You‚Äôre most welcome! üôå\u003c/p\u003e\n\u003cp\u003eDrop a ‚≠êÔ∏è on¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/launching-aim-on-hugging-face-spaces.md","_raw":{"sourceFilePath":"posts/launching-aim-on-hugging-face-spaces.md","sourceFileName":"launching-aim-on-hugging-face-spaces.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/launching-aim-on-hugging-face-spaces"},"type":"Post"},{"title":"Reproducible forecasting with Prophet and Aim","date":"2023-03-14T07:19:42.802Z","author":"Davit Grigoryan","description":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. ","slug":"reproducible-forecasting-with-prophet-and-aim","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaIDQvLbqy0KyVG22D0epw.png","draft":false,"categories":["Tutorials"],"body":{"raw":"You can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.\n\n# What is Aim?\n\n[Aim](https://aimstack.io/)¬†is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n# What is Prophet?\n\n[Prophet](https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/)¬†is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.\n\nA key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.\n\n# Tracking Prophet experiments with Aim\n\nProphet isn‚Äôt trained like neural networks, so you can‚Äôt track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost we‚Äôre going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check out¬†[the repo](https://github.com/aimhubio/aim/tree/main/examples).\n\nIn the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.\n\n```\nfrom aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n```\n\nAdditionally, if you‚Äôre working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Here‚Äôs a simple code snippet doing just that:\n\n```\n# Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n```\n\nNow, the extra feature(s) will be tracked as a hyperparameter called¬†**extra_regressors**.\n\nTake a look at a simple, end-to-end example¬†[here](https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py).\n\n# Viewing experiment logs\n\nAfter running the experiment, we can view the logs by executing¬†`aim up`¬†from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png)\n\nAdditionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif)\n\nThese features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.\n\n# Using Aim‚Äôs Pythonic search to filter metrics and hyperparameters\n\n\n\nGiven the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if we‚Äôre working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aim‚Äôs¬†*pythonic search*¬†functionality can be super useful. Say we only want to see the models with MAE ‚â§ 0.25. We can go to the metrics section and search exactly like we would in Python, say¬†`((metric.name == \"mae\") and (metric.last \u003c= 0.25))`¬†(the¬†**last**¬†part is meant for neural networks, where you might want to see the metric at the last epoch). Here‚Äôs a visual demonstration of this feature:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png)\n\nAs you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.\n\n# Conclusion\n\n\n\nTo sum up, Aim‚Äôs integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with its¬†*pythonic search*¬†functionality. All these features can make forecasting with Prophet a breeze!\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå\n\nTry out¬†[Aim](https://github.com/aimhubio/aim), join the¬†[Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs.","html":"\u003cp\u003eYou can now track your Prophet experiments with Aim! The recent Aim v3.16 release includes a built-in logger object for Prophet runs. It tracks Prophet hyperparameters, arbitrary user-defined metrics, extra feature variables, and system metrics. These features, along with the intuitive Aim UI and its pythonic search functionality can significantly improve your Prophet workflow and accelerate the model training and evaluation process.\u003c/p\u003e\n\u003ch1\u003eWhat is Aim?\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e¬†is the fastest open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. Aim is used to deeply inspect thousands of hyperparameter-sensitive training runs.\u003c/p\u003e\n\u003ch1\u003eWhat is Prophet?\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/\"\u003eProphet\u003c/a\u003e¬†is a time series forecasting algorithm developed by Meta. Its official implementation is open-sourced, with libraries for Python and R.\u003c/p\u003e\n\u003cp\u003eA key benefit of Prophet is that it does not assume stationarity and can automatically find trend changepoints and seasonality. This makes it easy to apply to arbitrary forecasting problems without many assumptions about the data.\u003c/p\u003e\n\u003ch1\u003eTracking Prophet experiments with Aim\u003c/h1\u003e\n\u003cp\u003eProphet isn‚Äôt trained like neural networks, so you can‚Äôt track per-epoch metrics or anything of the sort. However, Aim allows the user to track Prophet hyperparameters and arbitrary user-defined metrics to compare performance across models, as well as system-level metrics like CPU usage. In this blogpost we‚Äôre going to see how to integrate Aim with your Prophet experiments with minimal effort. For many more end-to-end examples of usage with other frameworks, check out¬†\u003ca href=\"https://github.com/aimhubio/aim/tree/main/examples\"\u003ethe repo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn the simple example below, we generate synthetic time series data in the format required by Prophet, then train and test a single model with default hyperparameters, tracking said hyperparameters using an AimLogger object. Then, we calculate MSE and MAE and add those metrics to the AimLogger. As you can see, this snippet can easily be extended to work with hyperparameter search workflows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.from aim.prophet import AimLogger\n...\n\nmodel = Prophet()\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_test\")\n...\n\nmetrics = {\n    \"mse\": mean_squared_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n    \"mae\": mean_absolute_error(test[\"y\"], preds.iloc[4000:][\"yhat\"]),\n}\nlogger.track_metrics(metrics)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdditionally, if you‚Äôre working with multivariate time series data, you can use Aim to track different dependent variable configurations to see which combination results in the best performance (however, be mindful of the fact that you need to know the future values of your features to forecast your target variable). Here‚Äôs a simple code snippet doing just that:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Here, we add an extra variable to our dataset\ndata = pd.DataFrame(\n    {\n   \"y\": np.random.rand(num_days, 1),\n   \"ds\": rng,\n   \"some_feature\": np.random.randint(10, 20, num_days),\n  }\n)\n\nmodel = Prophet()\n# Prophet won't use the \"some_feature\" variable without the following line\nmodel.add_regressor(\"some_feature\")\nlogger = AimLogger(prophet_model=model, repo=\".\", experiment=\"prophet_with_some_feature\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, the extra feature(s) will be tracked as a hyperparameter called¬†\u003cstrong\u003eextra_regressors\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTake a look at a simple, end-to-end example¬†\u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/prophet_track.py\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eViewing experiment logs\u003c/h1\u003e\n\u003cp\u003eAfter running the experiment, we can view the logs by executing¬†\u003ccode\u003eaim up\u003c/code\u003e¬†from the command line in the aim_logs directory. When the UI is opened, we can see the logs of all the experiments with their corresponding metrics and hyperparameters by navigating to prophet_test/runs and selecting the desired run.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aw3FjV81meA0C_UbMG7bnQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, we can monitor system metrics, environment variables, and packages installed in the virtual environment, among other things.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Syw9V1IQLf78fAqp-HYXRA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThese features make it easy to track and compare different Prophet experiments on an arbitrary number of metrics, both accuracy and performance-related.\u003c/p\u003e\n\u003ch1\u003eUsing Aim‚Äôs Pythonic search to filter metrics and hyperparameters\u003c/h1\u003e\n\u003cp\u003eGiven the same dataset and the same hyperparameters, Prophet is guaranteed to produce the same exact model, which means the metrics will be the same. However, if we‚Äôre working with several time series (e.g. forecasting demand using different factors), we might want to fit many different Prophet models to see which factors have a bigger effect on the target variable. Similarly, we might want to filter experiments by hyperparameter values. In cases like these, Aim‚Äôs¬†\u003cem\u003epythonic search\u003c/em\u003e¬†functionality can be super useful. Say we only want to see the models with MAE ‚â§ 0.25. We can go to the metrics section and search exactly like we would in Python, say¬†\u003ccode\u003e((metric.name == \"mae\") and (metric.last \u0026#x3C;= 0.25))\u003c/code\u003e¬†(the¬†\u003cstrong\u003elast\u003c/strong\u003e¬†part is meant for neural networks, where you might want to see the metric at the last epoch). Here‚Äôs a visual demonstration of this feature:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6tJtlwtY4v7RHXf5E_mBg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhsN41wOUX-giiwhJXg6NA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, filtering based on the metrics is super easy and convenient. The pythonic search functionality can also be used to filter based on other parameters.\u003c/p\u003e\n\u003ch1\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eTo sum up, Aim‚Äôs integration with Prophet allows one to easily track an arbitrary amount of Prophet runs with different hyperparameters and feature variable configurations, as well as arbitrary user-defined metrics, while also allowing one to monitor system performance and see how much resources model training consumes. Aim UI also makes it easy to filter runs based on hyperparameter and metric values with its¬†\u003cem\u003epythonic search\u003c/em\u003e¬†functionality. All these features can make forecasting with Prophet a breeze!\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It‚Äôs humbling and inspiring üôå\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the¬†\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs.\u003c/p\u003e"},"_id":"posts/reproducible-forecasting-with-prophet-and-aim.md","_raw":{"sourceFilePath":"posts/reproducible-forecasting-with-prophet-and-aim.md","sourceFileName":"reproducible-forecasting-with-prophet-and-aim.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/reproducible-forecasting-with-prophet-and-aim"},"type":"Post"},{"title":"Streamlining your Hugging Face Experiments with AimOS","date":"2023-12-29T13:33:30.989Z","author":"Hovhannes Tamoyan","description":"AimCallback¬†for¬†Hugging Face¬†is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy.","slug":"streamlining-your-hugging-face-experiments-with-aimos","image":"/images/dynamic/hugging-face-1-.png","draft":false,"categories":["Integrations"],"body":{"raw":"AimCallback¬†for¬†[Hugging Face](https://huggingface.co/)¬†is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy. Moreover, it offers comprehensive system usage tracking, keeping an eye on CPU and GPU memory utilization.\n\n## Introduction\n\nDiscover how [AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems) effortlessly monitors your Hugging Face experiments, capturing every detail of hyperparameters and metrics. We'll dive into a sample script to demonstrate the powerful integration between AimOS and Hugging Face.\n\n## Running a Sample Training with Hugging Face: A Step-by-Step Guide\n\nExplore a hands-on example using the `yelp_review_full` dataset consisting of Yelp reviews, with the review scores ranging from 1 to 5 to full-tune a BERT base model for sequence classification.\n\n\n\n## Setting the Stage\n\nBefore diving into the script, ensure that AimOS is installed. If not, simply run ‚Äúpip install aimos‚Äù, and head over to the [GitHub page](https://github.com/aimhubio/aimos) for more.\n\n## Sample Script\n\n\n\n### 1. Importing Required Modules\n\nBegin by importing the necessary modules.\n\n```\nimport evaluate\nimport numpy as np\nfrom aimstack.experiment_tracker.hugging_face import Callback as AimCallback\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n)\n```\n\n### 2. Preparing the Dataset and the Tokenizer\n\nDefine a function for tokenization.\n\n```\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n```\n\nTokenize the dataset using the BERT base-cased tokenizer.\n\n```\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\nSelect a small subset of the dataset for training and evaluation.\n\n```\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\nDefine the metrics for evaluation.\n\n```\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nmetric = evaluate.load(\"accuracy\")\n```\n\n### 3. Instantiate the Model\n\nInstantiate the BERT base-cased model for sequence classification.\n\n```\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-cased\", num_labels=5\n)\n```\n\n### 4. Initialize AimOS Callback\n\nInitialize the AimOS callback by providing the repository address and experiment name. Here the `aim://0.0.0.0:53800` is the default address of your aimos server if it‚Äôs running on your local machine:\n\n```\naim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:53800\", experiment_name=\"yelp_classification\"\n)\n```\n\nEnsure that your AimOS server is active on the default `53800` port. Start the AimOS server with the following command:\n\n```\naimos server\n```\n\n### 5. Training\n\nSet the training arguments, specifying the output directory, number of epochs, and learning rate. Train the model by providing the AimOS callback in the Trainer arguments.\n\n```\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=10, learning_rate=5e-7)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[aim_callback],\n)\n\ntrainer.train()\n```\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the .aim repository is initialized and execute:\n\n```\naimos ui\n```\n\nThis is the view you'll see after opening the provided URL: \n\n![AimOS Apps](/images/dynamic/seamless-experimentation-screenshot-dec-13.png \"AimOS Apps\")\n\n## Interpreting Experiment Results\n\nExplore the wealth of information provided by AimOS to gain insights into your Hugging Face experiment. We'll guide you through the key aspects to understand experiment's effectiveness.\n\n\n\n## AimOS Overview Page\n\nHead over to the AimOS Overview page to witness the containers and sequences overview tables of your Hugging Face experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\n\n![Overview](/images/dynamic/seam.png \"Overview\")\n\n## AimOS Runs Page\n\nVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\n\n![Runs page](/images/dynamic/seamless-experimentation-screenshot-dec-13-1-.png \"Runs page\")\n\nBy navigating to each run‚Äôs individual page, you'll have a detailed view of the activities within that run.\n\n## Run Overview\n\n\n\nNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\n\n![Run overview](/images/dynamic/seamless-experimentation-screenshot-dec-13-2-.png \"Run overview\")\n\n## Params Tab\n\nFor a more detailed view of all parameters, navigate to the Params tab.\n\n![Params tab](/images/dynamic/seamless-experimentation-screenshot-dec-13-3-.png \"Params tab\")\n\n## Metrics Tab\n\nIn the Metrics tab, explore all the tracked metrics graphs, including the change of learning_rate, loss, samples_per_second, steps_per_second, total_flos, and steps.\n\n![Metrics tab](/images/dynamic/seamless-experimentation-screenshot-dec-13-4-.png \"Metrics tab \")\n\nIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\n\n## Wrapping up\n\nAimOS provides a comprehensive suite of tools to analyze and understand your Hugging Face experiment thoroughly. By leveraging these insights, you can elevate your experimentation process and make data-driven decisions. \n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on [AimStack blog.](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\n## Learn more\n\n[](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\n[AimOS is on a mission to democratize AI Systems logging tools.¬†üôå](https://aimos.readthedocs.io/en/latest/apps/overview.html)[](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\nTry out¬†[AimOS](https://github.com/aimhubio/aimos), join the [Aim community](https://community.aimstack.io/)¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon‚Äôt forget to leave us a star on¬†[GitHub](https://github.com/aimhubio/aimos)¬†if you think AimOS is useful, and here is the repository of¬†[Aim](https://github.com/aimhubio/aim),¬†an easy-to-use \u0026 supercharged open-source experiment tracker.‚≠êÔ∏è","html":"\u003cp\u003eAimCallback¬†for¬†\u003ca href=\"https://huggingface.co/\"\u003eHugging Face\u003c/a\u003e¬†is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy. Moreover, it offers comprehensive system usage tracking, keeping an eye on CPU and GPU memory utilization.\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eDiscover how \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e effortlessly monitors your Hugging Face experiments, capturing every detail of hyperparameters and metrics. We'll dive into a sample script to demonstrate the powerful integration between AimOS and Hugging Face.\u003c/p\u003e\n\u003ch2\u003eRunning a Sample Training with Hugging Face: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eExplore a hands-on example using the \u003ccode\u003eyelp_review_full\u003c/code\u003e dataset consisting of Yelp reviews, with the review scores ranging from 1 to 5 to full-tune a BERT base model for sequence classification.\u003c/p\u003e\n\u003ch2\u003eSetting the Stage\u003c/h2\u003e\n\u003cp\u003eBefore diving into the script, ensure that AimOS is installed. If not, simply run ‚Äúpip install aimos‚Äù, and head over to the \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eGitHub page\u003c/a\u003e for more.\u003c/p\u003e\n\u003ch2\u003eSample Script\u003c/h2\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eBegin by importing the necessary modules.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport evaluate\nimport numpy as np\nfrom aimstack.experiment_tracker.hugging_face import Callback as AimCallback\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Preparing the Dataset and the Tokenizer\u003c/h3\u003e\n\u003cp\u003eDefine a function for tokenization.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTokenize the dataset using the BERT base-cased tokenizer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSelect a small subset of the dataset for training and evaluation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDefine the metrics for evaluation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nmetric = evaluate.load(\"accuracy\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Instantiate the Model\u003c/h3\u003e\n\u003cp\u003eInstantiate the BERT base-cased model for sequence classification.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-cased\", num_labels=5\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4. Initialize AimOS Callback\u003c/h3\u003e\n\u003cp\u003eInitialize the AimOS callback by providing the repository address and experiment name. Here the \u003ccode\u003eaim://0.0.0.0:53800\u003c/code\u003e is the default address of your aimos server if it‚Äôs running on your local machine:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:53800\", experiment_name=\"yelp_classification\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsure that your AimOS server is active on the default \u003ccode\u003e53800\u003c/code\u003e port. Start the AimOS server with the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Training\u003c/h3\u003e\n\u003cp\u003eSet the training arguments, specifying the output directory, number of epochs, and learning rate. Train the model by providing the AimOS callback in the Trainer arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etraining_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=10, learning_rate=5e-7)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[aim_callback],\n)\n\ntrainer.train()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the .aim repository is initialized and execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the view you'll see after opening the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13.png\" alt=\"AimOS Apps\" title=\"AimOS Apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting Experiment Results\u003c/h2\u003e\n\u003cp\u003eExplore the wealth of information provided by AimOS to gain insights into your Hugging Face experiment. We'll guide you through the key aspects to understand experiment's effectiveness.\u003c/p\u003e\n\u003ch2\u003eAimOS Overview Page\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Overview page to witness the containers and sequences overview tables of your Hugging Face experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seam.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eAimOS Runs Page\u003c/h2\u003e\n\u003cp\u003eVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-1-.png\" alt=\"Runs page\" title=\"Runs page\"\u003e\u003c/p\u003e\n\u003cp\u003eBy navigating to each run‚Äôs individual page, you'll have a detailed view of the activities within that run.\u003c/p\u003e\n\u003ch2\u003eRun Overview\u003c/h2\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-2-.png\" alt=\"Run overview\" title=\"Run overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eParams Tab\u003c/h2\u003e\n\u003cp\u003eFor a more detailed view of all parameters, navigate to the Params tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-3-.png\" alt=\"Params tab\" title=\"Params tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eMetrics Tab\u003c/h2\u003e\n\u003cp\u003eIn the Metrics tab, explore all the tracked metrics graphs, including the change of learning_rate, loss, samples_per_second, steps_per_second, total_flos, and steps.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-4-.png\" alt=\"Metrics tab\" title=\"Metrics tab \"\u003e\u003c/p\u003e\n\u003cp\u003eIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eAimOS provides a comprehensive suite of tools to analyze and understand your Hugging Face experiment thoroughly. By leveraging these insights, you can elevate your experimentation process and make data-driven decisions.\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools.¬†üôå\u003c/a\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTry out¬†\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e¬†and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon‚Äôt forget to leave us a star on¬†\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eGitHub\u003c/a\u003e¬†if you think AimOS is useful, and here is the repository of¬†\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e,¬†an easy-to-use \u0026#x26; supercharged open-source experiment tracker.‚≠êÔ∏è\u003c/p\u003e"},"_id":"posts/streamlining-your-hugging-face-experiments-with-aimos.md","_raw":{"sourceFilePath":"posts/streamlining-your-hugging-face-experiments-with-aimos.md","sourceFileName":"streamlining-your-hugging-face-experiments-with-aimos.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/streamlining-your-hugging-face-experiments-with-aimos"},"type":"Post"},{"title":"Training Models to Predict Heart Attack","date":"2024-02-14T16:36:40.128Z","author":"Hovhannes Tamoyan","description":"Explore heart attack prediction using machine learning.This article breaks down key factors and insights from the Heart Attack Prediction dataset. With the help of Aim we keep track of model performance.","slug":"training-models-to-predict-heart-attack","image":"/images/dynamic/medium.png","draft":false,"categories":["Tutorials"],"body":{"raw":"In today's blog post, we'll explore heart attack prediction using the Heart Attack Prediction dataset. Our aim is to understand the key factors behind heart attacks with Machine Learning algorithms. We're going also to use the Aim open-source experiment tracking tool, renowned for its great UI and lightning-fast insights.\n\nLet's dive straight into the action by setting up our environment and importing the essential libraries:\n\n```\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom aim import Image, Run\n```\n\nLet's dive into the dataset. We'll start by loading it to see the main features:\n\n```\ndf = pd.read_csv(\"heart.csv\")\ndf.head()\n```\n\nThis dataset gives us key information on heart health, with the \"target\" column indicating a heart attack occurrence (1) or absence (0). Here are the dataset's columns briefly:\n\n* age: Age of the individual\n* sex: Gender of the individual (1 for male, 0 for female)\n* cp: Chest pain type\n* trestbps: Resting blood pressure\n* chol: Serum cholesterol level\n* fbs: Fasting blood sugar level\n* restecg: Resting electrocardiographic results\n* thalach: Maximum heart rate achieved\n* exang: Exercise-induced angina (1 for yes, 0 for no)\n* oldpeak: ST depression induced by exercise relative to rest\n* slope: Slope of the peak exercise ST segment\n* ca: Number of major vessels colored by fluoroscopy\n* thal: Thalassemia\n* target: Level of heart attack (0.1 for lesser and 0.9 for stronger)\n\n\n\nNow, armed with this understanding, let's kickstart our Aim run to track our discoveries:\n\n```\nrun = Run()\n```\n\nIt's as simple as that! Let's begin by logging some key insights about our dataset into our Aim run:\n\n```\nrun[\"dataset_info\"] = {\n    \"name\": \"Heart Attack Prediction\",\n    \"features\": df.describe().to_dict(),\n}\n\n```\n\nEasy, isn't it? But as they say, simplicity often conceals complexity, and we're just scratching the surface of our exploration.\n\nTo observe these and the future metadata that we will track, let‚Äôs execute the following command:\n\n```\naim up\n```\n\nThe resulting view will provide us with:\n\n![Overview](/images/dynamic/untitled-2-.png \"Overview\")\n\nAnd by navigating to the Runs page we will see our newly created single run.\n\n![Runs Explorer](/images/dynamic/untitled-3-.png \"Runs Explorer\")\n\nClick on the run name/hash to see the details of this run:\n\n![Run: Overview](/images/dynamic/untitled-4-.png \"Run: Overview\")\n\n## Exploratory Data Analysis\n\nUpon a preliminary examination of the data, let's delve into our first qualitative analysis to explore the correlations among various metrics:\n\n```\ncorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5)\nplt.title(\"Correlation Matrix Heatmap\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Correlation Matrix heatmap\")\n```\n\nWe've generated a correlation matrix heatmap representing the correlation coefficients between different features in the heart attack dataset. Each cell contains the correlation coefficient between the corresponding row and column variables. Here are some key observations:\n\n* **Positive Correlations:**\n\n  * cp (Chest Pain Type) correlates positively with thalach (Maximum heart rate achieved) and slope.\n  * thalach correlates positively with slope.\n* **Negative Correlations:**\n\n  * age correlates negatively with thalach.\n  * sex correlates negatively with the target variable.\n* **Strong Correlations:** \n\n  * The absolute correlation coefficient between cp and the target is relatively high (0.42), indicating a moderate positive correlation.\n  * The absolute correlation coefficient between thalach and the target is also relatively high (0.40).\n* **Other Observations:**\n\n  * exang (Exercise induced angina) exhibits a strong negative correlation with thalach.\n  * oldpeak (ST depression induced by exercise relative to rest) correlates negatively with thalach.\n\n\n\nCorrelation coefficients range from -1 to 1, where 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\n\n![Correlation Matrix heatmap ](/images/dynamic/untitled-5-.png \"Correlation Matrix heatmap\")\n\nNext, let's visualize the distribution of the target variable and track it in Aim:\n\nIt would also be interesting to see how is the target variable distributed and again track it in Aim:\n\n```\nfig = plt.figure(figsize=(8, 6))\nsns.histplot(df[\"target\"], kde=True, color=\"skyblue\", bins=20)\nplt.title(\"Distribution of Target Variable\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Frequency\")\n\naim_img = Image(fig)\nrun.track(aim_img, name=\"Distribution of Target Variable\")\n```\n\n![Distribution of Target Variable](/images/dynamic/untitled-6-.png \"Distribution of Target Variable\")\n\nAlso let‚Äôs plot the distribution of the features such as age, trestbps, chol, thalach and oldpeak that are gonna help us predict the target value:\n\n```\nnumerical_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n\nfor i, feature in enumerate(numerical_features, 1):\n    fig = plt.figure(figsize=(10, 8))\n    sns.boxplot(x=df[feature], color=\"skyblue\")\n    aim_img = Image(fig)\n    run.track(aim_img, name=f\"Box Plots of {feature}\")\n```\n\nThe centre of distribution chol \u0026 oldpeak is positively skewed, because the whisker and half-box are longer on the right side of the median than on the left side. Distribution of trestbps is approximately symmetric, because both half-boxes are almost the same length. It‚Äôs the most concentrated distribution because the interquartile range. The centre of distribution thalach \u0026 age is negatively skewed because the whisker and half-box are longer on the left side of the median than on the right side.\n\n**To check out the boxplots of each feature we can use the Images Explorer:**\n\n![Images Explorer](/images/dynamic/screenshot-2024-02-13-at-4.36.58‚ÄØpm.png \"Images Explorer\")\n\nNow let‚Äôs normalize our features and check out their importance and track everything on Aim, starting with Impact of Age Categories on Heart Attack Risk:\n\n```\ndf[\"target\"] = df[\"target\"].apply(lambda x: 1 if x \u003e= 0.5 else 0)\n\ndf[\"age_category\"] = pd.cut(df[\"age\"], bins=[29, 45, 60, 100], labels=[\"Young\", \"Middle-aged\", \"Senior\"], right=False,)\nfig = plt.figure(figsize=(10, 6))\nsns.countplot(x=\"age_category\", hue=\"target\", data=df, palette=\"viridis\")\nplt.title(\"Impact of Age Categories on Heart Attack Risk\")\nplt.xlabel(\"Age Category\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Age Categories on Heart Attack Risk\")\n```\n\n![Impact of Age Categories on Heart Attack Risk](/images/dynamic/untitled-7-.png \"Impact of Age Categories on Heart Attack Risk\")\n\nUnderstanding the combined effect of blood pressure (trestbps) and cholesterol (chol) levels on heart attack risk is crucial. We computed the total cardiovascular risk by summing these two variables. The box plot below illustrates the distribution of total cardiovascular risk among individuals with and without heart attacks:\n\n```\ndf[\"total_risk\"] = df[\"trestbps\"] + df[\"chol\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"total_risk\", data=df)\nplt.title(\"Impact of Total Cardiovascular Risk on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Total Cardiovascular Risk on Heart Attack\")\n\n```\n\n![Impact of Total Cardiovascular Risk on Heart Attack](/images/dynamic/untitled-8-.png \"Impact of Total Cardiovascular Risk on Heart Attack\")\n\nExercise-induced angina, characterized by chest pain during physical exertion, could be a significant indicator of heart health. In this visualization, we explore how the presence of exercise-induced angina, coupled with maximum heart rate achieved (thalach), correlates with the occurrence of heart attacks:\n\n```\ndf[\"exercise_angina\"] = (df[\"exang\"] == 1) \u0026 (df[\"thalach\"] \u003e 150)\nfig = plt.figure(figsize=(10, 8))\nsns.countplot(x=\"exercise_angina\", hue=\"target\", data=df)\nplt.title(\"Impact of Exercise-induced Angina on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-induced Angina on Heart Attack\")\n\n```\n\n![Impact of Exercise-induced Angina on Heart Attack](/images/dynamic/untitled-9-.png \"Impact of Exercise-induced Angina on Heart Attack\")\n\nThe ratio of total cholesterol (chol) to high-density lipoprotein (HDL) cholesterol is a critical marker of cardiovascular health. A higher ratio may indicate an increased risk of heart disease. Here, we analyze the distribution of cholesterol-to-HDL ratios among individuals with and without heart attacks:\n\n```\ndf[\"cholesterol_hdl_ratio\"] = df[\"chol\"] / df[\"thalach\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"cholesterol_hdl_ratio\", data=df)\nplt.title(\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\n\n```\n\nThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\n\n![Impact of Cholesterol-to-HDL Ratio on Heart Attack](/images/dynamic/untitled-11-.png \"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\n\nExercise-induced angina can be a symptom of underlying heart conditions. In this analysis, we explore how the presence or absence of exercise-induced angina affects the likelihood of experiencing a heart attack:\n\n```\nfig = plt.figure(figsize=(8, 6))\nsns.countplot(x=\"exang\", hue=\"target\", data=df, palette=\"Set2\")\nplt.title(\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\nplt.xlabel(\"Exercise-Induced Angina (exang)\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\n\n```\n\nThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\n\n![Impact of Exercise-Induced Angina on Heart Attack Risk](/images/dynamic/untitled-12-.png \"Impact of Exercise-Induced Angina in Heart Attack Risk\")\n\n## Training Models to Predict Heart Attack\n\n\n\nIn our pursuit to predict heart attack risk, we‚Äôre going to train three simple machine learning algorithms using the provided features from the dataset.\n\n```\nX = df[[\"age\", \"sex\", \"chol\", \"trestbps\", \"thalach\", \"exang\", \"oldpeak\"]]\ny = df[\"target\"]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n```\n\nLet's dive into each model and examine their performance.\n\n### Logistic Regression\n\n\n\nWe kickstart our analysis with the classic Logistic Regression model:\n\n```\nLr_model = LogisticRegression(random_state=42)\nLr_model.fit(X_train_scaled, y_train)\ny_pred = Lr_model.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"logistic_regression\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: LogisticRegression\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"logistic_regression\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-logistic_regression\"] = classification_report(y_test, y_pred, output_dict=True)\n\n```\n\n![Run Params](/images/dynamic/untitled-13-.png \"Run Params\")\n\n### Random Forest Algorithm\n\nMoving forward, let's explore the Random Forest algorithm:\n\n```\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfmodel = RandomForestClassifier(random_state=42)\nrfmodel.fit(X_train, y_train)\ny_pred = rfmodel.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"random_forest_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: RandomForestClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"random_forest_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-random_forest\"] = classification_report(y_test, y_pred, output_dict=True)\n\n```\n\n![Run Params](/images/dynamic/untitled-14-.png \"Run Params\")\n\n### Decision Tree\n\nLastly, let's delve into the Decision Tree algorithm:\n\n```\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"decision_tree_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: DecisionTreeClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"decision_tree_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-decision_tree\"] = classification_report(y_test, y_pred, output_dict=True)\n\n```\n\n![Run Params](/images/dynamic/untitled-15-.png \"Run Params\")\n\n## Post-Training Analysis\n\nTo assess the accuracies of the three models we've trained, we can use the Metrics Explorer:\n\n![Metrics Explorer](/images/dynamic/untitled-16-.png \"Metrics Explorer\")\n\nAdditionally, we can compare the confusion matrices of each method using the Images Explorer:\n\n![Images Explorer](/images/dynamic/untitled-17-.png \"Images Explorer\")\n\nTo comprehend the importance of each feature provided by the Random Forest classifier, we conduct the following analysis:\n\n```\nfeature_importances = rfmodel.feature_importances_\n\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame(\n    {\"Feature\": feature_names, \"Importance\": feature_importances}\n)\n\nfeature_importance_df = feature_importance_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\nfig = plt.figure(figsize=(10, 6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\nplt.title(\"Feature Importances in Predicting Heart Attacks\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Feature Importances in Predicting Heart Attacks\")\n\n```\n\n![Feature Importances in Predicting Heart Attacks](/images/dynamic/untitled-18-.png \"Feature Importances in Predicting Heart Attacks\")\n\nFor more details, the process can be reviewed in the 'Logs' tab. For long and expensive experiments, the benefits are greater:\n\n![Logs](/images/dynamic/untitled-19-.png \"Logs\")\n\n## Conclusion\n\nIn wrapping up our exploration of heart attack prediction, we dove deep into the factors affecting heart health through machine learning.\n\nOur study showed how picking the right features and checking our models are super important for guessing heart attack chances well. We used Aim, a cool tool that helps track experiments. It's known for being easy to use and giving quick results. With Aim, we kept track of what we found while studying. The things we learned could help doctors and policymakers make better plans to help patients avoid heart attacks.\n\nWe used Aim to track every step and insight, showcasing its value in biomedical research. For more on Aim's features, check out [aimstack.io](https://aimstack.io/) or [GitHub repository](https://github.com/aimhubio/aim).\n\n# Learn more\n\nAim is on a mission to democratize AI dev tools. üôå\n\nTry out Aim, join the [Aim community](https://community.aimstack.io/), share your feedback.","html":"\u003cp\u003eIn today's blog post, we'll explore heart attack prediction using the Heart Attack Prediction dataset. Our aim is to understand the key factors behind heart attacks with Machine Learning algorithms. We're going also to use the Aim open-source experiment tracking tool, renowned for its great UI and lightning-fast insights.\u003c/p\u003e\n\u003cp\u003eLet's dive straight into the action by setting up our environment and importing the essential libraries:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom aim import Image, Run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's dive into the dataset. We'll start by loading it to see the main features:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf = pd.read_csv(\"heart.csv\")\ndf.head()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis dataset gives us key information on heart health, with the \"target\" column indicating a heart attack occurrence (1) or absence (0). Here are the dataset's columns briefly:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eage: Age of the individual\u003c/li\u003e\n\u003cli\u003esex: Gender of the individual (1 for male, 0 for female)\u003c/li\u003e\n\u003cli\u003ecp: Chest pain type\u003c/li\u003e\n\u003cli\u003etrestbps: Resting blood pressure\u003c/li\u003e\n\u003cli\u003echol: Serum cholesterol level\u003c/li\u003e\n\u003cli\u003efbs: Fasting blood sugar level\u003c/li\u003e\n\u003cli\u003erestecg: Resting electrocardiographic results\u003c/li\u003e\n\u003cli\u003ethalach: Maximum heart rate achieved\u003c/li\u003e\n\u003cli\u003eexang: Exercise-induced angina (1 for yes, 0 for no)\u003c/li\u003e\n\u003cli\u003eoldpeak: ST depression induced by exercise relative to rest\u003c/li\u003e\n\u003cli\u003eslope: Slope of the peak exercise ST segment\u003c/li\u003e\n\u003cli\u003eca: Number of major vessels colored by fluoroscopy\u003c/li\u003e\n\u003cli\u003ethal: Thalassemia\u003c/li\u003e\n\u003cli\u003etarget: Level of heart attack (0.1 for lesser and 0.9 for stronger)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, armed with this understanding, let's kickstart our Aim run to track our discoveries:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun = Run()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt's as simple as that! Let's begin by logging some key insights about our dataset into our Aim run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun[\"dataset_info\"] = {\n    \"name\": \"Heart Attack Prediction\",\n    \"features\": df.describe().to_dict(),\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEasy, isn't it? But as they say, simplicity often conceals complexity, and we're just scratching the surface of our exploration.\u003c/p\u003e\n\u003cp\u003eTo observe these and the future metadata that we will track, let‚Äôs execute the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe resulting view will provide us with:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-2-.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd by navigating to the Runs page we will see our newly created single run.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-3-.png\" alt=\"Runs Explorer\" title=\"Runs Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eClick on the run name/hash to see the details of this run:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-4-.png\" alt=\"Run: Overview\" title=\"Run: Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eExploratory Data Analysis\u003c/h2\u003e\n\u003cp\u003eUpon a preliminary examination of the data, let's delve into our first qualitative analysis to explore the correlations among various metrics:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5)\nplt.title(\"Correlation Matrix Heatmap\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Correlation Matrix heatmap\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe've generated a correlation matrix heatmap representing the correlation coefficients between different features in the heart attack dataset. Each cell contains the correlation coefficient between the corresponding row and column variables. Here are some key observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePositive Correlations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecp (Chest Pain Type) correlates positively with thalach (Maximum heart rate achieved) and slope.\u003c/li\u003e\n\u003cli\u003ethalach correlates positively with slope.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eNegative Correlations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eage correlates negatively with thalach.\u003c/li\u003e\n\u003cli\u003esex correlates negatively with the target variable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStrong Correlations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe absolute correlation coefficient between cp and the target is relatively high (0.42), indicating a moderate positive correlation.\u003c/li\u003e\n\u003cli\u003eThe absolute correlation coefficient between thalach and the target is also relatively high (0.40).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOther Observations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexang (Exercise induced angina) exhibits a strong negative correlation with thalach.\u003c/li\u003e\n\u003cli\u003eoldpeak (ST depression induced by exercise relative to rest) correlates negatively with thalach.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCorrelation coefficients range from -1 to 1, where 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-5-.png\" alt=\"Correlation Matrix heatmap \" title=\"Correlation Matrix heatmap\"\u003e\u003c/p\u003e\n\u003cp\u003eNext, let's visualize the distribution of the target variable and track it in Aim:\u003c/p\u003e\n\u003cp\u003eIt would also be interesting to see how is the target variable distributed and again track it in Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efig = plt.figure(figsize=(8, 6))\nsns.histplot(df[\"target\"], kde=True, color=\"skyblue\", bins=20)\nplt.title(\"Distribution of Target Variable\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Frequency\")\n\naim_img = Image(fig)\nrun.track(aim_img, name=\"Distribution of Target Variable\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-6-.png\" alt=\"Distribution of Target Variable\" title=\"Distribution of Target Variable\"\u003e\u003c/p\u003e\n\u003cp\u003eAlso let‚Äôs plot the distribution of the features such as age, trestbps, chol, thalach and oldpeak that are gonna help us predict the target value:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enumerical_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n\nfor i, feature in enumerate(numerical_features, 1):\n    fig = plt.figure(figsize=(10, 8))\n    sns.boxplot(x=df[feature], color=\"skyblue\")\n    aim_img = Image(fig)\n    run.track(aim_img, name=f\"Box Plots of {feature}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe centre of distribution chol \u0026#x26; oldpeak is positively skewed, because the whisker and half-box are longer on the right side of the median than on the left side. Distribution of trestbps is approximately symmetric, because both half-boxes are almost the same length. It‚Äôs the most concentrated distribution because the interquartile range. The centre of distribution thalach \u0026#x26; age is negatively skewed because the whisker and half-box are longer on the left side of the median than on the right side.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTo check out the boxplots of each feature we can use the Images Explorer:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2024-02-13-at-4.36.58%E2%80%AFpm.png\" alt=\"Images Explorer\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eNow let‚Äôs normalize our features and check out their importance and track everything on Aim, starting with Impact of Age Categories on Heart Attack Risk:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"target\"] = df[\"target\"].apply(lambda x: 1 if x \u003e= 0.5 else 0)\n\ndf[\"age_category\"] = pd.cut(df[\"age\"], bins=[29, 45, 60, 100], labels=[\"Young\", \"Middle-aged\", \"Senior\"], right=False,)\nfig = plt.figure(figsize=(10, 6))\nsns.countplot(x=\"age_category\", hue=\"target\", data=df, palette=\"viridis\")\nplt.title(\"Impact of Age Categories on Heart Attack Risk\")\nplt.xlabel(\"Age Category\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Age Categories on Heart Attack Risk\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-7-.png\" alt=\"Impact of Age Categories on Heart Attack Risk\" title=\"Impact of Age Categories on Heart Attack Risk\"\u003e\u003c/p\u003e\n\u003cp\u003eUnderstanding the combined effect of blood pressure (trestbps) and cholesterol (chol) levels on heart attack risk is crucial. We computed the total cardiovascular risk by summing these two variables. The box plot below illustrates the distribution of total cardiovascular risk among individuals with and without heart attacks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"total_risk\"] = df[\"trestbps\"] + df[\"chol\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"total_risk\", data=df)\nplt.title(\"Impact of Total Cardiovascular Risk on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Total Cardiovascular Risk on Heart Attack\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-8-.png\" alt=\"Impact of Total Cardiovascular Risk on Heart Attack\" title=\"Impact of Total Cardiovascular Risk on Heart Attack\"\u003e\u003c/p\u003e\n\u003cp\u003eExercise-induced angina, characterized by chest pain during physical exertion, could be a significant indicator of heart health. In this visualization, we explore how the presence of exercise-induced angina, coupled with maximum heart rate achieved (thalach), correlates with the occurrence of heart attacks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"exercise_angina\"] = (df[\"exang\"] == 1) \u0026#x26; (df[\"thalach\"] \u003e 150)\nfig = plt.figure(figsize=(10, 8))\nsns.countplot(x=\"exercise_angina\", hue=\"target\", data=df)\nplt.title(\"Impact of Exercise-induced Angina on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-induced Angina on Heart Attack\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-9-.png\" alt=\"Impact of Exercise-induced Angina on Heart Attack\" title=\"Impact of Exercise-induced Angina on Heart Attack\"\u003e\u003c/p\u003e\n\u003cp\u003eThe ratio of total cholesterol (chol) to high-density lipoprotein (HDL) cholesterol is a critical marker of cardiovascular health. A higher ratio may indicate an increased risk of heart disease. Here, we analyze the distribution of cholesterol-to-HDL ratios among individuals with and without heart attacks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf[\"cholesterol_hdl_ratio\"] = df[\"chol\"] / df[\"thalach\"]\nfig = plt.figure(figsize=(10, 8))\nsns.boxplot(x=\"target\", y=\"cholesterol_hdl_ratio\", data=df)\nplt.title(\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-11-.png\" alt=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\" title=\"Impact of Cholesterol-to-HDL Ratio on Heart Attack\"\u003e\u003c/p\u003e\n\u003cp\u003eExercise-induced angina can be a symptom of underlying heart conditions. In this analysis, we explore how the presence or absence of exercise-induced angina affects the likelihood of experiencing a heart attack:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efig = plt.figure(figsize=(8, 6))\nsns.countplot(x=\"exang\", hue=\"target\", data=df, palette=\"Set2\")\nplt.title(\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\nplt.xlabel(\"Exercise-Induced Angina (exang)\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Heart Attack\", labels=[\"No\", \"Yes\"])\naim_img = Image(fig)\nrun.track(aim_img, name=\"Impact of Exercise-Induced Angina on Heart Attack Risk\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese visualizations provide insights into various factors influencing heart attack risk and can aid in developing strategies for prevention and treatment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-12-.png\" alt=\"Impact of Exercise-Induced Angina on Heart Attack Risk\" title=\"Impact of Exercise-Induced Angina in Heart Attack Risk\"\u003e\u003c/p\u003e\n\u003ch2\u003eTraining Models to Predict Heart Attack\u003c/h2\u003e\n\u003cp\u003eIn our pursuit to predict heart attack risk, we‚Äôre going to train three simple machine learning algorithms using the provided features from the dataset.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eX = df[[\"age\", \"sex\", \"chol\", \"trestbps\", \"thalach\", \"exang\", \"oldpeak\"]]\ny = df[\"target\"]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's dive into each model and examine their performance.\u003c/p\u003e\n\u003ch3\u003eLogistic Regression\u003c/h3\u003e\n\u003cp\u003eWe kickstart our analysis with the classic Logistic Regression model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLr_model = LogisticRegression(random_state=42)\nLr_model.fit(X_train_scaled, y_train)\ny_pred = Lr_model.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"logistic_regression\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: LogisticRegression\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"logistic_regression\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-logistic_regression\"] = classification_report(y_test, y_pred, output_dict=True)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-13-.png\" alt=\"Run Params\" title=\"Run Params\"\u003e\u003c/p\u003e\n\u003ch3\u003eRandom Forest Algorithm\u003c/h3\u003e\n\u003cp\u003eMoving forward, let's explore the Random Forest algorithm:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.ensemble import RandomForestClassifier\n\nrfmodel = RandomForestClassifier(random_state=42)\nrfmodel.fit(X_train, y_train)\ny_pred = rfmodel.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"random_forest_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: RandomForestClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"random_forest_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-random_forest\"] = classification_report(y_test, y_pred, output_dict=True)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-14-.png\" alt=\"Run Params\" title=\"Run Params\"\u003e\u003c/p\u003e\n\u003ch3\u003eDecision Tree\u003c/h3\u003e\n\u003cp\u003eLastly, let's delve into the Decision Tree algorithm:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nrun.track(accuracy, name=\"accuracy\", context={\"model\": \"decision_tree_classifier\"})\n\nprint(f\"Confusion Matrix: {conf_matrix}\")\nfig = plt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"RdYlGn\", linewidths=0.5, fmt=\"g\")\nplt.title(\"Prediction Confusion Matrix: DecisionTreeClassifier\")\naim_img = Image(fig)\nrun.track(\n    aim_img,\n    name=\"Prediction Confusion Matrix\",\n    context={\"model\": \"decision_tree_classifier\"},\n)\n\nprint(f\"Classification Report: {classification_rep}\")\nrun[\"classification_report-decision_tree\"] = classification_report(y_test, y_pred, output_dict=True)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-15-.png\" alt=\"Run Params\" title=\"Run Params\"\u003e\u003c/p\u003e\n\u003ch2\u003ePost-Training Analysis\u003c/h2\u003e\n\u003cp\u003eTo assess the accuracies of the three models we've trained, we can use the Metrics Explorer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-16-.png\" alt=\"Metrics Explorer\" title=\"Metrics Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, we can compare the confusion matrices of each method using the Images Explorer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-17-.png\" alt=\"Images Explorer\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eTo comprehend the importance of each feature provided by the Random Forest classifier, we conduct the following analysis:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efeature_importances = rfmodel.feature_importances_\n\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame(\n    {\"Feature\": feature_names, \"Importance\": feature_importances}\n)\n\nfeature_importance_df = feature_importance_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\nfig = plt.figure(figsize=(10, 6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\nplt.title(\"Feature Importances in Predicting Heart Attacks\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\naim_img = Image(fig)\nrun.track(aim_img, name=\"Feature Importances in Predicting Heart Attacks\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-18-.png\" alt=\"Feature Importances in Predicting Heart Attacks\" title=\"Feature Importances in Predicting Heart Attacks\"\u003e\u003c/p\u003e\n\u003cp\u003eFor more details, the process can be reviewed in the 'Logs' tab. For long and expensive experiments, the benefits are greater:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/untitled-19-.png\" alt=\"Logs\" title=\"Logs\"\u003e\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn wrapping up our exploration of heart attack prediction, we dove deep into the factors affecting heart health through machine learning.\u003c/p\u003e\n\u003cp\u003eOur study showed how picking the right features and checking our models are super important for guessing heart attack chances well. We used Aim, a cool tool that helps track experiments. It's known for being easy to use and giving quick results. With Aim, we kept track of what we found while studying. The things we learned could help doctors and policymakers make better plans to help patients avoid heart attacks.\u003c/p\u003e\n\u003cp\u003eWe used Aim to track every step and insight, showcasing its value in biomedical research. For more on Aim's features, check out \u003ca href=\"https://aimstack.io/\"\u003eaimstack.io\u003c/a\u003e or \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub repository\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eAim is on a mission to democratize AI dev tools. üôå\u003c/p\u003e\n\u003cp\u003eTry out Aim, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e"},"_id":"posts/training-models-to-predict-heart-attack.md","_raw":{"sourceFilePath":"posts/training-models-to-predict-heart-attack.md","sourceFileName":"training-models-to-predict-heart-attack.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/training-models-to-predict-heart-attack"},"type":"Post"}]},"__N_SSG":true},"page":"/blog/[category]/[slug]","query":{"category":"integrations","slug":"exploring-your-pytorch-lightning-experiments-with-aimos"},"buildId":"sv29o-HT1NsDRREfQxVOj","isFallback":false,"gsp":true,"scriptLoader":[{"src":"https://www.googletagmanager.com/gtag/js?id=G-G8YKCN2HLS","strategy":"afterInteractive"},{"id":"google-analytics","strategy":"afterInteractive","children":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){window.dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config','G-G8YKCN2HLS');\n            "}]}</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-G8YKCN2HLS"
            height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript></body></html>