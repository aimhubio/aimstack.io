<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>New-releases | AimStack</title><meta name="robots" content="index,follow"/><meta name="description" content="Access your category related articles"/><meta property="og:title" content="New-releases | AimStack"/><meta property="og:description" content="Access your category related articles"/><meta property="og:url" content="https://aimstack.io/blog/new-releases"/><meta property="og:type" content="website"/><meta property="og:image" content="https://aimstack.io/banner.png"/><meta property="og:image:alt" content="New-releases | AimStack"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1224"/><meta property="og:image:height" content="724"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="AimStack"/><link rel="canonical" href="https://aimstack.io/blog/new-releases"/><link rel="preload" href="/_next/static/media/bg.eb38a857.svg" as="image" fetchpriority="high"/><meta name="next-head-count" content="18"/><style id="stitches">--sxs{--sxs:0 t-dSaWrp}@media{:root,.t-dSaWrp{--fonts-base:inherit;--colors-blue:#1093F2;--colors-blueHover:#0F7AC8;--colors-lightBlue:rgba(16, 147, 242, 0.1);--colors-lightBlueHover:rgba(16, 147, 242, 0.2);--colors-darkBlue:#0C1031;--colors-darkBlue500:rgba(12,16,49,0.5);--colors-bigStone:#191D3C;--colors-bigStoneHover:#313551;--colors-green:#14C89D;--colors-black:#000000;--colors-black003:rgba(0,0,0,0.03);--colors-black700:rgba(0,0,0,0.7);--colors-black800:rgba(0,0,0,0.8);--colors-white:#ffffff;--colors-white100:rgba(255,255,255,0.1);--colors-white500:rgba(255,255,255,0.5);--colors-white700:rgba(255,255,255,0.7);--colors-red:#CC231A;--colors-lightGrey:#F4F7F9;--colors-lightGreyHover:#ECEEF0;--colors-grey:#CFD3D6;--colors-darkGrey:#737379;--colors-darkGreyHover:#393940;--colors-primary:var(--colors-blue);--colors-primaryHover:var(--colors-blueHover);--colors-primaryLight:var(--colors-lightBlue);--colors-primaryLightHover:var(--colors-lightBlueHover);--colors-secondary:var(--colors-green);--colors-textColor:var(--colors-black);--colors-bgColor:var(--colors-darkBlue);--colors-danger:var(--colors-red);--fontSizes-1:14px;--fontSizes-2:16px;--fontSizes-3:18px;--fontSizes-4:20px;--fontSizes-5:22px;--fontSizes-6:24px;--fontSizes-7:32px;--fontSizes-8:44px;--fontSizes-9:56px;--fontSizes-10:64px;--fontSizes-11:68px;--fontSizes-baseSize:var(--fontSizes-2);--space-1:4px;--space-2:8px;--space-3:12px;--space-4:16px;--space-5:20px;--space-6:24px;--space-7:28px;--space-8:32px;--space-9:36px;--space-10:40px;--space-11:44px;--space-12:48px;--space-13:52px;--space-14:56px;--fontWeights-1:400;--fontWeights-2:500;--fontWeights-3:600;--fontWeights-4:700;--fontWeights-5:800;--fontWeights-6:900;--shadows-1:0px 60px 66px -65px rgba(11, 47, 97, 0.2);--shadows-2:0px 96px 66px -65px rgba(11, 47, 97, 0.2);--shadows-3:1px 1px 10px 3px  rgb(0, 0, 0, 10%);--shadows-4:0px 10px 24px -10px rgba(11, 47, 97, 0.4);--shadows-5:0px 10px 32px -10px rgba(11, 47, 97, 0.2);--radii-1:6px;--radii-2:8px;--transitions-main:0.2s ease-out}}--sxs{--sxs:1 iPqvQa k-iKtTFk}@media{html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,main,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section{display:block}*[hidden]{display:none}*{box-sizing:border-box}ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:none}table{border-spacing:0}strong{font-weight:var(--fontWeights-5)}body{background:var(--colors-white);color:var(--colors-textColor);font-family:var(--fonts-base);font-size:var(--fontSizes-baseSize);line-height:1.35;overscroll-behavior:none}img{user-drag:none;-webkit-user-drag:none}a{text-decoration:none;color:inherit}a.link{color:var(--colors-primary);font-weight:var(--fontWeights-4);transition:var(--transitions-main)}a.link:hover{color:var(--colors-primaryHover)}.text-center{text-align:center}@keyframes k-iKtTFk{0%{opacity:0}10%{opacity:1}90%{opacity:1}100%{opacity:0}}}--sxs{--sxs:2 c-iSkjJi c-guYHoi c-ckOUFN c-hCkBfr c-ePqJJt c-jafYUQ c-BqIMD c-iCFsHS c-kPczbf c-lizetl c-fOPBY c-cZmHrB c-iKzMen c-dhzjXW c-PJLV c-hpFSsv c-kSnMQa c-kXnqgD c-fgXcmv c-gPAtaN c-dMnFVR c-kHrbHn c-hJzboU c-ghrOTq c-kBJMsw c-fcTZTi c-gvTmKt c-FsNLu c-jgJYki c-bFmrFE c-MNZuo c-cChYXC c-dtSTJL c-caMtBg c-bMqEGV c-ioCbLy c-hIPllP c-kfcfix c-fXQtST c-iHpEPN c-jHDeUH c-fYCBFy c-gDDtX c-kpTGUt c-hdgScR c-kskmfB c-idhSbv c-fixGjY c-kHKySA c-iWPTnC c-dJzRen c-juttUG c-TJTRd c-foiehy c-hoGqzL c-QFqsL c-coAScu c-gPhuwe c-eJZVxt c-joiLKa c-IuYqv c-bCTsjq c-EjvQF c-bcXfYW c-kOFnHV c-bosfyl c-gswrlc c-jVwFOG c-eYYUfS c-bBICiS c-kDFBqB c-cXVAvf c-dsvWej c-cmpvrW c-crdKmK c-jpbidf c-jFppbe c-hnRRWM c-hQOWqi c-jiSXep c-dzopkw c-bzrsgE c-cgYDvy c-fAHQVo c-kvmSnp c-cPiarr c-iGygWh c-fVgYPG c-fxrEBZ c-hTKfPd c-kBUzHM c-lcVFNy c-kqsoHo c-eCJxrf c-hMVjTK c-cQwwIn c-fagtgx c-gsaXpY c-eOYVvF c-ekLXmJ c-hPoOJn c-ckoDwU c-covwPd c-eGPXIo c-igsMin c-bzbvcg c-hhjAKd c-bTiyxX}@media{.c-iSkjJi{position:relative}.c-iSkjJi .bg-top{z-index:2}.c-iSkjJi .bg-bottom{z-index:1;position:absolute;bottom:300px;left:0;right:0;object-fit:contain;width:100%;height:60%}.c-guYHoi{position:relative;z-index:3;display:flex;flex-direction:column;min-height:100vh}.c-ckOUFN{height:72px;position:fixed;top:0px;left:0px;right:0px;z-index:99;transition:var(--transitions-main);background-color:transparent}.c-ckOUFN.dark{background-color:var(--colors-darkBlue)}.c-ckOUFN.dark a{color:var(--colors-white)}.c-ckOUFN.dark.fixed a{color:var(--colors-textColor)}.c-ckOUFN.fixed{box-shadow:var(--shadows-3);background-color:var(--colors-white)}.c-hCkBfr{margin-left:auto;margin-right:auto;padding-left:var(--space-6);padding-right:var(--space-6);width:100%;max-width:1300px}.c-ePqJJt{display:flex;align-items:center;height:100%}.c-jafYUQ{margin-right:50px}.c-jafYUQ .logo{max-width:158px;width:100%;display:block}.c-jafYUQ .logo-image{display:block}@media (max-width: 1024px){.c-jafYUQ{position:relative;z-index:11}}.c-BqIMD{display:flex;justify-content:center}.c-BqIMD .nav-list{display:flex}.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-6)}@media (max-width: 1440px){.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-4)}}.c-BqIMD .nav-list li a{-webkit-backface-visibility:hidden;backface-visibility:hidden;display:inline-flex}.c-BqIMD .nav-list li a .text{transition:var(--transitions-main)}.c-BqIMD .nav-list li a:hover .text{opacity:.6}@media (max-width: 1024px){.c-BqIMD{position:fixed;top:72px;right:0;bottom:0;left:0;overflow-y:auto;z-index:10;height:0;transition:height 0.5s;background-color:var(--colors-white);flex-direction:column;justify-content:space-between}}@media (max-width: 1024px){.open .c-BqIMD{height:calc(100% - 72px)}}@media (max-width: 1024px){.c-BqIMD .nav-inner{width:100%;flex-direction:column;align-items:inherit;justify-content:space-between;padding-top:16px}}@media (max-width: 1024px){.c-BqIMD .nav-list{flex-direction:column;align-items:flex-start}}@media (max-width: 1024px){.c-BqIMD .nav-list > li{font-size:var(--fontSizes-2);font-weight:var(--fontWeights-3);width:100%}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a{display:block;padding:var(--space-3) var(--space-5)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a:active{background-color:var(--colors-primaryLight)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li:not(:last-child){margin-right:0}}.c-iCFsHS{display:inline-block;height:18px;line-height:18px;padding:0 4px;border-radius:2px;background-color:var(--colors-primary);color:var(--colors-white);font-weight:700;font-size:10px;margin-left:6px}.c-kPczbf{margin-left:auto}.c-kPczbf span span{display:flex;align-items:center}.c-kPczbf.desktop-btn span span{justify-content:flex-end}.c-lizetl{display:none}@media (max-width: 1024px){.c-lizetl{display:flex;justify-content:center;padding-top:var(--space-6);padding-bottom:var(--space-6)}}@media (max-width: 1024px){.c-lizetl > li{margin-right:var(--space-6)}}.c-fOPBY{display:none}@media (max-width: 1024px){.c-fOPBY{position:relative;z-index:11;display:inline-block;-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer}}.c-cZmHrB{flex:1}.c-iKzMen{text-align:center;padding-top:80px;padding-bottom:80px}.c-dhzjXW{display:flex}.c-hpFSsv{padding-top:80px;padding-bottom:80px;color:var(--colors-white);background-color:var(--colors-darkBlue);position:relative}.c-hpFSsv .bg-top{object-fit:cover;object-position:top}@media (max-width: 1024px){.c-hpFSsv{padding-top:44px;padding-bottom:44px}}.c-kSnMQa{display:flex;flex-wrap:wrap;margin-left:-60px;margin-top:60px}@media (max-width: 743px){.c-kSnMQa{margin-left:0;margin-top:var(--space-8)}}.c-kXnqgD{width:calc((100% / 2) - 60px);margin-left:60px;margin-bottom:60px}@media (max-width: 743px){.c-kXnqgD{width:100%;margin-left:0;margin-bottom:var(--space-10)}}.c-fgXcmv{max-width:650px}.c-fgXcmv img{width:100%;height:auto}@media (max-width: 1024px){.c-fgXcmv{margin:0 auto}}.c-gPAtaN{text-align:center;padding-top:100px}.c-dMnFVR{display:flex;flex-wrap:wrap;margin-left:-40px;margin-top:100px}@media (max-width: 1024px){.c-dMnFVR{justify-content:center}}@media (max-width: 743px){.c-dMnFVR{margin-left:0}}.c-kHrbHn{width:calc((100% / 4) - 40px);margin-left:40px;margin-bottom:60px}@media (max-width: 1024px){.c-kHrbHn{width:calc((100% / 2) - 40px);margin-bottom:40px}}@media (max-width: 743px){.c-kHrbHn{width:100%;margin-left:0}}.c-kHrbHn .card-content{margin-top:var(--space-6)}@media (max-width: 743px){.c-kHrbHn .card-content{text-align:left;margin-top:0}}.c-hJzboU{-webkit-mask-size:100%;mask-size:100%;-webkit-mask-repeat:no-repeat}.c-hJzboU img{object-fit:contain;height:100%;width:100%}@media (max-width: 743px){.c-hJzboU{width:100px;min-width:100px;margin-right:var(--space-6)}}.c-ghrOTq{overflow:hidden;width:100%;height:auto;padding-top:4px;display:flex;flex-direction:column}.c-kBJMsw{width:100%;height:auto;scale:1.02}.c-fcTZTi{min-height:100px;display:flex;flex-direction:column;background-color:#673AB7}.c-gvTmKt{margin-top:auto}.c-FsNLu{display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;color:var(--colors-white);padding-top:27px;padding-bottom:27px;border-bottom:1px solid var(--colors-white100)}.c-jgJYki{margin-right:var(--space-9)}.c-jgJYki img{display:block}@media (max-width: 743px){.c-jgJYki img{margin:0 auto}}@media (max-width: 1440px){.c-jgJYki{margin-right:var(--space-6)}}@media (max-width: 743px){.c-jgJYki{width:100%;margin-right:0;margin-bottom:var(--space-1);text-align:center}}.c-bFmrFE{display:flex;margin-right:auto}.c-bFmrFE li:not(:last-child){margin-right:var(--space-9)}@media (max-width: 1440px){.c-bFmrFE li:not(:last-child){margin-right:var(--space-4)}}.c-bFmrFE li{transition:var(--transitions-main)}.c-bFmrFE li:hover{opacity:.6}@media (max-width: 1024px){.c-bFmrFE{order:3;width:100%;margin-top:var(--space-7)}}@media (max-width: 1024px){.c-bFmrFE li:last-child{margin-right:0}}@media (max-width: 743px){.c-bFmrFE{order:2;width:100%;margin-bottom:var(--space-2);flex-direction:column;text-align:center}}@media (max-width: 743px){.c-bFmrFE li{margin-bottom:var(--space-6)}}@media (max-width: 743px){.c-bFmrFE li:not(:last-child){margin-right:0}}.c-MNZuo{display:flex}.c-MNZuo li:not(:last-child){margin-right:var(--space-6)}.c-MNZuo li a{transition:var(--transitions-main)}.c-MNZuo li a:hover{opacity:.6}@media (max-width: 743px){.c-MNZuo{margin:0 auto;order:3}}.c-cChYXC{padding-top:var(--space-4);padding-bottom:var(--space-4);color:var(--colors-white500);text-align:center}.c-dtSTJL{background-image:url(/images/static/hero/dots.svg), url(/images/static/hero/bg.svg);background-repeat:no-repeat;background-size:cover, contain;background-position:center, right;position:relative;height:100%;min-height:100vh;display:flex;text-align:center;overflow:hidden;padding:150px 0}@media (max-width: 1024px){.c-dtSTJL{padding:120px 0}}@media (max-width: 743px){.c-dtSTJL{padding:120px 0 24px}}.c-caMtBg{width:100%;position:relative;display:flex}@media (max-width: 1024px){.c-caMtBg{flex-direction:column;align-items:center;height:100%}}.c-caMtBg .hero-button-container{display:flex}@media (max-width: 1024px){.c-caMtBg .hero-button-container{justify-content:center}}@media (max-width: 425px){.c-caMtBg .hero-button-container{flex-direction:column}}@media (max-width: 425px){.c-caMtBg .hero-button-container .hero-try-demo{margin-bottom:var(--space-4)}}.c-caMtBg .hero-button-container .hero-quick-start{margin-left:var(--space-4)}@media (max-width: 425px){.c-caMtBg .hero-button-container .hero-quick-start{margin-left:0}}.c-caMtBg .hero-content{max-width:550px;min-width:550px;text-align:left}@media (max-width: 1024px){.c-caMtBg .hero-content{text-align:center;min-width:unset}}.c-bMqEGV{-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer;border-radius:var(--radii-1);display:inline-block;transition:var(--transitions-main)}.c-ioCbLy{position:absolute;top:50%;transform:translateY(-50%);right:-300px;border-radius:12px;border:1px solid #D8DADF;width:866px;height:auto;aspect-ratio:auto}@media (max-width: 1440px){.c-ioCbLy{width:700px}}@media (max-width: 1024px){.c-ioCbLy{position:unset;transform:unset;width:500px;margin-top:var(--space-12)}}@media (max-width: 743px){.c-ioCbLy{width:90%}}@media (max-width: 425px){.c-ioCbLy{display:none}}.c-hIPllP{padding:80px 0}@media (max-width: 1024px){.c-hIPllP{padding:60px 0}}@media (max-width: 743px){.c-hIPllP{padding:60px 0 24px}}.c-kfcfix{height:110px;display:flex;background-color:white}.c-fXQtST{background-color:white}.c-iHpEPN{display:flex;align-items:center;overflow:visible}.c-iHpEPN img{width:100%;height:auto}.c-jHDeUH{position:relative;overflow:hidden;padding:150px 0}@media (max-width: 1024px){.c-jHDeUH{padding:80px 0 24px}}.c-fYCBFy{object-fit:cover;z-index:-1}.c-gDDtX{display:flex;justify-content:space-between;align-items:center}@media (max-width: 1024px){.c-gDDtX{flex-direction:column}}.c-kpTGUt{display:flex;flex-direction:column;max-width:500px}@media (max-width: 1024px){.c-kpTGUt{text-align:center;align-items:center}}.c-kpTGUt .integrations-title{margin-bottom:var(--space-6);font-weight:800}.c-kpTGUt .integrations-subtitle{line-height:30px;margin-bottom:var(--space-14)}@media (max-width: 1024px){.c-kpTGUt .integrations-subtitle{margin-bottom:var(--space-11)}}.c-kpTGUt a{width:-moz-fit-content;width:fit-content}@media (max-width: 425px){.c-kpTGUt a{width:100%}}.c-hdgScR{width:50%;max-width:540px;height:auto}@media (max-width: 1024px){.c-hdgScR{margin-top:var(--space-7);width:100%}}@media (max-width: 425px){.c-hdgScR{display:none}}.c-kskmfB{position:relative;overflow:hidden;padding:150px 0}@media (max-width: 743px){.c-kskmfB{padding:80px 0 24px;text-align:center}}.c-kskmfB .features-title{margin-bottom:var(--space-6);font-weight:800}@media (max-width: 743px){.c-kskmfB .features-title{text-align:center}}.c-kskmfB .features-subtitle{line-height:30px;margin-bottom:var(--space-14)}@media (max-width: 743px){.c-kskmfB .features-subtitle{margin-bottom:var(--space-11)}}.c-idhSbv{object-fit:contain;z-index:-1}.c-fixGjY{display:flex;flex-direction:column}.c-kHKySA{display:flex;flex-direction:column;max-width:600px;margin:0 auto;text-align:center}@media (max-width: 743px){.c-kHKySA{max-width:100%}}.c-iWPTnC{display:flex;flex-direction:column;justify-content:space-between;width:100%}.c-dJzRen{display:flex;align-items:center;justify-content:space-between;padding-bottom:120px}@media (max-width: 743px){.c-dJzRen{flex-direction:column}}.c-juttUG{text-align:left;width:400px;min-width:300px}@media (max-width: 743px){.c-juttUG{width:100%}}.c-juttUG .step-title{display:flex;align-items:center;text-align:left;margin-bottom:var(--space-6)}.c-juttUG .step-title .badge{color:var(--colors-white);font-size:10px;padding:6px 4px;background:#5865F2;border-radius:4px;margin-left:var(--space-4)}.c-juttUG .step-description{margin-bottom:var(--space-5)}@media (max-width: 743px){.c-juttUG .step-description{text-align:left}}@media (max-width: 743px){.c-juttUG ul{text-align:left}}.c-juttUG ul li{margin-bottom:var(--space-3);position:relative;padding-left:24px}.c-juttUG ul li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:#5865F2}.c-TJTRd{padding:12px;display:inline-flex;border-radius:8px;margin-bottom:24px}.c-foiehy{max-width:800px;min-width:380px;height:auto;display:block;margin-left:60px}@media (max-width: 743px){.c-foiehy{display:none}}.c-foiehy.step-banner-image-mobile{display:none}@media (max-width: 743px){.c-foiehy.step-banner-image-mobile{margin:0 auto var(--space-10);display:block;min-width:unset;width:100%}}.c-hoGqzL{border-top:1px solid #E4E7EB;display:flex;align-items:flex-start;width:100%}@media (max-width: 743px){.c-hoGqzL{flex-direction:column}}.c-hoGqzL .step-coming-soon{flex:1;max-width:360px;padding-right:40px;margin-right:60px;padding:60px 0}@media (max-width: 743px){.c-hoGqzL .step-coming-soon{padding-right:0;margin-right:0}}.c-QFqsL{display:flex;flex-direction:column;overflow:hidden;position:relative;padding-bottom:100px}@media (max-width: 743px){.c-QFqsL{padding:80px 0 24px}}.c-QFqsL .top-bg{object-fit:cover}.c-QFqsL .bottom-bg{object-fit:cover}.c-coAScu{background-color:#04062E;padding:100px 0}.c-coAScu .quickstart-button{margin-top:var(--space-14);font-weight:var(--fontWeights-3)}@media (max-width: 1024px){.c-coAScu .quickstart-button{display:none}}.c-coAScu .quickstart-button-mobile{display:none;text-align:center;margin-top:var(--space-6)}@media (max-width: 1024px){.c-coAScu .quickstart-button-mobile{display:inline-block;margin-inline-start:auto;margin-inline-end:auto}}.c-gPhuwe{overflow:hidden;display:flex;justify-content:space-between}@media (max-width: 1024px){.c-gPhuwe{flex-direction:column;margin:0 auto}}.c-eJZVxt{max-width:480px}@media (max-width: 1024px){.c-eJZVxt{margin-bottom:var(--space-14)}}.c-eJZVxt .quickstart-title{color:var(--colors-white);margin-bottom:var(--space-6)}.c-eJZVxt .quickstart-subtitle{color:var(--colors-white)}.c-eJZVxt .quickstart-subtitle li{margin-bottom:var(--space-3);position:relative;padding-left:24px;line-height:26px}.c-eJZVxt .quickstart-subtitle li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:#1093F2}.c-joiLKa{margin-left:80px}@media (max-width: 1024px){.c-joiLKa{margin-left:0}}.c-IuYqv{padding-top:var(--space-5)}.c-IuYqv code{font-family:var(--fonts-Inconsolata)}.c-bCTsjq{position:relative;width:500px}@media (max-width: 1440px){.c-bCTsjq{width:unset}}.c-bCTsjq.light .hljs{background-color:var(--colors-white);color:var(--colors-textColor)}.c-bCTsjq button{position:absolute;right:12px;top:6px;background:none;border:none;cursor:pointer;height:42px;width:42px;transition:var(--transitions-main)}.c-bCTsjq button:hover{opacity:.6}.c-bCTsjq .copied{display:block;opacity:0;position:absolute;right:0;top:-20px;animation:k-iKtTFk 2.5s ease 0s alternate;color:var(--colors-secondary);font-size:var(--fontSizes-1);font-weight:var(--fontWeights-2);transition:var(--transitions-main);z-index:10}.c-bCTsjq .hljs{display:block;overflow-x:auto;color:#abb2bf;background:#0c0d34;margin-bottom:20px;border-radius:var(--radii-1);border:1px solid #3D3D5D;padding:var(--space-4)}.c-bCTsjq .hljs-comment,.c-bCTsjq .hljs-quote{color:#5c6370;font-style:italic}.c-bCTsjq .hljs-doctag,.c-bCTsjq .hljs-keyword,.c-bCTsjq .hljs-formula{color:#c678dd}.c-bCTsjq .hljs-section,.c-bCTsjq .hljs-name,.c-bCTsjq .hljs-selector-tag,.c-bCTsjq .hljs-deletion,.c-bCTsjq .hljs-subst{color:#e06c75}.c-bCTsjq .hljs-literal{color:#56b6c2}.c-bCTsjq .hljs-string,.c-bCTsjq .hljs-regexp,.c-bCTsjq .hljs-addition,.c-bCTsjq .hljs-attribute,.c-bCTsjq .hljs-meta-string{color:#98c379}.c-bCTsjq .hljs-built_in,.c-bCTsjq .hljs-class .hljs-title{color:#e6c07b}.c-bCTsjq .hljs-attr,.c-bCTsjq .hljs-variable,.c-bCTsjq .hljs-template-variable,.c-bCTsjq .hljs-type,.c-bCTsjq .hljs-selector-class,.c-bCTsjq .hljs-selector-attr,.c-bCTsjq .hljs-selector-pseudo,.c-bCTsjq .hljs-number{color:#d19a66}.c-bCTsjq .hljs-symbol,.c-bCTsjq .hljs-bullet,.c-bCTsjq .hljs-link,.c-bCTsjq .hljs-meta,.c-bCTsjq .hljs-selector-id,.c-bCTsjq .hljs-title{color:#61aeee}.c-bCTsjq .hljs-emphasis{font-style:italic}.c-bCTsjq .hljs-strong{font-weight:bold}.c-bCTsjq .hljs-link{text-decoration:underline}.c-EjvQF{background-image:linear-gradient(transparent, #d0cafe40, transparent);position:relative;overflow:hidden;padding:150px 0}@media (max-width: 743px){.c-EjvQF{padding:80px 0 24px}}.c-EjvQF .demos-title{text-align:center;margin-bottom:var(--space-6)}.c-EjvQF .demos-subtitle{text-align:center;margin-bottom:var(--space-14)}.c-bcXfYW{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:16px}@media (max-width: 1024px){.c-bcXfYW{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-bcXfYW{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 575px){.c-bcXfYW{grid-template-columns:repeat(1,minmax(0,1fr))}}.c-kOFnHV{min-width:260px;height:340px;display:inline-flex}@media (max-width: 575px){.c-kOFnHV{width:100%;max-width:360px;margin:0 auto}}.c-kOFnHV a{display:block;width:100%;height:100%;border-radius:var(--radii-2);transition:var(--transitions-main);border:1px solid #E2D7EB;background-color:var(--colors-white)}.c-kOFnHV a .demo-inner{padding:var(--space-5) var(--space-6)}.c-kOFnHV a:hover{background-color:#F3F5F9}.c-bosfyl{width:100%;height:66%;display:block;border-radius:var(--radii-1) var(--radii-1) 0 0;object-fit:cover;padding:10px;overflow:hidden}.c-gswrlc{padding:100px 0}.c-gswrlc .input{max-width:400px}@media (max-width: 1024px){.c-gswrlc{padding:80px 0;text-align:center}}@media (max-width: 1024px){.c-gswrlc .input{margin:0 auto}}@media (max-width: 743px){.c-gswrlc{padding:60px 0}}.c-jVwFOG{display:flex;align-items:center;justify-content:space-between}.c-eYYUfS{flex:1;margin-right:80px}@media (max-width: 1024px){.c-eYYUfS{margin-right:0}}.c-bBICiS{position:relative;display:block}.c-bBICiS .error-message{position:absolute;top:100%;transform:translateY(2px);color:var(--colors-danger);font-size:var(--fontSizes-1)}.c-bBICiS input{border:none;border-radius:var(--radii-1);width:100%;transition:var(--transitions-main)}.c-kDFBqB{flex:1}.c-kDFBqB img{max-width:100%;width:100%;height:auto}@media (max-width: 1024px){.c-kDFBqB{display:none}}.c-cXVAvf{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:40px}@media (max-width: 1024px){.c-cXVAvf{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-cXVAvf{gap:20px;grid-template-columns:repeat(1,minmax(0,1fr))}}.c-dsvWej{padding:10px;border:1px solid #E2D7EB;border-radius:var(--radii-2);transition:var(--transitions-main);height:100%}.c-dsvWej:hover{background-color:#F3F5F9}.c-cmpvrW{position:relative}.c-crdKmK{width:100%;object-fit:cover;height:200px;min-height:200px}.c-jpbidf{padding:20px 10px 0px 10px;position:relative}.c-jFppbe{margin-bottom:var(--space-2);display:flex;align-items:center;color:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe .icon{margin-right:var(--space-1);fill:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe:hover{color:var(--colors-darkGray)}.c-jFppbe:hover .icon{fill:var(--colors-darkGray)}.c-hnRRWM{display:flex;align-items:center;justify-content:flex-end}.c-hnRRWM .icon{margin-right:var(--space-1);fill:var(--colors-grey)}.c-hQOWqi{margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-jiSXep{display:flex;align-items:center;justify-content:center}.c-jiSXep li{height:44px;width:44px;font-size:var(--fontSizes-1);font-weight:var(--fontWeights-4);text-align:center;margin-right:var(--space-1)}.c-jiSXep li a{display:block;line-height:44px;transition:var(--transitions-main)}.c-jiSXep li a.active{background-color:var(--colors-primary);color:var(--colors-white)}.c-jiSXep li a:hover:not(.active){color:var(--colors-primary)}.c-jiSXep li a:hover .icon{fill:var(--colors-primary)}.c-dzopkw{padding:150px 0 64px;text-align:center}@media (max-width: 1024px){.c-dzopkw{padding:100px 0}}@media (max-width: 743px){.c-dzopkw{padding:100px 0 24px}}.c-bzrsgE{max-width:948px;width:100%;margin-left:auto;margin-right:auto;position:relative}.c-bzrsgE .github-btn{position:absolute;bottom:0;left:50%;translate:-50% 0}.c-cgYDvy{display:flex;justify-content:space-between;align-items:center;padding-top:150px;padding-bottom:150px}@media (max-width: 1024px){.c-cgYDvy{padding-top:80px;padding-bottom:80px}}@media (max-width: 743px){.c-cgYDvy{padding-top:60px;padding-bottom:60px;flex-direction:column}}.c-fAHQVo{display:flex;flex-direction:column;max-width:500px}.c-fAHQVo .Text_highlight{background-color:white;box-shadow:0px 2px 4px 0px rgba(11, 47, 97, 0.2);border-radius:4px;height:24px;padding:0 4px;color:#D02F61;display:inline-flex;align-items:center;justify-content:center}.c-fAHQVo a{width:-moz-fit-content;width:fit-content}@media (max-width: 743px){.c-fAHQVo h3,.c-fAHQVo a{text-align:center}}@media (max-width: 743px){.c-fAHQVo a{margin:var(--space-7) auto 0}}.c-kvmSnp{max-width:540px}@media (max-width: 743px){.c-kvmSnp{margin-top:var(--space-7);max-width:100%;width:80%}}.c-cPiarr{padding:64px 0 150px}@media (max-width: 1024px){.c-cPiarr{padding:44px 0 80px}}@media (max-width: 743px){.c-cPiarr{padding:24px 0 60px}}.c-iGygWh{position:relative;box-shadow:0 96px 45px -65px rgba(0, 0, 0, 0.2);border-radius:var(--radii-2)}.c-iGygWh .yt-lite{border-radius:var(--radii-2)}.c-iGygWh .yt-lite > .lty-playbtn{width:68px;height:48px;border:none;border-radius:25%}.c-iGygWh iframe{position:absolute;width:100%;height:100%;top:0;left:0;border-radius:var(--radii-2);box-shadow:var(--shadows-2)}.c-fVgYPG{position:relative}.c-fVgYPG .float-text{float:left;width:45%}@media (max-width: 1024px){.c-fVgYPG .float-text{width:100%}}.c-fxrEBZ{overflow:hidden}.c-hTKfPd{width:47%;padding-top:80px;padding-bottom:var(--space-9);float:left}@media (max-width: 1024px){.c-hTKfPd{width:100%;float:none;text-align:center}}.c-kBUzHM{float:right;margin-left:80px}@media (max-width: 1024px){.c-kBUzHM{float:none;margin-left:0}}.c-lcVFNy{position:relative;width:500px}@media (max-width: 1440px){.c-lcVFNy{width:unset}}.c-lcVFNy.light .hljs{background-color:var(--colors-white);color:var(--colors-textColor)}.c-lcVFNy button{position:absolute;right:12px;top:6px;background:none;border:none;cursor:pointer;height:42px;width:42px;transition:var(--transitions-main)}.c-lcVFNy button:hover{opacity:.6}.c-lcVFNy .copied{display:block;opacity:0;position:absolute;right:0;top:-20px;animation:k-iKtTFk 2.5s ease 0s alternate;color:var(--colors-secondary);font-size:var(--fontSizes-1);font-weight:var(--fontWeights-2);transition:var(--transitions-main);z-index:10}.c-lcVFNy .hljs{display:block;overflow-x:auto;color:#abb2bf;background:#282c34;margin-bottom:20px;border-radius:var(--radii-1);padding:var(--space-4)}.c-lcVFNy .hljs-comment,.c-lcVFNy .hljs-quote{color:#5c6370;font-style:italic}.c-lcVFNy .hljs-doctag,.c-lcVFNy .hljs-keyword,.c-lcVFNy .hljs-formula{color:#c678dd}.c-lcVFNy .hljs-section,.c-lcVFNy .hljs-name,.c-lcVFNy .hljs-selector-tag,.c-lcVFNy .hljs-deletion,.c-lcVFNy .hljs-subst{color:#e06c75}.c-lcVFNy .hljs-literal{color:#56b6c2}.c-lcVFNy .hljs-string,.c-lcVFNy .hljs-regexp,.c-lcVFNy .hljs-addition,.c-lcVFNy .hljs-attribute,.c-lcVFNy .hljs-meta-string{color:#98c379}.c-lcVFNy .hljs-built_in,.c-lcVFNy .hljs-class .hljs-title{color:#e6c07b}.c-lcVFNy .hljs-attr,.c-lcVFNy .hljs-variable,.c-lcVFNy .hljs-template-variable,.c-lcVFNy .hljs-type,.c-lcVFNy .hljs-selector-class,.c-lcVFNy .hljs-selector-attr,.c-lcVFNy .hljs-selector-pseudo,.c-lcVFNy .hljs-number{color:#d19a66}.c-lcVFNy .hljs-symbol,.c-lcVFNy .hljs-bullet,.c-lcVFNy .hljs-link,.c-lcVFNy .hljs-meta,.c-lcVFNy .hljs-selector-id,.c-lcVFNy .hljs-title{color:#61aeee}.c-lcVFNy .hljs-emphasis{font-style:italic}.c-lcVFNy .hljs-strong{font-weight:bold}.c-lcVFNy .hljs-link{text-decoration:underline}.c-kqsoHo{padding-top:150px;padding-bottom:150px}.c-kqsoHo .title{text-align:center;margin-bottom:100px}@media (max-width: 1024px){.c-kqsoHo{padding-top:80px;padding-bottom:80px}}@media (max-width: 1024px){.c-kqsoHo .title{margin-bottom:64px}}@media (max-width: 743px){.c-kqsoHo{padding-top:60px;padding-bottom:60px}}@media (max-width: 743px){.c-kqsoHo .title{margin-bottom:44px}}.c-eCJxrf{display:flex;align-items:center;justify-content:space-between}.c-eCJxrf .title-mobile{display:none}@media (max-width: 1024px){.c-eCJxrf{flex-wrap:wrap;align-items:flex-start}}@media (max-width: 1024px){.c-eCJxrf .title-mobile{display:block;width:100%}}@media (max-width: 743px){.c-eCJxrf{flex-direction:column;margin-bottom:60px}}.c-hMVjTK ul li{margin-bottom:var(--space-4);position:relative;padding-left:16px}.c-hMVjTK ul li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:var(--colors-secondary)}@media (max-width: 1024px){.c-hMVjTK ul{padding-top:var(--space-4)}}@media (max-width: 1024px){.c-hMVjTK{max-width:280px;width:100%}}@media (max-width: 1024px){.c-hMVjTK .title-desktop{display:none}}@media (max-width: 743px){.c-hMVjTK{max-width:100%;order:2}}.c-cQwwIn{display:flex;align-items:center;color:var(--colors-primary);font-family:var(--fonts-OpenSans600);text-decoration:none}.c-cQwwIn .icon{fill:var(--colors-primary);margin-left:var(--space-4)}.c-fagtgx{max-width:610px;width:100%;position:relative}.c-fagtgx img{width:100%;height:auto}@media (max-width: 1024px){.c-fagtgx{max-width:396px;margin-left:0}}@media (max-width: 743px){.c-fagtgx{max-width:100%;order:1}}.c-gsaXpY{padding:60px 0 100px;background-color:var(--colors-darkBlue);color:var(--colors-white)}@media (max-width: 1024px){.c-gsaXpY{padding:60px 0 26px}}.c-eOYVvF{display:flex;flex-wrap:wrap;margin-left:-24px}@media (max-width: 1024px){.c-eOYVvF{margin-left:-54px}}@media (max-width: 575px){.c-eOYVvF{margin-left:0}}.c-ekLXmJ{width:calc((100% / 4) - 24px);margin-left:24px;margin-bottom:24px}.c-ekLXmJ a{background-color:var(--colors-bigStone);display:block;height:100%;border-radius:var(--radii-1);transition:var(--transitions-main)}.c-ekLXmJ a .inner{padding:var(--space-6)}.c-ekLXmJ a:hover{background-color:var(--colors-bigStoneHover)}.c-ekLXmJ img{display:block;border-radius:var(--radii-1) var(--radii-1) 0 0;width:100%;height:auto}@media (max-width: 1024px){.c-ekLXmJ{width:calc((100% / 2) - 54px);margin-left:54px;margin-bottom:54px}}@media (max-width: 575px){.c-ekLXmJ{width:100%;margin-left:0;margin-bottom:24px}}.c-hPoOJn{padding-top:100px;padding-bottom:100px}.c-ckoDwU{display:flex;flex:1;box-shadow:var(--shadows-4);border-radius:var(--radii-2);padding:var(--space-6);background-color:var(--colors-white)}.c-covwPd{margin-bottom:var(--space-6);padding-left:var(--space-5);list-style-type:disc;font-size:var(--fontSizes-2)}.c-covwPd li{color:var(--colors-secondary)}.c-covwPd li:not(:last-child){margin-bottom:var(--space-3)}.c-covwPd li span{color:var(--colors-black700)}.c-eGPXIo{color:var(--colors-white);background-color:var(--colors-darkBlue);padding-top:60px;padding-bottom:60px}@media (max-width: 743px){.c-eGPXIo{padding-top:40px;padding-bottom:40px}}.c-eGPXIo .logo{min-width:45px;width:45px;height:auto}.c-eGPXIo .org{padding-left:24px}.c-eGPXIo .org-name{display:flex;align-items:center;font-size:40px}@media (max-width: 743px){.c-eGPXIo .org-name p,.c-eGPXIo .org-name span{font-size:var(--fontSizes-5)}}.c-eGPXIo .org-name :not(span){color:var(--colors-primary)}.c-eGPXIo .org-name span{padding:0 8px}.c-eGPXIo .highlight{max-width:428px;width:100%}@media (max-width: 1024px){.c-eGPXIo .highlight{margin-top:30px}}.c-eGPXIo .highlight .hljs{margin-bottom:0}.c-igsMin{width:calc(100% - 286px);padding:0 30px;border-left:1px solid var(--colors-grey);border-right:1px solid var(--colors-grey)}.c-igsMin .inner{margin:30px auto}@media (max-width: 1024px){.c-igsMin{width:100%;border:none;padding:0px}}.c-bzbvcg{margin:var(--space-10) auto;line-height:1.7;overflow-wrap:break-word}.c-bzbvcg h1,.c-bzbvcg h2,.c-bzbvcg h3,.c-bzbvcg h4,.c-bzbvcg h5,.c-bzbvcg h6{margin-top:var(--space-8);margin-bottom:var(--space-5)}.c-bzbvcg.blog-inner p,.c-bzbvcg.blog-inner ol,.c-bzbvcg.blog-inner ul,.c-bzbvcg.blog-inner a{font-family:var(--fonts-Lora)}.c-bzbvcg.blog-inner .blog-image{position:static !important}.c-bzbvcg h1{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-3);border-bottom:1px solid var(--colors-grey)}@media (max-width: 743px){.c-bzbvcg h1{font-size:var(--fontSizes-7)}}.c-bzbvcg h2{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-bzbvcg h2{font-size:var(--fontSizes-6)}}.c-bzbvcg h3{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-bzbvcg h3{font-size:var(--fontSizes-4)}}.c-bzbvcg p{margin-bottom:var(--space-5);font-size:var(--fontSizes-4);color:var(--colors-black800)}.c-bzbvcg ol,.c-bzbvcg ul{margin-bottom:var(--space-5);padding-left:var(--space-10);font-size:var(--fontSizes-3)}.c-bzbvcg ol li >ul,.c-bzbvcg ul li >ul{list-style-type:circle}.c-bzbvcg ol li:not(:last-child),.c-bzbvcg ul li:not(:last-child){margin-bottom:var(--space-2)}.c-bzbvcg ul{list-style-type:disc}.c-bzbvcg ul :has(input){list-style-type:none}.c-bzbvcg a{text-decoration:underline;word-break:break-all}.c-bzbvcg a:hover{color:var(--colors-primary)}.c-bzbvcg img{max-width:100%;height:auto}.c-bzbvcg strong{font-weight:var(--fontWeights-4)}.c-bzbvcg blockquote{padding-left:var(--space-9);border-left:2px solid var(--colors-grey);font-size:var(--fontSizes-2);font-style:italic}.c-bzbvcg em,.c-bzbvcg q{font-style:italic}.c-bzbvcg pre{font-size:var(--fontSizes-2);font-family:var(--fonts-Inconsolata);line-height:1.7;overflow:auto;white-space:pre;word-break:normal;margin:0 0 var(--space-5);padding:var(--space-10);color:var(--colors-darkGreyHover);background-color:var(--colors-lightGrey);border:none;border-radius:2px}.c-bzbvcg code{white-space:pre-wrap}.c-bzbvcg table{margin-bottom:var(--space-4)}.c-bzbvcg table,.c-bzbvcg th,.c-bzbvcg td{border:1px solid var(--colors-grey)}.c-bzbvcg th,.c-bzbvcg td{padding:6px 13px}.c-bzbvcg iframe{max-width:100%}.c-hhjAKd{width:286px;padding:30px;border-right:1px solid var(--colors-grey)}@media (max-width: 1024px){.c-hhjAKd{width:100%;border:none;padding:30px 0 0}}.c-bTiyxX{border-top:1px solid var(--colors-grey);margin-top:24px;padding-top:24px;padding-bottom:24px}@media (max-width: 1024px){.c-bTiyxX{border-bottom:1px solid var(--colors-grey)}}}--sxs{--sxs:3 c-dhzjXW-iTKOFX-direction-column c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-PJLV-ifRTQw-size-10 c-PJLV-dnvIlH-size-4 c-PJLV-ohsET-size-8 c-dhzjXW-ejCoEP-direction-row c-PJLV-ypqAk-size-3 c-PJLV-cBLpOj-size-2 c-PJLV-EDrvg-size-1 c-dhzjXW-bICGYT-justify-center c-PJLV-bWcAUL-size-9 c-bMqEGV-kwxnul-size-1 c-bMqEGV-gPhMRO-variant-outline c-bMqEGV-dxfqfJ-variant-primary c-PJLV-jszWhv-size-7 c-PJLV-hlFDyt-size-6 c-bBICiS-RBfUm-size-1 c-PJLV-iMgaFX-truncate-true c-PJLV-cllNQK-lineClamp-true c-dhzjXW-irEjuD-align-stretch c-dhzjXW-dOBaZS-gap-12 c-dhzjXW-knmidH-justify-between c-bMqEGV-jDfpYu-variant-secondary}@media{.c-dhzjXW-iTKOFX-direction-column{flex-direction:column}.c-dhzjXW-jroWjL-align-center{align-items:center}.c-dhzjXW-awKDG-justify-start{justify-content:flex-start}.c-dhzjXW-kVNAnR-wrap-noWrap{flex-wrap:nowrap}.c-PJLV-ifRTQw-size-10{font-size:var(--fontSizes-10);font-weight:var(--fontWeights-6);line-height:1.25}@media (max-width: 1024px){.c-PJLV-ifRTQw-size-10{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-ifRTQw-size-10{font-size:var(--fontSizes-7)}}.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-4)}@media (max-width: 1024px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-3)}}@media (max-width: 743px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-2)}}.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-5);line-height:1.5}@media (max-width: 743px){.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-7)}}.c-dhzjXW-ejCoEP-direction-row{flex-direction:row}.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-3)}@media (max-width: 743px){.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-2)}}.c-PJLV-cBLpOj-size-2{font-size:var(--fontSizes-2)}.c-PJLV-EDrvg-size-1{font-size:var(--fontSizes-1)}.c-dhzjXW-bICGYT-justify-center{justify-content:center}.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-9);font-weight:var(--fontWeights-6);line-height:normal}@media (max-width: 1024px){.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-7)}}.c-bMqEGV-kwxnul-size-1{height:53px;line-height:53px;font-size:var(--fontSizes-3);padding-left:var(--space-6);padding-right:var(--space-6)}@media (max-width: 1024px){.c-bMqEGV-kwxnul-size-1{height:50px;line-height:50px}}.c-bMqEGV-gPhMRO-variant-outline{background-color:var(--colors-white);color:#5865F2;border:1px solid #5865F2}.c-bMqEGV-gPhMRO-variant-outline:hover{background-color:#DEE0FC;border-color:var(--colors-primaryHover)}.c-bMqEGV-dxfqfJ-variant-primary{background-color:var(--colors-primary);color:var(--colors-white)}.c-bMqEGV-dxfqfJ-variant-primary:hover{background-color:var(--colors-primaryHover)}.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-2)}@media (max-width: 743px){.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-6)}}.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-4)}}.c-bBICiS-RBfUm-size-1 input{padding:17px 24px;font-size:var(--fontSizes-2);background-color:var(--colors-lightGrey)}.c-bBICiS-RBfUm-size-1 input:hover{background-color:var(--colors-lightGreyHover)}.c-PJLV-iMgaFX-truncate-true{max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.c-PJLV-cllNQK-lineClamp-true{display:-webkit-box;-webkit-line-clamp:var(---lineClamp);-webkit-box-orient:vertical;overflow:hidden}.c-dhzjXW-irEjuD-align-stretch{align-items:stretch}.c-dhzjXW-dOBaZS-gap-12{gap:var(--space-12)}.c-dhzjXW-knmidH-justify-between{justify-content:space-between}.c-bMqEGV-jDfpYu-variant-secondary{background-color:var(--colors-primaryLight);color:var(--colors-primary)}.c-bMqEGV-jDfpYu-variant-secondary:hover{background-color:var(--colors-primaryLightHover)}}--sxs{--sxs:4 c-dhzjXW-lelvPF-direction-columnReverse c-dhzjXW-cLKPGv-direction-row c-dhzjXW-ewYpoe-align-start c-dhzjXW-dAvekr-direction-column c-dhzjXW-dHjzmU-direction-column}@media{@media (max-width: 1024px){.c-dhzjXW-lelvPF-direction-columnReverse{flex-direction:column-reverse}}@media (max-width: 743px){.c-dhzjXW-cLKPGv-direction-row{flex-direction:row}}@media (max-width: 743px){.c-dhzjXW-ewYpoe-align-start{align-items:flex-start}}@media (max-width: 743px){.c-dhzjXW-dAvekr-direction-column{flex-direction:column}}@media (max-width: 1024px){.c-dhzjXW-dHjzmU-direction-column{flex-direction:column}}}--sxs{--sxs:6 c-hCkBfr-ilkBNdM-css c-kPczbf-igSvvfr-css c-kPczbf-idOghFk-css c-fOPBY-ieBuAdh-css c-dhzjXW-ielgvHS-css c-PJLV-ietOTGQ-css c-PJLV-ibrRWTZ-css c-hCkBfr-ikYMMba-css c-PJLV-igHOLBX-css c-PJLV-ijskIFF-css c-PJLV-ijpcBqa-css c-hJzboU-ieOwGK-css c-PJLV-idgasAY-css c-PJLV-ieFlRcC-css c-PJLV-iciGHnC-css c-hJzboU-icdsEyN-css c-hJzboU-iYSetQ-css c-hJzboU-iebhThe-css c-hJzboU-ifZMaZh-css c-hJzboU-ieVlAUk-css c-dhzjXW-iguWOGj-css c-PJLV-iftFgSo-css c-PJLV-idSlxtB-css c-PJLV-ibWHAEn-css c-fXQtST-ijfuMJQ-css c-TJTRd-ihKsdEP-css c-bMqEGV-ihZnFyC-css c-TJTRd-ihYCfXG-css c-TJTRd-igEZsbb-css c-PJLV-ifPSeYF-css c-PJLV-ibZBPXN-css c-PJLV-iejLhMq-css c-bMqEGV-icGsXGD-css c-PJLV-ieRZfPH-css c-PJLV-iUazGY-css c-PJLV-ikMWyde-css c-PJLV-iedNKou-css c-PJLV-iePtfAL-css c-bMqEGV-idKitzl-css c-PJLV-iqxEHy-css c-PJLV-ihdEvWZ-css c-bMqEGV-ibyPveq-css c-PJLV-ieBSoMY-css c-PJLV-igjdJOs-css c-PJLV-ihMnYQP-css c-PJLV-igVRrzN-css c-PJLV-iiMrHxl-css c-cQwwIn-ihZnFyC-css c-PJLV-iiBvIAS-css c-PJLV-ieNyba-css c-dhzjXW-ieHGUxU-css c-dhzjXW-iejLhMq-css c-PJLV-idHAijf-css c-PJLV-ieXfYvc-css c-PJLV-ijoSYcG-css c-bMqEGV-icBLpOj-css c-PJLV-ihjojsL-css c-PJLV-ikGRPtV-css c-PJLV-ignELVg-css}@media{.c-hCkBfr-ilkBNdM-css{height:100%}.c-kPczbf-igSvvfr-css{display:none}@media (max-width: 1024px){.c-kPczbf-igSvvfr-css{display:block;padding:var(--space-5)}}.c-kPczbf-idOghFk-css{flex:1}@media (max-width: 1024px){.c-kPczbf-idOghFk-css{display:none}}.c-fOPBY-ieBuAdh-css{margin-left:auto;padding:var(--space-3)}.c-dhzjXW-ielgvHS-css{padding-top:80px;padding-bottom:80px}.c-PJLV-ietOTGQ-css{margin-bottom:var(--space-6)}.c-PJLV-ibrRWTZ-css{max-width:848px}.c-hCkBfr-ikYMMba-css{position:relative;z-index:2}@media (max-width: 1024px){.c-PJLV-igHOLBX-css{text-align:center}}.c-PJLV-ijskIFF-css{font-weight:var(--fontWeights-3);padding:var(--space-3) 0 var(--space-2)}.c-PJLV-ijpcBqa-css{color:var(--colors-white700)}.c-hJzboU-ieOwGK-css{-webkit-mask-image:url(/images/static/about-us/shapes/1.svg);mask-image:url(/images/static/about-us/shapes/1.svg)}.c-PJLV-idgasAY-css{font-weight:var(--fontWeights-4)}.c-PJLV-ieFlRcC-css{padding-top:var(--space-2);padding-bottom:var(--space-2);font-weight:var(--fontWeights-3)}.c-PJLV-iciGHnC-css{color:var(--colors-black700)}.c-hJzboU-icdsEyN-css{-webkit-mask-image:url(/images/static/about-us/shapes/2.svg);mask-image:url(/images/static/about-us/shapes/2.svg)}.c-hJzboU-iYSetQ-css{-webkit-mask-image:url(/images/static/about-us/shapes/3.svg);mask-image:url(/images/static/about-us/shapes/3.svg)}.c-hJzboU-iebhThe-css{-webkit-mask-image:url(/images/static/about-us/shapes/5.svg);mask-image:url(/images/static/about-us/shapes/5.svg)}.c-hJzboU-ifZMaZh-css{-webkit-mask-image:url(/images/static/about-us/shapes/6.svg);mask-image:url(/images/static/about-us/shapes/6.svg)}.c-hJzboU-ieVlAUk-css{-webkit-mask-image:url(/images/static/about-us/shapes/7.svg);mask-image:url(/images/static/about-us/shapes/7.svg)}.c-dhzjXW-iguWOGj-css{height:100vh}.c-PJLV-iftFgSo-css{margin-bottom:var(--space-6);width:100%}.c-PJLV-idSlxtB-css{margin-bottom:56px;width:100%}.c-PJLV-ibWHAEn-css{margin-bottom:var(--space-6);font-weight:700;text-align:center}.c-fXQtST-ijfuMJQ-css{opacity:0;transition:var(--transitions-main)}.c-TJTRd-ihKsdEP-css{background:#DBEEFE}.c-bMqEGV-ihZnFyC-css{margin-top:var(--space-6);font-weight:var(--fontWeights-3)}.c-TJTRd-ihYCfXG-css{background:#E1DCFE}.c-TJTRd-igEZsbb-css{background:#FADAFE}.c-PJLV-ifPSeYF-css{color:var(--colors-white)}.c-PJLV-ibZBPXN-css{margin-bottom:var(--space-2);font-weight:var(--fontWeights-4)}.c-PJLV-iejLhMq-css{margin-bottom:var(--space-4)}.c-bMqEGV-icGsXGD-css{margin-top:var(--space-14)}.c-PJLV-ieRZfPH-css{text-align:center;margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-PJLV-iUazGY-css{display:flex;align-items:center}.c-PJLV-ikMWyde-css{margin-top:var(--space-6);margin-bottom:var(--space-6);---lineClamp:3;font-family:var(--fonts-Lora)}.c-PJLV-iedNKou-css{margin-bottom:48px}.c-PJLV-iePtfAL-css{font-weight:700}.c-bMqEGV-idKitzl-css{margin-bottom:var(--space-12)}.c-PJLV-iqxEHy-css{margin-bottom:var(--space-6);line-height:1.3;font-weight:800}.c-PJLV-ihdEvWZ-css{line-height:30px}.c-bMqEGV-ibyPveq-css{margin-top:var(--space-8)}.c-PJLV-ieBSoMY-css span{background-color:var(--colors-secondary);color:var(--colors-white)}.c-PJLV-igjdJOs-css{text-align:center}.c-PJLV-ihMnYQP-css{margin:12px 0;text-align:center}.c-PJLV-igVRrzN-css{margin-bottom:24px;text-align:center}.c-PJLV-iiMrHxl-css{margin-bottom:var(--space-6)}.c-PJLV-iiMrHxl-css strong{font-weight:var(--fontWeights-5)}.c-cQwwIn-ihZnFyC-css{margin-top:var(--space-6);font-weight:var(--fontWeights-3)}.c-PJLV-iiBvIAS-css{text-align:center;margin-bottom:var(--space-5)}.c-PJLV-ieNyba-css{margin:0 auto 60px;text-align:center;max-width:560px}@media (max-width: 1024px){.c-PJLV-ieNyba-css{margin-bottom:44px}}.c-dhzjXW-ieHGUxU-css{padding-bottom:104px}.c-dhzjXW-iejLhMq-css{margin-bottom:var(--space-4)}.c-PJLV-idHAijf-css{font-weight:var(--fontWeights-4);margin-left:var(--space-4)}.c-PJLV-ieXfYvc-css{margin-bottom:var(--space-5);color:var(--colors-black700)}.c-PJLV-ijoSYcG-css{margin-bottom:var(--space-6);font-weight:var(--fontWeights-3)}.c-bMqEGV-icBLpOj-css{font-size:var(--fontSizes-2)}.c-PJLV-ihjojsL-css{font-weight:var(--fontWeights-3)}.c-PJLV-ikGRPtV-css{font-weight:var(--fontWeights-5)}.c-PJLV-ignELVg-css{font-weight:var(--fontWeights-3);padding-bottom:var(--space-3)}}</style><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/a5b262126197d1b9.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/a5b262126197d1b9.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-a5cea090b09e5077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-64ec93240796860d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-e494fb9acc0620f0.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/962-688f32d50bed486d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bcategory%5D-23b87f99393a56cf.js" defer="" crossorigin=""></script><script src="/_next/static/tfECHI5ownHmk6F76cjV6/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/tfECHI5ownHmk6F76cjV6/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><main class="c-iSkjJi __className_e66fe9"><div class="c-guYHoi"><header class="c-ckOUFN"><div class="c-hCkBfr c-hCkBfr-ilkBNdM-css"><div class="c-ePqJJt"><div class="c-jafYUQ"><a class="logo" href="/"><img alt="AimStack" loading="lazy" width="156" height="37" decoding="async" data-nimg="1" class="logo-image next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 156 37&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/images/static/main/logo.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/static/main/logo.svg"/></a></div><nav class="c-BqIMD"><div class="nav-inner"><ul class="nav-list"><li><a target="_self" class="" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" class="" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" class="" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" class="" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" class="" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" class="active" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" class="" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" class="" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><div class="c-kPczbf c-kPczbf-igSvvfr-css"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div></div><ul class="c-lizetl"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></nav><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div><button type="button" aria-label="menu" class="c-fOPBY c-fOPBY-ieBuAdh-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:black" d="M0 146.286c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.289 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM0 512.008c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.29 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM60.952 816.748c-33.663 0-60.952 27.29-60.952 60.956 0 33.661 27.289 60.951 60.952 60.951h902.097c33.661 0 60.951-27.29 60.951-60.951 0-33.667-27.29-60.956-60.951-60.956h-902.097z"></path></svg></button></div></div></header><div class="c-cZmHrB"><div style="padding-block:100px" class="c-hCkBfr"><h1 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-ieRZfPH-css title">Category: <!-- -->New Releases</h1><ul class="c-cXVAvf"><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-4-0-to-a-new-repo-aimos-here-is-why"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim 4.0 to a new repo AimOS. Here is why?" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/images/dynamic/nextImageExportOptimizer/aimos-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/images/dynamic/nextImageExportOptimizer/aimos-opt-384.WEBP 1x, /images/dynamic/nextImageExportOptimizer/aimos-opt-640.WEBP 2x" src="/images/dynamic/nextImageExportOptimizer/aimos-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-4-0-to-a-new-repo-aimos-here-is-why">Aim 4.0 to a new repo AimOS. Here is why?</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-4-0-to-a-new-repo-aimos-here-is-why">We have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="AimOS: Open-source modular observability for AI Systems" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/images/dynamic/nextImageExportOptimizer/medium8-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/images/dynamic/nextImageExportOptimizer/medium8-opt-384.WEBP 1x, /images/dynamic/nextImageExportOptimizer/medium8-opt-640.WEBP 2x" src="/images/dynamic/nextImageExportOptimizer/medium8-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems">AimOS: Open-source modular observability for AI Systems</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems">Discover AimOS: Open-source modular observability for AI systems. Easily log, connect and observe any parts of your AI Systems from experiments to production to prompts to AI system monitoring</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-v3-16-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim v3.16 — Run messages in UI, TensorBoard real-time sync, integration with Hugging Face Datasets" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_G-AzW2_QT8nvsgLUQH9JeA-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_G-AzW2_QT8nvsgLUQH9JeA-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_G-AzW2_QT8nvsgLUQH9JeA-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_G-AzW2_QT8nvsgLUQH9JeA-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-v3-16-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets">Aim v3.16 — Run messages in UI, TensorBoard real-time sync, integration with Hugging Face Datasets</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-v3-16-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets">🚀 Aim v3.16 is out! Users will be able to view all Run messages right in the UI. Integration of Aim with TensorBoard, Hugging Face Datasets, Acme, Stable-Baselines3. </a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-2022-product-recap"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim 2022; Product Recap" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_mpgFSGReVaycf5644hM6xw-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_mpgFSGReVaycf5644hM6xw-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_mpgFSGReVaycf5644hM6xw-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_mpgFSGReVaycf5644hM6xw-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-2022-product-recap">Aim 2022; Product Recap</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-2022-product-recap">A retrospective look at the past year! Significant improvements enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-2022-community-report"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim 2022 community report" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_OvP3KCeUaBbqWj_jq4c5xg-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_OvP3KCeUaBbqWj_jq4c5xg-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_OvP3KCeUaBbqWj_jq4c5xg-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_OvP3KCeUaBbqWj_jq4c5xg-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-2022-community-report">Aim 2022 community report</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-2022-community-report">It’s time to review the 2022. In a year, Aim has become from a cool open-source project into a full-blown product. We have shipped 12 new Aim versions..</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-community-is-moving-to-discord-"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim Community is Moving to Discord!! 🎉" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_max_1400_1_c_ovadkKTTkZOokR3WiUTg-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_max_1400_1_c_ovadkKTTkZOokR3WiUTg-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_max_1400_1_c_ovadkKTTkZOokR3WiUTg-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_max_1400_1_c_ovadkKTTkZOokR3WiUTg-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-community-is-moving-to-discord-">Aim Community is Moving to Discord!! 🎉</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-community-is-moving-to-discord-">Aim community is officially moving to Discord!! 🎉 Let&#x27;s rock and build great things together!! The slack workspace is going to become deprecated by the end of 2022.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-3-15-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim 3.15 — Callbacks, logging and notifications, line chart legends, experiment page, Audios Explorer" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_max_1400_1_wQWwnwDeCSVQ-YbZapbHlQ-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_max_1400_1_wQWwnwDeCSVQ-YbZapbHlQ-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_max_1400_1_wQWwnwDeCSVQ-YbZapbHlQ-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_max_1400_1_wQWwnwDeCSVQ-YbZapbHlQ-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-3-15-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer">Aim 3.15 — Callbacks, logging and notifications, line chart legends, experiment page, Audios Explorer</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-3-15-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer">Aim 3.15 is out! Callbacks, Logging &amp; notifications, Line chart legends, Audios Explorer, Experiment page, Robust locking mechanism, PaddlePaddle &amp; Optuna integrations.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-3-14-brand-new-home-page"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim 3.14 — Brand new Home Page!" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_GEJ0_GtGyXC4sv2sZrSCRA-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_GEJ0_GtGyXC4sv2sZrSCRA-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_GEJ0_GtGyXC4sv2sZrSCRA-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_v2_resize_fit_1400_format_webp_1_GEJ0_GtGyXC4sv2sZrSCRA-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-3-14-brand-new-home-page">Aim 3.14 — Brand new Home Page!</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-3-14-brand-new-home-page">Hey team, Aim 3.14 is now available! We&#x27;re excited to introduce Brand new Home Page, MXNet and FastAI loggers, new tooltip positioning modes and ability to edit tags on the runs table!</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/new-releases/aim-3-13figures-explorer-and-notifications-on-stalled-runs"><noscript><style>
    .next-exported-image-blur-svg {
       filter: none !important;
    }
    </style></noscript><img alt="Aim 3.13–Figures Explorer and notifications on stalled runs" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(/nextImageExportOptimizer/miro.medium.com_max_1400_1_5hUcOciwufupoI4-nBbcyA-opt-10.WEBP);filter:url(#sharpBlur)" srcSet="/nextImageExportOptimizer/miro.medium.com_max_1400_1_5hUcOciwufupoI4-nBbcyA-opt-384.WEBP 1x, /nextImageExportOptimizer/miro.medium.com_max_1400_1_5hUcOciwufupoI4-nBbcyA-opt-640.WEBP 2x" src="/nextImageExportOptimizer/miro.medium.com_max_1400_1_5hUcOciwufupoI4-nBbcyA-opt-640.WEBP"/><svg style="border:0;clip:rect(0 0 0 0);height:0;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px"><filter id="sharpBlur"><feGaussianBlur stdDeviation="20" color-interpolation-filters="sRGB"></feGaussianBlur><feColorMatrix type="matrix" color-interpolation-filters="sRGB" values="1 0 0 0 0, 0 1 0 0 0, 0 0 1 0 0, 0 0 0 9 0"></feColorMatrix><feComposite in2="SourceGraphic" operator="in"></feComposite></filter></svg></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/new-releases"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>New Releases</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/new-releases/aim-3-13figures-explorer-and-notifications-on-stalled-runs">Aim 3.13–Figures Explorer and notifications on stalled runs</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-ikMWyde-css title"><a href="/blog/new-releases/aim-3-13figures-explorer-and-notifications-on-stalled-runs">In this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven’t had the chance to talk about much. </a></p><div class="c-hnRRWM"></div></div></div></li></ul><nav class="c-hQOWqi"><ul class="c-jiSXep"><li><a class="active" href="/blog/new-releases?page=1">1</a></li><li><a class="" href="/blog/new-releases?page=2">2</a></li><li><a class="" href="/blog/new-releases?page=3">3</a></li><li><a href="/blog/new-releases?page=2"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M663.040 548.198l36.198-36.198-289.638-289.638-72.397 72.397 217.19 217.242-217.19 217.242 72.397 72.397 253.44-253.44z"></path></svg></a></li></ul></nav></div></div><footer class="c-ghrOTq"><img alt="footer" fetchpriority="high" width="1440" height="246" decoding="async" data-nimg="1" class="c-kBJMsw next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1440 246&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/bg.eb38a857.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/bg.eb38a857.svg"/><div class="c-fcTZTi"><div class="c-hCkBfr c-gvTmKt"><div class="c-FsNLu"><div class="c-jgJYki"><a class="logo" href="/"><picture><source height="26" width="109" media="(max-width: 1199px)" srcSet="[object Object]"/><img alt="Aimstack" loading="lazy" width="26" height="26" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 26 26&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/aim-logo.e9fed5b2.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/aim-logo.e9fed5b2.svg"/></picture></a></div><ul class="c-bFmrFE"><li><a target="_self" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><ul class="c-MNZuo"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></div><div class="c-cChYXC"><p class="c-PJLV c-PJLV-EDrvg-size-1">Copyright © <!-- -->2023<!-- --> Aimstack</p></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"posts":[{"title":"Aim 1.3.5 — Activity View and X-axis alignment","date":"2021-02-05T11:16:41.748Z","author":"Gev Soghomonian","description":"Aim v1.3.5 is now available. Activity View on the Dashboard, X-axis alignment by epoch, Ordering runs both on Explore and on Dashboard. Check out!","slug":"aim-1-3-5-activity-view-and-x-axis-alignment","image":"/images/dynamic/1st.gif","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n## Aim Release — Activity View and X-axis alignment\n\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is now available. Thanks to the incredible [Aim community](https://discord.com/invite/zXq2NfVdtF) for the feedback and support on building democratized open source AI dev tools.\n\nCheck out the new features at play.aimstack.io\n\nHere are the highlights of the features:\n\n## Activity View on the Dashboard\n\nThe **Activity View** shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away\n\n**Statistics** displays the overall count of [Experiments and Runs. ](https://github.com/aimhubio/aim#concepts)\n\n![](/images/dynamic/1st.gif \"The new dashboard in action!\")\n\n## **X-axis alignment by epoch**\n\nX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.\n\n![](/images/dynamic/2nd.gif \"No more misaligned metrics!\")\n\n## **Ordering runs both on Explore and on Dashboard**\n\n\n\nNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\n\nOn Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\n\n![](/images/dynamic/3.gif \"Sort the runs both on Dashboard and Explore!\")\n\n## Learn More\n\n\n\nWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\n\nWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month 😊). Try out Aim, join the [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support 🙌.","html":"\u003ch2\u003eAim Release — Activity View and X-axis alignment\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e v1.3.5 is now available. Thanks to the incredible \u003ca href=\"https://discord.com/invite/zXq2NfVdtF\"\u003eAim community\u003c/a\u003e for the feedback and support on building democratized open source AI dev tools.\u003c/p\u003e\n\u003cp\u003eCheck out the new features at play.aimstack.io\u003c/p\u003e\n\u003cp\u003eHere are the highlights of the features:\u003c/p\u003e\n\u003ch2\u003eActivity View on the Dashboard\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eActivity View\u003c/strong\u003e shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStatistics\u003c/strong\u003e displays the overall count of \u003ca href=\"https://github.com/aimhubio/aim#concepts\"\u003eExperiments and Runs. \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1st.gif\" alt=\"\" title=\"The new dashboard in action!\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eX-axis alignment by epoch\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/2nd.gif\" alt=\"\" title=\"No more misaligned metrics!\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eOrdering runs both on Explore and on Dashboard\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\u003c/p\u003e\n\u003cp\u003eOn Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/3.gif\" alt=\"\" title=\"Sort the runs both on Dashboard and Explore!\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\u003c/p\u003e\n\u003cp\u003eWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month 😊). Try out Aim, join the \u003ca href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e"},"_id":"posts/aim-1-3-5-—-activity-view-and-x-axis-alignment.md","_raw":{"sourceFilePath":"posts/aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileName":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-1-3-5-—-activity-view-and-x-axis-alignment"},"type":"Post"},{"title":"Aim 1.3.8 — Enhanced Context Table and Advanced Group Coloring","date":"2021-03-01T11:09:06.858Z","author":"Gev Soghomonian","description":"Discover Aim 1.3.8's Exciting Features: Advanced Group Coloring \u0026 Enhanced Context Table. Streamlined AI metrics visualization for efficient analysis. Explore now!","slug":"aim-1-3-8-enhanced-context-table-and-advanced-group-coloring","image":"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n[Aim](https://aimstack.io/) 1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\n\nCheck out the new features at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=).\n\nHere are the more notable changes:\n\n### Enhanced Context Table\n\n\n\nThe context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\n\nSo we have made 3 changes to tackle this problem\n\n### New table groups view\n\nBelow is the new modified look of the table. Here is what’s changed:\n\n* The empty per group is removed\n* In addition to the group details popover, there is an in-place list group config is available for instant lookup\n\n![](https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp \"New improved Aim Context Table on Explore\")\n\n\n\n* **\\[In Progress for next release]** Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\n\n### Column Management\n\n\n\nThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\n\nYou can also re-order the columns by dragging the mid-section up/down.\n\nThis is super-handy when dealing with 100s of columns on the table for parameters.\n\n![](https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp \"Drag columns left and right to pin, search and disable less important ones\")\n\n\n\n### Advanced Group Coloring\n\n\n\nThis is the latest iteration over the group coloring.\n\n* Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\n* You can shuffle the colors in case they aren’t pick to your liking\n* You can pick two different color palette for better distribution of the colors.\n\n  ![](https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp \"Effective coloring of the groups is important!\")\n\n  ### Thanks to\n* [mahnerak](https://github.com/mahnerak), Lars, [Joe](https://github.com/jafioti) for detailed feedback — helped us immensely in this iteration.\n\n  ### Learn More\n\n  We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\n\n  Try out [Aim](https://aimstack.io/), join the [Aim community](https://slack.aimstack.io/), share your feedback.\n\n  We are building the next generation of democratized AI dev tools.\n\n  And don’t forget to leave [Aim](https://aimstack.io/) a star on GitHub for support 🙌.","html":"\u003cp\u003e\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e 1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\u003c/p\u003e\n\u003cp\u003eCheck out the new features at \u003ca href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere are the more notable changes:\u003c/p\u003e\n\u003ch3\u003eEnhanced Context Table\u003c/h3\u003e\n\u003cp\u003eThe context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\u003c/p\u003e\n\u003cp\u003eSo we have made 3 changes to tackle this problem\u003c/p\u003e\n\u003ch3\u003eNew table groups view\u003c/h3\u003e\n\u003cp\u003eBelow is the new modified look of the table. Here is what’s changed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe empty per group is removed\u003c/li\u003e\n\u003cli\u003eIn addition to the group details popover, there is an in-place list group config is available for instant lookup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp\" alt=\"\" title=\"New improved Aim Context Table on Explore\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e[In Progress for next release]\u003c/strong\u003e Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eColumn Management\u003c/h3\u003e\n\u003cp\u003eThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\u003c/p\u003e\n\u003cp\u003eYou can also re-order the columns by dragging the mid-section up/down.\u003c/p\u003e\n\u003cp\u003eThis is super-handy when dealing with 100s of columns on the table for parameters.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp\" alt=\"\" title=\"Drag columns left and right to pin, search and disable less important ones\"\u003e\u003c/p\u003e\n\u003ch3\u003eAdvanced Group Coloring\u003c/h3\u003e\n\u003cp\u003eThis is the latest iteration over the group coloring.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNow Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can shuffle the colors in case they aren’t pick to your liking\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can pick two different color palette for better distribution of the colors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp\" alt=\"\" title=\"Effective coloring of the groups is important!\"\u003e\u003c/p\u003e\n\u003ch3\u003eThanks to\u003c/h3\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mahnerak\"\u003emahnerak\u003c/a\u003e, Lars, \u003ca href=\"https://github.com/jafioti\"\u003eJoe\u003c/a\u003e for detailed feedback — helped us immensely in this iteration.\u003c/p\u003e\n\u003ch3\u003eLearn More\u003c/h3\u003e\n\u003cp\u003eWe have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eWe are building the next generation of democratized AI dev tools.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","_raw":{"sourceFilePath":"posts/aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileName":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring"},"type":"Post"},{"title":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","date":"2021-05-18T13:58:29.035Z","author":"Gev Soghomonian","description":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","slug":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0oLJAp9haSFzb92CSSyAg.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) 2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\n\nCheck out the updated Aim at [play.aimstack.io](http://play.aimstack.io:10001/).\n\nFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes [here](https://github.com/aimhubio/aim/milestone/6?closed=1).\n\n## XGBoost Integration\n\nTo begin with,  `aim.callback` is now available that exports `AimCallback` to be passed to the `xgb.train` as a callback to log the experiments.\n\nCheck out this [blogpost](https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library) for additional details on how to integrate Aim to your XGBoost code.\n\n## Confidence Interval as the aggregation method\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyjcE9k4-cZGl5g03F3Mjg.png)\n\nNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups. \n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nCheck out the latest [Aim integration](https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration)!\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e 2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim at \u003ca href=\"http://play.aimstack.io:10001/\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes \u003ca href=\"https://github.com/aimhubio/aim/milestone/6?closed=1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eXGBoost Integration\u003c/h2\u003e\n\u003cp\u003eTo begin with,  \u003ccode\u003eaim.callback\u003c/code\u003e is now available that exports \u003ccode\u003eAimCallback\u003c/code\u003e to be passed to the \u003ccode\u003exgb.train\u003c/code\u003e as a callback to log the experiments.\u003c/p\u003e\n\u003cp\u003eCheck out this \u003ca href=\"https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library\"\u003eblogpost\u003c/a\u003e for additional details on how to integrate Aim to your XGBoost code.\u003c/p\u003e\n\u003ch2\u003eConfidence Interval as the aggregation method\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyjcE9k4-cZGl5g03F3Mjg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups. \u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eCheck out the latest \u003ca href=\"https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration\"\u003eAim integration\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","_raw":{"sourceFilePath":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileName":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements"},"type":"Post"},{"title":"Aim 2022 community report","date":"2023-01-05T21:48:26.367Z","author":"Gev Soghomonian","description":"It’s time to review the 2022. In a year, Aim has become from a cool open-source project into a full-blown product. We have shipped 12 new Aim versions..","slug":"aim-2022-community-report","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OvP3KCeUaBbqWj_jq4c5xg.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey lovely Aimers, Happy New Year! 🎆\n\nNow that the 2022 is over, we are excited to share what has happened in 2022. We have achieved a lot together!\n\n# TLDR — Highlights\n\nIn 2022 we have taken Aim to completely different level. It has become from a cool open-source project into a full-blown product. An inspiring community-led effort!!\n\n* We have shipped [12 new Aim versions](https://github.com/aimhubio/aim/releaseshttps://github.com/aimhubio/aim/releases) and countless other patches\n* Almost every week for the past 80+ weeks a new Aim version has been shipped — **a blistering pace of innovation!! 😅**\n* Aim contributors have doubled to 48 in 2022\n* ~1300 GitHub issues have been created in 2022 and over 1100 of them have been resolved.\n* We have reached 3K stars on GitHub. ⭐️ (If you like our work, [drop us a star!](https://github.com/aimhubio/aim) )\n* We [launched](https://medium.com/aimstack/aim-community-is-moving-to-discord-da24a52169db) our community on [discord](https://community.aimstack.io/).\n\n# Product\n\nWe have shipped 100s of major features in 2022. It would take several chapters to talk about all of them.\n\nHowever, here are two fundamental features that we haven’t talked much about yet.\n\n## Base Explorer\n\n* A foundational work to help compare any type of renderable ML metadata at mass.\n* This is the first experimental step we have made to turn Aim into a one-stop-shop for multidimensional metadata analysis.\n\nThe Figures explorer runs on Base Explorer\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRbl_iVzy8FwRfj6eZWffA.png)\n\n## Aim Callbacks\n\nA surgical intervention into your ML runs. For everything!!\n\n* Aim Users are using the callbacks to create policies to automatically manage trainings. A major step towards reproducibility and true training management. Hours of GPU-time saved at the worst case scenario!!\n\nWe are going to talk a lot about the callbacks too! 🙌\n\nMore about callbacks [here](https://aimstack.readthedocs.io/en/latest/using/callbacks.html#)\n\n## Aim Roadmap\n\nHere is how the public Aim roadmap looks like now. This is just the subset of the additions made. A monumental work!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rYmolGfB_bEjGHFb4TB5Q.png)\n\n# Integrations\n\nAim is heavily integrated with the AI ecosystem now.\n\n* in 2022 Aim ecosystem integrations have grown 3x\n* ~10 more integrations are also in progress right now ❤️\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EMYy1Eh6qYqrLZo2i6UPw.png)\n\n\n\n* Lots of impactful repos have adopted Aim as well. Including these ones: [HF Accelerate](https://github.com/huggingface/accelerate), [FAIR Metaseq](https://github.com/facebookresearch/metaseqhttps://github.com/facebookresearch/metaseq) (default tracker), [FAIR Fairseq](https://github.com/facebookresearch/fairseq), [Ludwig](https://github.com/ludwig-ai/ludwig) etc\n* Community-requested Aim converters for Mlflow, TensorBoard and [Wandb](https://medium.com/aimstack/aim-tutorial-for-weights-and-biases-users-bfbdde76f21e?source=collection_home---4------0-----------------------).\n\n# Onwards to 2023\n\nIn 2023 we are going to work hard to put Aim into the hands of as many researchers and MLOps engineers as possible!\n\nSoon we will publish the Aim roadmap of 2023! Lots to share. :)\n\n**2023 here we come! 😊**","html":"\u003cp\u003eHey lovely Aimers, Happy New Year! 🎆\u003c/p\u003e\n\u003cp\u003eNow that the 2022 is over, we are excited to share what has happened in 2022. We have achieved a lot together!\u003c/p\u003e\n\u003ch1\u003eTLDR — Highlights\u003c/h1\u003e\n\u003cp\u003eIn 2022 we have taken Aim to completely different level. It has become from a cool open-source project into a full-blown product. An inspiring community-led effort!!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe have shipped \u003ca href=\"https://github.com/aimhubio/aim/releaseshttps://github.com/aimhubio/aim/releases\"\u003e12 new Aim versions\u003c/a\u003e and countless other patches\u003c/li\u003e\n\u003cli\u003eAlmost every week for the past 80+ weeks a new Aim version has been shipped — \u003cstrong\u003ea blistering pace of innovation!! 😅\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eAim contributors have doubled to 48 in 2022\u003c/li\u003e\n\u003cli\u003e~1300 GitHub issues have been created in 2022 and over 1100 of them have been resolved.\u003c/li\u003e\n\u003cli\u003eWe have reached 3K stars on GitHub. ⭐️ (If you like our work, \u003ca href=\"https://github.com/aimhubio/aim\"\u003edrop us a star!\u003c/a\u003e )\u003c/li\u003e\n\u003cli\u003eWe \u003ca href=\"https://medium.com/aimstack/aim-community-is-moving-to-discord-da24a52169db\"\u003elaunched\u003c/a\u003e our community on \u003ca href=\"https://community.aimstack.io/\"\u003ediscord\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eProduct\u003c/h1\u003e\n\u003cp\u003eWe have shipped 100s of major features in 2022. It would take several chapters to talk about all of them.\u003c/p\u003e\n\u003cp\u003eHowever, here are two fundamental features that we haven’t talked much about yet.\u003c/p\u003e\n\u003ch2\u003eBase Explorer\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA foundational work to help compare any type of renderable ML metadata at mass.\u003c/li\u003e\n\u003cli\u003eThis is the first experimental step we have made to turn Aim into a one-stop-shop for multidimensional metadata analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe Figures explorer runs on Base Explorer\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRbl_iVzy8FwRfj6eZWffA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAim Callbacks\u003c/h2\u003e\n\u003cp\u003eA surgical intervention into your ML runs. For everything!!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim Users are using the callbacks to create policies to automatically manage trainings. A major step towards reproducibility and true training management. Hours of GPU-time saved at the worst case scenario!!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe are going to talk a lot about the callbacks too! 🙌\u003c/p\u003e\n\u003cp\u003eMore about callbacks \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/callbacks.html#\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eAim Roadmap\u003c/h2\u003e\n\u003cp\u003eHere is how the public Aim roadmap looks like now. This is just the subset of the additions made. A monumental work!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rYmolGfB_bEjGHFb4TB5Q.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eIntegrations\u003c/h1\u003e\n\u003cp\u003eAim is heavily integrated with the AI ecosystem now.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ein 2022 Aim ecosystem integrations have grown 3x\u003c/li\u003e\n\u003cli\u003e~10 more integrations are also in progress right now ❤️\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EMYy1Eh6qYqrLZo2i6UPw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLots of impactful repos have adopted Aim as well. Including these ones: \u003ca href=\"https://github.com/huggingface/accelerate\"\u003eHF Accelerate\u003c/a\u003e, \u003ca href=\"https://github.com/facebookresearch/metaseqhttps://github.com/facebookresearch/metaseq\"\u003eFAIR Metaseq\u003c/a\u003e (default tracker), \u003ca href=\"https://github.com/facebookresearch/fairseq\"\u003eFAIR Fairseq\u003c/a\u003e, \u003ca href=\"https://github.com/ludwig-ai/ludwig\"\u003eLudwig\u003c/a\u003e etc\u003c/li\u003e\n\u003cli\u003eCommunity-requested Aim converters for Mlflow, TensorBoard and \u003ca href=\"https://medium.com/aimstack/aim-tutorial-for-weights-and-biases-users-bfbdde76f21e?source=collection_home---4------0-----------------------\"\u003eWandb\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eOnwards to 2023\u003c/h1\u003e\n\u003cp\u003eIn 2023 we are going to work hard to put Aim into the hands of as many researchers and MLOps engineers as possible!\u003c/p\u003e\n\u003cp\u003eSoon we will publish the Aim roadmap of 2023! Lots to share. :)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2023 here we come! 😊\u003c/strong\u003e\u003c/p\u003e"},"_id":"posts/aim-2022-community-report.md","_raw":{"sourceFilePath":"posts/aim-2022-community-report.md","sourceFileName":"aim-2022-community-report.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2022-community-report"},"type":"Post"},{"title":"Aim 2022; Product Recap","date":"2023-01-20T06:16:21.056Z","author":"Gor Arakelyan","description":"A retrospective look at the past year! Significant improvements enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.","slug":"aim-2022-product-recap","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mpgFSGReVaycf5644hM6xw.png","draft":false,"categories":["New Releases"],"body":{"raw":"# A retrospective look at the past year\n\n\n\nAim underwent significant improvements in the past year. Over the course of 50 releases, including 12 minor releases and 38 patch releases, Aim received contributions from 35 total contributors, including 18 new external contributors. These contributions included 769 merged pull requests and 487 submitted issues!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhiLjCXeyDNrMIuZU5_SXg.png \"Aim contributions pulse\")\n\nThere was a significant progress in various areas of the Aim, including adding more supported platforms, taking the UI to the whole new level, adding remote tracking capabilities, revamping the CLI, extending the QL and moving towards active monitoring for optimizing large-scale trainings.\n\nIn addition to adding new features and extending Aim, improvements in performance and usability were applied to enhance the overall user experience. Furthermore, new integrations with machine learning tools were implemented to make it easier to use Aim in a broader technology stack\n\nOverall, these improvements greatly enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.\n\n# Aim Community\n\n![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GitHotOjlY9WPsvQmWaqwQ.png \"Aim contributors\")\n\n\n\n\u003e We are on a mission to democratize AI dev tools!\n\nNumerous external contributions were pushed to “main” in 2022. Without community support, Aim would not be where it is today.\n\nCongratulations to [Sharathmk99](https://github.com/Sharathmk99), [YodaEmbedding](https://github.com/YodaEmbedding), [hjoonjang](https://github.com/hjoonjang), [jangop](https://github.com/jangop), [djwessel](https://github.com/djwessel), [GeeeekExplorer](https://github.com/GeeeekExplorer), [dsblank](https://github.com/dsblank), [timokau](https://github.com/timokau), [kumarshreshtha](https://github.com/kumarshreshtha), [kage08](https://github.com/kage08), [karan2801](https://github.com/karan2801), [hendriks73](https://github.com/hendriks73), [yeghiakoronian](https://github.com/yeghiakoronian), [uduse](https://github.com/uduse), [arnauddhaene](https://github.com/arnauddhaene), [lukoucky](https://github.com/lukoucky), [Arvind644](https://github.com/Arvind644), [shrinandj](https://github.com/shrinandj) for their first contributions over the past year! 🎉\n\nThese contributions have covered a range of areas, including documentation, user guides, the SDK and the UI!\n\n# Highlights\n\n* Supported platforms were expanded to include Docker, Apple M1, Conda, Google Colab and Jupyter Notebooks.\n* The UI was given a makeover, with upgraded home and run single pages, as well as brand new experiment page. The new audio and figure explorers now allow for the exploration of more types of metadata.\n* Aim remote tracking server was rolled out to enable tracking from multi-host environments.\n* Active monitoring capabilities were enabled, such as notifications for stalled runs, and the ability to programmatically define training heuristics using callbacks and notifications.\n* Aim was integrated with 9 different machine learning frameworks alongside with 3 convertors that help to migrate from other experiment trackers.\n* Last but not least, the documentation was completely revamped, including almost 40 pages of guides towards helping in effectively using and understanding Aim.\n\n# **Supported Platforms**\n\n\n\nImprovements were made to ensure Aim works seamlessly with widely used environments and platforms in ML/MLOps field, including adding support for new platforms.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYG-SSawh8Q8jte6BjyNMQ.png)\n\n\n\n* Support for running Aim on Apple M1 devices.\n* Support for using Aim inside Google Colab and Jupyter notebooks.\n* Docker images for Aim UI and server.\n* Support for running Aim in Kubernetes clusters.\n* Aim became available in Conda.\n\n# User Interface\n\n\n\nThe UI is one of key interfaces to interact with the ML training runs tracked by Aim. It’s super-powerful for comparing large number of trainings. The UI was significantly improved in the past year with new pages and explorers to help to explore more metadata types.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odqgX33xLVzYBI6sfRYRWQ.png \"Home Page\")\n\n\n\n* Revamped home page to see project’s contributions and overall statistics.\n* Revamped run page to deep-dive into an individual run.\n* Brand new experiment page to view experiment’s details, attached runs, etc.\n* New explorers, such as the “Figures Explorer” and “Audio Explorer”, to allow a cross-run comparison of Plotly figures and audio objects respectively.\n* “Metrics Explorer” key enhancements, including displaying chart legends and the ability of exporting charts as images.\n\n\n\n# Command Line Interface\n\n\n\nAim CLI offers a simple interface to easily manage tracked trainings. Two key groups of commands were added:\n\n* **Runs management** to enable the base operations for managing trainings — list, copy, remove, move, upload, close.\n* **Storage management** to help to manage Aim data storage, such as reindexing, pruning, managing data format versions (advanced usage).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZFkXFzmz5BxGYofUgWdTg.png)\n\n# Query Language\n\nAim enables a powerful query language to filter through all the stored metadata using python expressions. A few additions were done providing a more friendly and pythonic interface for working with date-time expressions, as well as querying runs based on its metrics results.\n\n![](https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6J9gouDEt9Mkhv9Cds4dNw.png)\n\n# Remote Tracking\n\nThe Aim remote tracking server, which allows running trainings in a multi-host environment and collect tracked data in a centralized location, used to be one of the most requested features.\n\nIt has been gradually rolled out from its experimental phase to a stable version that can scale up and handle an increasing number of parallel trainings.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CizfTBF8Nh4DZcun6TP8Pg.png)\n\nSwitching from a local environment to a centralized server is as easy as pie, as zero code changes are required.⚡\\\nThis is because the interfaces are completely compatible!\n\n# Active Monitoring\n\nAim made it first steps towards optimizing conducting long and large scale trainings, such as training or tuning GPT-like models. It reduces human-hours spent monitoring/restarting runs, improves reproducibility and ramps up time for people who need to train these models:\n\n* Ability to notify on stalled/stuck runs.\n* Callbacks and notifications to define training heuristics programmatically.\n\nA quote from “Scaling Laws for Generative Mixed-Modal Language Models” (Aghajanyan et al., 2023, [arXiv:2301.03728](https://arxiv.org/abs/2301.03728)):\n\n\u003e We tracked all experiments using the Aim experiment tracker (Arakelyan et al., 2020). To ensure consistent training strategies across our experiments, we implemented a model restart policy using the Aim experiment tracker and callbacks. Specifically, if training perplexities do not decrease after 500 million tokens, the training run is restarted with a reduced learning rate with a factor of 0.8 of the current time step. This policy helps remove variance in the scaling laws due to differences in training procedures and allows us to scale up the number of asynchronous experiments significantly. All experiments were conducted in a two-month time frame with a cluster of 768 80GB A100 GPUs. The majority of experiments used 64 GPUs at a time.\n\n# Key Performance and Usability Optimizations\n\n* Storage: long metrics sampling and retrieval (\u003e1M steps).\n* Storage: robust locking mechanism and automatic background indexing.\n* UI: The v3.12 milestone was released, addressing over 30 usability issues.\n* UI: virtualizations in various places across the UI to improve responsiveness when displaying large amount of data.\n* UI: optimizations in stream decoding and data encoding to enhance overall performance.\n* UI: live update optimizations to effectively update and display real-time data.\n\n# Integrations\n\nAim easily integrates with a large number of widely adopted machine learning frameworks and tools, reducing barriers to get started with Aim. During the past year, a number of integrations were added:\n\n* **ML Frameworks** — Pytorch Ignite, CatBoost, LightGBM, KerasTuner, fastai, MXNet, Optuna, PaddlePaddle\n* **Convertors** to easily migrate from other tools **—** TensorBoard to Aim, MLFlow to Aim, WandB to Aim.\n* **Tools that integrated Aim** — Meta’s fairseq and metaseq frameworks, HuggingFace’s accelerate, Ludwig and Kedro.\n\n![](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*YT4Yf1sJUqQLWRtiF8fJgQ.png)\n\n# Learn More\n\n\n\nIf you have any questions join [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. 🙌\n\nShow some love by dropping a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003ch1\u003eA retrospective look at the past year\u003c/h1\u003e\n\u003cp\u003eAim underwent significant improvements in the past year. Over the course of 50 releases, including 12 minor releases and 38 patch releases, Aim received contributions from 35 total contributors, including 18 new external contributors. These contributions included 769 merged pull requests and 487 submitted issues!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhiLjCXeyDNrMIuZU5_SXg.png\" alt=\"\" title=\"Aim contributions pulse\"\u003e\u003c/p\u003e\n\u003cp\u003eThere was a significant progress in various areas of the Aim, including adding more supported platforms, taking the UI to the whole new level, adding remote tracking capabilities, revamping the CLI, extending the QL and moving towards active monitoring for optimizing large-scale trainings.\u003c/p\u003e\n\u003cp\u003eIn addition to adding new features and extending Aim, improvements in performance and usability were applied to enhance the overall user experience. Furthermore, new integrations with machine learning tools were implemented to make it easier to use Aim in a broader technology stack\u003c/p\u003e\n\u003cp\u003eOverall, these improvements greatly enhanced the functionality and usability of Aim for tracking machine learning experiments for both small and large scale projects.\u003c/p\u003e\n\u003ch1\u003eAim Community\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GitHotOjlY9WPsvQmWaqwQ.png\" alt=\"\" title=\"Aim contributors\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNumerous external contributions were pushed to “main” in 2022. Without community support, Aim would not be where it is today.\u003c/p\u003e\n\u003cp\u003eCongratulations to \u003ca href=\"https://github.com/Sharathmk99\"\u003eSharathmk99\u003c/a\u003e, \u003ca href=\"https://github.com/YodaEmbedding\"\u003eYodaEmbedding\u003c/a\u003e, \u003ca href=\"https://github.com/hjoonjang\"\u003ehjoonjang\u003c/a\u003e, \u003ca href=\"https://github.com/jangop\"\u003ejangop\u003c/a\u003e, \u003ca href=\"https://github.com/djwessel\"\u003edjwessel\u003c/a\u003e, \u003ca href=\"https://github.com/GeeeekExplorer\"\u003eGeeeekExplorer\u003c/a\u003e, \u003ca href=\"https://github.com/dsblank\"\u003edsblank\u003c/a\u003e, \u003ca href=\"https://github.com/timokau\"\u003etimokau\u003c/a\u003e, \u003ca href=\"https://github.com/kumarshreshtha\"\u003ekumarshreshtha\u003c/a\u003e, \u003ca href=\"https://github.com/kage08\"\u003ekage08\u003c/a\u003e, \u003ca href=\"https://github.com/karan2801\"\u003ekaran2801\u003c/a\u003e, \u003ca href=\"https://github.com/hendriks73\"\u003ehendriks73\u003c/a\u003e, \u003ca href=\"https://github.com/yeghiakoronian\"\u003eyeghiakoronian\u003c/a\u003e, \u003ca href=\"https://github.com/uduse\"\u003euduse\u003c/a\u003e, \u003ca href=\"https://github.com/arnauddhaene\"\u003earnauddhaene\u003c/a\u003e, \u003ca href=\"https://github.com/lukoucky\"\u003elukoucky\u003c/a\u003e, \u003ca href=\"https://github.com/Arvind644\"\u003eArvind644\u003c/a\u003e, \u003ca href=\"https://github.com/shrinandj\"\u003eshrinandj\u003c/a\u003e for their first contributions over the past year! 🎉\u003c/p\u003e\n\u003cp\u003eThese contributions have covered a range of areas, including documentation, user guides, the SDK and the UI!\u003c/p\u003e\n\u003ch1\u003eHighlights\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eSupported platforms were expanded to include Docker, Apple M1, Conda, Google Colab and Jupyter Notebooks.\u003c/li\u003e\n\u003cli\u003eThe UI was given a makeover, with upgraded home and run single pages, as well as brand new experiment page. The new audio and figure explorers now allow for the exploration of more types of metadata.\u003c/li\u003e\n\u003cli\u003eAim remote tracking server was rolled out to enable tracking from multi-host environments.\u003c/li\u003e\n\u003cli\u003eActive monitoring capabilities were enabled, such as notifications for stalled runs, and the ability to programmatically define training heuristics using callbacks and notifications.\u003c/li\u003e\n\u003cli\u003eAim was integrated with 9 different machine learning frameworks alongside with 3 convertors that help to migrate from other experiment trackers.\u003c/li\u003e\n\u003cli\u003eLast but not least, the documentation was completely revamped, including almost 40 pages of guides towards helping in effectively using and understanding Aim.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\u003cstrong\u003eSupported Platforms\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eImprovements were made to ensure Aim works seamlessly with widely used environments and platforms in ML/MLOps field, including adding support for new platforms.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYG-SSawh8Q8jte6BjyNMQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSupport for running Aim on Apple M1 devices.\u003c/li\u003e\n\u003cli\u003eSupport for using Aim inside Google Colab and Jupyter notebooks.\u003c/li\u003e\n\u003cli\u003eDocker images for Aim UI and server.\u003c/li\u003e\n\u003cli\u003eSupport for running Aim in Kubernetes clusters.\u003c/li\u003e\n\u003cli\u003eAim became available in Conda.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eUser Interface\u003c/h1\u003e\n\u003cp\u003eThe UI is one of key interfaces to interact with the ML training runs tracked by Aim. It’s super-powerful for comparing large number of trainings. The UI was significantly improved in the past year with new pages and explorers to help to explore more metadata types.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odqgX33xLVzYBI6sfRYRWQ.png\" alt=\"\" title=\"Home Page\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRevamped home page to see project’s contributions and overall statistics.\u003c/li\u003e\n\u003cli\u003eRevamped run page to deep-dive into an individual run.\u003c/li\u003e\n\u003cli\u003eBrand new experiment page to view experiment’s details, attached runs, etc.\u003c/li\u003e\n\u003cli\u003eNew explorers, such as the “Figures Explorer” and “Audio Explorer”, to allow a cross-run comparison of Plotly figures and audio objects respectively.\u003c/li\u003e\n\u003cli\u003e“Metrics Explorer” key enhancements, including displaying chart legends and the ability of exporting charts as images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eCommand Line Interface\u003c/h1\u003e\n\u003cp\u003eAim CLI offers a simple interface to easily manage tracked trainings. Two key groups of commands were added:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRuns management\u003c/strong\u003e to enable the base operations for managing trainings — list, copy, remove, move, upload, close.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStorage management\u003c/strong\u003e to help to manage Aim data storage, such as reindexing, pruning, managing data format versions (advanced usage).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZFkXFzmz5BxGYofUgWdTg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eQuery Language\u003c/h1\u003e\n\u003cp\u003eAim enables a powerful query language to filter through all the stored metadata using python expressions. A few additions were done providing a more friendly and pythonic interface for working with date-time expressions, as well as querying runs based on its metrics results.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1284/format:webp/1*6J9gouDEt9Mkhv9Cds4dNw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eRemote Tracking\u003c/h1\u003e\n\u003cp\u003eThe Aim remote tracking server, which allows running trainings in a multi-host environment and collect tracked data in a centralized location, used to be one of the most requested features.\u003c/p\u003e\n\u003cp\u003eIt has been gradually rolled out from its experimental phase to a stable version that can scale up and handle an increasing number of parallel trainings.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CizfTBF8Nh4DZcun6TP8Pg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSwitching from a local environment to a centralized server is as easy as pie, as zero code changes are required.⚡\u003cbr\u003e\nThis is because the interfaces are completely compatible!\u003c/p\u003e\n\u003ch1\u003eActive Monitoring\u003c/h1\u003e\n\u003cp\u003eAim made it first steps towards optimizing conducting long and large scale trainings, such as training or tuning GPT-like models. It reduces human-hours spent monitoring/restarting runs, improves reproducibility and ramps up time for people who need to train these models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAbility to notify on stalled/stuck runs.\u003c/li\u003e\n\u003cli\u003eCallbacks and notifications to define training heuristics programmatically.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA quote from “Scaling Laws for Generative Mixed-Modal Language Models” (Aghajanyan et al., 2023, \u003ca href=\"https://arxiv.org/abs/2301.03728\"\u003earXiv:2301.03728\u003c/a\u003e):\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe tracked all experiments using the Aim experiment tracker (Arakelyan et al., 2020). To ensure consistent training strategies across our experiments, we implemented a model restart policy using the Aim experiment tracker and callbacks. Specifically, if training perplexities do not decrease after 500 million tokens, the training run is restarted with a reduced learning rate with a factor of 0.8 of the current time step. This policy helps remove variance in the scaling laws due to differences in training procedures and allows us to scale up the number of asynchronous experiments significantly. All experiments were conducted in a two-month time frame with a cluster of 768 80GB A100 GPUs. The majority of experiments used 64 GPUs at a time.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eKey Performance and Usability Optimizations\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eStorage: long metrics sampling and retrieval (\u003e1M steps).\u003c/li\u003e\n\u003cli\u003eStorage: robust locking mechanism and automatic background indexing.\u003c/li\u003e\n\u003cli\u003eUI: The v3.12 milestone was released, addressing over 30 usability issues.\u003c/li\u003e\n\u003cli\u003eUI: virtualizations in various places across the UI to improve responsiveness when displaying large amount of data.\u003c/li\u003e\n\u003cli\u003eUI: optimizations in stream decoding and data encoding to enhance overall performance.\u003c/li\u003e\n\u003cli\u003eUI: live update optimizations to effectively update and display real-time data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eIntegrations\u003c/h1\u003e\n\u003cp\u003eAim easily integrates with a large number of widely adopted machine learning frameworks and tools, reducing barriers to get started with Aim. During the past year, a number of integrations were added:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eML Frameworks\u003c/strong\u003e — Pytorch Ignite, CatBoost, LightGBM, KerasTuner, fastai, MXNet, Optuna, PaddlePaddle\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConvertors\u003c/strong\u003e to easily migrate from other tools \u003cstrong\u003e—\u003c/strong\u003e TensorBoard to Aim, MLFlow to Aim, WandB to Aim.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTools that integrated Aim\u003c/strong\u003e — Meta’s fairseq and metaseq frameworks, HuggingFace’s accelerate, Ludwig and Kedro.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1358/format:webp/1*YT4Yf1sJUqQLWRtiF8fJgQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you have any questions join \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. 🙌\u003c/p\u003e\n\u003cp\u003eShow some love by dropping a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/aim-2022-product-recap.md","_raw":{"sourceFilePath":"posts/aim-2022-product-recap.md","sourceFileName":"aim-2022-product-recap.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-2022-product-recap"},"type":"Post"},{"title":"Aim 3.1 — Images Tracker and Images Explorer","date":"2021-11-25T14:57:05.232Z","author":"Gev Soghomonian","description":"Excited to share with you the Aim 3.1! 😊 A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community","slug":"aim-3-1-images-tracker-and-images-explorer","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Excited to share with you the Aim 3.1! \n\nA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWe have added a new demo to showcase the new Explore Images feature. We have also forked the [lightweight-gan from lucidrains](https://github.com/lucidrains/lightweight-gan) and added the Aim integration.\n\nDemo code: \u003chttps://github.com/aimhubio/lightweight-gan\u003e\\\nDemo site: [play.aimstack.io:10002](http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq\u0026select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS\u0026images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK)\n\n**The notable changes in the Aim 3.1.0:**\n\n* Images tracker and explorer\n* Runs navigation from the single run page\n* Performance improvements and tests\n\nThanks to srikanth, Mohamad Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\n\n## Images Tracker and Explorer\n\nFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\n\nAll the regular grouping and search goodies are available on Images Explorer too \n\nAdd an image tracker in your training code like this:\n\n```\nrun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n```\n\nOnce you run your training, the images will be saved and become available on the Images Explorer tab.\n\nOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\n\nHere is a quick promo video of what it can do. The rest of the details can be found in the docs[ here](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png)\n\nA quick demo video.\n\n![](https://youtu.be/4mtUFV8yG_o)\n\nThe Images view on the single run page is coming soon …\n\n## Runs Navigation\n\nOne of the regressions from the Aim 2.x to [3.0.0](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative) was the ability to navigate between the runs of the same experiment from the single run page.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63UGGl2EDquqGHIY03VpQQ.png)\n\nWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\n\nSorry for the regression \n\n## Performance improvements and assurances\n\nTwo groups of performance improvements have been made— on the UI and on the SDK side.\n\n### **Column virtualization on Aim UI Table**\n\nThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\n\nWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem — and this is a key step towards  it. 🍻\n\n### **Storage performance tests**\n\nOne of the unique challenges in building Aim is to make sure each newly added data type won’t impact the overall performance. We have added continuously add more performance tests to ensure the new releases don’t impact the performance.\n\nIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\n\nPerformance reliability is key for Aim and this is a major iteration towards that!\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003eExcited to share with you the Aim 3.1! \u003c/p\u003e\n\u003cp\u003eA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\u003c/p\u003e\n\u003cp\u003eWe have added a new demo to showcase the new Explore Images feature. We have also forked the \u003ca href=\"https://github.com/lucidrains/lightweight-gan\"\u003elightweight-gan from lucidrains\u003c/a\u003e and added the Aim integration.\u003c/p\u003e\n\u003cp\u003eDemo code: \u003ca href=\"https://github.com/aimhubio/lightweight-gan\"\u003ehttps://github.com/aimhubio/lightweight-gan\u003c/a\u003e\u003cbr\u003e\nDemo site: \u003ca href=\"http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq\u0026#x26;select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS\u0026#x26;images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK\"\u003eplay.aimstack.io:10002\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe notable changes in the Aim 3.1.0:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImages tracker and explorer\u003c/li\u003e\n\u003cli\u003eRuns navigation from the single run page\u003c/li\u003e\n\u003cli\u003ePerformance improvements and tests\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThanks to srikanth, Mohamad Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\u003c/p\u003e\n\u003ch2\u003eImages Tracker and Explorer\u003c/h2\u003e\n\u003cp\u003eFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\u003c/p\u003e\n\u003cp\u003eAll the regular grouping and search goodies are available on Images Explorer too \u003c/p\u003e\n\u003cp\u003eAdd an image tracker in your training code like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you run your training, the images will be saved and become available on the Images Explorer tab.\u003c/p\u003e\n\u003cp\u003eOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\u003c/p\u003e\n\u003cp\u003eHere is a quick promo video of what it can do. The rest of the details can be found in the docs\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003e here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3IkmtAOLGy-e3eJneuv2bA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA quick demo video.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://youtu.be/4mtUFV8yG_o\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Images view on the single run page is coming soon …\u003c/p\u003e\n\u003ch2\u003eRuns Navigation\u003c/h2\u003e\n\u003cp\u003eOne of the regressions from the Aim 2.x to \u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003e3.0.0\u003c/a\u003e was the ability to navigate between the runs of the same experiment from the single run page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63UGGl2EDquqGHIY03VpQQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\u003c/p\u003e\n\u003cp\u003eSorry for the regression \u003c/p\u003e\n\u003ch2\u003ePerformance improvements and assurances\u003c/h2\u003e\n\u003cp\u003eTwo groups of performance improvements have been made— on the UI and on the SDK side.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eColumn virtualization on Aim UI Table\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\u003c/p\u003e\n\u003cp\u003eWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem — and this is a key step towards  it. 🍻\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eStorage performance tests\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eOne of the unique challenges in building Aim is to make sure each newly added data type won’t impact the overall performance. We have added continuously add more performance tests to ensure the new releases don’t impact the performance.\u003c/p\u003e\n\u003cp\u003eIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\u003c/p\u003e\n\u003cp\u003ePerformance reliability is key for Aim and this is a major iteration towards that!\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-1-—-images-tracker-and-images-explorer.md","_raw":{"sourceFilePath":"posts/aim-3-1-—-images-tracker-and-images-explorer.md","sourceFileName":"aim-3-1-—-images-tracker-and-images-explorer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-1-—-images-tracker-and-images-explorer"},"type":"Post"},{"title":"Aim 3.10 — Visualize terminal logs, M1 support \u0026 better query autocomplete","date":"2022-05-25T20:49:49.499Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.10 is now available! What's new? A lot! Visualize terminal logs, M1 support, better autocomplete experience, integration with CatBoost and LightGBM.","slug":"aim-3-10-visualize-terminal-logs-m1-support-better-query-autocomplete","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5HbX-z8qa0ORFDRKArmFw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.10 is now available!\n\nWe are on a mission to [democratize AI dev tools](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative). Thanks to the awesome Aim community for the help and contributions.\n\nHere is what’s new in Aim 3.10:\n\n* Visualize terminal logs\n* M1 support\n* Better autocomplete experience\n* Aim citation available\n* CatBoost integration\n* LightGBM integration\n\nCheck out the 3.10 release [milestone](https://github.com/aimhubio/aim/milestone/33) and the [GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.10.0)\n\n\u003e *Shout out to Daniel Wessel, [arnauddhaene](https://github.com/arnauddhaene),[ uduse](https://github.com/uduse), [lukoucky](https://github.com/lukoucky), Armen Aghajanyan and others for contributions and feedback. We really appreciate it.*\n\n## Visualize terminal logs\n\nWhen it comes to automating training of multiple runs with job schedulers or workload managers on a cluster, it becomes hard to track the **terminal** **logs** of the runs.\n\nNow Aim automatically streams the terminal logs to the UI. Near-real-time.\n\nThe terminal logs can be turned off if the run instance is created with the following flag in place:\n\n```\naim_run = Run(capture_terminal_logs=False)\n```\n\nCheck out more about this feature [in Aim docs](https://aimstack.readthedocs.io/en/latest/using/configure_runs.html?highlight=terminal%20logs#capturing-terminal-logs).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K60XrQq0iH6NlO0NFz44EQ.png)\n\n## M1 support\n\n[With the awesome work by the PyTorch team](https://twitter.com/PyTorch/status/1526944876478144512) on enabling support for GPU-accelerated PyTorch training on Mac, there has been a huge demand to enable aim on M1 as well.\n\nAim now supports M1 too. Now you can use Aim with [PyTorch](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/) on Mac to track and deeply compare your experiments.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp5LKw55RgE4gnPXC1cWIg.png)\n\n## Better autocomplete experience\n\nWe have integrated a rich code autocomplete system as the Aim community loves to deeply query their training runs. This has been a highly requested improvement and a huge productivity booster. Expect more improvements here.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*vEQnAJSks5YGNCXHllDnNA.gif)\n\n## Aim citation available\n\nNow you can cite Aim from your paper if you are using Aim to compare your experiments. [The citation file](https://github.com/aimhubio/aim/blob/main/CITATION.cff).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLHweR2at8tbpHiTkelbfA.png)\n\n[Thanks to the researchers from Meta AI Research for the incentive and initiative.](https://arxiv.org/abs/2205.10770)  🙌\n\n## CatBoost integration\n\n[CatBoost](https://catboost.ai/) is one of the fastest Gradient Boosting on Decision Trees library. Aim now supports CatBoost out of the box. Check out the[ Aim CatBoost docs here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost).\n\nHere is a short [example](https://github.com/aimhubio/aim/blob/main/examples/catboost_track.py) that shows how to use AimLogger for CatBoost.\n\n## LightGBM integration\n\n[LightGBM](https://lightgbm.readthedocs.io/en/latest/) is one of the most widely-used and battle-tested gradient boosting frameworks. Due to high-demand we have also added an out-of-the-box Aim LightGBM integration.\n\n[Here is a short code example](https://github.com/aimhubio/aim/blob/main/examples/lightgbm_track.py) and [the docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm) on how to use the integration.\n\n- - -\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n* [](https://twitter.com/share?url=https://aimstack.io/aim-3-10-release-catboost-integration/\u0026text=Aim%203.10%20%E2%80%94%20Visualize%20terminal%20logs%2C%20M1%20support%20%26%23038%3B%20better%20query%20autocomplete)","html":"\u003cp\u003eHey team, Aim 3.10 is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to \u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003edemocratize AI dev tools\u003c/a\u003e. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what’s new in Aim 3.10:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVisualize terminal logs\u003c/li\u003e\n\u003cli\u003eM1 support\u003c/li\u003e\n\u003cli\u003eBetter autocomplete experience\u003c/li\u003e\n\u003cli\u003eAim citation available\u003c/li\u003e\n\u003cli\u003eCatBoost integration\u003c/li\u003e\n\u003cli\u003eLightGBM integration\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the 3.10 release \u003ca href=\"https://github.com/aimhubio/aim/milestone/33\"\u003emilestone\u003c/a\u003e and the \u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.10.0\"\u003eGitHub Release Notes\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eShout out to Daniel Wessel, \u003ca href=\"https://github.com/arnauddhaene\"\u003earnauddhaene\u003c/a\u003e,\u003ca href=\"https://github.com/uduse\"\u003e uduse\u003c/a\u003e, \u003ca href=\"https://github.com/lukoucky\"\u003elukoucky\u003c/a\u003e, Armen Aghajanyan and others for contributions and feedback. We really appreciate it.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eVisualize terminal logs\u003c/h2\u003e\n\u003cp\u003eWhen it comes to automating training of multiple runs with job schedulers or workload managers on a cluster, it becomes hard to track the \u003cstrong\u003eterminal\u003c/strong\u003e \u003cstrong\u003elogs\u003c/strong\u003e of the runs.\u003c/p\u003e\n\u003cp\u003eNow Aim automatically streams the terminal logs to the UI. Near-real-time.\u003c/p\u003e\n\u003cp\u003eThe terminal logs can be turned off if the run instance is created with the following flag in place:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = Run(capture_terminal_logs=False)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck out more about this feature \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/configure_runs.html?highlight=terminal%20logs#capturing-terminal-logs\"\u003ein Aim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K60XrQq0iH6NlO0NFz44EQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eM1 support\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/PyTorch/status/1526944876478144512\"\u003eWith the awesome work by the PyTorch team\u003c/a\u003e on enabling support for GPU-accelerated PyTorch training on Mac, there has been a huge demand to enable aim on M1 as well.\u003c/p\u003e\n\u003cp\u003eAim now supports M1 too. Now you can use Aim with \u003ca href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\"\u003ePyTorch\u003c/a\u003e on Mac to track and deeply compare your experiments.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mp5LKw55RgE4gnPXC1cWIg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eBetter autocomplete experience\u003c/h2\u003e\n\u003cp\u003eWe have integrated a rich code autocomplete system as the Aim community loves to deeply query their training runs. This has been a highly requested improvement and a huge productivity booster. Expect more improvements here.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*vEQnAJSks5YGNCXHllDnNA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAim citation available\u003c/h2\u003e\n\u003cp\u003eNow you can cite Aim from your paper if you are using Aim to compare your experiments. \u003ca href=\"https://github.com/aimhubio/aim/blob/main/CITATION.cff\"\u003eThe citation file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLHweR2at8tbpHiTkelbfA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2205.10770\"\u003eThanks to the researchers from Meta AI Research for the incentive and initiative.\u003c/a\u003e  🙌\u003c/p\u003e\n\u003ch2\u003eCatBoost integration\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://catboost.ai/\"\u003eCatBoost\u003c/a\u003e is one of the fastest Gradient Boosting on Decision Trees library. Aim now supports CatBoost out of the box. Check out the\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\"\u003e Aim CatBoost docs here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere is a short \u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/catboost_track.py\"\u003eexample\u003c/a\u003e that shows how to use AimLogger for CatBoost.\u003c/p\u003e\n\u003ch2\u003eLightGBM integration\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://lightgbm.readthedocs.io/en/latest/\"\u003eLightGBM\u003c/a\u003e is one of the most widely-used and battle-tested gradient boosting frameworks. Due to high-demand we have also added an out-of-the-box Aim LightGBM integration.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/lightgbm_track.py\"\u003eHere is a short code example\u003c/a\u003e and \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\"\u003ethe docs\u003c/a\u003e on how to use the integration.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/share?url=https://aimstack.io/aim-3-10-release-catboost-integration/\u0026#x26;text=Aim%203.10%20%E2%80%94%20Visualize%20terminal%20logs%2C%20M1%20support%20%26%23038%3B%20better%20query%20autocomplete\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"posts/aim-3-10-—-visualize-terminal-logs-m1-support-better-query-autocomplete.md","_raw":{"sourceFilePath":"posts/aim-3-10-—-visualize-terminal-logs-m1-support-better-query-autocomplete.md","sourceFileName":"aim-3-10-—-visualize-terminal-logs-m1-support-better-query-autocomplete.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-10-—-visualize-terminal-logs-m1-support-better-query-autocomplete"},"type":"Post"},{"title":"Aim 3.13–Figures Explorer and notifications on stalled runs","date":"2022-08-24T21:08:50.210Z","author":"Gev Soghomonian","description":"In this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven’t had the chance to talk about much. ","slug":"aim-3-13figures-explorer-and-notifications-on-stalled-runs","image":"https://miro.medium.com/max/1400/1*5hUcOciwufupoI4-nBbcyA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"# Hey team, Aim 3.13 is now available!\n\nIn this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven’t had the chance to talk about much. Stay tuned on the [AimStack](https://twitter.com/aimstackio) twitter where we will share them and more use-cases.\n\n**Aim is evolving really quickly! 🚀**\n\n\u003e *We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.*\n\u003e\n\u003e *Thanks to [Sharathmk99 and](https://github.com/Sharathmk99) [tmynn](https://github.com/tmynn) for their first contributions!! 🔥🔥🔥*\n\n**TLDR;**\n\nAim 3.13 is full of impactful changes. Here are the main highlights!\n\n* **Figures Explorer**\\\n  Explore and compare 100s of Plotly figures within a few clicks.\n* **Notify about stalled runs**\\\n  Configure Aim to notify your team when the run is stalled (slack, email, workspace are available)\n* **Stable Aim Remote Server**\n* **KerasTuner integration**\n* **Weights and Biases log converter**\n\nCheck out the Aim 3.13 release [milestone](https://github.com/aimhubio/aim/issues?q=is%3Aissue+milestone%3Av3.13.0+is%3Aopen) and the [GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.13.0) for more information.\n\n# Figures Explorer\n\nFigures Explorer is a new addition to the mighty Aim explorers. With this feature now you can compare 1000s of Plotly figures within a few clicks.\n\nThis is just the first iteration 😊. Expect lots of improvements of the experience over time. This is how the figures explorer works:\n\n![](https://miro.medium.com/max/1400/1*WKwbKRBJPK8fErSKVItf6Q.gif)\n\n# Notify about stalled runs\n\nOne of the ways the precious training time is prolonged / wasted when for some reasons the runs stall. The training runs can stall in so many different ways:\n\n* Hardware issues\n* exception in the code\n* driver issues\n\nAnd many other things we never anticipate. This feature allows Aim users to configure notifications to their preferred channel (slack, workspace) when the run stalls.\n\nCheck out the [docs](https://aimstack.readthedocs.io/en/latest/using/run_status_notifications.html#) for more info. Here is how it looks on your slack:\n\n![](https://miro.medium.com/max/1400/1*OFZh37yiEa_vo-14gv7UFw.webp)\n\n# Stable Aim Remote Server\n\nAim Remote allows to set up a remote Aim tracking server so the tracked logs can be sent to a centralized location.\n\nIt’s been a while since Aim Remote Server was an experimental feature. We have been lucky as the users have started using it and shared lots of feedbacks, issues that we have fixed.\n\nNow we are excited to announce that the Aim Remote Server is stable\n\n![](https://miro.medium.com/max/1400/1*zC-crWCgWncraYJFQRAd3g.webp)\n\n# KerasTuner integration\n\nA highly requested integration with KerasTuner.\n\nHere are the [docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tunerhttps://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tuner) and a short code snippet. Super-easy to integrate Aim with your KerasTuner project now.\n\n```\nfrom aim.keras_tuner import AimCallback\n\n...\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n```\n\n# Weights and Biases log converter\n\nThe W\u0026B users who also wanted to use Aim in their work, have been asking about this feature for a while now.\n\nExcited to share that now you can easily convert your W\u0026B logs to Aim.\n\nHere is how it works:\n\n```\ncd project-directory\n# Init the Aim repo\naim init \n# Run the converter to migrate logs from WandB to Aim\naim convert wandb --entity 'my_team' --project 'my_project'\n```\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html) 🙌\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aim) if you think Aim is useful ⭐️","html":"\u003ch1\u003eHey team, Aim 3.13 is now available!\u003c/h1\u003e\n\u003cp\u003eIn this blogpost I will share the improvements Aim 3.13. We had released 2 more versions (Aim 3.11 and 3.12) that we haven’t had the chance to talk about much. Stay tuned on the \u003ca href=\"https://twitter.com/aimstackio\"\u003eAimStack\u003c/a\u003e twitter where we will share them and more use-cases.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAim is evolving really quickly! 🚀\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThanks to \u003ca href=\"https://github.com/Sharathmk99\"\u003eSharathmk99 and\u003c/a\u003e \u003ca href=\"https://github.com/tmynn\"\u003etmynn\u003c/a\u003e for their first contributions!! 🔥🔥🔥\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim 3.13 is full of impactful changes. Here are the main highlights!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFigures Explorer\u003c/strong\u003e\u003cbr\u003e\nExplore and compare 100s of Plotly figures within a few clicks.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNotify about stalled runs\u003c/strong\u003e\u003cbr\u003e\nConfigure Aim to notify your team when the run is stalled (slack, email, workspace are available)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStable Aim Remote Server\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKerasTuner integration\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWeights and Biases log converter\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the Aim 3.13 release \u003ca href=\"https://github.com/aimhubio/aim/issues?q=is%3Aissue+milestone%3Av3.13.0+is%3Aopen\"\u003emilestone\u003c/a\u003e and the \u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.13.0\"\u003eGitHub Release Notes\u003c/a\u003e for more information.\u003c/p\u003e\n\u003ch1\u003eFigures Explorer\u003c/h1\u003e\n\u003cp\u003eFigures Explorer is a new addition to the mighty Aim explorers. With this feature now you can compare 1000s of Plotly figures within a few clicks.\u003c/p\u003e\n\u003cp\u003eThis is just the first iteration 😊. Expect lots of improvements of the experience over time. This is how the figures explorer works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*WKwbKRBJPK8fErSKVItf6Q.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eNotify about stalled runs\u003c/h1\u003e\n\u003cp\u003eOne of the ways the precious training time is prolonged / wasted when for some reasons the runs stall. The training runs can stall in so many different ways:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHardware issues\u003c/li\u003e\n\u003cli\u003eexception in the code\u003c/li\u003e\n\u003cli\u003edriver issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd many other things we never anticipate. This feature allows Aim users to configure notifications to their preferred channel (slack, workspace) when the run stalls.\u003c/p\u003e\n\u003cp\u003eCheck out the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/run_status_notifications.html#\"\u003edocs\u003c/a\u003e for more info. Here is how it looks on your slack:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*OFZh37yiEa_vo-14gv7UFw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eStable Aim Remote Server\u003c/h1\u003e\n\u003cp\u003eAim Remote allows to set up a remote Aim tracking server so the tracked logs can be sent to a centralized location.\u003c/p\u003e\n\u003cp\u003eIt’s been a while since Aim Remote Server was an experimental feature. We have been lucky as the users have started using it and shared lots of feedbacks, issues that we have fixed.\u003c/p\u003e\n\u003cp\u003eNow we are excited to announce that the Aim Remote Server is stable\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*zC-crWCgWncraYJFQRAd3g.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eKerasTuner integration\u003c/h1\u003e\n\u003cp\u003eA highly requested integration with KerasTuner.\u003c/p\u003e\n\u003cp\u003eHere are the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tunerhttps://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tuner\"\u003edocs\u003c/a\u003e and a short code snippet. Super-easy to integrate Aim with your KerasTuner project now.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.keras_tuner import AimCallback\n\n...\n\ntuner = kt.Hyperband(\n    build_model, objective=\"val_accuracy\", max_epochs=30, hyperband_iterations=2\n)\n...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eWeights and Biases log converter\u003c/h1\u003e\n\u003cp\u003eThe W\u0026#x26;B users who also wanted to use Aim in their work, have been asking about this feature for a while now.\u003c/p\u003e\n\u003cp\u003eExcited to share that now you can easily convert your W\u0026#x26;B logs to Aim.\u003c/p\u003e\n\u003cp\u003eHere is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd project-directory\n# Init the Aim repo\naim init \n# Run the converter to migrate logs from WandB to Aim\naim convert wandb --entity 'my_team' --project 'my_project'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e if you think Aim is useful ⭐️\u003c/p\u003e"},"_id":"posts/aim-3-13–figures-explorer-and-notifications-on-stalled-runs.md","_raw":{"sourceFilePath":"posts/aim-3-13–figures-explorer-and-notifications-on-stalled-runs.md","sourceFileName":"aim-3-13–figures-explorer-and-notifications-on-stalled-runs.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-13–figures-explorer-and-notifications-on-stalled-runs"},"type":"Post"},{"title":"Aim 3.14 — Brand new Home Page!","date":"2022-11-03T21:31:15.756Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.14 is now available! We're excited to introduce Brand new Home Page, MXNet and FastAI loggers, new tooltip positioning modes and ability to edit tags on the runs table!","slug":"aim-3-14-brand-new-home-page","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEJ0_GtGyXC4sv2sZrSCRA.png","draft":false,"categories":["New Releases"],"body":{"raw":"# Hey team, Aim 3.14 is now available!\n\nWe are releasing a new Aim version every 4–5 weeks with community-contributed features and fixes.\n\n**A brand new Aim release every few weeks**. With new features and fixes! 🚀\n\n\u003e We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\u003e\n\u003e Thanks to [djwessel](https://github.com/djwessel) and [Vahram-aimhub](https://github.com/Vahram-aimhub) for their first contributions🔥🔥🔥\n\n# Aim 3.14 Overview\n\nHere is what’s new in the latest release:\n\n* **Brand new Home Page**\n* **MXNet and FastAI loggers**\n* **New tooltip positioning modes**\n* **Ability to edit tags on the runs table**\n\nCheck out the [Aim 3.14 release milestone](https://github.com/aimhubio/aim/milestone/41) and the [GitHub Release Notes](https://github.com/aimhubio/aim/releases/tag/v3.14.0) for more information.\n\n\u003e Note: at the time of writing this, Aim is at [3.14.3](https://github.com/aimhubio/aim/releases) — check those intermediate versions too.\n\n# Brand new Home Page\n\nAfter months of requests from the community to build a quick access page, improve the onboarding, we are excited to introduce new Home page.\n\nThe home page is pretty self-descriptive. It’s an overview of whatever actions have been taken using Aim.\n\nThis is in no way the final version, we continue to iterate on it. Would love to hear your feedbacks.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhWVuIETasWSsUc3oEYdfQ.png)\n\n# MXNet and fastai loggers\n\nNow there are loggers available both for MxNet and FastAI.\n\nIt takes only a few lines to incorporate Aim to your code.\n\nHere is how it looks for MXNet in your code. More [MXNet integration details here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-mxnet)\n\n\n\n```\nfrom aim.mxnet import AimLoggingHandler\n\naim_log_handler = AimLoggingHandler(repo='.', experiment_name='mxnet_example',\n                                    log_interval=1, metrics=[train_acc, train_loss, val_acc])\n\nest.fit(train_data=train_data_loader, val_data=val_data_loader,\n        epochs=num_epochs, event_handlers=[aim_log_handler])\n```\n\nThis is the code for fastai Aim integrations. [More details here.](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)\n\n```\nfrom aim.fastai import AimCallback\n\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_example'))\n```\n\n## New tooltip positioning modes\n\nWe have added three buttons to the Explorers tooltip on click.\n\nThese buttons allow to fix where you’d want to see these tooltips appear when hovering over the metrics.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71V5hEnGIOg2-QIi01ok1A.png)\n\nIn this case we have chosen to place the tooltip at the top of the page.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vDsrHdRa42jxpTcG66nkg.png)\n\n# Ability to edit tags on the runs table\n\nTags have become one of the widely used features lately and we are iterating so you can make the best out of them.\n\nNow you can add tags to your runs from within the Runs Explorer.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vk0DJyFCGdw9A_TPzGDMRg.png)\n\n# Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html) 🙌\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://slack.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aim) if you think Aim is useful ⭐️.","html":"\u003ch1\u003eHey team, Aim 3.14 is now available!\u003c/h1\u003e\n\u003cp\u003eWe are releasing a new Aim version every 4–5 weeks with community-contributed features and fixes.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA brand new Aim release every few weeks\u003c/strong\u003e. With new features and fixes! 🚀\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eThanks to \u003ca href=\"https://github.com/djwessel\"\u003edjwessel\u003c/a\u003e and \u003ca href=\"https://github.com/Vahram-aimhub\"\u003eVahram-aimhub\u003c/a\u003e for their first contributions🔥🔥🔥\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eAim 3.14 Overview\u003c/h1\u003e\n\u003cp\u003eHere is what’s new in the latest release:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBrand new Home Page\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMXNet and FastAI loggers\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNew tooltip positioning modes\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAbility to edit tags on the runs table\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the \u003ca href=\"https://github.com/aimhubio/aim/milestone/41\"\u003eAim 3.14 release milestone\u003c/a\u003e and the \u003ca href=\"https://github.com/aimhubio/aim/releases/tag/v3.14.0\"\u003eGitHub Release Notes\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: at the time of writing this, Aim is at \u003ca href=\"https://github.com/aimhubio/aim/releases\"\u003e3.14.3\u003c/a\u003e — check those intermediate versions too.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eBrand new Home Page\u003c/h1\u003e\n\u003cp\u003eAfter months of requests from the community to build a quick access page, improve the onboarding, we are excited to introduce new Home page.\u003c/p\u003e\n\u003cp\u003eThe home page is pretty self-descriptive. It’s an overview of whatever actions have been taken using Aim.\u003c/p\u003e\n\u003cp\u003eThis is in no way the final version, we continue to iterate on it. Would love to hear your feedbacks.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhWVuIETasWSsUc3oEYdfQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eMXNet and fastai loggers\u003c/h1\u003e\n\u003cp\u003eNow there are loggers available both for MxNet and FastAI.\u003c/p\u003e\n\u003cp\u003eIt takes only a few lines to incorporate Aim to your code.\u003c/p\u003e\n\u003cp\u003eHere is how it looks for MXNet in your code. More \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-mxnet\"\u003eMXNet integration details here\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.mxnet import AimLoggingHandler\n\naim_log_handler = AimLoggingHandler(repo='.', experiment_name='mxnet_example',\n                                    log_interval=1, metrics=[train_acc, train_loss, val_acc])\n\nest.fit(train_data=train_data_loader, val_data=val_data_loader,\n        epochs=num_epochs, event_handlers=[aim_log_handler])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the code for fastai Aim integrations. \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\"\u003eMore details here.\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.fastai import AimCallback\n\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_example'))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eNew tooltip positioning modes\u003c/h2\u003e\n\u003cp\u003eWe have added three buttons to the Explorers tooltip on click.\u003c/p\u003e\n\u003cp\u003eThese buttons allow to fix where you’d want to see these tooltips appear when hovering over the metrics.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71V5hEnGIOg2-QIi01ok1A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this case we have chosen to place the tooltip at the top of the page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vDsrHdRa42jxpTcG66nkg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eAbility to edit tags on the runs table\u003c/h1\u003e\n\u003cp\u003eTags have become one of the widely used features lately and we are iterating so you can make the best out of them.\u003c/p\u003e\n\u003cp\u003eNow you can add tags to your runs from within the Runs Explorer.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vk0DJyFCGdw9A_TPzGDMRg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e if you think Aim is useful ⭐️.\u003c/p\u003e"},"_id":"posts/aim-3-14-—-brand-new-home-page.md","_raw":{"sourceFilePath":"posts/aim-3-14-—-brand-new-home-page.md","sourceFileName":"aim-3-14-—-brand-new-home-page.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-14-—-brand-new-home-page"},"type":"Post"},{"title":"Aim 3.15 — Callbacks, logging and notifications, line chart legends, experiment page, Audios Explorer","date":"2022-12-06T21:40:50.164Z","author":"Gor Arakelyan","description":"Aim 3.15 is out! Callbacks, Logging \u0026 notifications, Line chart legends, Audios Explorer, Experiment page, Robust locking mechanism, PaddlePaddle \u0026 Optuna integrations.","slug":"aim-3-15-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer","image":"https://miro.medium.com/max/1400/1*wQWwnwDeCSVQ-YbZapbHlQ.webp","draft":false,"categories":["New Releases"],"body":{"raw":"## Hey team, get ready for a major upgrade!\n\nWe are thrilled to announce the release of Aim v3.15, packed with new features and improvements.\n\n\u003e *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Without your help, Aim would not be where it is today.*\n\u003e\n\u003e *Congratulations to [jangop](https://github.com/jangop), [hjoonjang](https://github.com/hjoonjang) and [kumarshreshtha](https://github.com/kumarshreshtha)* *for their first contributions.*\n\n**TL;DR** The new version includes lots of significant and highly requested enhancements, as well as key improvements. Here are the highlights:\n\n* **Callbacks**\n* **Logging and notifications**\n* **Line chart legends**\n* **Audios Explorer**\n* **Experiment page**\n* **Robust locking mechanism**\n* **Integrations with PaddlePaddle and Optuna**\n\n# Aim SDK\n\n## **Callbacks**\n\nMany things can go wrong during ML training that could result in wasted GPUs and time. The Aim callbacks API helps to define custom callbacks to be executed at any point during ML training.\n\nFind API docs and examples [here](https://aimstack.readthedocs.io/en/latest/using/callbacks.html).\n\nAn end-to-end toy script below:\n\n*Send a notification and exits the process, when the loss explodes.*\n\n```\nimport random\nimport time\n\nfrom aim import Run\nfrom aim import TrainingFlow\nfrom aim.sdk.callbacks import events\n\n\n# Define the callbacks\nclass MyCallbacks:\n    @events.on.training_metrics_collected\n    def check_loss_explosion(self, metrics, run, **kwargs):\n        if metrics.get('loss') \u003e 1:\n            # Log warning message and send the corresponding notification\n            run.log_warning(f'loss exploded: {metrics[\"loss\"]}')\n            # Exit the process\n            exit()\n\n\n# Initialize Aim run and register callbacks\nrun = Run()\ntraining_flow = TrainingFlow(run=run)\ntraining_flow.register(MyCallbacks())\n\n\n# Training simulation\nfor _ in range(100):\n    loss = random.random() * 2\n    print(loss)\n    run.track(loss, name='loss', context={'subset': 'train'})\n    training_flow.training_metrics_collected(\n        metrics={'loss': loss},\n        run=run\n    )\n    time.sleep(1)\n```\n\n## Logging and notifications\n\nDuring the training many key events happen. Use new Aim logging API to log any text message and send the corresponding notification to slack or workplace. It is as easy as calling `aim_run.log_error(message)`.\n\nIt perfectly fits with callbacks API and enables programmatically defining training heuristics and including it as part of the code. A simple example is “let me know when the training run starts to overfit”.\n\nRead more in the [docs](https://aimstack.readthedocs.io/en/latest/using/logging.html).\n\n# Aim UI\n\n## Chart legends\n\nSince the early days of Aim’s launch, one of the most requested features has been displaying chart legends. And finally it is here! The legends section makes it easy to learn metrics corresponding fields and hyper-params at a glance.\n\n![](https://miro.medium.com/max/1400/1*TFNCZjjwXFg7JZv1F1qSpg.webp)\n\n## **Audios Explorer**\n\nTo track the performance of ML models that process speech data, it is important to track audio objects.\n\nThe new Audios Explorer makes it super easy to query and play tracked audio objects across the runs. Flexible grouping options enable arranging them into rows and columns to make exploration even easier.\n\n![](https://miro.medium.com/max/1400/1*mnP4qbmlQuJE-nGqe-36lw.webp)\n\n\n\n## Experiment page\n\nThe experiment page provides an holistic view of the results of the trainings in order to identify trends and patterns that can help inform further experimentation and optimization.\n\nExperiment page consists from the following tabs:\n\n* 🏠 **Overview** — Experiment overview, number of active/finished runs, timeline of the runs.\n* 🏃 **Runs** —Holistic view of the experiment runs.\n* 📓 **Notes** — Experiment notes and insights with notion-like rich editor.\n* ⚙️ **Settings** — Experiment settings, such as name and description.\n\n![](https://miro.medium.com/max/1400/1*h-DLE3XnBM5uGKBDRcu_iA.webp)\n\n# Integrations\n\n## Integration with PaddlePaddle\n\nWith Aim v3.15 it takes just two steps to integrate Aim into your PaddlePaddle training script.\n\n```\n# Step 1: Explicitly import the AimCallback for tracking training metadata\nfrom aim.paddle import AimCallback\n\n...\n\n# Step 2: Pass the callback to callbacks list upon initiating your training\ncallback = AimCallback(repo='.', experiment='paddle_test')\nmodel.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)\n\n```\n\nSee docs [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-paddlepaddle).\n\n## Integration with Optuna\n\nSmoothly integrate Aim into your Optuna project.\n\nHere are the [docs](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-optuna) and a short code snippet.\n\n```\nfrom aim.optuna import AimCallback\n\n...\naim_callback = AimCallback(experiment_name=\"optuna_single_run\")\nstudy.optimize(objective, n_trials=10, callbacks=[aim_callback])\n```\n\n# *Key optimizations and fixes*\n\n\n\n## Runs locking mechanism and finished runs detection\n\nNumber of issues were reported w.r.t runs locking. More robust locking mechanism is applied starting from Aim v3.15.\n\n## Remote server scaling\n\nNow it is possible to spin up several workers to achieve higher processing speed for remote tracking server. Just run the server passing “workers” argument: `aim server --workers=N`\n\n# Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html) 🙌\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://discord.gg/F5TpzUeY), share your feedback, open issues for new features, bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aim) if you think Aim is useful ⭐️","html":"\u003ch2\u003eHey team, get ready for a major upgrade!\u003c/h2\u003e\n\u003cp\u003eWe are thrilled to announce the release of Aim v3.15, packed with new features and improvements.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Without your help, Aim would not be where it is today.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eCongratulations to \u003ca href=\"https://github.com/jangop\"\u003ejangop\u003c/a\u003e, \u003ca href=\"https://github.com/hjoonjang\"\u003ehjoonjang\u003c/a\u003e and \u003ca href=\"https://github.com/kumarshreshtha\"\u003ekumarshreshtha\u003c/a\u003e\u003c/em\u003e \u003cem\u003efor their first contributions.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e The new version includes lots of significant and highly requested enhancements, as well as key improvements. Here are the highlights:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCallbacks\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLogging and notifications\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLine chart legends\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAudios Explorer\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExperiment page\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust locking mechanism\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntegrations with PaddlePaddle and Optuna\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eAim SDK\u003c/h1\u003e\n\u003ch2\u003e\u003cstrong\u003eCallbacks\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eMany things can go wrong during ML training that could result in wasted GPUs and time. The Aim callbacks API helps to define custom callbacks to be executed at any point during ML training.\u003c/p\u003e\n\u003cp\u003eFind API docs and examples \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/callbacks.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAn end-to-end toy script below:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eSend a notification and exits the process, when the loss explodes.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport random\nimport time\n\nfrom aim import Run\nfrom aim import TrainingFlow\nfrom aim.sdk.callbacks import events\n\n\n# Define the callbacks\nclass MyCallbacks:\n    @events.on.training_metrics_collected\n    def check_loss_explosion(self, metrics, run, **kwargs):\n        if metrics.get('loss') \u003e 1:\n            # Log warning message and send the corresponding notification\n            run.log_warning(f'loss exploded: {metrics[\"loss\"]}')\n            # Exit the process\n            exit()\n\n\n# Initialize Aim run and register callbacks\nrun = Run()\ntraining_flow = TrainingFlow(run=run)\ntraining_flow.register(MyCallbacks())\n\n\n# Training simulation\nfor _ in range(100):\n    loss = random.random() * 2\n    print(loss)\n    run.track(loss, name='loss', context={'subset': 'train'})\n    training_flow.training_metrics_collected(\n        metrics={'loss': loss},\n        run=run\n    )\n    time.sleep(1)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLogging and notifications\u003c/h2\u003e\n\u003cp\u003eDuring the training many key events happen. Use new Aim logging API to log any text message and send the corresponding notification to slack or workplace. It is as easy as calling \u003ccode\u003eaim_run.log_error(message)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt perfectly fits with callbacks API and enables programmatically defining training heuristics and including it as part of the code. A simple example is “let me know when the training run starts to overfit”.\u003c/p\u003e\n\u003cp\u003eRead more in the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\"\u003edocs\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eAim UI\u003c/h1\u003e\n\u003ch2\u003eChart legends\u003c/h2\u003e\n\u003cp\u003eSince the early days of Aim’s launch, one of the most requested features has been displaying chart legends. And finally it is here! The legends section makes it easy to learn metrics corresponding fields and hyper-params at a glance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TFNCZjjwXFg7JZv1F1qSpg.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAudios Explorer\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eTo track the performance of ML models that process speech data, it is important to track audio objects.\u003c/p\u003e\n\u003cp\u003eThe new Audios Explorer makes it super easy to query and play tracked audio objects across the runs. Flexible grouping options enable arranging them into rows and columns to make exploration even easier.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*mnP4qbmlQuJE-nGqe-36lw.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eExperiment page\u003c/h2\u003e\n\u003cp\u003eThe experiment page provides an holistic view of the results of the trainings in order to identify trends and patterns that can help inform further experimentation and optimization.\u003c/p\u003e\n\u003cp\u003eExperiment page consists from the following tabs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e🏠 \u003cstrong\u003eOverview\u003c/strong\u003e — Experiment overview, number of active/finished runs, timeline of the runs.\u003c/li\u003e\n\u003cli\u003e🏃 \u003cstrong\u003eRuns\u003c/strong\u003e —Holistic view of the experiment runs.\u003c/li\u003e\n\u003cli\u003e📓 \u003cstrong\u003eNotes\u003c/strong\u003e — Experiment notes and insights with notion-like rich editor.\u003c/li\u003e\n\u003cli\u003e⚙️ \u003cstrong\u003eSettings\u003c/strong\u003e — Experiment settings, such as name and description.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*h-DLE3XnBM5uGKBDRcu_iA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eIntegrations\u003c/h1\u003e\n\u003ch2\u003eIntegration with PaddlePaddle\u003c/h2\u003e\n\u003cp\u003eWith Aim v3.15 it takes just two steps to integrate Aim into your PaddlePaddle training script.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Step 1: Explicitly import the AimCallback for tracking training metadata\nfrom aim.paddle import AimCallback\n\n...\n\n# Step 2: Pass the callback to callbacks list upon initiating your training\ncallback = AimCallback(repo='.', experiment='paddle_test')\nmodel.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-paddlepaddle\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eIntegration with Optuna\u003c/h2\u003e\n\u003cp\u003eSmoothly integrate Aim into your Optuna project.\u003c/p\u003e\n\u003cp\u003eHere are the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-optuna\"\u003edocs\u003c/a\u003e and a short code snippet.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.optuna import AimCallback\n\n...\naim_callback = AimCallback(experiment_name=\"optuna_single_run\")\nstudy.optimize(objective, n_trials=10, callbacks=[aim_callback])\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\u003cem\u003eKey optimizations and fixes\u003c/em\u003e\u003c/h1\u003e\n\u003ch2\u003eRuns locking mechanism and finished runs detection\u003c/h2\u003e\n\u003cp\u003eNumber of issues were reported w.r.t runs locking. More robust locking mechanism is applied starting from Aim v3.15.\u003c/p\u003e\n\u003ch2\u003eRemote server scaling\u003c/h2\u003e\n\u003cp\u003eNow it is possible to spin up several workers to achieve higher processing speed for remote tracking server. Just run the server passing “workers” argument: \u003ccode\u003eaim server --workers=N\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://discord.gg/F5TpzUeY\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e if you think Aim is useful ⭐️\u003c/p\u003e"},"_id":"posts/aim-3-15-—-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","_raw":{"sourceFilePath":"posts/aim-3-15-—-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","sourceFileName":"aim-3-15-—-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-15-—-callbacks-logging-and-notifications-line-chart-legends-experiment-page-audios-explorer"},"type":"Post"},{"title":"Aim 3.2 — Jupyter Notebook integration \u0026 Histograms","date":"2021-12-08T15:09:19.264Z","author":"Gev Soghomonian","description":"Aim 3.2 featuring Jupyter notebook integration is now available! 😊 We are on a mission to democratize AI dev tools. ","slug":"aim-3-2-jupyter-notebook-integration-histograms","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQCHweFLuEHppnFsQOaUnw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.2 featuring Jupyter notebook integration is now available! \n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWithin a week after the [v3.1](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer) we are releasing the v3.2 with a number of key highly requested features in:\n\n* Aim Jupyter Notebook extension (Colab is coming soon…)\n* Histogram tracking and visualization\n* Full-size images, images resize and render modes\n* Search autofill and suggest on the UI\n\n## Aim \u0026 Jupyter notebook\n\n\n\nNow you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with the `aim` package.\n\nYou don’t need to install anything other than Aim. Here are the [docs](https://aimstack.readthedocs.io/en/latest/index.html).\\\nA quick demo of integrating Aim \u0026 Jupyter Notebook::\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*7Xqp7esswOPWZpRoDqSawg.gif \"It’s that easy to use Aim on your Jupyter notebook!\")\n\n## Histogram tracking and Visualization\n\nStarting from 3.2 you can track Distributions and visualize them as histograms on the run page.\n\nThis is how to track the weights distribution in your training code:\n\n```\nfrom aim import Distribution\n\nfor step in range(1000):\n    my_run.track(\n        Distribution(tensor), # Pass distribution\n        name='gradients', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'type': 'weights',\n        },\n    )\n```\n\nThe [demo](http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51), [docs](https://aimstack.readthedocs.io/en/latest/index.html) and [examples](https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py) \n\nAnd this is how it looks on the UI. Aim lets you visualize the distributions every layer at every tracked step. Super powerful!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Il1FjcQVpYHyuKHUVAtS0A.gif \"Visualize every layer and every tracked step.\")\n\n## Full-size images, images resize and render modes\n\nNow you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.\n\nHere is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*zYuodUJ1ykDhrFD9rdYC_w.gif \"Easily resize the queried images\")\n\nWe have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*pTG1Z34twqOOo7x6yoo1YQ.gif \"Toggle between pixelated and smooth images render modes\")\n\nYou can also view full-size images by clicking on the top right corner magnifier on each images.\n\nBTW the smooth / pixelated config also translates to the full-size images.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*qvhjgoBWWCaT0Hm_6eXzOw.gif \"full-size images: easy!\")\n\n## Query Language autosuggest editor\n\nAnother highly requested feature.\n\nNow there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.\n\nHopefully this makes it much easier to get started with Aim.Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*eHTlgK8CdxpUsWC_edw1rw.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eAim 3.2 featuring Jupyter notebook integration is now available! \u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\u003c/p\u003e\n\u003cp\u003eWithin a week after the \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003ev3.1\u003c/a\u003e we are releasing the v3.2 with a number of key highly requested features in:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim Jupyter Notebook extension (Colab is coming soon…)\u003c/li\u003e\n\u003cli\u003eHistogram tracking and visualization\u003c/li\u003e\n\u003cli\u003eFull-size images, images resize and render modes\u003c/li\u003e\n\u003cli\u003eSearch autofill and suggest on the UI\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAim \u0026#x26; Jupyter notebook\u003c/h2\u003e\n\u003cp\u003eNow you can use Aim fully from your Jupyter notebook. We have built an Aim Jupyter notebook extension that comes along with the \u003ccode\u003eaim\u003c/code\u003e package.\u003c/p\u003e\n\u003cp\u003eYou don’t need to install anything other than Aim. Here are the \u003ca href=\"https://aimstack.readthedocs.io/en/latest/index.html\"\u003edocs\u003c/a\u003e.\u003cbr\u003e\nA quick demo of integrating Aim \u0026#x26; Jupyter Notebook::\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*7Xqp7esswOPWZpRoDqSawg.gif\" alt=\"\" title=\"It’s that easy to use Aim on your Jupyter notebook!\"\u003e\u003c/p\u003e\n\u003ch2\u003eHistogram tracking and Visualization\u003c/h2\u003e\n\u003cp\u003eStarting from 3.2 you can track Distributions and visualize them as histograms on the run page.\u003c/p\u003e\n\u003cp\u003eThis is how to track the weights distribution in your training code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Distribution\n\nfor step in range(1000):\n    my_run.track(\n        Distribution(tensor), # Pass distribution\n        name='gradients', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'type': 'weights',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ca href=\"http://play.aimstack.io:10003/runs/426032ad2d7e4b0385bc6c51\"\u003edemo\u003c/a\u003e, \u003ca href=\"https://aimstack.readthedocs.io/en/latest/index.html\"\u003edocs\u003c/a\u003e and \u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/pytorch_track.py\"\u003eexamples\u003c/a\u003e \u003c/p\u003e\n\u003cp\u003eAnd this is how it looks on the UI. Aim lets you visualize the distributions every layer at every tracked step. Super powerful!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Il1FjcQVpYHyuKHUVAtS0A.gif\" alt=\"\" title=\"Visualize every layer and every tracked step.\"\u003e\u003c/p\u003e\n\u003ch2\u003eFull-size images, images resize and render modes\u003c/h2\u003e\n\u003cp\u003eNow you can resize the tracked images size using the images resize tool available on the right hand tool-pain on the Images Explorer.\u003c/p\u003e\n\u003cp\u003eHere is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*zYuodUJ1ykDhrFD9rdYC_w.gif\" alt=\"\" title=\"Easily resize the queried images\"\u003e\u003c/p\u003e\n\u003cp\u003eWe have also enabled smooth and pixelated images render modes. Especially for sensitive type of images, this allows to see the detailed version of the image without any filter etc.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*pTG1Z34twqOOo7x6yoo1YQ.gif\" alt=\"\" title=\"Toggle between pixelated and smooth images render modes\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can also view full-size images by clicking on the top right corner magnifier on each images.\u003c/p\u003e\n\u003cp\u003eBTW the smooth / pixelated config also translates to the full-size images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*qvhjgoBWWCaT0Hm_6eXzOw.gif\" alt=\"\" title=\"full-size images: easy!\"\u003e\u003c/p\u003e\n\u003ch2\u003eQuery Language autosuggest editor\u003c/h2\u003e\n\u003cp\u003eAnother highly requested feature.\u003c/p\u003e\n\u003cp\u003eNow there is a QL autosuggest on all query inputs on the Aim UI. You will have all your tracked queryable metadata (params, run properties, contexts) at your hand at all times.\u003c/p\u003e\n\u003cp\u003eHopefully this makes it much easier to get started with Aim.Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*eHTlgK8CdxpUsWC_edw1rw.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community,\u003c/a\u003e share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-2-—-jupyter-notebook-integration-histograms.md","_raw":{"sourceFilePath":"posts/aim-3-2-—-jupyter-notebook-integration-histograms.md","sourceFileName":"aim-3-2-—-jupyter-notebook-integration-histograms.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-2-—-jupyter-notebook-integration-histograms"},"type":"Post"},{"title":"Aim 3.3 — Audio \u0026 Text tracking, Plotly \u0026 Colab integrations","date":"2022-01-13T15:33:59.865Z","author":"Gev Soghomonian","description":"Introducing Aim 3.3: Audio \u0026 Text Tracking, Plotly \u0026 Colab Integration, Images visualization on run details page. Democratizing AI dev tools with new features. Explore the enhancements now!","slug":"aim-3-3-audio-text-tracking-plotly-colab-integrations","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*300ZXHcgylKBit_O2KDeMA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, Aim 3.3 is now available! 😊\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.\n\nAim 3.3 is full of highly requested features. Lots of items scratched from the public Aim [roadmap](https://github.com/aimhubio/aim#roadmap).\n\n* Audio tracking and exploration\n* Text tracking and visualization\n* Scatter plot explorer\n* Colab integration\n* Plotly integration\n* Images visualization on run details page\n\nNew Demo: [play.aimstack.io:10004](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a)\n\nDemo Code: [github.com/osoblanco/FastSpeech2](https://github.com/osoblanco/FastSpeech2/blob/master/train.py)\n\nSpecial thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.\n\n## Audio tracking and exploration\n\nNow you can track audio files with Aim during your speech-to-text or other audio-involving experiments.\n\nIt will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.\n\nJust like tracking the metrics, we have enabled a simple API to track the audio.\n\n```\nfrom aim import Audio\n\nfor step in range(1000):\n    my_run.track(\n        Audio(arr), # Pass audio file or numpy array\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nHere is how it looks like on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*LkeFigHPnQM1drADXJ3yZQ.gif)\n\n## **Text tracking and visualization**\n\nUse this to compare text inputs and outputs during training\n\nSimilarly, you can also track text with Aim during your NLP experiments.\n\nHere is how the code looks like:\n\n```\nfrom aim import Text\n\nfor step in range(1000):\n    my_run.track(\n        Text(string), # Pass a string you want to track\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nThis is the end result on the UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*nkqUz9DecxaMmJoCUe_8Rw.gif \"Training info tracked as a text\")\n\n## Colab integration\n\nAfter we have integrated [Aim to Jupyter](https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms), there were many requests to enable Aim on Colab too. Now it has arrived! \n\nWith fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!\n\nHere is an example [colab to get started with](https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing).\n\n\u003e Note: Please make sure to run all the cells to be able to use the UI as well\n\nSo this is how it looks on your browser:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*SjYF_Kq4WzWOodVzsjemvA.gif)\n\n## Plotly integration\n\nNow you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.\n\nThis is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.\n\n```\nfrom aim import Figure\n\nfor step in range(1000):\n    my_run.track(\n        Figure(fig_obj), # Pass any plotly figure\n        name='plotly_bars', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n```\n\nThe end-result on the Aim Web UI.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*8xn3PH_Hzk6fotyZQqy-AQ.gif)\n\n## Images visualization on run details page\n\n\n\nAs we had launched the images tracking and visualization in[ 3.1](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer), we haven’t enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.\n\nHere is how it looks on the UI\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*jnUogPgbzwX6GOmDaXHKMg.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs.\n\nAnd don’t forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eHey community, Aim 3.3 is now available! 😊\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for helping test early versions and thanks for their contributions.\u003c/p\u003e\n\u003cp\u003eAim 3.3 is full of highly requested features. Lots of items scratched from the public Aim \u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eroadmap\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAudio tracking and exploration\u003c/li\u003e\n\u003cli\u003eText tracking and visualization\u003c/li\u003e\n\u003cli\u003eScatter plot explorer\u003c/li\u003e\n\u003cli\u003eColab integration\u003c/li\u003e\n\u003cli\u003ePlotly integration\u003c/li\u003e\n\u003cli\u003eImages visualization on run details page\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNew Demo: \u003ca href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a\"\u003eplay.aimstack.io:10004\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDemo Code: \u003ca href=\"https://github.com/osoblanco/FastSpeech2/blob/master/train.py\"\u003egithub.com/osoblanco/FastSpeech2\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSpecial thanks to Erik Arakelyan (osoblanco), Krystian Pavzkowski (krstp), Flaviu Vadan, SSamDav and AminSlk for their contributions and feedback.\u003c/p\u003e\n\u003ch2\u003eAudio tracking and exploration\u003c/h2\u003e\n\u003cp\u003eNow you can track audio files with Aim during your speech-to-text or other audio-involving experiments.\u003c/p\u003e\n\u003cp\u003eIt will allow you to track generated audio with context, query them, observe the evolution for audio-related experiments (e.g. speech-to-text). Both input, output and ground truth.\u003c/p\u003e\n\u003cp\u003eJust like tracking the metrics, we have enabled a simple API to track the audio.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Audio\n\nfor step in range(1000):\n    my_run.track(\n        Audio(arr), # Pass audio file or numpy array\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is how it looks like on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LkeFigHPnQM1drADXJ3yZQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eText tracking and visualization\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eUse this to compare text inputs and outputs during training\u003c/p\u003e\n\u003cp\u003eSimilarly, you can also track text with Aim during your NLP experiments.\u003c/p\u003e\n\u003cp\u003eHere is how the code looks like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Text\n\nfor step in range(1000):\n    my_run.track(\n        Text(string), # Pass a string you want to track\n        name='outputs', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the end result on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*nkqUz9DecxaMmJoCUe_8Rw.gif\" alt=\"\" title=\"Training info tracked as a text\"\u003e\u003c/p\u003e\n\u003ch2\u003eColab integration\u003c/h2\u003e\n\u003cp\u003eAfter we have integrated \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-2-jupyter-notebook-integration-histograms\"\u003eAim to Jupyter\u003c/a\u003e, there were many requests to enable Aim on Colab too. Now it has arrived! \u003c/p\u003e\n\u003cp\u003eWith fully embedded Aim UI, now you can track and follow your experiments live without leaving your colab environment!\u003c/p\u003e\n\u003cp\u003eHere is an example \u003ca href=\"https://colab.research.google.com/drive/1vlXVEtsKCf1390-gAfDhrvLPC14PN3-r?usp=sharing\"\u003ecolab to get started with\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: Please make sure to run all the cells to be able to use the UI as well\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSo this is how it looks on your browser:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*SjYF_Kq4WzWOodVzsjemvA.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePlotly integration\u003c/h2\u003e\n\u003cp\u003eNow you can track your custom plotly charts and visualize them on Aim with full native plotly interactive capabilities baked in.\u003c/p\u003e\n\u003cp\u003eThis is a great way to also track all your relevant plotly visualizations per step and have them rendered, navigated in Aim along with everything else already in there.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim import Figure\n\nfor step in range(1000):\n    my_run.track(\n        Figure(fig_obj), # Pass any plotly figure\n        name='plotly_bars', # The name of distributions\n        step=step,   # Step index (optional)\n        epoch=0,     # Epoch (optional)\n        context={    # Context (optional)\n            'subset': 'train',\n        },\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe end-result on the Aim Web UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*8xn3PH_Hzk6fotyZQqy-AQ.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eImages visualization on run details page\u003c/h2\u003e\n\u003cp\u003eAs we had launched the images tracking and visualization in\u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003e 3.1\u003c/a\u003e, we haven’t enabled the images on the single run page. Besides the explorer, now you can observe and search through the images on the single run page as well.\u003c/p\u003e\n\u003cp\u003eHere is how it looks on the UI\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*jnUogPgbzwX6GOmDaXHKMg.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-3-—-audio-text-tracking-plotly-colab-integrations.md","_raw":{"sourceFilePath":"posts/aim-3-3-—-audio-text-tracking-plotly-colab-integrations.md","sourceFileName":"aim-3-3-—-audio-text-tracking-plotly-colab-integrations.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-3-—-audio-text-tracking-plotly-colab-integrations"},"type":"Post"},{"title":"Aim 3.4 – Remote Tracking Alpha, Sorting \u0026 deleting runs","date":"2022-01-24T16:06:51.383Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.4  featuring remote-tracking is now available! 😊 Aim is an open source AI experiment tracking tool.","slug":"aim-3-4-remote-tracking-alpha-sorting-deleting-runs","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-Ubp6reeVEAahicQEz-SQ.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.4  featuring remote-tracking is now available! \n\nAim is an open source AI experiment tracking tool. We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\nThis is a milestone release with lots of anticipated new features and one that’s super-anticipated (remote-tracking!). Brace yourselves, the collaborative Aim is here one commit at a time!!!\n\n## Here is what’s new:\n\n* Remote tracking \\[experimental]\n* Run delete and archive: batch and single\n* Ability to stack images on the [Images Explorer](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)\n* Text filtering via regexp\n* Trendline on scatterplots\n* More images features: display images by original size, align by width, images reordering\n\nSpecial thanks to Geoffrey Chen, gormat, sjakkampudi, osoblanco, Sennevs, gloryVine, krstp and others for continuous feedback and help.\n\n## Remote tracking \\[experimental]\n\nThis is a milestone release with lots of anticipated new features and one that’s super-anticipated. Brace yourselves, the collaborative Aim is here one commit at a time!!!\n\nAim Remote Tracking is very simple and easy to get started with \n\nHere are the steps to make it work. For more details pls check out [the end-to-end guide on Aim docs](https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html).\n\n````\n### Ensure Aim version 3.4.x\n```sh\n$ pip install \"aim\u003e=3.4.0\"\n```\n\n### Initialize the remote server\n```sh\n$ aim init # (optional)\n$ aim server --repo \u003cREPO_PATH\u003e\n```\nOutput:\n```\n\u003e Server is mounted on 0.0.0.0:53800\n\u003e Press Ctrl+C to exit\n```\n### Start the Aim UI\n```\n$ aim up --repo \u003cREPO_PATH\u003e\n```\n\n### Change only 1 line in your training code\nPoint your repo to the remote location\n```py\naim_run = Run(repo='aim://172.3.66.145:53800')# replace example IP with your tracking server IP/hostname\n```\n````\n\n## Run, delete and archive: Batch and single\n\nOne of the very highly anticipated features by the Aim community. Now you can archive or hard delete your premature, interrupted runs from all the explorer pages as well as Run Details Settings tab.\n\nIt’s really easy to use. just select the run and the `Delete` / `Archive` buttons will appear.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qu9172RudbYfvFaVXVvNg.png)\n\n## Ability to stack images on the Images Explorer\n\nThis feature allows to not only view the tracked images sequentially, but also stack them by the last grouped param.\n\nOnce stacked, a slider appears that will allow you to navigate between the layers of stacked images. This feature will allow to closely observe the evolution of output images (GANs, wave outputs as img etc)\n\nHere is a quick demo [link](http://play.aimstack.io:10002/images?grouping=7sKkRob88jEDScwcb7RLVkxZMAGNj8QPqfbeDBN11Hau1HzW4sr7FGSESSWwWH7qPYX4NvhSCtkgWXMTkHXEixu6soDSnPEYZ6CCa5fqUhbpWZ17EGMYDCknV4JErUVLaxyDtt2VjKEVSgEfGdWghLKLL5euM3YAF34MwNEtRVJxuXGAreiKEtXYbwbMHrs5JJ5kZ\u0026select=gTSaqzN7Mb8GdSizQfqNj9qCzd61EVTqUzAWmhAMFt1NVE2ABWM3PcYXbvEoHXrHBBwG51ytmWtG2AXtTVbUr8PVynYCpDzJqg4ZFEGauDS7PFw91DQ7nnxPzstQYFwbxWBQTR8x2eiUdScMgXh5XvkpPpn1HQpTbKz7ZgU1GXuezCrjcVXN9KGatx2S5iJAwNS4pLWpuE6HthgD9ww8sNzQzDBJuDHHHUSgkXDV2CoJqzAEE6RKf8gsdrWwP9GLovkHk43oMqPzwcmXCnUG4TFG7oL2HGaqXk2MSAN44RAMDbhwBH9nwV4KAyCHZD6kcrvdyBdadH92iPJMsW4cDQD26XaMsRxN3mWAqkvZPvAznnppWaVNFHv4V8q4kmemmoTNRi5jCD2r4b1ZgpuVN639cNsniPdQxPHm38bw56\u0026images=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) to try out. Here is a glimpse of the UI:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdJPM1ggTMtdcri1htvZdg.png)\n\n## Text Search\n\nNow a comprehensive text searching is available to be able to outline the desired text. Regular Expressions are available among others.\n\nHere is a link for a quick [demo](http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/texts). And a glimpse of the UI:\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt8sjFzpbhomdqRDwKb-Mw.png)\n\n## Trendline on scatterplots\n\nNow you can render a trendline over your scatterplots on Scatters Explorer. We have enabled linear and Loess trendline.\n\nThanks to [sjakkampudi](https://github.com/aimhubio/aim/issues/1160) for the help on this one.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pndAq3SIevuTckBZ20wwqg.png)\n\n## More Images Features\n\nWe have added a number of new tools to the Images Explorer\n\n## Display Images by original size\n\nNow you can view \u0026 compare the tracked images by their original size. This has been a key missing ingredient for effective images comparison.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXgyBHiVPAV_8l6-im_P3A.png)\n\n## Align by width\n\nYou can also align images by their width. This is especially handy when long images are tracked, such as sound spectograms (40px / 1000px). This feature will allow to effectively compare such images.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHHMux9rYd1sLRX4OjLCeQ.png)\n\n## Images Sorting\n\nYou can also apply sorting of images by any tracked available parameter. In this case I have enabled sorting by step in descending order to see the later generated images in my GAN experiments. A lot smoother than the ones above.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4gA2EO7b0AI39YjcpLwIA.png)\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eHey team, Aim 3.4  featuring remote-tracking is now available! \u003c/p\u003e\n\u003cp\u003eAim is an open source AI experiment tracking tool. We are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eThis is a milestone release with lots of anticipated new features and one that’s super-anticipated (remote-tracking!). Brace yourselves, the collaborative Aim is here one commit at a time!!!\u003c/p\u003e\n\u003ch2\u003eHere is what’s new:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemote tracking [experimental]\u003c/li\u003e\n\u003cli\u003eRun delete and archive: batch and single\u003c/li\u003e\n\u003cli\u003eAbility to stack images on the \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003eImages Explorer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eText filtering via regexp\u003c/li\u003e\n\u003cli\u003eTrendline on scatterplots\u003c/li\u003e\n\u003cli\u003eMore images features: display images by original size, align by width, images reordering\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpecial thanks to Geoffrey Chen, gormat, sjakkampudi, osoblanco, Sennevs, gloryVine, krstp and others for continuous feedback and help.\u003c/p\u003e\n\u003ch2\u003eRemote tracking [experimental]\u003c/h2\u003e\n\u003cp\u003eThis is a milestone release with lots of anticipated new features and one that’s super-anticipated. Brace yourselves, the collaborative Aim is here one commit at a time!!!\u003c/p\u003e\n\u003cp\u003eAim Remote Tracking is very simple and easy to get started with \u003c/p\u003e\n\u003cp\u003eHere are the steps to make it work. For more details pls check out \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html\"\u003ethe end-to-end guide on Aim docs\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e### Ensure Aim version 3.4.x\n```sh\n$ pip install \"aim\u003e=3.4.0\"\n```\n\n### Initialize the remote server\n```sh\n$ aim init # (optional)\n$ aim server --repo \u0026#x3C;REPO_PATH\u003e\n```\nOutput:\n```\n\u003e Server is mounted on 0.0.0.0:53800\n\u003e Press Ctrl+C to exit\n```\n### Start the Aim UI\n```\n$ aim up --repo \u0026#x3C;REPO_PATH\u003e\n```\n\n### Change only 1 line in your training code\nPoint your repo to the remote location\n```py\naim_run = Run(repo='aim://172.3.66.145:53800')# replace example IP with your tracking server IP/hostname\n```\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eRun, delete and archive: Batch and single\u003c/h2\u003e\n\u003cp\u003eOne of the very highly anticipated features by the Aim community. Now you can archive or hard delete your premature, interrupted runs from all the explorer pages as well as Run Details Settings tab.\u003c/p\u003e\n\u003cp\u003eIt’s really easy to use. just select the run and the \u003ccode\u003eDelete\u003c/code\u003e / \u003ccode\u003eArchive\u003c/code\u003e buttons will appear.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Qu9172RudbYfvFaVXVvNg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAbility to stack images on the Images Explorer\u003c/h2\u003e\n\u003cp\u003eThis feature allows to not only view the tracked images sequentially, but also stack them by the last grouped param.\u003c/p\u003e\n\u003cp\u003eOnce stacked, a slider appears that will allow you to navigate between the layers of stacked images. This feature will allow to closely observe the evolution of output images (GANs, wave outputs as img etc)\u003c/p\u003e\n\u003cp\u003eHere is a quick demo \u003ca href=\"http://play.aimstack.io:10002/images?grouping=7sKkRob88jEDScwcb7RLVkxZMAGNj8QPqfbeDBN11Hau1HzW4sr7FGSESSWwWH7qPYX4NvhSCtkgWXMTkHXEixu6soDSnPEYZ6CCa5fqUhbpWZ17EGMYDCknV4JErUVLaxyDtt2VjKEVSgEfGdWghLKLL5euM3YAF34MwNEtRVJxuXGAreiKEtXYbwbMHrs5JJ5kZ\u0026#x26;select=gTSaqzN7Mb8GdSizQfqNj9qCzd61EVTqUzAWmhAMFt1NVE2ABWM3PcYXbvEoHXrHBBwG51ytmWtG2AXtTVbUr8PVynYCpDzJqg4ZFEGauDS7PFw91DQ7nnxPzstQYFwbxWBQTR8x2eiUdScMgXh5XvkpPpn1HQpTbKz7ZgU1GXuezCrjcVXN9KGatx2S5iJAwNS4pLWpuE6HthgD9ww8sNzQzDBJuDHHHUSgkXDV2CoJqzAEE6RKf8gsdrWwP9GLovkHk43oMqPzwcmXCnUG4TFG7oL2HGaqXk2MSAN44RAMDbhwBH9nwV4KAyCHZD6kcrvdyBdadH92iPJMsW4cDQD26XaMsRxN3mWAqkvZPvAznnppWaVNFHv4V8q4kmemmoTNRi5jCD2r4b1ZgpuVN639cNsniPdQxPHm38bw56\u0026#x26;images=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\"\u003elink\u003c/a\u003e to try out. Here is a glimpse of the UI:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdJPM1ggTMtdcri1htvZdg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eText Search\u003c/h2\u003e\n\u003cp\u003eNow a comprehensive text searching is available to be able to outline the desired text. Regular Expressions are available among others.\u003c/p\u003e\n\u003cp\u003eHere is a link for a quick \u003ca href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/texts\"\u003edemo\u003c/a\u003e. And a glimpse of the UI:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt8sjFzpbhomdqRDwKb-Mw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eTrendline on scatterplots\u003c/h2\u003e\n\u003cp\u003eNow you can render a trendline over your scatterplots on Scatters Explorer. We have enabled linear and Loess trendline.\u003c/p\u003e\n\u003cp\u003eThanks to \u003ca href=\"https://github.com/aimhubio/aim/issues/1160\"\u003esjakkampudi\u003c/a\u003e for the help on this one.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pndAq3SIevuTckBZ20wwqg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eMore Images Features\u003c/h2\u003e\n\u003cp\u003eWe have added a number of new tools to the Images Explorer\u003c/p\u003e\n\u003ch2\u003eDisplay Images by original size\u003c/h2\u003e\n\u003cp\u003eNow you can view \u0026#x26; compare the tracked images by their original size. This has been a key missing ingredient for effective images comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXgyBHiVPAV_8l6-im_P3A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eAlign by width\u003c/h2\u003e\n\u003cp\u003eYou can also align images by their width. This is especially handy when long images are tracked, such as sound spectograms (40px / 1000px). This feature will allow to effectively compare such images.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHHMux9rYd1sLRX4OjLCeQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eImages Sorting\u003c/h2\u003e\n\u003cp\u003eYou can also apply sorting of images by any tracked available parameter. In this case I have enabled sorting by step in descending order to see the later generated images in my GAN experiments. A lot smoother than the ones above.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4gA2EO7b0AI39YjcpLwIA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-4-–-remote-tracking-alpha-sorting-deleting-runs.md","_raw":{"sourceFilePath":"posts/aim-3-4-–-remote-tracking-alpha-sorting-deleting-runs.md","sourceFileName":"aim-3-4-–-remote-tracking-alpha-sorting-deleting-runs.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-4-–-remote-tracking-alpha-sorting-deleting-runs"},"type":"Post"},{"title":"Aim 3.5 — TensorBoard logs support, Matplotlib integration \u0026 System Params logging","date":"2022-02-10T16:15:32.942Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.5 featuring TensorBoard logs support is now available!! We are on a mission to democratize MLOps tools. ","slug":"aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xQnlTBk3BmsoMgsaE_0oQ.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.5 featuring TensorBoard logs support is now available!! We are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is what’s new:\n\n* Aim supports TensorBoard logs\n* Track system params, CLI ,Env, Executable, Git, Installed packages\n* Matplotlib figure tracking and visualization\n* Ability move runs between Aim repos\n\n\u003e Special thanks to [gloryVine](https://github.com/gloryVine), [hughperkins](https://github.com/hughperkins), [Ssamdav](https://github.com/SSamDav), [ptaejoon](https://github.com/ptaejoon), mahnerak, gormat and [Mike](https://github.com/mikel-brostrom)l for feedback, issues and help.\n\n## TensorBoard logs support\n\nIt’s been one of the most highly requested features by the Aim community. This feature was available for Aim 2.x and folks used to love this it. However, we had to drop it due to the backend changes.\n\nNow its back! Better than it was before and this is how it works:\n\n```\n$ cd /path/to/.aim\n$ aim convert tf --logdir ~/tensorflow/logdir\n```\n\nThis command will  scan then convert the `scalar` and `image` type logs from your directory into Aim runs.\n\nRead more about how it works [here](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-tensorboard-logs-in-aim).\n\n## Tracking Env info, git info with Aim\n\nTracking your ENV variables, CLI argument, git info, etc could be a lot of details to care about.\n\nNow there is a way to enable Aim to track the environment info automatically and they will be available as params.\n\nIt takes a small tweak to enable that:\n\n```\nrun = Run(log_system_params=True)\n```\n\nThen once tracked, you can search experiments based on these values too:\n\n```\nrun.__system_params.git_info.branch == 'feature/testing'\n```\n\nMore on this feature find out [here](https://aimstack.readthedocs.io/en/latest/using/configure_runs.html#how-to-enable-system-params-automatic-logging).\n\n\u003e A special UI for these tracked data is to be shipped with the next version.\n\n## Tracking Matplotlib figures\n\nStarting Aim 3.5 you can also track [Matplotlib](https://matplotlib.org/) figures with Aim. During research (especially with Jupyter Notebooks) Matplotlib is very helpful in rendering intermediate images for analysis.\n\nNow you can track all such figures on Aim (both as Matplotlib figure and as an image). When tracking as an image, you can query and compare them too at scale on the [Images Explorer.](https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYdPka8XdPLAOxLdITESAw.png)\n\n[Here are the docs for more details](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#figure-tracking-with-aim).\n\n## Moving Runs between Aim repos\n\nWe have added a CLI command to move runs between folders.\n\nThis will allow to easily move your best runs from a draft scratch project to your main one with one command. Here is how it works:\n\n```\nim mv --destination /new/path/to/.aim \u003cmy_run_hash_1\u003e ...\n```\n\n\\\nFor more info, check the runs subcommand docs [here](https://aimstack.readthedocs.io/en/latest/refs/cli.html#runs).\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling  and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003eHey team, Aim 3.5 featuring TensorBoard logs support is now available!! We are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what’s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim supports TensorBoard logs\u003c/li\u003e\n\u003cli\u003eTrack system params, CLI ,Env, Executable, Git, Installed packages\u003c/li\u003e\n\u003cli\u003eMatplotlib figure tracking and visualization\u003c/li\u003e\n\u003cli\u003eAbility move runs between Aim repos\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to \u003ca href=\"https://github.com/gloryVine\"\u003egloryVine\u003c/a\u003e, \u003ca href=\"https://github.com/hughperkins\"\u003ehughperkins\u003c/a\u003e, \u003ca href=\"https://github.com/SSamDav\"\u003eSsamdav\u003c/a\u003e, \u003ca href=\"https://github.com/ptaejoon\"\u003eptaejoon\u003c/a\u003e, mahnerak, gormat and \u003ca href=\"https://github.com/mikel-brostrom\"\u003eMike\u003c/a\u003el for feedback, issues and help.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eTensorBoard logs support\u003c/h2\u003e\n\u003cp\u003eIt’s been one of the most highly requested features by the Aim community. This feature was available for Aim 2.x and folks used to love this it. However, we had to drop it due to the backend changes.\u003c/p\u003e\n\u003cp\u003eNow its back! Better than it was before and this is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd /path/to/.aim\n$ aim convert tf --logdir ~/tensorflow/logdir\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis command will  scan then convert the \u003ccode\u003escalar\u003c/code\u003e and \u003ccode\u003eimage\u003c/code\u003e type logs from your directory into Aim runs.\u003c/p\u003e\n\u003cp\u003eRead more about how it works \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-tensorboard-logs-in-aim\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eTracking Env info, git info with Aim\u003c/h2\u003e\n\u003cp\u003eTracking your ENV variables, CLI argument, git info, etc could be a lot of details to care about.\u003c/p\u003e\n\u003cp\u003eNow there is a way to enable Aim to track the environment info automatically and they will be available as params.\u003c/p\u003e\n\u003cp\u003eIt takes a small tweak to enable that:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun = Run(log_system_params=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen once tracked, you can search experiments based on these values too:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.__system_params.git_info.branch == 'feature/testing'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMore on this feature find out \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/configure_runs.html#how-to-enable-system-params-automatic-logging\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA special UI for these tracked data is to be shipped with the next version.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eTracking Matplotlib figures\u003c/h2\u003e\n\u003cp\u003eStarting Aim 3.5 you can also track \u003ca href=\"https://matplotlib.org/\"\u003eMatplotlib\u003c/a\u003e figures with Aim. During research (especially with Jupyter Notebooks) Matplotlib is very helpful in rendering intermediate images for analysis.\u003c/p\u003e\n\u003cp\u003eNow you can track all such figures on Aim (both as Matplotlib figure and as an image). When tracking as an image, you can query and compare them too at scale on the \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-1-images-tracker-and-images-explorer\"\u003eImages Explorer.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYdPka8XdPLAOxLdITESAw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#figure-tracking-with-aim\"\u003eHere are the docs for more details\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eMoving Runs between Aim repos\u003c/h2\u003e\n\u003cp\u003eWe have added a CLI command to move runs between folders.\u003c/p\u003e\n\u003cp\u003eThis will allow to easily move your best runs from a draft scratch project to your main one with one command. Here is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eim mv --destination /new/path/to/.aim \u0026#x3C;my_run_hash_1\u003e ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cbr\u003e\nFor more info, check the runs subcommand docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/refs/cli.html#runs\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling  and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-5-—-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","_raw":{"sourceFilePath":"posts/aim-3-5-—-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","sourceFileName":"aim-3-5-—-tensorboard-logs-support-matplotlib-integration-system-params-logging.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-5-—-tensorboard-logs-support-matplotlib-integration-system-params-logging"},"type":"Post"},{"title":"Aim 3.6 — Chart export, PyTorch Ignite \u0026 Activeloop Hub integrations","date":"2022-02-24T16:27:07.254Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite \u0026 Activeloop Hub integrations is now available!","slug":"aim-3-6-chart-export-pytorch-ignite-activeloop-hub-integrations","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-4kxMg5kF4ntVhW5889Xw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite \u0026 Activeloop Hub integrations is now available!\n\nWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is what’s new in addition to [Aim 3.5](https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging).\n\n* Export chart as image\n* MLflow to Aim logs converter\n* Pytorch Ignite integration\n* Activeloop Hub integration\n* Wildcard support for *aim runs* subcommands\n\n*Special thanks to [krstp](https://github.com/krstp), [ptaejoon](https://github.com/ptaejoon), [farizrahman4u](https://github.com/farizrahman4u) from Activeloop, [vfdev-5](https://github.com/vfdev-5) from PyTorch Ignite, [hughperkins](https://github.com/hughperkins), [Ssamdav](https://github.com/ssamdav) and [mahnerak](https://github.com/mahnerak) for feedback, reviews and help.*\n\n## Export chart as image\n\nHey awesome Aim community, sorry for shipping [this feature](https://github.com/aimhubio/aim/issues/339) so late. We know it has been so highly requested, just couldn’t get our hands on it over and over again.\n\nNow the images export is here! You can export charts as JPEG, PNG and SVG so you can include the awesome Aim visuals in your reports and research papers.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0-dHK5OoYjUAWhyKfBWA.png)\n\nYou can control the width, height, the format of the downloadable image.\n\n\u003e The ability to add / edit legends will be added in the coming versions.\n\nWould still love to hear feedback on how would you like us to tune this feature.\n\n## MLflow to Aim logs converter\n\nIts been also [requested](https://github.com/aimhubio/aim/issues/409) across the board by many users. Now you can convert your MLflow logs to Aim so you can compare all of them regardless on where it was tracked.\n\nHere is how it works:\n\n```\n$ aim init\n$ aim convert mlflow --tracking_uri 'file:///Users/aim_user/mlruns'\n```\n\nBesides the metrics, You can convert Images, Texts and Sound/Audios. More on that in [here](https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-mlflow-logs-in-aim).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idkzKK4Ua8N3ATiokIx_rQ.png)\n\n## Pytorch Ignite integration\n\nFor all the Aim users who use PyTorch Ignite, there is now a native PI callback available so you can track your basic metrics automatically!\n\nHere is a code snippet on how to use the PI integration:\n\n```\n# call aim sdk designed for pytorch ignite\nfrom aim.pytorch_ignite import AimLogger\n\n# track experimential data by using Aim\naim_logger = AimLogger(\n    experiment='aim_on_pt_ignite',\n    train_metric_prefix='train_',\n    val_metric_prefix='val_',\n    test_metric_prefix='test_',\n)\n\n# track experimential data by using Aim\naim_logger.attach_output_handler(\n    train_evaluator,\n    event_name=Events.EPOCH_COMPLETED,\n    tag=\"train\",\n    metric_names=[\"nll\", \"accuracy\"],\n    global_step_transform=global_step_from_engine(trainer),\n)\n```\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKznHJq6W2XxmzyYpF17og.png)\n\n## Activeloop Hub integration\n\nExcited to share that thanks to [an integration with Activeloop](https://activeloop.ai/), starting Aim 3.6 you can add your [Hub dataset](https://github.com/activeloopai/Hub) info as a run param and use them to search and compare your runs across all Explorers.\n\nHere is a code snippet to demonstrate the usage:\n\n```\nimport hub\n\nfrom aim.sdk.objects.plugins.hub_dataset import HubDataset\nfrom aim.sdk import Run\n\n# create dataset object\nds = hub.dataset('hub://activeloop/cifar100-test')\n\n# log dataset metadata\nrun = Run(system_tracking_interval=None)\nrun['hub_ds'] = HubDataset(ds)\n```\n\nThe following information becomes available among others:\n\n```\nrun.hub_ds.version\nrun.hub_ds.num_samples\nrun.hub_ds.tensors.name\nrun.hub_ds.tensors.compression_type\n```\n\nHub is an awesome tool to build, manage, query \u0026 visualize datasets for deep learning, as well as stream data real-time to PyTorch/TensorFlow \u0026 version-control it. Check out the [Hub docs](https://docs.activeloop.ai/).\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XlJrxPe3g9RYdqkvMaVUg.png)\n\nWildcard support for the \\`aim runs\\` command\n\nOn [Aim 3.5](https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging) we made available the `run mv` command which allows to move single runs between folders.\n\nNow we have also added the wildcard to move the whole folder altogether\n\nHere is the CLI interface:\n\n```\n$ aim runs --repo \u003csource_repo/.aim\u003e mv * --destination \u003cdest_repo/.aim\u003e\n```\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](http://github.com/aimhubio/aim) a star on GitHub for support 🙌.","html":"\u003cp\u003eHey team, Aim 3.6 featuring Mlflow logs converter, Chart export, PyTorch Ignite \u0026#x26; Activeloop Hub integrations is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what’s new in addition to \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging\"\u003eAim 3.5\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExport chart as image\u003c/li\u003e\n\u003cli\u003eMLflow to Aim logs converter\u003c/li\u003e\n\u003cli\u003ePytorch Ignite integration\u003c/li\u003e\n\u003cli\u003eActiveloop Hub integration\u003c/li\u003e\n\u003cli\u003eWildcard support for \u003cem\u003eaim runs\u003c/em\u003e subcommands\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eSpecial thanks to \u003ca href=\"https://github.com/krstp\"\u003ekrstp\u003c/a\u003e, \u003ca href=\"https://github.com/ptaejoon\"\u003eptaejoon\u003c/a\u003e, \u003ca href=\"https://github.com/farizrahman4u\"\u003efarizrahman4u\u003c/a\u003e from Activeloop, \u003ca href=\"https://github.com/vfdev-5\"\u003evfdev-5\u003c/a\u003e from PyTorch Ignite, \u003ca href=\"https://github.com/hughperkins\"\u003ehughperkins\u003c/a\u003e, \u003ca href=\"https://github.com/ssamdav\"\u003eSsamdav\u003c/a\u003e and \u003ca href=\"https://github.com/mahnerak\"\u003emahnerak\u003c/a\u003e for feedback, reviews and help.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003eExport chart as image\u003c/h2\u003e\n\u003cp\u003eHey awesome Aim community, sorry for shipping \u003ca href=\"https://github.com/aimhubio/aim/issues/339\"\u003ethis feature\u003c/a\u003e so late. We know it has been so highly requested, just couldn’t get our hands on it over and over again.\u003c/p\u003e\n\u003cp\u003eNow the images export is here! You can export charts as JPEG, PNG and SVG so you can include the awesome Aim visuals in your reports and research papers.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0-dHK5OoYjUAWhyKfBWA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can control the width, height, the format of the downloadable image.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe ability to add / edit legends will be added in the coming versions.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWould still love to hear feedback on how would you like us to tune this feature.\u003c/p\u003e\n\u003ch2\u003eMLflow to Aim logs converter\u003c/h2\u003e\n\u003cp\u003eIts been also \u003ca href=\"https://github.com/aimhubio/aim/issues/409\"\u003erequested\u003c/a\u003e across the board by many users. Now you can convert your MLflow logs to Aim so you can compare all of them regardless on where it was tracked.\u003c/p\u003e\n\u003cp\u003eHere is how it works:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim init\n$ aim convert mlflow --tracking_uri 'file:///Users/aim_user/mlruns'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBesides the metrics, You can convert Images, Texts and Sound/Audios. More on that in \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/convert_data.html#show-mlflow-logs-in-aim\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idkzKK4Ua8N3ATiokIx_rQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePytorch Ignite integration\u003c/h2\u003e\n\u003cp\u003eFor all the Aim users who use PyTorch Ignite, there is now a native PI callback available so you can track your basic metrics automatically!\u003c/p\u003e\n\u003cp\u003eHere is a code snippet on how to use the PI integration:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# call aim sdk designed for pytorch ignite\nfrom aim.pytorch_ignite import AimLogger\n\n# track experimential data by using Aim\naim_logger = AimLogger(\n    experiment='aim_on_pt_ignite',\n    train_metric_prefix='train_',\n    val_metric_prefix='val_',\n    test_metric_prefix='test_',\n)\n\n# track experimential data by using Aim\naim_logger.attach_output_handler(\n    train_evaluator,\n    event_name=Events.EPOCH_COMPLETED,\n    tag=\"train\",\n    metric_names=[\"nll\", \"accuracy\"],\n    global_step_transform=global_step_from_engine(trainer),\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKznHJq6W2XxmzyYpF17og.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eActiveloop Hub integration\u003c/h2\u003e\n\u003cp\u003eExcited to share that thanks to \u003ca href=\"https://activeloop.ai/\"\u003ean integration with Activeloop\u003c/a\u003e, starting Aim 3.6 you can add your \u003ca href=\"https://github.com/activeloopai/Hub\"\u003eHub dataset\u003c/a\u003e info as a run param and use them to search and compare your runs across all Explorers.\u003c/p\u003e\n\u003cp\u003eHere is a code snippet to demonstrate the usage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport hub\n\nfrom aim.sdk.objects.plugins.hub_dataset import HubDataset\nfrom aim.sdk import Run\n\n# create dataset object\nds = hub.dataset('hub://activeloop/cifar100-test')\n\n# log dataset metadata\nrun = Run(system_tracking_interval=None)\nrun['hub_ds'] = HubDataset(ds)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following information becomes available among others:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.hub_ds.version\nrun.hub_ds.num_samples\nrun.hub_ds.tensors.name\nrun.hub_ds.tensors.compression_type\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHub is an awesome tool to build, manage, query \u0026#x26; visualize datasets for deep learning, as well as stream data real-time to PyTorch/TensorFlow \u0026#x26; version-control it. Check out the \u003ca href=\"https://docs.activeloop.ai/\"\u003eHub docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XlJrxPe3g9RYdqkvMaVUg.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWildcard support for the `aim runs` command\u003c/p\u003e\n\u003cp\u003eOn \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-5-tensorboard-logs-support-matplotlib-integration-system-params-logging\"\u003eAim 3.5\u003c/a\u003e we made available the \u003ccode\u003erun mv\u003c/code\u003e command which allows to move single runs between folders.\u003c/p\u003e\n\u003cp\u003eNow we have also added the wildcard to move the whole folder altogether\u003c/p\u003e\n\u003cp\u003eHere is the CLI interface:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ aim runs --repo \u0026#x3C;source_repo/.aim\u003e mv * --destination \u0026#x3C;dest_repo/.aim\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"http://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e"},"_id":"posts/aim-3-6-—-chart-export-pytorch-ignite-activeloop-hub-integrations.md","_raw":{"sourceFilePath":"posts/aim-3-6-—-chart-export-pytorch-ignite-activeloop-hub-integrations.md","sourceFileName":"aim-3-6-—-chart-export-pytorch-ignite-activeloop-hub-integrations.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-6-—-chart-export-pytorch-ignite-activeloop-hub-integrations"},"type":"Post"},{"title":"Aim 3.7 — Revamped Run Single Page and Aim Docker Image","date":"2022-03-16T16:59:53.238Z","author":"Gev Soghomonian","description":"Hey team, Aim 3.7 featuring Aim Docker Image is now available! We are on a mission to democratize MLOps tools.","slug":"aim-3-7-revamped-run-single-page-and-aim-docker-image","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INjZ3YZMiIgIc3EhLos_sw.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.7 featuring Aim Docker Image is now available!\n\nWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\n\n[Aim](https://github.com/aimhubio/aim) is an open-source, self-hosted ML experiment tracking tool. It’s good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\n\nYou can use not only the great Aim UI but also its SDK to query your runs’ metadata programmatically. That’s especially useful for automations and additional analysis on a Jupyter Notebook.\n\nHere is what’s new in the V3.7 release:\n\n* Completely revamped Run Single Page\n* TF/Keras adaptors refactoring\n* Aim Docker Image\n\n\u003e Special thanks to osoblanco for the help, insights and feedback.\n\n## **Run Single Page Overview**\n\nWe have created an overview section on the Run Single page to quickly observe the relevant info about your before getting deep into it.\n\n* The last values of metrics\n* Flattened list of key params\n* Overview information about your run\n* Tracked reproducibility fields (env and git info, etc)\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQXh2J_Clq2SE1EDq_ooQ.png)\n\nAim Docker Image\n\nNow you can pull the Aim docker image from the [docker hub](https://hub.docker.com/r/aimstack/aim)!\n\nFor the users who are trying to set up Aim on their clusters or server infrastructure via docker, now you can use the native container — released with each release.\n\nThe Aim docker container is pre-packaged Aim that can be mounted on `.aim` directory and UI could be ran.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjSOM1o9Dk4WNHgDEcE64w.png \"Aimstack docker image\")\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAlso check the recent blog post on [Aim 3.6](https://aimstack.io/blog/new-releases/aim-3-6-chart-export-pytorch-ignite-activeloop-hub-integrations) release featuring Mlflow logs converter \u0026 more!","html":"\u003cp\u003eHey team, Aim 3.7 featuring Aim Docker Image is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize MLOps tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e is an open-source, self-hosted ML experiment tracking tool. It’s good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\u003c/p\u003e\n\u003cp\u003eYou can use not only the great Aim UI but also its SDK to query your runs’ metadata programmatically. That’s especially useful for automations and additional analysis on a Jupyter Notebook.\u003c/p\u003e\n\u003cp\u003eHere is what’s new in the V3.7 release:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompletely revamped Run Single Page\u003c/li\u003e\n\u003cli\u003eTF/Keras adaptors refactoring\u003c/li\u003e\n\u003cli\u003eAim Docker Image\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to osoblanco for the help, insights and feedback.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\u003cstrong\u003eRun Single Page Overview\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eWe have created an overview section on the Run Single page to quickly observe the relevant info about your before getting deep into it.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe last values of metrics\u003c/li\u003e\n\u003cli\u003eFlattened list of key params\u003c/li\u003e\n\u003cli\u003eOverview information about your run\u003c/li\u003e\n\u003cli\u003eTracked reproducibility fields (env and git info, etc)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQXh2J_Clq2SE1EDq_ooQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim Docker Image\u003c/p\u003e\n\u003cp\u003eNow you can pull the Aim docker image from the \u003ca href=\"https://hub.docker.com/r/aimstack/aim\"\u003edocker hub\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eFor the users who are trying to set up Aim on their clusters or server infrastructure via docker, now you can use the native container — released with each release.\u003c/p\u003e\n\u003cp\u003eThe Aim docker container is pre-packaged Aim that can be mounted on \u003ccode\u003e.aim\u003c/code\u003e directory and UI could be ran.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjSOM1o9Dk4WNHgDEcE64w.png\" alt=\"\" title=\"Aimstack docker image\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAlso check the recent blog post on \u003ca href=\"https://aimstack.io/blog/new-releases/aim-3-6-chart-export-pytorch-ignite-activeloop-hub-integrations\"\u003eAim 3.6\u003c/a\u003e release featuring Mlflow logs converter \u0026#x26; more!\u003c/p\u003e"},"_id":"posts/aim-3-7-—-revamped-run-single-page-and-aim-docker-image.md","_raw":{"sourceFilePath":"posts/aim-3-7-—-revamped-run-single-page-and-aim-docker-image.md","sourceFileName":"aim-3-7-—-revamped-run-single-page-and-aim-docker-image.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-7-—-revamped-run-single-page-and-aim-docker-image"},"type":"Post"},{"title":"Aim 3.8 — DVC integration \u0026 Extensible HuggingFace callbacks","date":"2022-04-04T20:27:53.206Z","author":"Gev Soghomonian","description":"Aim 3.8 featuring extensible HuggingFace trainer callbacks is out ! We are on a mission to democratize AI dev tools. ","slug":"aim-3-8-dvc-integration-extensible-huggingface-callbacks","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIfbNRqB72W36j8gooKi4Q.png","draft":false,"categories":["New Releases"],"body":{"raw":"Aim 3.8 featuring extensible HuggingFace trainer callbacks is out !\n\nWe are on a mission to[ democratize AI dev tools](https://aimstack.io/blog/tutorials/aim-from-zero-to-hero). Thanks to the awesome Aim community for the help and contributions.\n\nHere is what’s new:\n\n* Color scale of numeric values\n* DVC integration\n* Extensible HuggingFace and XGBoost callbacks\n\n\u003e Special thanks to osoblanco, ashutoshsaboo and mohamadelgaar for the help and feedback\n\n\\\nColor scale of numeric values\n\nDue to popular demand an Aim 2.x feature is back!\n\nNow you can color-code the aggregated metric values on the Runs Explorer. The highest values will be in green and the lowest in yellow. The color changes from yellow to green at the median.\n\nWe are surely going to iterate over this feature. Further feedbacks are welcome!\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOgBqVnIeq3fc_EduNAAfw.png \"Aimstack: Color scale of numeric values\")\n\n## DVC integration\n\nNow you can track the info about your DVC tracked files and datasets on Aim. With this we are iterating towards easy connection between Aim and DVC so you can connect your tracked artifacts with experiment tracking.\n\nShare more feedback with us. How would you like to see this evolve?\n\nThe code looks as simple as this:\n\n```\nfrom aim.sdk import Run\nfrom aim.sdk.objects.plugins.dvc_metadata import DvcData\n\nrun = Run(system_tracking_interval=None)\n\npath_to_dvc_repo = '.'\nrun['dvc_info'] = DvcData(path_to_dvc_repo)\n```\n\n## Extensible HuggingFace and XGBoost callbacks\n\nWhen using Aim with your favorite frameworks, the metadata is logged through `AimCallback` which is limited as it allows only specific group of logged metrics per framework.\n\nNow you can extend the AimCallback and log any other metadata made available by the framework. [Detailed docs here.](https://aimstack.readthedocs.io/en/latest/using/integration_guides.html)\n\nHere is an example on how to extend the HuggingFace callback to track texts with Aim:\n\n```\nfrom aim.hugging_face import AimCallback\nfrom aim import Text\n\n\nclass CustomCallback(AimCallback):\n    def on_log(self, args, state, control,\n               model=None, logs=None, **kwargs):\n        super().on_log(args, state, control, model, logs, **kwargs)\n\n        context = {\n            'subset': self._current_shift,\n        }\n        for log_name, log_value in logs.items():\n            if isinstance(log_value, str):\n                self.experiment.track(Text(log_value), name=log_name, context=context)\n```\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave Aim a star on GitHub for support.","html":"\u003cp\u003eAim 3.8 featuring extensible HuggingFace trainer callbacks is out !\u003c/p\u003e\n\u003cp\u003eWe are on a mission to\u003ca href=\"https://aimstack.io/blog/tutorials/aim-from-zero-to-hero\"\u003e democratize AI dev tools\u003c/a\u003e. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what’s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eColor scale of numeric values\u003c/li\u003e\n\u003cli\u003eDVC integration\u003c/li\u003e\n\u003cli\u003eExtensible HuggingFace and XGBoost callbacks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to osoblanco, ashutoshsaboo and mohamadelgaar for the help and feedback\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cbr\u003e\nColor scale of numeric values\u003c/p\u003e\n\u003cp\u003eDue to popular demand an Aim 2.x feature is back!\u003c/p\u003e\n\u003cp\u003eNow you can color-code the aggregated metric values on the Runs Explorer. The highest values will be in green and the lowest in yellow. The color changes from yellow to green at the median.\u003c/p\u003e\n\u003cp\u003eWe are surely going to iterate over this feature. Further feedbacks are welcome!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOgBqVnIeq3fc_EduNAAfw.png\" alt=\"\" title=\"Aimstack: Color scale of numeric values\"\u003e\u003c/p\u003e\n\u003ch2\u003eDVC integration\u003c/h2\u003e\n\u003cp\u003eNow you can track the info about your DVC tracked files and datasets on Aim. With this we are iterating towards easy connection between Aim and DVC so you can connect your tracked artifacts with experiment tracking.\u003c/p\u003e\n\u003cp\u003eShare more feedback with us. How would you like to see this evolve?\u003c/p\u003e\n\u003cp\u003eThe code looks as simple as this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.sdk import Run\nfrom aim.sdk.objects.plugins.dvc_metadata import DvcData\n\nrun = Run(system_tracking_interval=None)\n\npath_to_dvc_repo = '.'\nrun['dvc_info'] = DvcData(path_to_dvc_repo)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eExtensible HuggingFace and XGBoost callbacks\u003c/h2\u003e\n\u003cp\u003eWhen using Aim with your favorite frameworks, the metadata is logged through \u003ccode\u003eAimCallback\u003c/code\u003e which is limited as it allows only specific group of logged metrics per framework.\u003c/p\u003e\n\u003cp\u003eNow you can extend the AimCallback and log any other metadata made available by the framework. \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/integration_guides.html\"\u003eDetailed docs here.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example on how to extend the HuggingFace callback to track texts with Aim:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hugging_face import AimCallback\nfrom aim import Text\n\n\nclass CustomCallback(AimCallback):\n    def on_log(self, args, state, control,\n               model=None, logs=None, **kwargs):\n        super().on_log(args, state, control, model, logs, **kwargs)\n\n        context = {\n            'subset': self._current_shift,\n        }\n        for log_name, log_value in logs.items():\n            if isinstance(log_value, str):\n                self.experiment.track(Text(log_value), name=log_name, context=context)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave Aim a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-3-8-—-dvc-integration-extensible-huggingface-callbacks.md","_raw":{"sourceFilePath":"posts/aim-3-8-—-dvc-integration-extensible-huggingface-callbacks.md","sourceFileName":"aim-3-8-—-dvc-integration-extensible-huggingface-callbacks.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-8-—-dvc-integration-extensible-huggingface-callbacks"},"type":"Post"},{"title":"Aim 3.9 — Notes on training runs and upgrade to Pytorch Lightning 1.6 Pytorch Lightning","date":"2022-05-08T20:34:11.966Z","author":"Gev Soghomonian","description":"Discover Aim 3.9's latest features! Add notes on Single Run page, enhance Table component usability, and more. Read the article for full details!","slug":"aim-3-9-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoVjVDwfOytJ-FyRqp_n0Q.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey team, Aim 3.9 is now available!\n\nWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\n\nHere is what’s new:\n\n* Notes on training runs\n* Upgrade to Pytorch Lightning 1.6\n\n\u003e Special thanks to ashutoshsaboo, haritsahm and Justin Burton for the help and feedback.\n\n## Notes on the training runs\n\nNow there is an ability to add rich metadata edit (similar to notion) over the runs. It’s been a highly requested feature to be able to leave relevant code, result-related notes, next steps etc on the runs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghbd3OrbVatbwSwnwpIm-A.png)\n\n## Upgrade to Pytorch Lightning 1.6\n\nWe have upgraded the Aim Logger for Pytorch Lightning 1.6 updates.\n\nLearn more in the following thread: \u003chttps://github.com/aimhubio/aim/issues/1619\u003e\n\nMeanwhile, check out an earlier [tutorial](https://aimstack.io/blog/tutorials/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim) on how to tune hyperparams with fixed seeds using PyTorch Lightning and Aim.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave Aim a star on GitHub for support","html":"\u003cp\u003eHey team, Aim 3.9 is now available!\u003c/p\u003e\n\u003cp\u003eWe are on a mission to democratize AI dev tools. Thanks to the awesome Aim community for the help and contributions.\u003c/p\u003e\n\u003cp\u003eHere is what’s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNotes on training runs\u003c/li\u003e\n\u003cli\u003eUpgrade to Pytorch Lightning 1.6\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpecial thanks to ashutoshsaboo, haritsahm and Justin Burton for the help and feedback.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eNotes on the training runs\u003c/h2\u003e\n\u003cp\u003eNow there is an ability to add rich metadata edit (similar to notion) over the runs. It’s been a highly requested feature to be able to leave relevant code, result-related notes, next steps etc on the runs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghbd3OrbVatbwSwnwpIm-A.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eUpgrade to Pytorch Lightning 1.6\u003c/h2\u003e\n\u003cp\u003eWe have upgraded the Aim Logger for Pytorch Lightning 1.6 updates.\u003c/p\u003e\n\u003cp\u003eLearn more in the following thread: \u003ca href=\"https://github.com/aimhubio/aim/issues/1619\"\u003ehttps://github.com/aimhubio/aim/issues/1619\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMeanwhile, check out an earlier \u003ca href=\"https://aimstack.io/blog/tutorials/how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim\"\u003etutorial\u003c/a\u003e on how to tune hyperparams with fixed seeds using PyTorch Lightning and Aim.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave Aim a star on GitHub for support\u003c/p\u003e"},"_id":"posts/aim-3-9-—-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","_raw":{"sourceFilePath":"posts/aim-3-9-—-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","sourceFileName":"aim-3-9-—-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-3-9-—-notes-on-training-runs-and-upgrade-to-pytorch-lightning-1-6-pytorch-lightning"},"type":"Post"},{"title":"AimOS: Open-source modular observability for AI Systems","date":"2023-10-12T20:09:25.239Z","author":"Gev Sogomonian, Gor Arakelyan, Ben Lorica","description":"Discover AimOS: Open-source modular observability for AI systems. Easily log, connect and observe any parts of your AI Systems from experiments to production to prompts to AI system monitoring","slug":"aim-4-0-open-source-modular-observability-for-ai-systems","image":"/images/dynamic/medium8.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The software lifecycle is broken\n\nAI is changing how software is built and deployed. AI Systems are not deterministic, so observability is a key layer in the AI Systems development stack.\n\nChanging the code is no longer enough to change the behavior of the AI system - a kind of systems that are inherently unreliable and hard to iterate on.\n\nWith the rise of the LLMs and Generative AI, there are many more use-cases that are built with AI. This means more software needs to be monitored and observed.\n\n\u003e The mainstream ideas of software logging and observability may not work anymore as the use-cases and needs have increased dramatically.\n\n## Software systems need to be logged\n\nLogging is inseparable from software all along - from code versioning to production monitoring to AI experiment tracking to agent tracing to full observability.\n\n![](/images/dynamic/1111-1.png)\n\n**With AI systems, we now have to deal with many more dimensions. Developing an AI System is as demanding as deploying it to production and integrating it with other software components.**\n\nWe are now faced with problems of AI alignment and safety. AI regulation conversations are moving at full speed. The AI system governance and lineage are key when tackling any of these problems.\n\nFor this, teams are trying to log as much as they can at every possible dimension.\n\nTo satisfy these needs, we must be able to:\n\n* Log anything from every part of the AI Infra\n* Observe and interpret the logged data at scale in a flexible fashion\n* Add layers and layers of automations around the logged data\n\nAltogether these form the observability and logging layer for AI Systems.\n\n\u003e The observability layer for AI Systems is inherently complex and needs to encompass end-to-end development + production and beyond to allow to fully connect the dots.\n\n## What do developers need from the AI Systems observability layer?\n\n\n\nWe need to log and observe at every step of building AI Systems.\n\n![Basic AI system observability](/images/dynamic/222-1.png \"Basic AI system observability\")\n\nOver the last three years, we have collaborated with hundreds of teams to enhance our understanding of observability requirements in AI systems. The capability to track diverse data sources stands out, given that logs are invaluable for discerning system health and facilitating infrastructure evolution.\n\nIn addition, the ability to customize log visualizations increases the depth of analysis, enabling teams to discern intricate relationships within their setups. The use of automation ensures not only optimal performance but also increased reliability for AI systems.\n\nThe lineage of an AI System is also vitally important, covering a wide range of considerations from governance to regulatory to best practices. The need for robust governance over AI Systems grows as compliance needs increase. In the end, we aim for an integrated approach to governance that provides detailed lineage insights and unwavering traceability.\n\n![Key requirements: AI Observability Layer](/images/dynamic/444-1.png \"Key requirements: AI Observability Layer\")\n\n### We need well-developed open-source tools for observability use-cases\n\nLogs are now essential for overcoming the fundamental challenges of AI adoption. It is crucial for organizations to maintain ownership of their logs and ensure that they remain in an open format without third-party control. As AI systems evolve, the tools used for observability must also mature.\n\n\u003e Ideally, these tools should be easy to use but powerful enough to meet next-stage infrastructure needs.\n\nUnfortunately, current mainstream observability tools are centralized, lack interoperability, and mostly run on closed-source platforms. These tools are not inherently scalable and often only address a fraction of the overall observability requirements.\n\n## Introducing AimOS - Operating system for logs\n\n\n\nAim began as a tool for tracking experiments, and it ran on a single node\n\n* Aim 2.0 - AI experiment logger for metrics and hyperparameters.\n* [Aim 3.0 ](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative)- AI experiment logger for any kind of Python objects.\n* **AimOS - Operating system for logs.** 💫\n\nAimOS runs logging applications that can easily be composed and extended to larger ones encompassing both the main and long-tail use-cases.\n\n![Modular observability for AI Systems](/images/dynamic/333-1.png \"Modular observability for AI Systems\")\n\nWithin the context of AimOS, logging applications are Python packages that AimOS can run.\n\n## There are many logging apps that need to exist\n\n![AimOS logging app structure](/images/dynamic/555-1.png \"AimOS logging app structure\")\n\nThere are many logging uses-cases for AI systems, which necessitates collaboration across the community. Many of these use-cases are at the intersection of different tools and frameworks used in development and production.\n\nHere are some examples of apps that need to be built:\n\n* data visualization apps (+ automations)\n* model performance apps (+ automations),\n* model explainability apps + automations,\n* workflow visualization apps + automations,\n* model monitoring apps + automations,\n* specific regulation-apps + automations,\n* connector and integration apps + automations,\n* lineage apps + automations.\n\n## AimOS is simple first, then powerful\n\n\n\nUsers don’t have to know AimOS in-depth to be able to use it. You don’t even need to know if it’s an operating system - it takes a few commands and built-in packages to start with.\n\n![AimOS default apps](/images/dynamic/666-1-.png \"AimOS default apps\")\n\nUsing AimOS means installing AimOS and running an AimOS logging app. It comes with a number of built- apps that can go a long way:\n\n* Base app,\n* Experiment tracking app,\n* AI Systems tracing app.\n\nIt takes only a couple of commands to start with AimOS.\n\nWe have published high-level roadmap about the next steps. \n\n\u003e Checkout the AimOS documentation [here](https://aimos.readthedocs.io/en/latest/#).\n\nThe contributors to core Aim and package creators are very welcome to help us build out the future of logging infrastructure for AI Systems and beyond.\n\n## **Learn more**\n\n\n\n[AimOS is on a mission to democratize AI Systems logging tools.](https://aimos.readthedocs.io/en/latest/apps/overview.html) 🙌\n\nTry out [AimOS](https://github.com/aimhubio/aimos), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aimos/tree/main) if you think AimOS is useful, and here is the link to [Aim,](https://github.com/aimhubio/aim) an easy-to-use \u0026 supercharged open-source experiment tracker.⭐️","html":"\u003ch2\u003eThe software lifecycle is broken\u003c/h2\u003e\n\u003cp\u003eAI is changing how software is built and deployed. AI Systems are not deterministic, so observability is a key layer in the AI Systems development stack.\u003c/p\u003e\n\u003cp\u003eChanging the code is no longer enough to change the behavior of the AI system - a kind of systems that are inherently unreliable and hard to iterate on.\u003c/p\u003e\n\u003cp\u003eWith the rise of the LLMs and Generative AI, there are many more use-cases that are built with AI. This means more software needs to be monitored and observed.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe mainstream ideas of software logging and observability may not work anymore as the use-cases and needs have increased dramatically.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eSoftware systems need to be logged\u003c/h2\u003e\n\u003cp\u003eLogging is inseparable from software all along - from code versioning to production monitoring to AI experiment tracking to agent tracing to full observability.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1111-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith AI systems, we now have to deal with many more dimensions. Developing an AI System is as demanding as deploying it to production and integrating it with other software components.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe are now faced with problems of AI alignment and safety. AI regulation conversations are moving at full speed. The AI system governance and lineage are key when tackling any of these problems.\u003c/p\u003e\n\u003cp\u003eFor this, teams are trying to log as much as they can at every possible dimension.\u003c/p\u003e\n\u003cp\u003eTo satisfy these needs, we must be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLog anything from every part of the AI Infra\u003c/li\u003e\n\u003cli\u003eObserve and interpret the logged data at scale in a flexible fashion\u003c/li\u003e\n\u003cli\u003eAdd layers and layers of automations around the logged data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAltogether these form the observability and logging layer for AI Systems.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe observability layer for AI Systems is inherently complex and needs to encompass end-to-end development + production and beyond to allow to fully connect the dots.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eWhat do developers need from the AI Systems observability layer?\u003c/h2\u003e\n\u003cp\u003eWe need to log and observe at every step of building AI Systems.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/222-1.png\" alt=\"Basic AI system observability\" title=\"Basic AI system observability\"\u003e\u003c/p\u003e\n\u003cp\u003eOver the last three years, we have collaborated with hundreds of teams to enhance our understanding of observability requirements in AI systems. The capability to track diverse data sources stands out, given that logs are invaluable for discerning system health and facilitating infrastructure evolution.\u003c/p\u003e\n\u003cp\u003eIn addition, the ability to customize log visualizations increases the depth of analysis, enabling teams to discern intricate relationships within their setups. The use of automation ensures not only optimal performance but also increased reliability for AI systems.\u003c/p\u003e\n\u003cp\u003eThe lineage of an AI System is also vitally important, covering a wide range of considerations from governance to regulatory to best practices. The need for robust governance over AI Systems grows as compliance needs increase. In the end, we aim for an integrated approach to governance that provides detailed lineage insights and unwavering traceability.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/444-1.png\" alt=\"Key requirements: AI Observability Layer\" title=\"Key requirements: AI Observability Layer\"\u003e\u003c/p\u003e\n\u003ch3\u003eWe need well-developed open-source tools for observability use-cases\u003c/h3\u003e\n\u003cp\u003eLogs are now essential for overcoming the fundamental challenges of AI adoption. It is crucial for organizations to maintain ownership of their logs and ensure that they remain in an open format without third-party control. As AI systems evolve, the tools used for observability must also mature.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIdeally, these tools should be easy to use but powerful enough to meet next-stage infrastructure needs.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eUnfortunately, current mainstream observability tools are centralized, lack interoperability, and mostly run on closed-source platforms. These tools are not inherently scalable and often only address a fraction of the overall observability requirements.\u003c/p\u003e\n\u003ch2\u003eIntroducing AimOS - Operating system for logs\u003c/h2\u003e\n\u003cp\u003eAim began as a tool for tracking experiments, and it ran on a single node\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim 2.0 - AI experiment logger for metrics and hyperparameters.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003eAim 3.0 \u003c/a\u003e- AI experiment logger for any kind of Python objects.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAimOS - Operating system for logs.\u003c/strong\u003e 💫\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAimOS runs logging applications that can easily be composed and extended to larger ones encompassing both the main and long-tail use-cases.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/333-1.png\" alt=\"Modular observability for AI Systems\" title=\"Modular observability for AI Systems\"\u003e\u003c/p\u003e\n\u003cp\u003eWithin the context of AimOS, logging applications are Python packages that AimOS can run.\u003c/p\u003e\n\u003ch2\u003eThere are many logging apps that need to exist\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/555-1.png\" alt=\"AimOS logging app structure\" title=\"AimOS logging app structure\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are many logging uses-cases for AI systems, which necessitates collaboration across the community. Many of these use-cases are at the intersection of different tools and frameworks used in development and production.\u003c/p\u003e\n\u003cp\u003eHere are some examples of apps that need to be built:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edata visualization apps (+ automations)\u003c/li\u003e\n\u003cli\u003emodel performance apps (+ automations),\u003c/li\u003e\n\u003cli\u003emodel explainability apps + automations,\u003c/li\u003e\n\u003cli\u003eworkflow visualization apps + automations,\u003c/li\u003e\n\u003cli\u003emodel monitoring apps + automations,\u003c/li\u003e\n\u003cli\u003especific regulation-apps + automations,\u003c/li\u003e\n\u003cli\u003econnector and integration apps + automations,\u003c/li\u003e\n\u003cli\u003elineage apps + automations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAimOS is simple first, then powerful\u003c/h2\u003e\n\u003cp\u003eUsers don’t have to know AimOS in-depth to be able to use it. You don’t even need to know if it’s an operating system - it takes a few commands and built-in packages to start with.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/666-1-.png\" alt=\"AimOS default apps\" title=\"AimOS default apps\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing AimOS means installing AimOS and running an AimOS logging app. It comes with a number of built- apps that can go a long way:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBase app,\u003c/li\u003e\n\u003cli\u003eExperiment tracking app,\u003c/li\u003e\n\u003cli\u003eAI Systems tracing app.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt takes only a couple of commands to start with AimOS.\u003c/p\u003e\n\u003cp\u003eWe have published high-level roadmap about the next steps.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCheckout the AimOS documentation \u003ca href=\"https://aimos.readthedocs.io/en/latest/#\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe contributors to core Aim and package creators are very welcome to help us build out the future of logging infrastructure for AI Systems and beyond.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eLearn more\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aimos/tree/main\"\u003eGitHub\u003c/a\u003e if you think AimOS is useful, and here is the link to \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim,\u003c/a\u003e an easy-to-use \u0026#x26; supercharged open-source experiment tracker.⭐️\u003c/p\u003e"},"_id":"posts/aim-4-0-open-source-modular-observability-for-ai-systems.md","_raw":{"sourceFilePath":"posts/aim-4-0-open-source-modular-observability-for-ai-systems.md","sourceFileName":"aim-4-0-open-source-modular-observability-for-ai-systems.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-4-0-open-source-modular-observability-for-ai-systems"},"type":"Post"},{"title":"Aim 4.0 to a new repo AimOS. Here is why?","date":"2023-10-13T20:01:12.714Z","author":"Gev Sogomonian","description":"We have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.","slug":"aim-4-0-to-a-new-repo-aimos-here-is-why","image":"/images/dynamic/aimos.png","draft":false,"categories":["New Releases"],"body":{"raw":"We have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.\n\nRight now you can access the original Aim Experiment Tracker as is through the Aim repo. You can also access the Aim 4.0 as AimOS - an end-to-end modular observability tool for AI systems.\n\n## Why the move?\n\n\n\nThe [AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems) mission is to enable a modular end-to-end observability for the AI Systems.\n\n\n\nIn the first week of the release and user feedback, we admit that Aim 4.0 and [Aim 3.0](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative) are two different products and that the users want both. So we oblige.\n\nThe Aim 3.0 is used by many teams and we are sorry for the disruption.\n\nGoing forward we are going to maintain both projects. \n\nAim 3.0 roadmap has also been updated. Stay tuned for that.\n\n## Learn more\n\n\n\n[AimOS is on a mission to democratize AI Systems logging tools.](https://aimos.readthedocs.io/en/latest/apps/overview.html) 🙌\n\nTry out[ AimOS](https://github.com/aimhubio/aimos), join the [Aim community,](https://community.aimstack.io/) share your feedback, open issues for new features, bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aimos/tree/main) if you think AimOS is useful, and here is the link to[ Aim](https://github.com/aimhubio/aim), an easy-to-use \u0026 supercharged open-source experiment tracker.⭐️","html":"\u003cp\u003eWe have moved the Aim 4.0 to a new repo AimOS and reinstated Aim to its previous version.\u003c/p\u003e\n\u003cp\u003eRight now you can access the original Aim Experiment Tracker as is through the Aim repo. You can also access the Aim 4.0 as AimOS - an end-to-end modular observability tool for AI systems.\u003c/p\u003e\n\u003ch2\u003eWhy the move?\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e mission is to enable a modular end-to-end observability for the AI Systems.\u003c/p\u003e\n\u003cp\u003eIn the first week of the release and user feedback, we admit that Aim 4.0 and \u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003eAim 3.0\u003c/a\u003e are two different products and that the users want both. So we oblige.\u003c/p\u003e\n\u003cp\u003eThe Aim 3.0 is used by many teams and we are sorry for the disruption.\u003c/p\u003e\n\u003cp\u003eGoing forward we are going to maintain both projects.\u003c/p\u003e\n\u003cp\u003eAim 3.0 roadmap has also been updated. Stay tuned for that.\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eTry out\u003ca href=\"https://github.com/aimhubio/aimos\"\u003e AimOS\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community,\u003c/a\u003e share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aimos/tree/main\"\u003eGitHub\u003c/a\u003e if you think AimOS is useful, and here is the link to\u003ca href=\"https://github.com/aimhubio/aim\"\u003e Aim\u003c/a\u003e, an easy-to-use \u0026#x26; supercharged open-source experiment tracker.⭐️\u003c/p\u003e"},"_id":"posts/aim-4-0-to-a-new-repo-aimos-here-is-why.md","_raw":{"sourceFilePath":"posts/aim-4-0-to-a-new-repo-aimos-here-is-why.md","sourceFileName":"aim-4-0-to-a-new-repo-aimos-here-is-why.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-4-0-to-a-new-repo-aimos-here-is-why"},"type":"Post"},{"title":"Aim Community is Moving to Discord!! 🎉","date":"2022-12-21T21:45:47.434Z","author":"Gev Soghomonian","description":"Aim community is officially moving to Discord!! 🎉 Let's rock and build great things together!! The slack workspace is going to become deprecated by the end of 2022.","slug":"aim-community-is-moving-to-discord-","image":"https://miro.medium.com/max/1400/1*c_ovadkKTTkZOokR3WiUTg.webp","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) community is officially moving to Discord!! 🎉\n\n[Join us on Discord](https://community.aimstack.io/), let’s rock and build great things together!!\n\nIn this new space we are going to\n\n* be very proactive in sharing next steps, areas of focus and plans\n* share weekly roadmap / commitments\n* share more community-specific content\n* of course continue answering the user questions\n* work with the users to build the best open-source experiment tracking tools.\n\n***The slack workspace is going to become deprecated by the end of 2022.***\n\nI also asked a friend on the technical reasons for moving to Discord. Here is what he responded. \n\n![](https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp)\n\nAim High!! 🚀","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e community is officially moving to Discord!! 🎉\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://community.aimstack.io/\"\u003eJoin us on Discord\u003c/a\u003e, let’s rock and build great things together!!\u003c/p\u003e\n\u003cp\u003eIn this new space we are going to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebe very proactive in sharing next steps, areas of focus and plans\u003c/li\u003e\n\u003cli\u003eshare weekly roadmap / commitments\u003c/li\u003e\n\u003cli\u003eshare more community-specific content\u003c/li\u003e\n\u003cli\u003eof course continue answering the user questions\u003c/li\u003e\n\u003cli\u003ework with the users to build the best open-source experiment tracking tools.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eThe slack workspace is going to become deprecated by the end of 2022.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI also asked a friend on the technical reasons for moving to Discord. Here is what he responded.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*TyjpfDg_-GabhaZ2cm76SA.webp\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim High!! 🚀\u003c/p\u003e"},"_id":"posts/aim-community-is-moving-to-discord-🎉.md","_raw":{"sourceFilePath":"posts/aim-community-is-moving-to-discord-🎉.md","sourceFileName":"aim-community-is-moving-to-discord-🎉.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-community-is-moving-to-discord-🎉"},"type":"Post"},{"title":"Aim v2.2.0 — Hugging Face integration","date":"2021-03-24T13:20:24.937Z","author":"Gev Soghomonian","description":"Aim 2.2.0 featuring Hugging Face integration is out! Hugging Face integration has been one of the most requested features and we are excited to finally ship it.","slug":"aim-v2-2-0-hugging-face-integration","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MqbllMOE307ZvLLuH0MhvA.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\n\nThanks to [siddk](https://github.com/siddk), [TommasoBendinelli](https://github.com/TommasoBendinelli) and [Khazhak](https://github.com/Khazhak) for their contribution to this release.\n\n\u003e **Note on the Aim versioning:** The previous two release posts: [Aim 1.3.5](https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment) and [Aim 1.3.8](https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring) had used the version number of [AimUI](https://aimstack.readthedocs.io/en/latest/ui/overview.html) as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion. [Check out the Aim CHANGELOG](https://github.com/aimhubio/aim/blob/main/CHANGELOG.md).\n\nWe have also added [milestones](https://github.com/aimhubio/aim/milestones) for each version. As well as the [Roadmap](https://github.com/aimhubio/aim#roadmap).\n\nCheck out the new features at [play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\n## Hugging Face integration\n\nHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\n\n```\nfrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n```\n\nHere is how it works: `aim_callback` will automatically open an Aim `Session` and close it when the trainer is done. When `trainer.train()`, `trainer.evaluate()` or `trainer.predict()` is called, `aim_callback` will automatically log the hyper-params and respective metrics.\n\nFind a full example [here](https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py).\n\n## Metric Visibility Control\n\nWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\n\nA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*yZAWw55lUWVCa35zptdCrA.gif \"Hide individual metrics as well as collectively\")\n\n## Column resize\n\nWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table. Resize is available both on Explore and Dashboard tables.Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*KFUs6pmpqOCVhfdWMO6CVg.gif \"Drag column edges back-and-forth to resize\")\n\n## Hide columns with similar values\n\nMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\n\nThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*klN4dR6T8nSHLqyvqIUjOg.gif \"Leave only different columns with a button click, if needed customize afterwards\")\n\n## Logscale\n\nOne of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*cin-2u7a14bj9fB46WmZJw.gif \"log-scale on Aim\")\n\n## New methods for aggregation\n\nNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*xfp0OtVsz6s4vhIdQ1nLIw.gif)\n\n## Learn More\n\n[Aim is on the mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support 🙌.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\u003c/p\u003e\n\u003cp\u003eThanks to \u003ca href=\"https://github.com/siddk\"\u003esiddk\u003c/a\u003e, \u003ca href=\"https://github.com/TommasoBendinelli\"\u003eTommasoBendinelli\u003c/a\u003e and \u003ca href=\"https://github.com/Khazhak\"\u003eKhazhak\u003c/a\u003e for their contribution to this release.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote on the Aim versioning:\u003c/strong\u003e The previous two release posts: \u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-5-activity-view-and-x-axis-alignment\"\u003eAim 1.3.5\u003c/a\u003e and \u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring\"\u003eAim 1.3.8\u003c/a\u003e had used the version number of \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\"\u003eAimUI\u003c/a\u003e as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion. \u003ca href=\"https://github.com/aimhubio/aim/blob/main/CHANGELOG.md\"\u003eCheck out the Aim CHANGELOG\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe have also added \u003ca href=\"https://github.com/aimhubio/aim/milestones\"\u003emilestones\u003c/a\u003e for each version. As well as the \u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eRoadmap\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCheck out the new features at \u003ca href=\"http://play.aimstack.io:43900/dashboard\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eHugging Face integration\u003c/h2\u003e\n\u003cp\u003eHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is how it works: \u003ccode\u003eaim_callback\u003c/code\u003e will automatically open an Aim \u003ccode\u003eSession\u003c/code\u003e and close it when the trainer is done. When \u003ccode\u003etrainer.train()\u003c/code\u003e, \u003ccode\u003etrainer.evaluate()\u003c/code\u003e or \u003ccode\u003etrainer.predict()\u003c/code\u003e is called, \u003ccode\u003eaim_callback\u003c/code\u003e will automatically log the hyper-params and respective metrics.\u003c/p\u003e\n\u003cp\u003eFind a full example \u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eMetric Visibility Control\u003c/h2\u003e\n\u003cp\u003eWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\u003c/p\u003e\n\u003cp\u003eA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*yZAWw55lUWVCa35zptdCrA.gif\" alt=\"\" title=\"Hide individual metrics as well as collectively\"\u003e\u003c/p\u003e\n\u003ch2\u003eColumn resize\u003c/h2\u003e\n\u003cp\u003eWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table. Resize is available both on Explore and Dashboard tables.Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*KFUs6pmpqOCVhfdWMO6CVg.gif\" alt=\"\" title=\"Drag column edges back-and-forth to resize\"\u003e\u003c/p\u003e\n\u003ch2\u003eHide columns with similar values\u003c/h2\u003e\n\u003cp\u003eMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\u003c/p\u003e\n\u003cp\u003eThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*klN4dR6T8nSHLqyvqIUjOg.gif\" alt=\"\" title=\"Leave only different columns with a button click, if needed customize afterwards\"\u003e\u003c/p\u003e\n\u003ch2\u003eLogscale\u003c/h2\u003e\n\u003cp\u003eOne of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*cin-2u7a14bj9fB46WmZJw.gif\" alt=\"\" title=\"log-scale on Aim\"\u003e\u003c/p\u003e\n\u003ch2\u003eNew methods for aggregation\u003c/h2\u003e\n\u003cp\u003eNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*xfp0OtVsz6s4vhIdQ1nLIw.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on the mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e"},"_id":"posts/aim-v2-2-0-—-hugging-face-integration.md","_raw":{"sourceFilePath":"posts/aim-v2-2-0-—-hugging-face-integration.md","sourceFileName":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-2-0-—-hugging-face-integration"},"type":"Post"},{"title":"Aim v2.3.0 — System Resource Usage and Reverse Grouping","date":"2021-04-20T13:39:39.516Z","author":"Gev Soghomonian","description":"Aim 2.3.0 allowing System Resource Usage optimization is out! Some highlights: Reverse grouping, Line Chart Smoothing, New aggregation modes. Check out! ","slug":"aim-v2-3-0-system-resource-usage-and-reverse-grouping","image":"https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) 2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\n\nCheck out the updated Aim at [play.aimstack.io](http://play.aimstack.io:10001/).\n\nBelow are the highlight features of the update. Find the full list of changes [here](https://github.com/aimhubio/aim/milestone/3?closed=1).\n\n## System Resource Usage\n\nNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with `system_tracking_interval==0`.\n\nOnce you run the training, you will see the following. Here is a demo:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*pOMxDdOpUDIOedrNIM53pQ.gif \"Aim automatic system resource tracking demo\")\n\nOf course you can also query all those new metrics in the Explore page and compare, group, aggregate with the rest of the metrics!\n\n## Reverse grouping (“against” the param)\n\nAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a Reverse Grouping. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif \"Aim Reverse grouping demo\")\n\nUse the toggle next to switch between grouping modes.\n\n## Line Chart Smoothing\n\nNow you can apply smoothing on metrics. Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*MgNuAaH6gzm7u-H2QUh15g.gif \"Aim metric smoothing\")\n\nThere are two type of smoothing options available: `Exponential Moving Average` and `Central Moving Average`. Further info of [how](https://en.wikipedia.org/wiki/Moving_average) it’s calculated.\n\n## New aggregation modes\n\nWe have additionally added two more aggregation modes: `standard error` and `standard deviation` . Here is how it works:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*LS17WlLey5aJfMyJbtgtBg.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e 2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim at \u003ca href=\"http://play.aimstack.io:10001/\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow are the highlight features of the update. Find the full list of changes \u003ca href=\"https://github.com/aimhubio/aim/milestone/3?closed=1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSystem Resource Usage\u003c/h2\u003e\n\u003cp\u003eNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with \u003ccode\u003esystem_tracking_interval==0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce you run the training, you will see the following. Here is a demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*pOMxDdOpUDIOedrNIM53pQ.gif\" alt=\"\" title=\"Aim automatic system resource tracking demo\"\u003e\u003c/p\u003e\n\u003cp\u003eOf course you can also query all those new metrics in the Explore page and compare, group, aggregate with the rest of the metrics!\u003c/p\u003e\n\u003ch2\u003eReverse grouping (“against” the param)\u003c/h2\u003e\n\u003cp\u003eAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a Reverse Grouping. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*txmn1LOXJ7nGift0T82lng.gif\" alt=\"\" title=\"Aim Reverse grouping demo\"\u003e\u003c/p\u003e\n\u003cp\u003eUse the toggle next to switch between grouping modes.\u003c/p\u003e\n\u003ch2\u003eLine Chart Smoothing\u003c/h2\u003e\n\u003cp\u003eNow you can apply smoothing on metrics. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*MgNuAaH6gzm7u-H2QUh15g.gif\" alt=\"\" title=\"Aim metric smoothing\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are two type of smoothing options available: \u003ccode\u003eExponential Moving Average\u003c/code\u003e and \u003ccode\u003eCentral Moving Average\u003c/code\u003e. Further info of \u003ca href=\"https://en.wikipedia.org/wiki/Moving_average\"\u003ehow\u003c/a\u003e it’s calculated.\u003c/p\u003e\n\u003ch2\u003eNew aggregation modes\u003c/h2\u003e\n\u003cp\u003eWe have additionally added two more aggregation modes: \u003ccode\u003estandard error\u003c/code\u003e and \u003ccode\u003estandard deviation\u003c/code\u003e . Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*LS17WlLey5aJfMyJbtgtBg.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","_raw":{"sourceFilePath":"posts/aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileName":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-3-0-—-system-resource-usage-and-reverse-grouping"},"type":"Post"},{"title":"Aim v2.6.0 — Docker requirement removed and Metric as x-axis","date":"2021-06-24T14:25:34.071Z","author":"Gev Soghomonian","description":"Explore Aim v2.6.0: No More Docker Requirement, Metric as x-axis. Enhance your analysis with alternate axes, improving metric correlations. Learn how!","slug":"aim-v2-6-0-docker-requirement-removed-and-metric-as-x-axis","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8Nw162Czv6_lzfQeh2m9A.png","draft":false,"categories":["New Releases"],"body":{"raw":"## Metric as an alternative x-axis\n\nSometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.\n\nFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared — `loss` and`bleu`. One of the main indicators of a performant model in NMT, is when higher values of `bleu` correlates with the lower values of `loss`.\n\nJust plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use `loss` as the x-axis to directly see the correlation between the `loss`and the`bleu` metrics for two groups of runs ( `max_k==3`and `max_k == 1`).\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Y0wAL9nZ7IhgLOUr-cCRRQ.gif \"Demo for metric as alternative x-axis\")\n\nUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\n\n\u003e **Note**: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003ch2\u003eMetric as an alternative x-axis\u003c/h2\u003e\n\u003cp\u003eSometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.\u003c/p\u003e\n\u003cp\u003eFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared — \u003ccode\u003eloss\u003c/code\u003e and\u003ccode\u003ebleu\u003c/code\u003e. One of the main indicators of a performant model in NMT, is when higher values of \u003ccode\u003ebleu\u003c/code\u003e correlates with the lower values of \u003ccode\u003eloss\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eJust plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use \u003ccode\u003eloss\u003c/code\u003e as the x-axis to directly see the correlation between the \u003ccode\u003eloss\u003c/code\u003eand the\u003ccode\u003ebleu\u003c/code\u003e metrics for two groups of runs ( \u003ccode\u003emax_k==3\u003c/code\u003eand \u003ccode\u003emax_k == 1\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Y0wAL9nZ7IhgLOUr-cCRRQ.gif\" alt=\"\" title=\"Demo for metric as alternative x-axis\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"posts/aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","_raw":{"sourceFilePath":"posts/aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileName":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis"},"type":"Post"},{"title":"Aim v2.7.1 — Table export and Explore view bookmarks","date":"2021-07-15T14:31:32.317Z","author":"Gev Soghomonian","description":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback! Check out the updated","slug":"aim-v2-7-1-table-export-and-explore-view-bookmarks","image":"https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif","draft":false,"categories":["New Releases"],"body":{"raw":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\n\nCheck out the updated Aim demo at [play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\nBelow are the two main highlights of Aim v2.7.1.\n\n## Aim Table CSV export\n\nNow you can export both tables from Explore and Dashboard to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with [Pandas](https://pandas.pydata.org/)) for your MLOps needs.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif \"Exporting CSV from the Explore table\")\n\n## Bookmark/Save the Explore State \\[experimental]\n\nThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you’d like to revisit.\n\nIn the future [releases](https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative), we will build on top of this feature to potentially add notes, fully manage and share them. *For now, this feature is in experimental mode*. Please feel free to share more feedback and how we can improve its experience.\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*U5a36lDx7-xgSONKh1-D0A.gif \"Use bookmarks to save important explore states\")\n\n## Learn More\n\n[Aim is on a mission to democratize MLOps tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support .","html":"\u003cp\u003eAim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim demo at \u003ca href=\"http://play.aimstack.io:43900/dashboard\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow are the two main highlights of Aim v2.7.1.\u003c/p\u003e\n\u003ch2\u003eAim Table CSV export\u003c/h2\u003e\n\u003cp\u003eNow you can export both tables from Explore and Dashboard to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with \u003ca href=\"https://pandas.pydata.org/\"\u003ePandas\u003c/a\u003e) for your MLOps needs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*Xp8sY5HA6DUjRPXVAboB3A.gif\" alt=\"\" title=\"Exporting CSV from the Explore table\"\u003e\u003c/p\u003e\n\u003ch2\u003eBookmark/Save the Explore State [experimental]\u003c/h2\u003e\n\u003cp\u003eThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you’d like to revisit.\u003c/p\u003e\n\u003cp\u003eIn the future \u003ca href=\"https://aimstack.io/blog/new-releases/aims-foundations-why-were-building-a-tensorboard-alternative\"\u003ereleases\u003c/a\u003e, we will build on top of this feature to potentially add notes, fully manage and share them. \u003cem\u003eFor now, this feature is in experimental mode\u003c/em\u003e. Please feel free to share more feedback and how we can improve its experience.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*U5a36lDx7-xgSONKh1-D0A.gif\" alt=\"\" title=\"Use bookmarks to save important explore states\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize MLOps tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support .\u003c/p\u003e"},"_id":"posts/aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","_raw":{"sourceFilePath":"posts/aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","sourceFileName":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v2-7-1-—-table-export-and-explore-view-bookmarks"},"type":"Post"},{"title":"Aim v3.16 — Run messages in UI, TensorBoard real-time sync, integration with Hugging Face Datasets","date":"2023-02-15T06:56:43.141Z","author":"Gor Arakelyan","description":"🚀 Aim v3.16 is out! Users will be able to view all Run messages right in the UI. Integration of Aim with TensorBoard, Hugging Face Datasets, Acme, Stable-Baselines3. ","slug":"aim-v3-16-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-AzW2_QT8nvsgLUQH9JeA.png","draft":false,"categories":["New Releases"],"body":{"raw":"Hey community, excited to announce Aim v3.16 is out! 🚀 It is packed with new integrations and key enhancements.\n\n\u003e *We are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Every question and every issue makes Aim better!*\n\u003e\n\u003e *Congratulations to [timokau,](https://github.com/timokau) [dsblank](https://github.com/dsblank) and [grigoryan-davit](https://github.com/grigoryan-davit) for their first contributions. 🙌*\n\n# Key highlights\n\n## **Run messages tab in UI**\n\nStarting from version 3.15, Aim has enabled an ability to [log training messages](https://aimstack.readthedocs.io/en/latest/using/logging.html) and even [send notifications](https://aimstack.readthedocs.io/en/latest/using/notifications.html) to Slack or Workplace.\n\nWith the version 3.16, you’ll be able to view all your [run messages](https://aimstack.readthedocs.io/en/latest/using/logging.html) right in the UI. All the information you need is just a click away! 💫\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxqbmJJcqYaH1tLmcg59Xw.png)\n\n## **Real-time sync with TensorBoard logs**\n\n\n\nAim smoothly integrates with experiment tracking tools. If you’re part of a team that’s already deeply integrated TensorBoard into your projects and pipelines, you will love this enhancement.\n\nWith just one simple line of code, you can integrate Aim with your existing TensorBoard projects. This means that all your logs will be automatically converted to Aim format in real-time. 🔥🔥🔥\n\n```\nfrom aim.ext.tensorboard_tracker import Run\n\naim_run = Run(\n    sync_tensorboard_log_dir='TB_LOG_DIR_TO_SYNC_RUNS'\n)\n\n```\n\n## **Support for Python 3.11**\n\nThe Python community has received fantastic news towards the end of 2022. The stable Python 3.11 has been officially released!\n\nWith Aim 3.16 release, you can install and use Aim in your python 3.11 projects. 🎉\n\n```\npip3.11 install aim\n```\n\n## **Dropped support for Python 3.6**\n\nTime to say goodbye: Python 3.6 has reached the end-of-life. 💀\n\n![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpHtGUIfWc5RXtDk0TNKTQ.png)\n\nThis change allows us to take advantage of the latest advancements in the Python language and provide you with a more robust and reliable library.\n\nPlease note that if you have been using Aim in your Python 3.6 projects, you will need to upgrade to newer Python versions in order to continue using Aim.\n\n**We apologize for any inconvenience this may cause, but rest assured that the improved stability of Aim will make it worth the transition.**\n\n# New integrations\n\nAim 3.16 is packed with new integrations with favorite ML tools!\n\n## **Aim + Hugging Face Datasets = ❤**\n\nHappy to share now you can easily track and store dataset metadata in Aim run and explore it on the UI.\n\n```\nfrom datasets import load_dataset\n\nfrom aim import Run\nfrom aim.hf_dataset import HFDataset\n\n# Load the dataset\ndataset = load_dataset('rotten_tomatoes')\n\n# Store the dataset metadata\nrun = Run()\nrun['datasets_info'] = HFDataset(dataset)\n```\n\nSee the docs [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim).\n\n## **Aim + Acme = ❤**\n\nAim’s been added a built-in support for tracking [Acme](https://dm-acme.readthedocs.io/en/latest/) trainings. It takes few simple steps to integrate Aim into your training script.\n\n1. Explicitly import the `AimCallback` and `AimWriter` for tracking training metadata:\n\n```\nfrom aim.sdk.acme import AimCallback, AimWriter\n```\n\n2. Initialize an Aim Run via `AimCallback`, and create a log factory:\n\n```\naim_run = AimCallback(repo=\".\", experiment_name=\"acme_test\")\n\ndef logger_factory(\n    name: str,\n    steps_key: Optional[str] = None,\n    task_id: Optional[int] = None,\n) -\u003e loggers.Logger:\n    return AimWriter(aim_run, name, steps_key, task_id)\n```\n\n3. Pass the logger factory to `logger_factory` upon initiating your training:\n\n```\nexperiment_config = experiments.ExperimentConfig(\n    builder=d4pg_builder,\n    environment_factory=make_environment,\n    network_factory=network_factory,\n    logger_factory=logger_factory,\n    seed=0,\n    max_num_actor_steps=5000)\n```\n\n\n\nSee the docs [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-acme).\n\n## **Aim + Stable-Baselines3 = ❤**\n\nNow you can easily track [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) trainings with Aim. It takes two steps to integrate Aim into your training script.\n\n1. Explicitly import the `AimCallback` for tracking training metadata\n\n   ```\n   from aim.sb3 import AimCallback\n   ```\n\n   2. Pass the callback to `callback` upon initiating your training:\n\n   ```\n   model.learn(total_timesteps=10_000, callback=AimCallback(repo='.', experiment_name='sb3_test'))\n   ```\n\n   See the docs [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-stable-baselines3).\n\n   # Learn more\n\n   [Aim is on a mission to democratize AI dev tools.](https://aimstack.readthedocs.io/en/latest/overview.html) 🙌\n\n   Try out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features, bugs.\n\n   Don’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aim) if you think Aim is useful. ⭐️","html":"\u003cp\u003eHey community, excited to announce Aim v3.16 is out! 🚀 It is packed with new integrations and key enhancements.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWe are on a mission to democratize AI dev tools and are incredibly lucky to have the support of the community. Every question and every issue makes Aim better!\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eCongratulations to \u003ca href=\"https://github.com/timokau\"\u003etimokau,\u003c/a\u003e \u003ca href=\"https://github.com/dsblank\"\u003edsblank\u003c/a\u003e and \u003ca href=\"https://github.com/grigoryan-davit\"\u003egrigoryan-davit\u003c/a\u003e for their first contributions. 🙌\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003eKey highlights\u003c/h1\u003e\n\u003ch2\u003e\u003cstrong\u003eRun messages tab in UI\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eStarting from version 3.15, Aim has enabled an ability to \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\"\u003elog training messages\u003c/a\u003e and even \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/notifications.html\"\u003esend notifications\u003c/a\u003e to Slack or Workplace.\u003c/p\u003e\n\u003cp\u003eWith the version 3.16, you’ll be able to view all your \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/logging.html\"\u003erun messages\u003c/a\u003e right in the UI. All the information you need is just a click away! 💫\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxqbmJJcqYaH1tLmcg59Xw.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eReal-time sync with TensorBoard logs\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAim smoothly integrates with experiment tracking tools. If you’re part of a team that’s already deeply integrated TensorBoard into your projects and pipelines, you will love this enhancement.\u003c/p\u003e\n\u003cp\u003eWith just one simple line of code, you can integrate Aim with your existing TensorBoard projects. This means that all your logs will be automatically converted to Aim format in real-time. 🔥🔥🔥\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.ext.tensorboard_tracker import Run\n\naim_run = Run(\n    sync_tensorboard_log_dir='TB_LOG_DIR_TO_SYNC_RUNS'\n)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cstrong\u003eSupport for Python 3.11\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThe Python community has received fantastic news towards the end of 2022. The stable Python 3.11 has been officially released!\u003c/p\u003e\n\u003cp\u003eWith Aim 3.16 release, you can install and use Aim in your python 3.11 projects. 🎉\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3.11 install aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cstrong\u003eDropped support for Python 3.6\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eTime to say goodbye: Python 3.6 has reached the end-of-life. 💀\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpHtGUIfWc5RXtDk0TNKTQ.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis change allows us to take advantage of the latest advancements in the Python language and provide you with a more robust and reliable library.\u003c/p\u003e\n\u003cp\u003ePlease note that if you have been using Aim in your Python 3.6 projects, you will need to upgrade to newer Python versions in order to continue using Aim.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWe apologize for any inconvenience this may cause, but rest assured that the improved stability of Aim will make it worth the transition.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eNew integrations\u003c/h1\u003e\n\u003cp\u003eAim 3.16 is packed with new integrations with favorite ML tools!\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAim + Hugging Face Datasets = ❤\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eHappy to share now you can easily track and store dataset metadata in Aim run and explore it on the UI.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom datasets import load_dataset\n\nfrom aim import Run\nfrom aim.hf_dataset import HFDataset\n\n# Load the dataset\ndataset = load_dataset('rotten_tomatoes')\n\n# Store the dataset metadata\nrun = Run()\nrun['datasets_info'] = HFDataset(dataset)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html#logging-huggingface-datasets-dataset-info-with-aim\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAim + Acme = ❤\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAim’s been added a built-in support for tracking \u003ca href=\"https://dm-acme.readthedocs.io/en/latest/\"\u003eAcme\u003c/a\u003e trainings. It takes few simple steps to integrate Aim into your training script.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExplicitly import the \u003ccode\u003eAimCallback\u003c/code\u003e and \u003ccode\u003eAimWriter\u003c/code\u003e for tracking training metadata:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.sdk.acme import AimCallback, AimWriter\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eInitialize an Aim Run via \u003ccode\u003eAimCallback\u003c/code\u003e, and create a log factory:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eaim_run = AimCallback(repo=\".\", experiment_name=\"acme_test\")\n\ndef logger_factory(\n    name: str,\n    steps_key: Optional[str] = None,\n    task_id: Optional[int] = None,\n) -\u003e loggers.Logger:\n    return AimWriter(aim_run, name, steps_key, task_id)\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003ePass the logger factory to \u003ccode\u003elogger_factory\u003c/code\u003e upon initiating your training:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eexperiment_config = experiments.ExperimentConfig(\n    builder=d4pg_builder,\n    environment_factory=make_environment,\n    network_factory=network_factory,\n    logger_factory=logger_factory,\n    seed=0,\n    max_num_actor_steps=5000)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-acme\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eAim + Stable-Baselines3 = ❤\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNow you can easily track \u003ca href=\"https://stable-baselines3.readthedocs.io/en/master/\"\u003eStable-Baselines3\u003c/a\u003e trainings with Aim. It takes two steps to integrate Aim into your training script.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eExplicitly import the \u003ccode\u003eAimCallback\u003c/code\u003e for tracking training metadata\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.sb3 import AimCallback\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ePass the callback to \u003ccode\u003ecallback\u003c/code\u003e upon initiating your training:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emodel.learn(total_timesteps=10_000, callback=AimCallback(repo='.', experiment_name='sb3_test'))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the docs \u003ca href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-stable-baselines3\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.readthedocs.io/en/latest/overview.html\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e if you think Aim is useful. ⭐️\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e"},"_id":"posts/aim-v3-16-—-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","_raw":{"sourceFilePath":"posts/aim-v3-16-—-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","sourceFileName":"aim-v3-16-—-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim-v3-16-—-run-messages-in-ui-tensorboard-real-time-sync-integration-with-hugging-face-datasets"},"type":"Post"},{"title":"Aim’s foundations \u0026 why we’re building a Tensorboard alternative","date":"2021-10-22T14:48:48.452Z","author":"Gev Soghomonian","description":"The origins of Aim. In the fateful summer of 2020, our friend mahnerak – a researcher at a non-profit lab was hitting the limits of Tensorboard. He wasn’t","slug":"aims-foundations-why-were-building-a-tensorboard-alternative","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd5FbXnYnB3fbELmRLTPwA.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The origins of Aim\n\nIn the fateful summer of 2020, our friend [mahnerak](https://twitter.com/mahnerak) – a researcher at [a non-profit lab](https://yerevann.com/) was hitting the limits of Tensorboard. He wasn’t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That’s how we decided to build a Tensorboard alternative.\n\n[Gor](https://github.com/gorarakelyan) and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\n\nBy fall 2020, [Aim 2.0](https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration) launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even [r/MachineLearning loved it](https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/).\n\n By spring 2021, mahnerak co-authoerd a paper [WARP](https://aclanthology.org/2021.acl-long.381/) ([code](https://github.com/yerevann/warp)): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\n\n## A scale problem\n\nBut Aim’s power users — who often do 5K+ runs —were hitting issues.\n\nAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests. *Live updates, image tracking, distribution tracking*… and Aim 2.0 was hitting the limits of Aim 1.0’s design.\n\nIn order to support the future, we had to make changes to the foundation now.\n\n## Launching Aim 3.0.0\n\nAn additional [317 pull requests](https://github.com/aimhubio/aim/milestone/13?closed=1) later, we are excited to launch Aim v3.0.0 !!!\n\nAs a result, the most important changes include:\n\n**A completely revamped UI**\n\n* Home page and run detail page\n* Runs, metrics and params explorers\n* Bookmarks and Tags\n\n**A completely revamped Aim Python SDK**\n\n* New and much more intuitive (but still quite vanilla) API to track your training runs\n* New and 10x faster embedded storage based on [Rocksdb](http://rocksdb.org/). This will allow us to store virtually any type of AI metadata. On the contrary, [AimRecords](https://github.com/aimhubio/aimrecords) was designed for metrics and hyperparams only.\n\nEnjoy the changes!\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*xbg148dzileheVdOALBMBg.gif)\n\n## Performance improvements\n\n* Average run query execution time on ~2000 runs: 0.784s.\n* Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\n* New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (*for comparison, v2 was performant with limitation for only 100 metrics*).\n\n## Comparisons to familiar tools\n\n### Tensorboard\n\n**Training run comparison**\n\n* The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\n* With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params. After all, TensorBoard doesn’t have features to group, aggregate the metrics\n\n**Scalability**\n\n* Aim can handle 1000s of training runs both on the backend and on the UI.\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\n\nAim will have the beloved TB visualizations\n\n* Embedding projector.\n* Neural network visualization.\n\n### [](https://github.com/aimhubio/aim#mlflow)MLFlow\n\nMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\n\n**Run comparison**\n\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\n* MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\n\n**UI Scalability**\n\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\n\n### [](https://github.com/aimhubio/aim#weights-and-biases)Weights and Biases\n\n**Hosted vs self-hosted**\n\n* Weights and Biases is a hosted closed-source MLOps platform.\n* Aim is self-hosted, free and open-source experiment tracking tool.\n\n## Aim Roadmap[](https://github.com/aimhubio/aim#tensorboard)\n\nWith this version we are also publishing the Aim [roadmap](https://github.com/aimhubio/aim#roadmap) for the next 3 months.\n\nThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\n\nWe are also [inviting community contributors](https://github.com/aimhubio/aim#community) to help us get there faster!\n\n## Why are we building Aim?\n\nWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\n\nExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\n\nHowever we see lots of improvements to be made. Especially around issues like:\n\n* ability to handle 1000s of large-scale experiments\n* actionable, beautiful and performant visualizations\n* extensibility — how easy are the apis for extension/democratization?\n\nWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\n\nOur mission…\n\nAim’s mission is to democratize AI dev tools. We believe that the best AI tools need to be:\n\n* open-source, open-data-format, community-driven\n* have great UI/UX, CLI and other interfaces for automation\n* performant both on UI and data\n* extensible — enable ways to build around for so many use-cases\n\n## Thanks to\n\n[Ruben Karapetyan](https://twitter.com/roubkar) for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\n\n[Mahnerak](https://twitter.com/mahnerak) for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\n\nAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\n\nThe contributors who have been relentlessly iterating over the course of the summer.\n\nOn to the next generation of ML tools!!\n\n## Join Us!\n\n\n\nJoin the [Aim community](https://community.aimstack.io/), test Aim out, ask questions, help us build the future of AI tooling!\n\nIf you find Aim useful, drop by and star the repo ⭐","html":"\u003ch2\u003eThe origins of Aim\u003c/h2\u003e\n\u003cp\u003eIn the fateful summer of 2020, our friend \u003ca href=\"https://twitter.com/mahnerak\"\u003emahnerak\u003c/a\u003e – a researcher at \u003ca href=\"https://yerevann.com/\"\u003ea non-profit lab\u003c/a\u003e was hitting the limits of Tensorboard. He wasn’t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That’s how we decided to build a Tensorboard alternative.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/gorarakelyan\"\u003eGor\u003c/a\u003e and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\u003c/p\u003e\n\u003cp\u003eBy fall 2020, \u003ca href=\"https://aimstack.io/blog/new-releases/aim-v2-2-0-hugging-face-integration\"\u003eAim 2.0\u003c/a\u003e launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even \u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/\"\u003er/MachineLearning loved it\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBy spring 2021, mahnerak co-authoerd a paper \u003ca href=\"https://aclanthology.org/2021.acl-long.381/\"\u003eWARP\u003c/a\u003e (\u003ca href=\"https://github.com/yerevann/warp\"\u003ecode\u003c/a\u003e): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\u003c/p\u003e\n\u003ch2\u003eA scale problem\u003c/h2\u003e\n\u003cp\u003eBut Aim’s power users — who often do 5K+ runs —were hitting issues.\u003c/p\u003e\n\u003cp\u003eAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests. \u003cem\u003eLive updates, image tracking, distribution tracking\u003c/em\u003e… and Aim 2.0 was hitting the limits of Aim 1.0’s design.\u003c/p\u003e\n\u003cp\u003eIn order to support the future, we had to make changes to the foundation now.\u003c/p\u003e\n\u003ch2\u003eLaunching Aim 3.0.0\u003c/h2\u003e\n\u003cp\u003eAn additional \u003ca href=\"https://github.com/aimhubio/aim/milestone/13?closed=1\"\u003e317 pull requests\u003c/a\u003e later, we are excited to launch Aim v3.0.0 !!!\u003c/p\u003e\n\u003cp\u003eAs a result, the most important changes include:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA completely revamped UI\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHome page and run detail page\u003c/li\u003e\n\u003cli\u003eRuns, metrics and params explorers\u003c/li\u003e\n\u003cli\u003eBookmarks and Tags\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eA completely revamped Aim Python SDK\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNew and much more intuitive (but still quite vanilla) API to track your training runs\u003c/li\u003e\n\u003cli\u003eNew and 10x faster embedded storage based on \u003ca href=\"http://rocksdb.org/\"\u003eRocksdb\u003c/a\u003e. This will allow us to store virtually any type of AI metadata. On the contrary, \u003ca href=\"https://github.com/aimhubio/aimrecords\"\u003eAimRecords\u003c/a\u003e was designed for metrics and hyperparams only.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEnjoy the changes!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*xbg148dzileheVdOALBMBg.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePerformance improvements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAverage run query execution time on ~2000 runs: 0.784s.\u003c/li\u003e\n\u003cli\u003eAverage metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\u003c/li\u003e\n\u003cli\u003eNew UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (\u003cem\u003efor comparison, v2 was performant with limitation for only 100 metrics\u003c/em\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eComparisons to familiar tools\u003c/h2\u003e\n\u003ch3\u003eTensorboard\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTraining run comparison\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\u003c/li\u003e\n\u003cli\u003eWith Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params. After all, TensorBoard doesn’t have features to group, aggregate the metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eScalability\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim can handle 1000s of training runs both on the backend and on the UI.\u003c/li\u003e\n\u003cli\u003eTensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAim will have the beloved TB visualizations\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEmbedding projector.\u003c/li\u003e\n\u003cli\u003eNeural network visualization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003ca href=\"https://github.com/aimhubio/aim#mlflow\"\u003e\u003c/a\u003eMLFlow\u003c/h3\u003e\n\u003cp\u003eMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRun comparison\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\u003c/li\u003e\n\u003cli\u003eMLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eUI Scalability\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\u003c/li\u003e\n\u003cli\u003eMLflow UI becomes slow to use when there are a few hundreds of runs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003ca href=\"https://github.com/aimhubio/aim#weights-and-biases\"\u003e\u003c/a\u003eWeights and Biases\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHosted vs self-hosted\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWeights and Biases is a hosted closed-source MLOps platform.\u003c/li\u003e\n\u003cli\u003eAim is self-hosted, free and open-source experiment tracking tool.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAim Roadmap\u003ca href=\"https://github.com/aimhubio/aim#tensorboard\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWith this version we are also publishing the Aim \u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eroadmap\u003c/a\u003e for the next 3 months.\u003c/p\u003e\n\u003cp\u003eThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\u003c/p\u003e\n\u003cp\u003eWe are also \u003ca href=\"https://github.com/aimhubio/aim#community\"\u003einviting community contributors\u003c/a\u003e to help us get there faster!\u003c/p\u003e\n\u003ch2\u003eWhy are we building Aim?\u003c/h2\u003e\n\u003cp\u003eWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\u003c/p\u003e\n\u003cp\u003eExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\u003c/p\u003e\n\u003cp\u003eHowever we see lots of improvements to be made. Especially around issues like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eability to handle 1000s of large-scale experiments\u003c/li\u003e\n\u003cli\u003eactionable, beautiful and performant visualizations\u003c/li\u003e\n\u003cli\u003eextensibility — how easy are the apis for extension/democratization?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\u003c/p\u003e\n\u003cp\u003eOur mission…\u003c/p\u003e\n\u003cp\u003eAim’s mission is to democratize AI dev tools. We believe that the best AI tools need to be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen-source, open-data-format, community-driven\u003c/li\u003e\n\u003cli\u003ehave great UI/UX, CLI and other interfaces for automation\u003c/li\u003e\n\u003cli\u003eperformant both on UI and data\u003c/li\u003e\n\u003cli\u003eextensible — enable ways to build around for so many use-cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThanks to\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/roubkar\"\u003eRuben Karapetyan\u003c/a\u003e for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/mahnerak\"\u003eMahnerak\u003c/a\u003e for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\u003c/p\u003e\n\u003cp\u003eAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\u003c/p\u003e\n\u003cp\u003eThe contributors who have been relentlessly iterating over the course of the summer.\u003c/p\u003e\n\u003cp\u003eOn to the next generation of ML tools!!\u003c/p\u003e\n\u003ch2\u003eJoin Us!\u003c/h2\u003e\n\u003cp\u003eJoin the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, test Aim out, ask questions, help us build the future of AI tooling!\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, drop by and star the repo ⭐\u003c/p\u003e"},"_id":"posts/aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","_raw":{"sourceFilePath":"posts/aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","sourceFileName":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/aim’s-foundations-why-we’re-building-a-tensorboard-alternative"},"type":"Post"}]},"__N_SSG":true},"page":"/blog/[category]","query":{"category":"new-releases"},"buildId":"tfECHI5ownHmk6F76cjV6","isFallback":false,"gsp":true,"scriptLoader":[{"src":"https://www.googletagmanager.com/gtag/js?id=G-G8YKCN2HLS","strategy":"afterInteractive"},{"id":"google-analytics","strategy":"afterInteractive","children":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){window.dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config','G-G8YKCN2HLS');\n            "}]}</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-G8YKCN2HLS"
            height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript></body></html>