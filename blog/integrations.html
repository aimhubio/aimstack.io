<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Integrations | AimStack</title><meta name="robots" content="index,follow"/><meta name="description" content="Access your category related articles"/><meta property="og:title" content="Integrations | AimStack"/><meta property="og:description" content="Access your category related articles"/><meta property="og:url" content="https://aimstack.io/blog/integrations"/><meta property="og:type" content="website"/><meta property="og:image" content="https://aimstack.io/banner.png"/><meta property="og:image:alt" content="Integrations | AimStack"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1224"/><meta property="og:image:height" content="724"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="AimStack"/><link rel="canonical" href="https://aimstack.io/blog/integrations"/><link rel="preload" href="/_next/static/media/bg.eb38a857.svg" as="image" fetchpriority="high"/><meta name="next-head-count" content="18"/><style id="stitches">--sxs{--sxs:0 t-dSaWrp}@media{:root,.t-dSaWrp{--fonts-base:inherit;--colors-blue:#1093F2;--colors-blueHover:#0F7AC8;--colors-lightBlue:rgba(16, 147, 242, 0.1);--colors-lightBlueHover:rgba(16, 147, 242, 0.2);--colors-darkBlue:#0C1031;--colors-darkBlue500:rgba(12,16,49,0.5);--colors-bigStone:#191D3C;--colors-bigStoneHover:#313551;--colors-green:#14C89D;--colors-black:#000000;--colors-black003:rgba(0,0,0,0.03);--colors-black700:rgba(0,0,0,0.7);--colors-black800:rgba(0,0,0,0.8);--colors-white:#ffffff;--colors-white100:rgba(255,255,255,0.1);--colors-white500:rgba(255,255,255,0.5);--colors-white700:rgba(255,255,255,0.7);--colors-red:#CC231A;--colors-lightGrey:#F4F7F9;--colors-lightGreyHover:#ECEEF0;--colors-grey:#CFD3D6;--colors-darkGrey:#737379;--colors-darkGreyHover:#393940;--colors-primary:var(--colors-blue);--colors-primaryHover:var(--colors-blueHover);--colors-primaryLight:var(--colors-lightBlue);--colors-primaryLightHover:var(--colors-lightBlueHover);--colors-secondary:var(--colors-green);--colors-textColor:var(--colors-black);--colors-bgColor:var(--colors-darkBlue);--colors-danger:var(--colors-red);--fontSizes-1:14px;--fontSizes-2:16px;--fontSizes-3:18px;--fontSizes-4:20px;--fontSizes-5:22px;--fontSizes-6:24px;--fontSizes-7:32px;--fontSizes-8:44px;--fontSizes-9:56px;--fontSizes-10:64px;--fontSizes-11:68px;--fontSizes-baseSize:var(--fontSizes-2);--space-1:4px;--space-2:8px;--space-3:12px;--space-4:16px;--space-5:20px;--space-6:24px;--space-7:28px;--space-8:32px;--space-9:36px;--space-10:40px;--space-11:44px;--space-12:48px;--space-13:52px;--space-14:56px;--fontWeights-1:400;--fontWeights-2:500;--fontWeights-3:600;--fontWeights-4:700;--fontWeights-5:800;--fontWeights-6:900;--shadows-1:0px 60px 66px -65px rgba(11, 47, 97, 0.2);--shadows-2:0px 96px 66px -65px rgba(11, 47, 97, 0.2);--shadows-3:1px 1px 10px 3px  rgb(0, 0, 0, 10%);--shadows-4:0px 10px 24px -10px rgba(11, 47, 97, 0.4);--shadows-5:0px 10px 32px -10px rgba(11, 47, 97, 0.2);--radii-1:6px;--radii-2:8px;--transitions-main:0.2s ease-out}}--sxs{--sxs:1 iPqvQa k-iKtTFk}@media{html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,main,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section{display:block}*[hidden]{display:none}*{box-sizing:border-box}ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:none}table{border-spacing:0}strong{font-weight:var(--fontWeights-5)}body{background:var(--colors-white);color:var(--colors-textColor);font-family:var(--fonts-base);font-size:var(--fontSizes-baseSize);line-height:1.35;overscroll-behavior:none}img{user-drag:none;-webkit-user-drag:none}a{text-decoration:none;color:inherit}a.link{color:var(--colors-primary);font-weight:var(--fontWeights-4);transition:var(--transitions-main)}a.link:hover{color:var(--colors-primaryHover)}.text-center{text-align:center}@keyframes k-iKtTFk{0%{opacity:0}10%{opacity:1}90%{opacity:1}100%{opacity:0}}}--sxs{--sxs:2 c-iSkjJi c-dhzjXW c-guYHoi c-ckOUFN c-hCkBfr c-ePqJJt c-jafYUQ c-BqIMD c-iCFsHS c-kPczbf c-bMqEGV c-fSprKn c-lizetl c-fOPBY c-cZmHrB c-dtSTJL c-caMtBg c-PJLV c-ioCbLy c-hIPllP c-kfcfix c-fXQtST c-iHpEPN c-jHDeUH c-fYCBFy c-gDDtX c-kpTGUt c-hdgScR c-kskmfB c-idhSbv c-fixGjY c-kHKySA c-iWPTnC c-dJzRen c-juttUG c-TJTRd c-foiehy c-hoGqzL c-QFqsL c-kBJMsw c-coAScu c-gPhuwe c-eJZVxt c-joiLKa c-IuYqv c-bCTsjq c-EjvQF c-bcXfYW c-kOFnHV c-bosfyl c-gswrlc c-jVwFOG c-eYYUfS c-bBICiS c-kDFBqB c-ghrOTq c-fcTZTi c-gvTmKt c-FsNLu c-jgJYki c-bFmrFE c-MNZuo c-cChYXC c-cXVAvf c-dsvWej c-cmpvrW c-crdKmK c-jpbidf c-jFppbe c-hnRRWM}@media{.c-iSkjJi{position:relative}.c-iSkjJi .bg-top{z-index:2}.c-iSkjJi .bg-bottom{z-index:1;position:absolute;bottom:300px;left:0;right:0;object-fit:contain;width:100%;height:60%}.c-dhzjXW{display:flex}.c-guYHoi{position:relative;z-index:3;display:flex;flex-direction:column;min-height:100vh}.c-ckOUFN{height:72px;position:fixed;top:0px;left:0px;right:0px;z-index:99;transition:var(--transitions-main);background-color:transparent}.c-ckOUFN.dark{background-color:var(--colors-darkBlue)}.c-ckOUFN.dark a{color:var(--colors-white)}.c-ckOUFN.dark.fixed a{color:var(--colors-textColor)}.c-ckOUFN.fixed{box-shadow:var(--shadows-3);background-color:var(--colors-white)}.c-hCkBfr{margin-left:auto;margin-right:auto;padding-left:var(--space-6);padding-right:var(--space-6);width:100%;max-width:1300px}.c-ePqJJt{display:flex;align-items:center;height:100%}.c-jafYUQ{margin-right:50px}.c-jafYUQ .logo{max-width:158px;width:100%;display:block}.c-jafYUQ .logo-image{display:block}@media (max-width: 1024px){.c-jafYUQ{position:relative;z-index:11}}.c-BqIMD{display:flex;justify-content:center}.c-BqIMD .nav-list{display:flex}.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-6)}@media (max-width: 1440px){.c-BqIMD .nav-list li:not(:last-child){margin-right:var(--space-4)}}.c-BqIMD .nav-list li a{-webkit-backface-visibility:hidden;backface-visibility:hidden;display:inline-flex}.c-BqIMD .nav-list li a .text{transition:var(--transitions-main)}.c-BqIMD .nav-list li a:hover .text{opacity:.6}@media (max-width: 1024px){.c-BqIMD{position:fixed;top:72px;right:0;bottom:0;left:0;overflow-y:auto;z-index:10;height:0;transition:height 0.5s;background-color:var(--colors-white);flex-direction:column;justify-content:space-between}}@media (max-width: 1024px){.open .c-BqIMD{height:calc(100% - 72px)}}@media (max-width: 1024px){.c-BqIMD .nav-inner{width:100%;flex-direction:column;align-items:inherit;justify-content:space-between;padding-top:16px}}@media (max-width: 1024px){.c-BqIMD .nav-list{flex-direction:column;align-items:flex-start}}@media (max-width: 1024px){.c-BqIMD .nav-list > li{font-size:var(--fontSizes-2);font-weight:var(--fontWeights-3);width:100%}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a{display:block;padding:var(--space-3) var(--space-5)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li a:active{background-color:var(--colors-primaryLight)}}@media (max-width: 1024px){.c-BqIMD .nav-list > li:not(:last-child){margin-right:0}}.c-iCFsHS{display:inline-block;height:18px;line-height:18px;padding:0 4px;border-radius:2px;background-color:var(--colors-primary);color:var(--colors-white);font-weight:700;font-size:10px;margin-left:6px}.c-kPczbf{margin-left:auto}.c-kPczbf span span{display:flex;align-items:center}.c-kPczbf.desktop-btn span span{justify-content:flex-end}.c-bMqEGV{-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer;border-radius:var(--radii-1);display:inline-block;transition:var(--transitions-main)}.c-fSprKn{display:flex;gap:10px;align-items:center;height:100%;margin-right:2px;margin-left:2px}.c-lizetl{display:none}@media (max-width: 1024px){.c-lizetl{display:flex;justify-content:center;padding-top:var(--space-6);padding-bottom:var(--space-6)}}@media (max-width: 1024px){.c-lizetl > li{margin-right:var(--space-6)}}.c-fOPBY{display:none}@media (max-width: 1024px){.c-fOPBY{position:relative;z-index:11;display:inline-block;-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer}}.c-cZmHrB{flex:1}.c-dtSTJL{background-image:url(/images/static/hero/dots.svg), url(/images/static/hero/bg.svg);background-repeat:no-repeat;background-size:cover, contain;background-position:center, right;position:relative;height:100%;min-height:100vh;display:flex;text-align:center;overflow:hidden;padding:150px 0}@media (max-width: 1024px){.c-dtSTJL{padding:120px 0}}@media (max-width: 743px){.c-dtSTJL{padding:120px 0 24px}}.c-caMtBg{width:100%;position:relative;display:flex}@media (max-width: 1024px){.c-caMtBg{flex-direction:column;align-items:center;height:100%}}.c-caMtBg .hero-button-container{display:flex}@media (max-width: 1024px){.c-caMtBg .hero-button-container{justify-content:center}}@media (max-width: 425px){.c-caMtBg .hero-button-container{flex-direction:column}}@media (max-width: 425px){.c-caMtBg .hero-button-container .hero-try-demo{margin-bottom:var(--space-4)}}.c-caMtBg .hero-button-container .hero-quick-start{margin-left:var(--space-4)}@media (max-width: 425px){.c-caMtBg .hero-button-container .hero-quick-start{margin-left:0}}.c-caMtBg .hero-content{max-width:550px;min-width:550px;text-align:left}@media (max-width: 1024px){.c-caMtBg .hero-content{text-align:center;min-width:unset}}.c-ioCbLy{position:absolute;top:50%;transform:translateY(-50%);right:-300px;border-radius:12px;border:1px solid #D8DADF;width:866px;height:auto;aspect-ratio:auto}@media (max-width: 1440px){.c-ioCbLy{width:700px}}@media (max-width: 1024px){.c-ioCbLy{position:unset;transform:unset;width:500px;margin-top:var(--space-12)}}@media (max-width: 743px){.c-ioCbLy{width:90%}}@media (max-width: 425px){.c-ioCbLy{display:none}}.c-hIPllP{padding:80px 0}@media (max-width: 1024px){.c-hIPllP{padding:60px 0}}@media (max-width: 743px){.c-hIPllP{padding:60px 0 24px}}.c-kfcfix{height:110px;display:flex;background-color:white}.c-fXQtST{background-color:white}.c-iHpEPN{display:flex;align-items:center;overflow:visible}.c-iHpEPN img{width:100%;height:auto}.c-jHDeUH{position:relative;overflow:hidden;padding:150px 0}@media (max-width: 1024px){.c-jHDeUH{padding:80px 0 24px}}.c-fYCBFy{object-fit:cover;z-index:-1}.c-gDDtX{display:flex;justify-content:space-between;align-items:center}@media (max-width: 1024px){.c-gDDtX{flex-direction:column}}.c-kpTGUt{display:flex;flex-direction:column;max-width:500px}@media (max-width: 1024px){.c-kpTGUt{text-align:center;align-items:center}}.c-kpTGUt .integrations-title{margin-bottom:var(--space-6);font-weight:800}.c-kpTGUt .integrations-subtitle{line-height:30px;margin-bottom:var(--space-14)}@media (max-width: 1024px){.c-kpTGUt .integrations-subtitle{margin-bottom:var(--space-11)}}.c-kpTGUt a{width:-moz-fit-content;width:fit-content}@media (max-width: 425px){.c-kpTGUt a{width:100%}}.c-hdgScR{width:50%;max-width:540px;height:auto}@media (max-width: 1024px){.c-hdgScR{margin-top:var(--space-7);width:100%}}@media (max-width: 425px){.c-hdgScR{display:none}}.c-kskmfB{position:relative;overflow:hidden;padding:150px 0}@media (max-width: 743px){.c-kskmfB{padding:80px 0 24px;text-align:center}}.c-kskmfB .features-title{margin-bottom:var(--space-6);font-weight:800}@media (max-width: 743px){.c-kskmfB .features-title{text-align:center}}.c-kskmfB .features-subtitle{line-height:30px;margin-bottom:var(--space-14)}@media (max-width: 743px){.c-kskmfB .features-subtitle{margin-bottom:var(--space-11)}}.c-idhSbv{object-fit:contain;z-index:-1}.c-fixGjY{display:flex;flex-direction:column}.c-kHKySA{display:flex;flex-direction:column;max-width:600px;margin:0 auto;text-align:center}@media (max-width: 743px){.c-kHKySA{max-width:100%}}.c-iWPTnC{display:flex;flex-direction:column;justify-content:space-between;width:100%}.c-dJzRen{display:flex;align-items:center;justify-content:space-between;padding-bottom:120px}@media (max-width: 743px){.c-dJzRen{flex-direction:column}}.c-juttUG{text-align:left;width:400px;min-width:300px}@media (max-width: 743px){.c-juttUG{width:100%}}.c-juttUG .step-title{display:flex;align-items:center;text-align:left;margin-bottom:var(--space-6)}.c-juttUG .step-title .badge{color:var(--colors-white);font-size:10px;padding:6px 4px;background:#5865F2;border-radius:4px;margin-left:var(--space-4)}.c-juttUG .step-description{margin-bottom:var(--space-5)}@media (max-width: 743px){.c-juttUG .step-description{text-align:left}}@media (max-width: 743px){.c-juttUG ul{text-align:left}}.c-juttUG ul li{margin-bottom:var(--space-3);position:relative;padding-left:24px}.c-juttUG ul li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:#5865F2}.c-TJTRd{padding:12px;display:inline-flex;border-radius:8px;margin-bottom:24px}.c-foiehy{max-width:800px;min-width:380px;height:auto;display:block;margin-left:60px}@media (max-width: 743px){.c-foiehy{display:none}}.c-foiehy.step-banner-image-mobile{display:none}@media (max-width: 743px){.c-foiehy.step-banner-image-mobile{margin:0 auto var(--space-10);display:block;min-width:unset;width:100%}}.c-hoGqzL{border-top:1px solid #E4E7EB;display:flex;align-items:flex-start;width:100%}@media (max-width: 743px){.c-hoGqzL{flex-direction:column}}.c-hoGqzL .step-coming-soon{flex:1;max-width:360px;padding-right:40px;margin-right:60px;padding:60px 0}@media (max-width: 743px){.c-hoGqzL .step-coming-soon{padding-right:0;margin-right:0}}.c-QFqsL{display:flex;flex-direction:column;overflow:hidden;position:relative;padding-bottom:100px}@media (max-width: 743px){.c-QFqsL{padding:80px 0 24px}}.c-QFqsL .top-bg{object-fit:cover}.c-QFqsL .bottom-bg{object-fit:cover}.c-kBJMsw{width:100%;height:auto;scale:1.02}.c-coAScu{background-color:#04062E;padding:100px 0}.c-coAScu .quickstart-button{margin-top:var(--space-14);font-weight:var(--fontWeights-3)}@media (max-width: 1024px){.c-coAScu .quickstart-button{display:none}}.c-coAScu .quickstart-button-mobile{display:none;text-align:center;margin-top:var(--space-6)}@media (max-width: 1024px){.c-coAScu .quickstart-button-mobile{display:inline-block;margin-inline-start:auto;margin-inline-end:auto}}.c-gPhuwe{overflow:hidden;display:flex;justify-content:space-between}@media (max-width: 1024px){.c-gPhuwe{flex-direction:column;margin:0 auto}}.c-eJZVxt{max-width:480px}@media (max-width: 1024px){.c-eJZVxt{margin-bottom:var(--space-14)}}.c-eJZVxt .quickstart-title{color:var(--colors-white);margin-bottom:var(--space-6)}.c-eJZVxt .quickstart-subtitle{color:var(--colors-white)}.c-eJZVxt .quickstart-subtitle li{margin-bottom:var(--space-3);position:relative;padding-left:24px;line-height:26px}.c-eJZVxt .quickstart-subtitle li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:#1093F2}.c-joiLKa{margin-left:80px}@media (max-width: 1024px){.c-joiLKa{margin-left:0}}.c-IuYqv{padding-top:var(--space-5)}.c-IuYqv code{font-family:var(--fonts-Inconsolata)}.c-bCTsjq{position:relative;width:500px}@media (max-width: 1440px){.c-bCTsjq{width:unset}}.c-bCTsjq.light .hljs{background-color:var(--colors-white);color:var(--colors-textColor)}.c-bCTsjq button{position:absolute;right:12px;top:6px;background:none;border:none;cursor:pointer;height:42px;width:42px;transition:var(--transitions-main)}.c-bCTsjq button:hover{opacity:.6}.c-bCTsjq .copied{display:block;opacity:0;position:absolute;right:0;top:-20px;animation:k-iKtTFk 2.5s ease 0s alternate;color:var(--colors-secondary);font-size:var(--fontSizes-1);font-weight:var(--fontWeights-2);transition:var(--transitions-main);z-index:10}.c-bCTsjq .hljs{display:block;overflow-x:auto;color:#abb2bf;background:#0c0d34;margin-bottom:20px;border-radius:var(--radii-1);border:1px solid #3D3D5D;padding:var(--space-4)}.c-bCTsjq .hljs-comment,.c-bCTsjq .hljs-quote{color:#5c6370;font-style:italic}.c-bCTsjq .hljs-doctag,.c-bCTsjq .hljs-keyword,.c-bCTsjq .hljs-formula{color:#c678dd}.c-bCTsjq .hljs-section,.c-bCTsjq .hljs-name,.c-bCTsjq .hljs-selector-tag,.c-bCTsjq .hljs-deletion,.c-bCTsjq .hljs-subst{color:#e06c75}.c-bCTsjq .hljs-literal{color:#56b6c2}.c-bCTsjq .hljs-string,.c-bCTsjq .hljs-regexp,.c-bCTsjq .hljs-addition,.c-bCTsjq .hljs-attribute,.c-bCTsjq .hljs-meta-string{color:#98c379}.c-bCTsjq .hljs-built_in,.c-bCTsjq .hljs-class .hljs-title{color:#e6c07b}.c-bCTsjq .hljs-attr,.c-bCTsjq .hljs-variable,.c-bCTsjq .hljs-template-variable,.c-bCTsjq .hljs-type,.c-bCTsjq .hljs-selector-class,.c-bCTsjq .hljs-selector-attr,.c-bCTsjq .hljs-selector-pseudo,.c-bCTsjq .hljs-number{color:#d19a66}.c-bCTsjq .hljs-symbol,.c-bCTsjq .hljs-bullet,.c-bCTsjq .hljs-link,.c-bCTsjq .hljs-meta,.c-bCTsjq .hljs-selector-id,.c-bCTsjq .hljs-title{color:#61aeee}.c-bCTsjq .hljs-emphasis{font-style:italic}.c-bCTsjq .hljs-strong{font-weight:bold}.c-bCTsjq .hljs-link{text-decoration:underline}.c-EjvQF{background-image:linear-gradient(transparent, #d0cafe40, transparent);position:relative;overflow:hidden;padding:150px 0}@media (max-width: 743px){.c-EjvQF{padding:80px 0 24px}}.c-EjvQF .demos-title{text-align:center;margin-bottom:var(--space-6)}.c-EjvQF .demos-subtitle{text-align:center;margin-bottom:var(--space-14)}.c-bcXfYW{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:16px}@media (max-width: 1024px){.c-bcXfYW{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-bcXfYW{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 575px){.c-bcXfYW{grid-template-columns:repeat(1,minmax(0,1fr))}}.c-kOFnHV{min-width:260px;height:340px;display:inline-flex}@media (max-width: 575px){.c-kOFnHV{width:100%;max-width:360px;margin:0 auto}}.c-kOFnHV a{display:block;width:100%;height:100%;border-radius:var(--radii-2);transition:var(--transitions-main);border:1px solid #E2D7EB;background-color:var(--colors-white)}.c-kOFnHV a .demo-inner{padding:var(--space-5) var(--space-6)}.c-kOFnHV a:hover{background-color:#F3F5F9}.c-bosfyl{width:100%;height:66%;display:block;border-radius:var(--radii-1) var(--radii-1) 0 0;object-fit:cover;padding:10px;overflow:hidden}.c-gswrlc{padding:100px 0}.c-gswrlc .input{max-width:400px}@media (max-width: 1024px){.c-gswrlc{padding:80px 0;text-align:center}}@media (max-width: 1024px){.c-gswrlc .input{margin:0 auto}}@media (max-width: 743px){.c-gswrlc{padding:60px 0}}.c-jVwFOG{display:flex;align-items:center;justify-content:space-between}.c-eYYUfS{flex:1;margin-right:80px}@media (max-width: 1024px){.c-eYYUfS{margin-right:0}}.c-bBICiS{position:relative;display:block}.c-bBICiS .error-message{position:absolute;top:100%;transform:translateY(2px);color:var(--colors-danger);font-size:var(--fontSizes-1)}.c-bBICiS input{border:none;border-radius:var(--radii-1);width:100%;transition:var(--transitions-main)}.c-kDFBqB{flex:1}.c-kDFBqB img{max-width:100%;width:100%;height:auto}@media (max-width: 1024px){.c-kDFBqB{display:none}}.c-ghrOTq{overflow:hidden;width:100%;height:auto;padding-top:4px;display:flex;flex-direction:column}.c-fcTZTi{min-height:100px;display:flex;flex-direction:column;background-color:#673AB7}.c-gvTmKt{margin-top:auto}.c-FsNLu{display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;color:var(--colors-white);padding-top:27px;padding-bottom:27px;border-bottom:1px solid var(--colors-white100)}.c-jgJYki{margin-right:var(--space-9)}.c-jgJYki img{display:block}@media (max-width: 743px){.c-jgJYki img{margin:0 auto}}@media (max-width: 1440px){.c-jgJYki{margin-right:var(--space-6)}}@media (max-width: 743px){.c-jgJYki{width:100%;margin-right:0;margin-bottom:var(--space-1);text-align:center}}.c-bFmrFE{display:flex;margin-right:auto}.c-bFmrFE li:not(:last-child){margin-right:var(--space-9)}@media (max-width: 1440px){.c-bFmrFE li:not(:last-child){margin-right:var(--space-4)}}.c-bFmrFE li{transition:var(--transitions-main)}.c-bFmrFE li:hover{opacity:.6}@media (max-width: 1024px){.c-bFmrFE{order:3;width:100%;margin-top:var(--space-7)}}@media (max-width: 1024px){.c-bFmrFE li:last-child{margin-right:0}}@media (max-width: 743px){.c-bFmrFE{order:2;width:100%;margin-bottom:var(--space-2);flex-direction:column;text-align:center}}@media (max-width: 743px){.c-bFmrFE li{margin-bottom:var(--space-6)}}@media (max-width: 743px){.c-bFmrFE li:not(:last-child){margin-right:0}}.c-MNZuo{display:flex}.c-MNZuo li:not(:last-child){margin-right:var(--space-6)}.c-MNZuo li a{transition:var(--transitions-main)}.c-MNZuo li a:hover{opacity:.6}@media (max-width: 743px){.c-MNZuo{margin:0 auto;order:3}}.c-cChYXC{padding-top:var(--space-4);padding-bottom:var(--space-4);color:var(--colors-white500);text-align:center}.c-cXVAvf{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:40px}@media (max-width: 1024px){.c-cXVAvf{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (max-width: 743px){.c-cXVAvf{gap:20px;grid-template-columns:repeat(1,minmax(0,1fr))}}.c-dsvWej{padding:10px;border:1px solid #E2D7EB;border-radius:var(--radii-2);transition:var(--transitions-main);height:100%}.c-dsvWej:hover{background-color:#F3F5F9}.c-cmpvrW{position:relative}.c-crdKmK{width:100%;object-fit:cover;height:200px;min-height:200px}.c-jpbidf{padding:20px 10px 0px 10px;position:relative}.c-jFppbe{margin-bottom:var(--space-2);display:flex;align-items:center;color:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe .icon{margin-right:var(--space-1);fill:var(--colors-darkGrey);transition:var(--transitions-main)}.c-jFppbe:hover{color:var(--colors-darkGray)}.c-jFppbe:hover .icon{fill:var(--colors-darkGray)}.c-hnRRWM{display:flex;align-items:center;justify-content:flex-end}.c-hnRRWM .icon{margin-right:var(--space-1);fill:var(--colors-grey)}}--sxs{--sxs:3 c-dhzjXW-iTKOFX-direction-column c-dhzjXW-jroWjL-align-center c-dhzjXW-bICGYT-justify-center c-dhzjXW-kVNAnR-wrap-noWrap c-bMqEGV-jIiqPO-size-2 c-bMqEGV-fTPcDy-variant-community c-PJLV-bWcAUL-size-9 c-PJLV-ypqAk-size-3 c-bMqEGV-kwxnul-size-1 c-bMqEGV-gPhMRO-variant-outline c-bMqEGV-dxfqfJ-variant-primary c-PJLV-jszWhv-size-7 c-PJLV-hlFDyt-size-6 c-PJLV-dnvIlH-size-4 c-PJLV-cBLpOj-size-2 c-bBICiS-RBfUm-size-1 c-PJLV-EDrvg-size-1 c-PJLV-iMgaFX-truncate-true c-PJLV-cllNQK-lineClamp-true}@media{.c-dhzjXW-iTKOFX-direction-column{flex-direction:column}.c-dhzjXW-jroWjL-align-center{align-items:center}.c-dhzjXW-bICGYT-justify-center{justify-content:center}.c-dhzjXW-kVNAnR-wrap-noWrap{flex-wrap:nowrap}.c-bMqEGV-jIiqPO-size-2{height:32px;line-height:32px;font-size:var(--fontSizes-1);padding-left:var(--space-2);padding-right:var(--space-2)}.c-bMqEGV-fTPcDy-variant-community{background-color:#5865F2;color:var(--colors-white);border:1px solid #5865F2}.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-9);font-weight:var(--fontWeights-6);line-height:normal}@media (max-width: 1024px){.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-bWcAUL-size-9{font-size:var(--fontSizes-7)}}.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-3)}@media (max-width: 743px){.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-2)}}.c-bMqEGV-kwxnul-size-1{height:53px;line-height:53px;font-size:var(--fontSizes-3);padding-left:var(--space-6);padding-right:var(--space-6)}@media (max-width: 1024px){.c-bMqEGV-kwxnul-size-1{height:50px;line-height:50px}}.c-bMqEGV-gPhMRO-variant-outline{background-color:var(--colors-white);color:#5865F2;border:1px solid #5865F2}.c-bMqEGV-gPhMRO-variant-outline:hover{background-color:#DEE0FC;border-color:var(--colors-primaryHover)}.c-bMqEGV-dxfqfJ-variant-primary{background-color:var(--colors-primary);color:var(--colors-white)}.c-bMqEGV-dxfqfJ-variant-primary:hover{background-color:var(--colors-primaryHover)}.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-2)}@media (max-width: 743px){.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-6)}}.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-4)}}.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-4)}@media (max-width: 1024px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-3)}}@media (max-width: 743px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-2)}}.c-PJLV-cBLpOj-size-2{font-size:var(--fontSizes-2)}.c-bBICiS-RBfUm-size-1 input{padding:17px 24px;font-size:var(--fontSizes-2);background-color:var(--colors-lightGrey)}.c-bBICiS-RBfUm-size-1 input:hover{background-color:var(--colors-lightGreyHover)}.c-PJLV-EDrvg-size-1{font-size:var(--fontSizes-1)}.c-PJLV-iMgaFX-truncate-true{max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.c-PJLV-cllNQK-lineClamp-true{display:-webkit-box;-webkit-line-clamp:var(---lineClamp);-webkit-box-orient:vertical;overflow:hidden}}--sxs{--sxs:6 c-dhzjXW-iguWOGj-css c-hCkBfr-ilkBNdM-css c-kPczbf-igSvvfr-css c-kPczbf-idOghFk-css c-bMqEGV-ifNxlSP-css c-fOPBY-ieBuAdh-css c-PJLV-iftFgSo-css c-PJLV-idSlxtB-css c-PJLV-ibWHAEn-css c-fXQtST-ijfuMJQ-css c-TJTRd-ihKsdEP-css c-bMqEGV-ihZnFyC-css c-TJTRd-ihYCfXG-css c-TJTRd-igEZsbb-css c-PJLV-ifPSeYF-css c-PJLV-ibZBPXN-css c-PJLV-iejLhMq-css c-PJLV-ietOTGQ-css c-bMqEGV-icGsXGD-css c-PJLV-ieRZfPH-css c-PJLV-iUazGY-css c-PJLV-icsdZpM-css}@media{.c-dhzjXW-iguWOGj-css{height:100vh}.c-hCkBfr-ilkBNdM-css{height:100%}.c-kPczbf-igSvvfr-css{display:none}@media (max-width: 1024px){.c-kPczbf-igSvvfr-css{display:block;padding:var(--space-5)}}.c-kPczbf-idOghFk-css{flex:1}@media (max-width: 1024px){.c-kPczbf-idOghFk-css{display:none}}.c-bMqEGV-ifNxlSP-css{float:right}.c-fOPBY-ieBuAdh-css{margin-left:auto;padding:var(--space-3)}.c-PJLV-iftFgSo-css{margin-bottom:var(--space-6);width:100%}.c-PJLV-idSlxtB-css{margin-bottom:56px;width:100%}.c-PJLV-ibWHAEn-css{margin-bottom:var(--space-6);font-weight:700;text-align:center}.c-fXQtST-ijfuMJQ-css{opacity:0;transition:var(--transitions-main)}.c-TJTRd-ihKsdEP-css{background:#DBEEFE}.c-bMqEGV-ihZnFyC-css{margin-top:var(--space-6);font-weight:var(--fontWeights-3)}.c-TJTRd-ihYCfXG-css{background:#E1DCFE}.c-TJTRd-igEZsbb-css{background:#FADAFE}.c-PJLV-ifPSeYF-css{color:var(--colors-white)}.c-PJLV-ibZBPXN-css{margin-bottom:var(--space-2);font-weight:var(--fontWeights-4)}.c-PJLV-iejLhMq-css{margin-bottom:var(--space-4)}.c-PJLV-ietOTGQ-css{margin-bottom:var(--space-6)}.c-bMqEGV-icGsXGD-css{margin-top:var(--space-14)}.c-PJLV-ieRZfPH-css{text-align:center;margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-PJLV-iUazGY-css{display:flex;align-items:center}.c-PJLV-icsdZpM-css{margin-top:var(--space-4);margin-bottom:var(--space-4);---lineClamp:3;font-family:var(--fonts-Lora)}}</style><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/cc5e782db96f5926.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/cc5e782db96f5926.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-46696d78b9933bc6.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-64ec93240796860d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-408fe53479a88159.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/962-688f32d50bed486d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bcategory%5D-c13ffc473bcb306e.js" defer="" crossorigin=""></script><script src="/_next/static/XVbXvPM_xexbgQ-lZ1_Ds/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/XVbXvPM_xexbgQ-lZ1_Ds/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><main class="c-iSkjJi __className_d65c78"><div class="c-guYHoi"><header class="c-ckOUFN"><div class="c-hCkBfr c-hCkBfr-ilkBNdM-css"><div class="c-ePqJJt"><div class="c-jafYUQ"><a class="logo" href="/"><img alt="AimStack" loading="lazy" width="156" height="37" decoding="async" data-nimg="1" class="logo-image next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 156 37&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/images/static/main/logo.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/images/static/main/logo.svg"/></a></div><nav class="c-BqIMD"><div class="nav-inner"><ul class="nav-list"><li><a target="_self" class="" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" class="" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" class="" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" class="" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" class="" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" class="active" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" class="" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" class="" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><div class="c-kPczbf c-kPczbf-igSvvfr-css"><a class="c-bMqEGV c-bMqEGV-jIiqPO-size-2 c-bMqEGV-fTPcDy-variant-community community-btn" href="https://community.aimstack.io"><div class="c-fSprKn"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/>Join Community</div></a></div><div class="c-kPczbf c-kPczbf-igSvvfr-css"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div></div><ul class="c-lizetl"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></nav><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><a class="c-bMqEGV c-bMqEGV-jIiqPO-size-2 c-bMqEGV-fTPcDy-variant-community c-bMqEGV-ifNxlSP-css community-btn" href="https://community.aimstack.io"><div class="c-fSprKn"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/>Join Community</div></a></div><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div><button type="button" aria-label="menu" class="c-fOPBY c-fOPBY-ieBuAdh-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:black" d="M0 146.286c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.289 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM0 512.008c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.29 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM60.952 816.748c-33.663 0-60.952 27.29-60.952 60.956 0 33.661 27.289 60.951 60.952 60.951h902.097c33.661 0 60.951-27.29 60.951-60.951 0-33.667-27.29-60.956-60.951-60.956h-902.097z"></path></svg></button></div></div></header><div class="c-cZmHrB"><div style="padding-block:100px" class="c-hCkBfr"><h1 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-ieRZfPH-css title">Category: <!-- -->Integrations</h1><ul class="c-cXVAvf"><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/integrations/exploring-your-pytorch-lightning-experiments-with-aimos"><img alt="Exploring your PyTorch Lightning experiments with AimOS" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/banner1.png 1x, /images/dynamic/banner1.png 2x" src="/images/dynamic/banner1.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/integrations/exploring-your-pytorch-lightning-experiments-with-aimos">Exploring your PyTorch Lightning experiments with AimOS</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/integrations/exploring-your-pytorch-lightning-experiments-with-aimos">Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/integrations/streamlining-your-hugging-face-experiments-with-aimos"><img alt="Streamlining your Hugging Face Experiments with AimOS" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/hugging-face-1-.png 1x, /images/dynamic/hugging-face-1-.png 2x" src="/images/dynamic/hugging-face-1-.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/integrations/streamlining-your-hugging-face-experiments-with-aimos">Streamlining your Hugging Face Experiments with AimOS</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/integrations/streamlining-your-hugging-face-experiments-with-aimos">AimCallback for Hugging Face is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/integrations/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger"><img alt="AI Observability with AimOS: A Deep Dive into the LangChain Debugger" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/langchain.png 1x, /images/dynamic/langchain.png 2x" src="/images/dynamic/langchain.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/integrations/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger">AI Observability with AimOS: A Deep Dive into the LangChain Debugger</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/integrations/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger">Explore step-by-step guide for LangChain Debugger, a tool designed for in-depth tracking and visualization of LangChain scripts. The guide features a hands-on example emphasizing its abilities to log LLMs prompts, tool inputs/outputs, and chain metadata, providing a clear understanding of its functionalities.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/integrations/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app"><img alt="AI Observability with AimOS: A Deep Dive into the LlamaIndex Observer App" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/llamaindex.png 1x, /images/dynamic/llamaindex.png 2x" src="/images/dynamic/llamaindex.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/integrations/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app">AI Observability with AimOS: A Deep Dive into the LlamaIndex Observer App</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/integrations/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app">Discover AI observability with AimOS&#x27;s LlamaIndex Observer. Essential insights for efficient AI model tracking and analysis. Check out our guide for observability solutions in the world of AI.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/integrations/launching-aim-on-hugging-face-spaces"><img alt="Launching Aim on Hugging Face Spaces" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/hf_space_header.png 1x, /images/dynamic/hf_space_header.png 2x" src="/images/dynamic/hf_space_header.png"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/integrations/launching-aim-on-hugging-face-spaces">Launching Aim on Hugging Face Spaces</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/integrations/launching-aim-on-hugging-face-spaces">Deploy Aim on Hugging Face Spaces using the Docker template. Aim empowers you to explore logs with interactive visualizations, easily compare training runs at scale and be on top of ML development insights.</a></p><div class="c-hnRRWM"></div></div></div></li><li class="c-PJLV"><div class="c-dsvWej"><div class="c-cmpvrW"><a href="/blog/integrations/langchain-aim-building-and-debugging-ai-systems-made-easy"><img alt="LangChain + Aim: Building and Debugging AI Systems Made EASY!" loading="lazy" width="300" height="220" decoding="async" data-nimg="1" class="c-crdKmK" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 300 220&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mO8bQwAAe8BEAdB0H4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)" srcSet="/images/dynamic/langchain_header.jpg 1x, /images/dynamic/langchain_header.jpg 2x" src="/images/dynamic/langchain_header.jpg"/></a></div><div class="c-jpbidf"><div class="c-jFppbe"><a href="/blog/integrations"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Integrations</p></a></div><h3 class="c-PJLV c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true title"><a rel="canonical" href="/blog/integrations/langchain-aim-building-and-debugging-ai-systems-made-easy">LangChain + Aim: Building and Debugging AI Systems Made EASY!</a></h3><p class="c-PJLV c-PJLV-cBLpOj-size-2 c-PJLV-cllNQK-lineClamp-true c-PJLV-icsdZpM-css title"><a href="/blog/integrations/langchain-aim-building-and-debugging-ai-systems-made-easy">As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial. Use Aim to easily trace complex AI systems.</a></p><div class="c-hnRRWM"></div></div></div></li></ul></div></div><footer class="c-ghrOTq"><img alt="footer" fetchpriority="high" width="1440" height="246" decoding="async" data-nimg="1" class="c-kBJMsw next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1440 246&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/bg.eb38a857.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/bg.eb38a857.svg"/><div class="c-fcTZTi"><div class="c-hCkBfr c-gvTmKt"><div class="c-FsNLu"><div class="c-jgJYki"><a class="logo" href="/"><picture><source height="26" width="109" media="(max-width: 1199px)" srcSet="[object Object]"/><img alt="Aimstack" loading="lazy" width="26" height="26" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 26 26&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/aim-logo.e9fed5b2.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/aim-logo.e9fed5b2.svg"/></picture></a></div><ul class="c-bFmrFE"><li><a target="_self" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" href="https://aimstack.notion.site/Working-at-AimStack-7f5d93e04f0645129dd314d5f077511b"><span class="text">Career</span><span class="c-iCFsHS">Hiring</span></a></li></ul><ul class="c-MNZuo"><li><a href="https://community.aimstack.io/" rel="noopener noreferrer" target="_blank" aria-label="Discord"><img alt="Discord" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/discord.c20737ca.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/discord.c20737ca.svg"/></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="X"><img alt="X" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/twitterx.ecd550d9.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/twitterx.ecd550d9.svg"/></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="LinkedIn"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/linkedin.caa3c4fb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/linkedin.caa3c4fb.svg"/></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="FaceBook"><img alt="FaceBook" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="undefined next-exported-image-blur-svg" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 24 24&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;/_next/static/media/facebook.af8656bb.svg&#x27;/%3E%3C/svg%3E&quot;)" src="/_next/static/media/facebook.af8656bb.svg"/></a></li></ul></div><div class="c-cChYXC"><p class="c-PJLV c-PJLV-EDrvg-size-1">Copyright © <!-- -->2024<!-- --> Aimstack</p></div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"posts":[{"title":"AI Observability with AimOS: A Deep Dive into the LangChain Debugger","date":"2023-11-24T20:58:19.490Z","author":"Hovhannes Tamoyan","description":"Explore step-by-step guide for LangChain Debugger, a tool designed for in-depth tracking and visualization of LangChain scripts. The guide features a hands-on example emphasizing its abilities to log LLMs prompts, tool inputs/outputs, and chain metadata, providing a clear understanding of its functionalities.","slug":"ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger","image":"/images/dynamic/langchain.png","draft":false,"categories":["Integrations"],"body":{"raw":"Imagine being able to track and visualize every part of your LangChain script in detail. That's what LangChain Debugger does, working smoothly with AimOS. \n\nWe'll look at a sample script to see how well LangChain Debugger and the scripts work together. It will highlight the main features of LangChain Debugger which logs LLMs prompts and generations, tools inputs/outputs, and chains metadata.\n\nIn this guide, we will prompt \"What are the number of parameters in OpenAI’s GPT5 (based on rumors) and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\n\n\u003e [AimOS 🔍 ](https://github.com/aimhubio/aimos)— A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\n\n## Executing LangChain Debugger: A Step-by-Step Guide\n\nLet's do a practical example using a script to interact with LangChain to show how well it works with AimOS and LangChain Debugger.\n\n### Setting Stage\n\nBefore diving into the script, ensure that LangChain Debugger is installed and properly configured. If not, head over to the AimOS GitHub page for installation instructions: https://github.com/aimhubio/aimos.\n\n### Sample Script\n\n### 1. Importing Required Modules\n\nWe begin by importing the necessary modules for LangChain and AimOS interaction.\n\n```\nfrom aimstack.langchain_debugger.callback_handlers import GenericCallbackHandler\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n```\n\n### 2. Configuring Callbacks and LangChain LLM\n\nEstablish a callback mechanism using LangChain Debugger's **`GenericCallbackHandler`** and set up the LangChain LLM (Large Language Model) using OpenAI.\n\n```\naim_callback = GenericCallbackHandler(repo_path=\"aim://0.0.0.0:53800\")\ncallbacks = [aim_callback]\nllm = OpenAI(temperature=0, callbacks=callbacks)\n```\n\nEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server effortlessly with the following command:\n\n```\naimos server\n```\n\n### 3. Loading Tools and Initializing Agent\n\nLoad the required tools for your LangChain script, initialize the agent, and set the agent type as **`ZERO_SHOT_REACT_DESCRIPTION`**.\n\n```\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callbacks=callbacks)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    callbacks=callbacks,\n)\n```\n\n### 4. Running LangChain Script\n\nExecute your LangChain script within the agent using the **`run`** method.\n\n```\nagent.run(\n    \"What are the number of parameters in GPT5 and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\"\n)\n```\n\n### 5. Flushing Callbacks\n\nEnsure that callbacks are flushed after running the script to capture the trace.\n\n```\naim_callback.flush()\n```\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the **`.aim`** repository is initialized and simply execute:\n\n```\naimos ui\n```\n\n😊 Enjoy this preview of what you'll see once you visit the provided URL:\n\n![AimOS Apps](/images/dynamic/navigating-depths-debugger-1-1-.png \"AimOS Apps\")\n\n## Interpreting the results\n\nHead over to the AimOS Traces page to witness the detailed trace of your LangChain script. This page provides a comprehensive overview of every action and interaction within your LangChain environment.\n\nHere you'll find a new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show the details about the script’s execution, token use, and the costs involved.\n\n![LangChain Debugger](/images/dynamic/screenshot-2023-11-23-at-11.57.51 pm-1-.png \"LangChain Debugger\")\n\n## Overview: Key Elements\n\nNavigate through the Overview tab to grasp essential information:\n\n* Trace: A unique identifier for each trace, allowing you to easily reference and analyze specific interactions. \n\n* Total Steps: The number of actions or queries performed in a trace, providing a quick measure of complexity\n\n* Total Tokens: The total number of tokens is represented to understand the computational load of your queries, ensuring optimal performance.\n\n* Cost: This represents the total cost of the trace which is a critical factor in resource management, the cost breakdown helps you make informed decisions.\n\n![Overview tab](/images/dynamic/screenshot-2023-11-23-at-11.58.15 pm-1-.png \"Overview tab\")\n\n## Steps: Exploring the Process\n\nThe Steps tab provides a detailed walkthrough of the sequence of actions undertaken throughout the pipeline to achieve the specified goal. In our example, the Agent executed a total of 18 steps to successfully accomplish the task.\n\nHere's a breakdown of the pivotal actions:\n\n1. LLM Response: The Language Model (LLM) initially determines that, for the given question, it needs to utilize the Search tool to identify the person in question.\n2. Search Tool Execution: The LLM activates the Search tool to acquire the necessary information, successfully obtaining the desired output.\n3. LLM Decision: Subsequently, the LLM decides to employ the Search tool once again to retrieve additional details, such as the the number of parameters in GPT5 (based on rumors) and GPT4.\n4. Search Tool Execution: The Agent, following the LLM's decision, triggers the Search tool once more, fetching the required information.\n5. Calculator Tool Usage: With the obtained number, the Agent employs the Calculator tool to calculate the logarithm (base e) of the difference between the number of parameters.\n6. Final Answer: Lastly, armed with all the necessary information, the LLM crafts the complete answer to the original question and returns it.\n\n![Steps tab](/images/dynamic/screenshot-2023-11-23-at-11.58.42 pm-1-.png \"Steps tab\")\n\n![Steps tab](/images/dynamic/screenshot-2023-11-24-at-12.00.36 am-1-.png \"Steps tab\")\n\n## Cost: Evaluating Resource Usage\n\nIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LangChain activities.\n\n* On the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\n* On the y-axis, you can see the number of tokens sent or received.\n\n![Cost tab](/images/dynamic/screenshot-2023-11-24-at-12.00.17 am-1-.png \"Cost tab\")\n\n## Wrapping up\n\nThe LangChain Debugger in AimOS is a powerful tool for deepening your understanding and gaining in-depth insights into your LangChain script. It logs LLMs prompts and generations, tools inputs/outputs, and chains metadata. It's great for both experienced users and beginners, offering a comprehensive observability.\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the [AimStack blog.](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\n## Learn more\n\n[AimOS is on a mission to democratize AI Systems logging tools. ](https://aimos.readthedocs.io/en/latest/apps/overview.html)🙌\n\nTry out [AimOS](https://github.com/aimhubio/aimos), join the [Aim community](https://community.aimstack.io/), and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aimos/) if you think AimOS is useful, and here is the repository of [Aim](https://github.com/aimhubio/aim), an easy-to-use \u0026 supercharged open-source experiment tracker.⭐️","html":"\u003cp\u003eImagine being able to track and visualize every part of your LangChain script in detail. That's what LangChain Debugger does, working smoothly with AimOS.\u003c/p\u003e\n\u003cp\u003eWe'll look at a sample script to see how well LangChain Debugger and the scripts work together. It will highlight the main features of LangChain Debugger which logs LLMs prompts and generations, tools inputs/outputs, and chains metadata.\u003c/p\u003e\n\u003cp\u003eIn this guide, we will prompt \"What are the number of parameters in OpenAI’s GPT5 (based on rumors) and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS 🔍 \u003c/a\u003e— A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eExecuting LangChain Debugger: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eLet's do a practical example using a script to interact with LangChain to show how well it works with AimOS and LangChain Debugger.\u003c/p\u003e\n\u003ch3\u003eSetting Stage\u003c/h3\u003e\n\u003cp\u003eBefore diving into the script, ensure that LangChain Debugger is installed and properly configured. If not, head over to the AimOS GitHub page for installation instructions: https://github.com/aimhubio/aimos.\u003c/p\u003e\n\u003ch3\u003eSample Script\u003c/h3\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eWe begin by importing the necessary modules for LangChain and AimOS interaction.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aimstack.langchain_debugger.callback_handlers import GenericCallbackHandler\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Configuring Callbacks and LangChain LLM\u003c/h3\u003e\n\u003cp\u003eEstablish a callback mechanism using LangChain Debugger's \u003cstrong\u003e\u003ccode\u003eGenericCallbackHandler\u003c/code\u003e\u003c/strong\u003e and set up the LangChain LLM (Large Language Model) using OpenAI.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = GenericCallbackHandler(repo_path=\"aim://0.0.0.0:53800\")\ncallbacks = [aim_callback]\nllm = OpenAI(temperature=0, callbacks=callbacks)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server effortlessly with the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Loading Tools and Initializing Agent\u003c/h3\u003e\n\u003cp\u003eLoad the required tools for your LangChain script, initialize the agent, and set the agent type as \u003cstrong\u003e\u003ccode\u003eZERO_SHOT_REACT_DESCRIPTION\u003c/code\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callbacks=callbacks)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    callbacks=callbacks,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4. Running LangChain Script\u003c/h3\u003e\n\u003cp\u003eExecute your LangChain script within the agent using the \u003cstrong\u003e\u003ccode\u003erun\u003c/code\u003e\u003c/strong\u003e method.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eagent.run(\n    \"What are the number of parameters in GPT5 and GPT4? What is the logarithm (base e) of the difference between the number of parameters?\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Flushing Callbacks\u003c/h3\u003e\n\u003cp\u003eEnsure that callbacks are flushed after running the script to capture the trace.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback.flush()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the \u003cstrong\u003e\u003ccode\u003e.aim\u003c/code\u003e\u003c/strong\u003e repository is initialized and simply execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e😊 Enjoy this preview of what you'll see once you visit the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/navigating-depths-debugger-1-1-.png\" alt=\"AimOS Apps\" title=\"AimOS Apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting the results\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Traces page to witness the detailed trace of your LangChain script. This page provides a comprehensive overview of every action and interaction within your LangChain environment.\u003c/p\u003e\n\u003cp\u003eHere you'll find a new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show the details about the script’s execution, token use, and the costs involved.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-23-at-11.57.51%E2%80%AFpm-1-.png\" alt=\"LangChain Debugger\" title=\"LangChain Debugger\"\u003e\u003c/p\u003e\n\u003ch2\u003eOverview: Key Elements\u003c/h2\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTrace: A unique identifier for each trace, allowing you to easily reference and analyze specific interactions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTotal Steps: The number of actions or queries performed in a trace, providing a quick measure of complexity\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTotal Tokens: The total number of tokens is represented to understand the computational load of your queries, ensuring optimal performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCost: This represents the total cost of the trace which is a critical factor in resource management, the cost breakdown helps you make informed decisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-23-at-11.58.15%E2%80%AFpm-1-.png\" alt=\"Overview tab\" title=\"Overview tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eSteps: Exploring the Process\u003c/h2\u003e\n\u003cp\u003eThe Steps tab provides a detailed walkthrough of the sequence of actions undertaken throughout the pipeline to achieve the specified goal. In our example, the Agent executed a total of 18 steps to successfully accomplish the task.\u003c/p\u003e\n\u003cp\u003eHere's a breakdown of the pivotal actions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLLM Response: The Language Model (LLM) initially determines that, for the given question, it needs to utilize the Search tool to identify the person in question.\u003c/li\u003e\n\u003cli\u003eSearch Tool Execution: The LLM activates the Search tool to acquire the necessary information, successfully obtaining the desired output.\u003c/li\u003e\n\u003cli\u003eLLM Decision: Subsequently, the LLM decides to employ the Search tool once again to retrieve additional details, such as the the number of parameters in GPT5 (based on rumors) and GPT4.\u003c/li\u003e\n\u003cli\u003eSearch Tool Execution: The Agent, following the LLM's decision, triggers the Search tool once more, fetching the required information.\u003c/li\u003e\n\u003cli\u003eCalculator Tool Usage: With the obtained number, the Agent employs the Calculator tool to calculate the logarithm (base e) of the difference between the number of parameters.\u003c/li\u003e\n\u003cli\u003eFinal Answer: Lastly, armed with all the necessary information, the LLM crafts the complete answer to the original question and returns it.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-23-at-11.58.42%E2%80%AFpm-1-.png\" alt=\"Steps tab\" title=\"Steps tab\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-24-at-12.00.36%E2%80%AFam-1-.png\" alt=\"Steps tab\" title=\"Steps tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eCost: Evaluating Resource Usage\u003c/h2\u003e\n\u003cp\u003eIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LangChain activities.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOn the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\u003c/li\u003e\n\u003cli\u003eOn the y-axis, you can see the number of tokens sent or received.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/screenshot-2023-11-24-at-12.00.17%E2%80%AFam-1-.png\" alt=\"Cost tab\" title=\"Cost tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eThe LangChain Debugger in AimOS is a powerful tool for deepening your understanding and gaining in-depth insights into your LangChain script. It logs LLMs prompts and generations, tools inputs/outputs, and chains metadata. It's great for both experienced users and beginners, offering a comprehensive observability.\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools. \u003c/a\u003e🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aimos/\"\u003eGitHub\u003c/a\u003e if you think AimOS is useful, and here is the repository of \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, an easy-to-use \u0026#x26; supercharged open-source experiment tracker.⭐️\u003c/p\u003e"},"_id":"posts/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger.md","_raw":{"sourceFilePath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger.md","sourceFileName":"ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-langchain-debugger"},"type":"Post"},{"title":"AI Observability with AimOS: A Deep Dive into the LlamaIndex Observer App","date":"2023-11-16T18:12:14.363Z","author":"Hovhannes Tamoyan","description":"Discover AI observability with AimOS's LlamaIndex Observer. Essential insights for efficient AI model tracking and analysis. Check out our guide for observability solutions in the world of AI.","slug":"ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app","image":"/images/dynamic/llamaindex.png","draft":false,"categories":["Integrations"],"body":{"raw":"Welcome to the world of seamless tracking and visualization! LlamaIndex Observer, a tool that unites the robust capabilities of LlamaIndex with the blazingly fast [AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems). This integration offers a full-featured platform for effectively monitoring and understanding every detail in your LlamaIndex environment.\n\nIn this guide, we're going to highlight the main aspects of the LlamaIndex Observer application, with a special focus on its Trace page, a key component within the LlamaIndex Observer package in Aim.\n\n# LlamaIndex + AimOS = ❤\n\nImagine being able to see everything that happens in your LlamaIndex system from a high-level perspective. **The LlamaIndex Observer** makes this possible by using AimOS to provide a detailed record of each interaction.\n\n\u003e AimOS 🔍 — A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\n\nWith [AimOS](https://github.com/aimhubio/aimos), you can set up various types of AI monitoring applications, like experiment tracking, production monitoring, and usage analysis.\n\nNow, let’s see how it works with an example!\n\n## Putting LlamaIndex Observer into Action: A Step-by-Step Guide\n\nWe'll now look at a hands-on example. We'll use a Python script to interact with LlamaIndex and observe how LlamaIndex Tracker records each action. This example demonstrates the smooth interaction between AimOS and LlamaIndex.\n\nBefore starting with the script, ensure AimOS is installed and set up. If you need to install it, visit [AimOS repository](https://github.com/aimhubio/aimos) for the guidelines.\n\n## Sample Script\n\n### 1. Importing Required Modules\n\nFirst we’ll start by importing the necessary Python modules for file manipulation, HTTP requests, and working with LlamaIndex and AimOS.\n\n```\nimport os\nfrom pathlib import Path\nimport requests\nfrom aimstack.llamaindex_observer.callback_handlers import GenericCallbackHandler\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.callbacks.base import CallbackManager\n```\n\n### 2. Downloading Paul Graham's Essay\n\nIn this step, the script downloads Paul Graham's essay from a given URL and saves it locally in a specified directory. \n\nIt’s important to ensure that the essay is successfully downloaded before moving on to the next steps. Next, we will be asking questions referring this essay.\n\n```\npaul_graham_essay_url = \"\u003chttps://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\u003e\"\nresponse = requests.get(paul_graham_essay_url)\noutput_dir = Path(\"aimos/examples/paul_graham_essay/data/\")\n\nif response.status_code == 200:\n    os.system(f\"mkdir -p {output_dir}\")\n    with open(output_dir.joinpath(\"Paul_Graham_Essay.txt\"), \"wb\") as file:\n        file.write(response.content)\nelse:\n    print(\"Failed to download the file.\")\n```\n\n### 3. Loading Document into LlamaIndex\n\nThe script reads the downloaded document using LlamaIndex's **`SimpleDirectoryReader`** and loads it into the system as a document.\n\n```\ndocuments = SimpleDirectoryReader(output_dir).load_data()\n```\n\n### 4. Setting Up AimOS Callback for LlamaIndex Observer\n\n```\naim_callback = GenericCallbackHandler(repo=\"aim://0.0.0.0:53800\")\ncallback_manager = CallbackManager([aim_callback])\n```\n\nEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server with this simple command command:\n\n```\naimos server\n```\n\n### 5. Creating LlamaIndex Service Context\n\nHere, we configure the LlamaIndex service context, create a vector store index using the loaded documents, and obtain a query engine for interacting with the index.\n\n```\nservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\nquery_engine = index.as_query_engine()\n```\n\n### 6. Performing Queries and Flushing Callbacks\n\nThis section triggers queries on the LlamaIndex using the query engine and ensures that the callbacks are flushed, capturing the corresponding traces.\n\n```\naim_callback.flush()\nquery_engine.query(\"How does Graham address the topic of competition and the importance (or lack thereof) of being the first mover in a market?\")\naim_callback.flush()\nquery_engine.query(\"What are Paul Graham's notable projects or companies?\")\naim_callback.flush()\nquery_engine.query(\"What problems did the author encounter with the early AI programs?\")\naim_callback.flush()\n```\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the .aim repository is initialized and simply run.\n\n```\naimos ui\n```\n\n😊 Enjoy this preview of what you'll see once you visit the provided URL:\n\n![AimOS Overview](/images/dynamic/1-.png \"AimOS Overview\")\n\nNavigate to the **`llamaindex_observer`** package under AI Systems Debugging.\n\nNow, we will delve into the tracked information.\n\n## Interpreting the Results\n\nOnce you’ve run the script, go to the Traces page in AimOS. There, you'll find a  new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show you details about the script’s execution, token use, and the costs involved.\n\n![AimOS Traces](/images/dynamic/llamaindex-insights-2-.png \"AimOS Traces\")\n\n### Overview Tab: A Quick Summary\n\nThe Overview tab in AimOS gives you a snapshot of what happened. Here are the main things you can see:\n\n**Trace ID:** A unique identifier for each trace, allowing you to easily reference and analyze specific interactions.\n\n**Date and Time:** Timestamps help you understand when actions took place, facilitating a chronological understanding of your LlamaIndex usage.\n\n**Total Steps:** The number of actions or queries performed in a trace, providing a quick measure of complexity.\n\n**Total Tokens:** Understand the computational load of your queries, ensuring optimal performance.\n\n**Cost:** A critical factor in resource management, the cost breakdown helps you make informed decisions.\n\n![Trace ID](/images/dynamic/llamaindex-insights-3.png \"Trace ID\")\n\n## Understanding the Process\n\nAt the top of the Steps tab, you'll find a slider labeled \"Select the step,\" enabling seamless navigation through the LlamaIndex processes. These steps are logged using the 'flush()' command.\n\n### Step 1: Chunking and Embedding\n\nIn the initial step, the provided essay document undergoes a chunking process, dividing it into multiple segments. Simultaneously, embeddings are extracted for each chunk, forming a foundational representation of the document's content.\n\n![LlamaIndex Observer insights](/images/dynamic/llamaindex-insights-4.png \"LlamaIndex Observer insights\")\n\n### Step 2: Query Processing\n\n**Question Formulation:** A specific question is asked about the document, such as \"How does Graham address the topic of competition and the importance of being the first mover in a market?\"\n\n![LlamaIndex Observer question formulation](/images/dynamic/llamaindex-insights-5.png \"LlamaIndex Observer question formulation\")\n\n**Query Initiation:** LlamaIndex initiates a query, retrieving the most relevant chunks from the previously embedded document.\n\n![LlamaIndex Observer query initiation](/images/dynamic/6.png \"LlamaIndex Observer query initiation\")\n\n**Prompt Generation:** A prompt for the language model is crafted. This involves combining the context strings (document chunks) and the question to create an input for the language model.\n\n![LlamaIndex Observer prompt generation](/images/dynamic/7.png \"LlamaIndex Observer prompt generation\")\n\n**Language Model Response:** The language model processes the prompt, generating a response that addresses the asked question.\n\n![LlamaIndex Observer - Language model response](/images/dynamic/8.png \"LlamaIndex Observer - Language model response\")\n\n### Step 3: Iterative Questioning\n\nWhile Step 1 is a one-time occurrence, our example involves asking three distinct questions. The process for generating responses to the second and third questions aligns with the methodology outlined in Step 2, utilizing the previously embedded chunks of the document.\n\nThe additional questions asked are:\n\n* \"What are Paul Graham's notable projects or companies?\" \n\n![Prompt](/images/dynamic/9.png \"Prompt\")\n\nGenerated response:\n\n![Generated response](/images/dynamic/10.png \"Generated response\")\n\n* \"What problems did the author encounter with the early AI programs?\"\n\n![Prompt](/images/dynamic/11.png \"Prompt\")\n\nGenerated response:\n\n![Generated response](/images/dynamic/12.png)\n\n## Cost: Evaluating Resource Usage\n\nIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LlamaIndex activities.\n\n* On the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\n* On the y-axis, you can see the number of tokens sent or received.\n\n![Token-usage](/images/dynamic/13.png \"Cost tab\")\n\n## Wrapping up\n\nThe “LlamaIndex Observer” in AimOS is a powerful tool for deepening your understanding of how your LlamaIndex models operate. It's great for both experienced users and beginners, offering a comprehensive observability of model performance.\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the [AimStack blog](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems). \n\n## Learn more\n\n[AimOS is on a mission to democratize AI Systems logging tools. ](https://aimos.readthedocs.io/en/latest/apps/overview.html)🙌\n\nTry out **[AimOS](https://github.com/aimhubio/aimos)**, join the **[Aim community](https://community.aimstack.io/),** and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aimos/tree/main) if you think AimOS is useful, and here is the repository of [Aim](https://github.com/aimhubio/aim), an easy-to-use \u0026 supercharged open-source experiment tracker.⭐️","html":"\u003cp\u003eWelcome to the world of seamless tracking and visualization! LlamaIndex Observer, a tool that unites the robust capabilities of LlamaIndex with the blazingly fast \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e. This integration offers a full-featured platform for effectively monitoring and understanding every detail in your LlamaIndex environment.\u003c/p\u003e\n\u003cp\u003eIn this guide, we're going to highlight the main aspects of the LlamaIndex Observer application, with a special focus on its Trace page, a key component within the LlamaIndex Observer package in Aim.\u003c/p\u003e\n\u003ch1\u003eLlamaIndex + AimOS = ❤\u003c/h1\u003e\n\u003cp\u003eImagine being able to see everything that happens in your LlamaIndex system from a high-level perspective. \u003cstrong\u003eThe LlamaIndex Observer\u003c/strong\u003e makes this possible by using AimOS to provide a detailed record of each interaction.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAimOS 🔍 — A user-friendly observability platform for AI systems. It's adaptable, scalable, and versatile.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWith \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, you can set up various types of AI monitoring applications, like experiment tracking, production monitoring, and usage analysis.\u003c/p\u003e\n\u003cp\u003eNow, let’s see how it works with an example!\u003c/p\u003e\n\u003ch2\u003ePutting LlamaIndex Observer into Action: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eWe'll now look at a hands-on example. We'll use a Python script to interact with LlamaIndex and observe how LlamaIndex Tracker records each action. This example demonstrates the smooth interaction between AimOS and LlamaIndex.\u003c/p\u003e\n\u003cp\u003eBefore starting with the script, ensure AimOS is installed and set up. If you need to install it, visit \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS repository\u003c/a\u003e for the guidelines.\u003c/p\u003e\n\u003ch2\u003eSample Script\u003c/h2\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eFirst we’ll start by importing the necessary Python modules for file manipulation, HTTP requests, and working with LlamaIndex and AimOS.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\nfrom pathlib import Path\nimport requests\nfrom aimstack.llamaindex_observer.callback_handlers import GenericCallbackHandler\nfrom llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.callbacks.base import CallbackManager\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Downloading Paul Graham's Essay\u003c/h3\u003e\n\u003cp\u003eIn this step, the script downloads Paul Graham's essay from a given URL and saves it locally in a specified directory.\u003c/p\u003e\n\u003cp\u003eIt’s important to ensure that the essay is successfully downloaded before moving on to the next steps. Next, we will be asking questions referring this essay.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epaul_graham_essay_url = \"\u0026#x3C;https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\u003e\"\nresponse = requests.get(paul_graham_essay_url)\noutput_dir = Path(\"aimos/examples/paul_graham_essay/data/\")\n\nif response.status_code == 200:\n    os.system(f\"mkdir -p {output_dir}\")\n    with open(output_dir.joinpath(\"Paul_Graham_Essay.txt\"), \"wb\") as file:\n        file.write(response.content)\nelse:\n    print(\"Failed to download the file.\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Loading Document into LlamaIndex\u003c/h3\u003e\n\u003cp\u003eThe script reads the downloaded document using LlamaIndex's \u003cstrong\u003e\u003ccode\u003eSimpleDirectoryReader\u003c/code\u003e\u003c/strong\u003e and loads it into the system as a document.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocuments = SimpleDirectoryReader(output_dir).load_data()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4. Setting Up AimOS Callback for LlamaIndex Observer\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = GenericCallbackHandler(repo=\"aim://0.0.0.0:53800\")\ncallback_manager = CallbackManager([aim_callback])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsure that your AimOS server is active on the default 53800 port. In this instance, the server is running on the local machine. You can start an AimOS server with this simple command command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Creating LlamaIndex Service Context\u003c/h3\u003e\n\u003cp\u003eHere, we configure the LlamaIndex service context, create a vector store index using the loaded documents, and obtain a query engine for interacting with the index.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\nquery_engine = index.as_query_engine()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e6. Performing Queries and Flushing Callbacks\u003c/h3\u003e\n\u003cp\u003eThis section triggers queries on the LlamaIndex using the query engine and ensures that the callbacks are flushed, capturing the corresponding traces.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback.flush()\nquery_engine.query(\"How does Graham address the topic of competition and the importance (or lack thereof) of being the first mover in a market?\")\naim_callback.flush()\nquery_engine.query(\"What are Paul Graham's notable projects or companies?\")\naim_callback.flush()\nquery_engine.query(\"What problems did the author encounter with the early AI programs?\")\naim_callback.flush()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, let's run the AimOS UI to observe the tracked metadata. To do this, navigate to the folder where the .aim repository is initialized and simply run.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e😊 Enjoy this preview of what you'll see once you visit the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1-.png\" alt=\"AimOS Overview\" title=\"AimOS Overview\"\u003e\u003c/p\u003e\n\u003cp\u003eNavigate to the \u003cstrong\u003e\u003ccode\u003ellamaindex_observer\u003c/code\u003e\u003c/strong\u003e package under AI Systems Debugging.\u003c/p\u003e\n\u003cp\u003eNow, we will delve into the tracked information.\u003c/p\u003e\n\u003ch2\u003eInterpreting the Results\u003c/h2\u003e\n\u003cp\u003eOnce you’ve run the script, go to the Traces page in AimOS. There, you'll find a  new trace corresponding to each query in the script. You can get valuable information by checking the Overview, Steps, and Cost tabs. These will show you details about the script’s execution, token use, and the costs involved.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-2-.png\" alt=\"AimOS Traces\" title=\"AimOS Traces\"\u003e\u003c/p\u003e\n\u003ch3\u003eOverview Tab: A Quick Summary\u003c/h3\u003e\n\u003cp\u003eThe Overview tab in AimOS gives you a snapshot of what happened. Here are the main things you can see:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrace ID:\u003c/strong\u003e A unique identifier for each trace, allowing you to easily reference and analyze specific interactions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDate and Time:\u003c/strong\u003e Timestamps help you understand when actions took place, facilitating a chronological understanding of your LlamaIndex usage.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTotal Steps:\u003c/strong\u003e The number of actions or queries performed in a trace, providing a quick measure of complexity.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTotal Tokens:\u003c/strong\u003e Understand the computational load of your queries, ensuring optimal performance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCost:\u003c/strong\u003e A critical factor in resource management, the cost breakdown helps you make informed decisions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-3.png\" alt=\"Trace ID\" title=\"Trace ID\"\u003e\u003c/p\u003e\n\u003ch2\u003eUnderstanding the Process\u003c/h2\u003e\n\u003cp\u003eAt the top of the Steps tab, you'll find a slider labeled \"Select the step,\" enabling seamless navigation through the LlamaIndex processes. These steps are logged using the 'flush()' command.\u003c/p\u003e\n\u003ch3\u003eStep 1: Chunking and Embedding\u003c/h3\u003e\n\u003cp\u003eIn the initial step, the provided essay document undergoes a chunking process, dividing it into multiple segments. Simultaneously, embeddings are extracted for each chunk, forming a foundational representation of the document's content.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-4.png\" alt=\"LlamaIndex Observer insights\" title=\"LlamaIndex Observer insights\"\u003e\u003c/p\u003e\n\u003ch3\u003eStep 2: Query Processing\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eQuestion Formulation:\u003c/strong\u003e A specific question is asked about the document, such as \"How does Graham address the topic of competition and the importance of being the first mover in a market?\"\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/llamaindex-insights-5.png\" alt=\"LlamaIndex Observer question formulation\" title=\"LlamaIndex Observer question formulation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eQuery Initiation:\u003c/strong\u003e LlamaIndex initiates a query, retrieving the most relevant chunks from the previously embedded document.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/6.png\" alt=\"LlamaIndex Observer query initiation\" title=\"LlamaIndex Observer query initiation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt Generation:\u003c/strong\u003e A prompt for the language model is crafted. This involves combining the context strings (document chunks) and the question to create an input for the language model.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/7.png\" alt=\"LlamaIndex Observer prompt generation\" title=\"LlamaIndex Observer prompt generation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Model Response:\u003c/strong\u003e The language model processes the prompt, generating a response that addresses the asked question.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/8.png\" alt=\"LlamaIndex Observer - Language model response\" title=\"LlamaIndex Observer - Language model response\"\u003e\u003c/p\u003e\n\u003ch3\u003eStep 3: Iterative Questioning\u003c/h3\u003e\n\u003cp\u003eWhile Step 1 is a one-time occurrence, our example involves asking three distinct questions. The process for generating responses to the second and third questions aligns with the methodology outlined in Step 2, utilizing the previously embedded chunks of the document.\u003c/p\u003e\n\u003cp\u003eThe additional questions asked are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"What are Paul Graham's notable projects or companies?\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/9.png\" alt=\"Prompt\" title=\"Prompt\"\u003e\u003c/p\u003e\n\u003cp\u003eGenerated response:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/10.png\" alt=\"Generated response\" title=\"Generated response\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"What problems did the author encounter with the early AI programs?\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/11.png\" alt=\"Prompt\" title=\"Prompt\"\u003e\u003c/p\u003e\n\u003cp\u003eGenerated response:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/12.png\" alt=\"Generated response\"\u003e\u003c/p\u003e\n\u003ch2\u003eCost: Evaluating Resource Usage\u003c/h2\u003e\n\u003cp\u003eIn the Cost tab, you can examine three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated with your LlamaIndex activities.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOn the x-axis, you can see the steps, which represent the number of times the system made an API request to an LLM (Language Model).\u003c/li\u003e\n\u003cli\u003eOn the y-axis, you can see the number of tokens sent or received.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/13.png\" alt=\"Token-usage\" title=\"Cost tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eThe “LlamaIndex Observer” in AimOS is a powerful tool for deepening your understanding of how your LlamaIndex models operate. It's great for both experienced users and beginners, offering a comprehensive observability of model performance.\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on the \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimStack blog\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools. \u003c/a\u003e🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003cstrong\u003e\u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e\u003c/strong\u003e, join the \u003cstrong\u003e\u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e,\u003c/strong\u003e and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aimos/tree/main\"\u003eGitHub\u003c/a\u003e if you think AimOS is useful, and here is the repository of \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, an easy-to-use \u0026#x26; supercharged open-source experiment tracker.⭐️\u003c/p\u003e"},"_id":"posts/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app.md","_raw":{"sourceFilePath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app.md","sourceFileName":"ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/ai-observability-with-aimos-a-deep-dive-into-the-llamaindex-observer-app"},"type":"Post"},{"title":"Exploring your PyTorch Lightning experiments with AimOS","date":"2024-01-11T13:41:33.464Z","author":"Hovhannes Tamoyan","description":"Explore how AimOS can boost your PyTorch Lightning experiments. This article provides a comprehensive guide with a practical example, emphasizing the integration of PyTorch Lightning into AimOS.","slug":"exploring-your-pytorch-lightning-experiments-with-aimos","image":"/images/dynamic/banner1.png","draft":false,"categories":["Integrations"],"body":{"raw":"Discover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.\n\n## Introduction\n\n\n\n[AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems), through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let's dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.\n\n\u003e AimOS 🔍 — An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.\n\n## Running a Sample Training with PyTorch Lightning: A Step-by-Step Guide\n\n\n\nLet's take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.\n\n\n\n### Setting the Stage\n\n\n\nBefore diving into the script, ensure that AimOS is installed. If not, simply run: \\`pip3 install aimos\\`\n\n### 1. Importing Required Modules\n\n\n\nStart by importing the necessary modules.\n\n```\nimport os\n\nimport lightning.pytorch as pl\nimport torch\nfrom aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger\nfrom torch import nn, optim, utils\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n```\n\n### 2. Preparing the Dataset and the DataLoaders\n\nDownload the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.\n\n```\ndataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)\nval_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)\ntest_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)\n```\n\n### 3. Instantiate the Model\n\nLet's create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.\n\n```\nclass LitModule(pl.LightningModule):\n    def __init__(self, hidden_size=64, lr=5e-3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lr = lr\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n```\n\nTo log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.\n\n### 4. Initialize AimOS Callback\n\n\n\nInitialize the AimOS callback by providing the experiment name.\n\n```\naim_logger = AimLogger(\n    experiment_name=\"example_experiment\"\n)\n```\n\nMake sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.\n\n```\naimos server\n```\n\n### 5. Training\n\nInstantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. 🚀\n\n```\nmodel = LitModule(lr=lr)\naim_logger.experiment.set(\"lr\", lr, strict=False)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=aim_logger)\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\ntrainer.test(dataloaders=test_loader)\n```\n\nLet’s use these learning rates to have more understanding about the learning curve of the task: `[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]`.\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the `.aim `repository is initialized and execute:\n\n```\naimos ui\n```\n\nThis is the view you'll see after following the provided URL:\n\n![AimOS apps](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png \"AimOS apps\")\n\n## Interpreting Experiment Results\n\nNow let’s dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment's performance.\n\n\n\n## AimOS Overview Page\n\nHead over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\n\n![Overview](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png \"Overview\")\n\n## AimOS Runs Page\n\nVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\n\n![Runs page](/images/dynamic/dec-21-screenshot-from-aimos.png \"Runs page\")\n\n## Run Overview\n\nBy navigating to each run’s individual page, you'll have a detailed view of the activities within that run.\n\nNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\n\n![Run page](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png \"Run page\")\n\n## Params Tab\n\nFor a more detailed view of all parameters, navigate to the Params tab.\n\n![Params tab](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png \"Params tan\")\n\n## Metrics Tab\n\nIn the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let's apply color grouping to the runs based on their context.\n\n![Metrics tab](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png \"Metrics tab\")\n\nIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\n\n## Comparing all Runs\n\n\n\nLet's use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the `loss` and `acc `metrics, apply the stroke by `metric.context.subset `, and use a log-scale on the y-axis.\n\n![Metrics explorer](/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png \"Metrics explorer\")\n\n## Wrapping up\n\nAimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!\n\n\n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on [AimStack blog.](https://aimstack.io/blog)\n\n## Learn more\n\n\n\n[AimOS is on a mission to democratize AI Systems logging tools](https://aimos.readthedocs.io/en/latest/apps/overview.html). 🙌\n\nTry out [AimOS,](https://github.com/aimhubio/aimos) join the [Aim community](https://community.aimstack.io/) and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon’t forget to leave us a star on GitHub if you think [AimOS](https://github.com/aimhubio/aimos) is useful, and here is the repository of [Aim](https://github.com/aimhubio/aim), an easy-to-use \u0026 supercharged open-source experiment tracker.⭐️","html":"\u003cp\u003eDiscover seamless tracking and exploration of your PyTorch Lightning experiments with AimOS callback handler. Store all the metadata of your experiments and track the corresponding metrics. This article provides a straightforward walkthrough of how it works with a practical example, focusing on integrating PyTorch Lightning into AimOS.\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e, through its seamless integration, meticulously tracks all your hyperparameters and metrics for PyTorch Lightning experiments. To see this work, let's dive into an example script to witness the synergy between AimOS and your PyTorch Lightning experiments.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAimOS 🔍 — An easy-to-use modular observability for AI Systems. Extensible, scalable and modular.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eRunning a Sample Training with PyTorch Lightning: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eLet's take a look at a practical example using a script to train a simple NN for image classification on MNIST dataset.\u003c/p\u003e\n\u003ch3\u003eSetting the Stage\u003c/h3\u003e\n\u003cp\u003eBefore diving into the script, ensure that AimOS is installed. If not, simply run: `pip3 install aimos`\u003c/p\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eStart by importing the necessary modules.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\n\nimport lightning.pytorch as pl\nimport torch\nfrom aimstack.experiment_tracker.pytorch_lightning import Logger as AimLogger\nfrom torch import nn, optim, utils\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Preparing the Dataset and the DataLoaders\u003c/h3\u003e\n\u003cp\u003eDownload the training and test subsets of the MNIST dataset and load it, after which do random split of the training set dividing it into training and the validation sets. Afterwards, simply define the DataLoaders with large batch sizes and multiple workers to speed up the process.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset, num_workers=2, batch_size=256)\nval_loader = utils.data.DataLoader(val_dataset, num_workers=2, batch_size=256)\ntest_loader = utils.data.DataLoader(test_dataset, num_workers=2, batch_size=256)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Instantiate the Model\u003c/h3\u003e\n\u003cp\u003eLet's create a PyTorch Lightning module that will contain the model and the logic for performing forward passes through the model during training, validation, and testing. We will also configure the optimizer within the module.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclass LitModule(pl.LightningModule):\n    def __init__(self, hidden_size=64, lr=5e-3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lr = lr\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo log metrics and the loss, we will use the self.log method provided by the PyTorch Lightning module. Simply provide the name of the metric and the corresponding value to track. For validation and test steps we will also calculate the multi-class accuracy and track it in their respective contexts.\u003c/p\u003e\n\u003ch3\u003e4. Initialize AimOS Callback\u003c/h3\u003e\n\u003cp\u003eInitialize the AimOS callback by providing the experiment name.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_logger = AimLogger(\n    experiment_name=\"example_experiment\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake sure that your AimOS server is active and running. If it is not, use the following command to start it up. By default, it should be on port 53800.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Training\u003c/h3\u003e\n\u003cp\u003eInstantiate the module we created recently, set the learning rate of the experiment as we will run experiments with different learning rates. After which simply define the trainer and let the training begin. 🚀\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = LitModule(lr=lr)\naim_logger.experiment.set(\"lr\", lr, strict=False)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=aim_logger)\ntrainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\ntrainer.test(dataloaders=test_loader)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s use these learning rates to have more understanding about the learning curve of the task: \u003ccode\u003e[5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6]\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the \u003ccode\u003e.aim \u003c/code\u003erepository is initialized and execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the view you'll see after following the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21.png\" alt=\"AimOS apps\" title=\"AimOS apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting Experiment Results\u003c/h2\u003e\n\u003cp\u003eNow let’s dive into the UI and explore the wealth of information tracked by AimOS to gain insights about your PyTorch Lightning experiment. Navigate through the following sections for a comprehensive understanding of your experiment's performance.\u003c/p\u003e\n\u003ch2\u003eAimOS Overview Page\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Overview page to witness the containers and sequences overview tables of your PyTorch Lightning experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-1-.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eAimOS Runs Page\u003c/h2\u003e\n\u003cp\u003eVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/dec-21-screenshot-from-aimos.png\" alt=\"Runs page\" title=\"Runs page\"\u003e\u003c/p\u003e\n\u003ch2\u003eRun Overview\u003c/h2\u003e\n\u003cp\u003eBy navigating to each run’s individual page, you'll have a detailed view of the activities within that run.\u003c/p\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-2-.png\" alt=\"Run page\" title=\"Run page\"\u003e\u003c/p\u003e\n\u003ch2\u003eParams Tab\u003c/h2\u003e\n\u003cp\u003eFor a more detailed view of all parameters, navigate to the Params tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-3-.png\" alt=\"Params tab\" title=\"Params tan\"\u003e\u003c/p\u003e\n\u003ch2\u003eMetrics Tab\u003c/h2\u003e\n\u003cp\u003eIn the Metrics tab, you can explore the graphs of all the tracked metrics. In this example, we tracked the loss and accuracy metrics. To have a better view, let's apply color grouping to the runs based on their context.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-4-.png\" alt=\"Metrics tab\" title=\"Metrics tab\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\u003c/p\u003e\n\u003ch2\u003eComparing all Runs\u003c/h2\u003e\n\u003cp\u003eLet's use the Metrics explorer to observe the relationship between accuracy and loss based on the learning rate. To do this, select the \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003eacc \u003c/code\u003emetrics, apply the stroke by \u003ccode\u003emetric.context.subset \u003c/code\u003e, and use a log-scale on the y-axis.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/pytorch-lightning-aimos-screenshot-dec-21-5-1-.png\" alt=\"Metrics explorer\" title=\"Metrics explorer\"\u003e\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eAimOS provides a comprehensive suite of tools to analyze and understand your PyTorch Lightning experiment thoroughly. By leveraging these insights, you can streamline your experimentation process and make data-driven decisions. Embrace the power of AimOS to enhance your PyTorch Lightning experiment experience today!\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on \u003ca href=\"https://aimstack.io/blog\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools\u003c/a\u003e. 🙌\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS,\u003c/a\u003e join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on GitHub if you think \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e is useful, and here is the repository of \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, an easy-to-use \u0026#x26; supercharged open-source experiment tracker.⭐️\u003c/p\u003e"},"_id":"posts/exploring-your-pytorch-lightning-experiments-with-aimos.md","_raw":{"sourceFilePath":"posts/exploring-your-pytorch-lightning-experiments-with-aimos.md","sourceFileName":"exploring-your-pytorch-lightning-experiments-with-aimos.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/exploring-your-pytorch-lightning-experiments-with-aimos"},"type":"Post"},{"title":"LangChain + Aim: Building and Debugging AI Systems Made EASY!","date":"2023-04-06T17:51:14.014Z","author":"Gor Arakelyan","description":"As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial. Use Aim to easily trace complex AI systems.","slug":"langchain-aim-building-and-debugging-ai-systems-made-easy","image":"/images/dynamic/langchain_header.jpg","draft":false,"categories":["Integrations"],"body":{"raw":"\n\n# The Rise of Complex AI Systems\n\nWith the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.\n\nBut the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.\n\n**As AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.** Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.\n\nIn this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:\n\n* track all inputs and outputs of chains,\n* visualize and explore individual chains,\n* compare several chains side-by-side.\n\n# LangChain: Building AI Systems with LLMs\n\nLangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.\n\nIt provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!\n\n# **Aim:** Upgraded Debugging Experience for AI Systems\n\nMonitoring and debugging AI systems requires more than just scanning output logs on a terminal.\n\n**Introducing Aim!**\n\nAim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.\n\nImportantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.\n\nIn that way, Aim makes debugging, monitoring, comparing different executions a breeze.\n\n**Experience the ultimate control with Aim!**\n\nCheck out Aim on GitHub: **[github.com/aimhubio/aim](http://github.com/aimhubio/aim)**\n\n![](/images/dynamic/explorer.jpg)\n\n# Aim + LangChain = 🚀\n\nWith the release of LangChain **[v0.0.127](https://github.com/hwchase17/langchain/releases/tag/v0.0.127)**, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! **All you need to do is configure the Aim callback and run your executions as usual.**\n\nAim does the rest for you by tracking tools and LLMs’ inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.\n\n![](/images/dynamic/langchain_aim.jpg)\n\nLet's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.\n\n# Hands-On Example: Building a Multi-Task AI Agent\n\n## Setting up the agent and the Aim callback\n\nLet’s build an agent equipped with two tools:\n\n* the SerpApi tool to access Google search results,\n* the LLM-math tool to perform required mathematical operations.\n\nIn this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:\n\n```python\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n```\n\nNow that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.\n\n```python\nfrom langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n```\n\n\u003e **Aim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.**\n\nFind the full script and more examples in the official LangChain docs: [](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html)**\u003chttps://python.langchain.com/en/latest/ecosystem/aim_tracking.html\u003e**\n\n## **Executing the agent and running Aim**\n\nBefore executing the agent, ensure that Aim is installed by executing the following command:\n\n```shell\npip install aim\n```\n\nNow, let's run multiple executions and launch the Aim UI to visualize and explore the results:\n\n1. execute the script by running `python example.py`,\n2. then, start the UI with `aim up` command.\n\nWith the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.\n\n## Exploring executions via Aim\n\n### Home page\n\nOn the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.\n\n![](/images/dynamic/home.jpg)\n\n\n\n### Deep dive into a single execution\n\nWhen navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:\n\n* CLI command and arguments,\n* Environment variables,\n* Packages,\n* Git information,\n* System resource usage,\n* and other relevant information about an individual execution.\n\n![](/images/dynamic/individual_exec.jpg)\n\nAim automatically captures terminal outputs during execution. Access these logs in the “Logs” tab to easily keep track of the progress of your AI system and identify issues.\n\n![](/images/dynamic/logs.jpg)\n\nIn the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.\n\n![](/images/dynamic/text_tab.jpg)\n\nWith Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.\n\n![](/images/dynamic/explorer.jpg)\n\nFor instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.\n\n**With Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.**\n\n# Wrapping Up\n\nIn conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!\n\n# Learn more\n\nCheck out the Aim + LangChain integration docs [here](https://python.langchain.com/en/latest/ecosystem/aim_tracking.html).\n\nLangChain repo: [](https://github.com/hwchase17/langchain)\u003chttps://github.com/hwchase17/langchain\u003e\n\nAim repo: [](https://github.com/aimhubio/aim)\u003chttps://github.com/aimhubio/aim\u003e\n\nIf you have questions, join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You’re most welcome! 🙌\n\nDrop a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003ch1\u003eThe Rise of Complex AI Systems\u003c/h1\u003e\n\u003cp\u003eWith the introduction of ChatGPT and large language models (LLMs) such as GPT3.5-turbo and GPT4, AI progress has skyrocketed. These models have enabled tons of AI-based applications, bringing the power of LLMs to real-world use cases.\u003c/p\u003e\n\u003cp\u003eBut the true power of AI comes when we combine LLMs with other tools, scripts, and sources of computation to create much more powerful AI systems than standalone models.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAs AI systems get increasingly complex, the ability to effectively debug and monitor them becomes crucial.\u003c/strong\u003e Without comprehensive tracing and debugging, the improvement, monitoring and understanding of these systems become extremely challenging.\u003c/p\u003e\n\u003cp\u003eIn this article, we will take a look at how to use Aim to easily trace complex AI systems built with LangChain. Specifically, we will go over how to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etrack all inputs and outputs of chains,\u003c/li\u003e\n\u003cli\u003evisualize and explore individual chains,\u003c/li\u003e\n\u003cli\u003ecompare several chains side-by-side.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eLangChain: Building AI Systems with LLMs\u003c/h1\u003e\n\u003cp\u003eLangChain is a library designed to enable the development of powerful applications by integrating LLMs with other computational resources or knowledge sources. It streamlines the process of creating applications such as question answering systems, chatbots, and intelligent agents.\u003c/p\u003e\n\u003cp\u003eIt provides a unified interface for managing and optimizing prompts, creating sequences of calls to LLMs or other utilities (chains), interacting with external data sources, making decisions and taking actions. LangChain empowers developers to build sophisticated, cutting-edge applications by making the most of LLMs and easily connecting them with other tools!\u003c/p\u003e\n\u003ch1\u003e\u003cstrong\u003eAim:\u003c/strong\u003e Upgraded Debugging Experience for AI Systems\u003c/h1\u003e\n\u003cp\u003eMonitoring and debugging AI systems requires more than just scanning output logs on a terminal.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroducing Aim!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim is an open-source AI metadata library that tracks all aspects of your AI system's execution, facilitating in-depth exploration, monitoring, and reproducibility.\u003c/p\u003e\n\u003cp\u003eImportantly, Aim helps to query all the tracked metadata programmatically and is equipped with a powerful UI / observability layer for the AI metadata.\u003c/p\u003e\n\u003cp\u003eIn that way, Aim makes debugging, monitoring, comparing different executions a breeze.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExperience the ultimate control with Aim!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCheck out Aim on GitHub: \u003cstrong\u003e\u003ca href=\"http://github.com/aimhubio/aim\"\u003egithub.com/aimhubio/aim\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/explorer.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eAim + LangChain = 🚀\u003c/h1\u003e\n\u003cp\u003eWith the release of LangChain \u003cstrong\u003e\u003ca href=\"https://github.com/hwchase17/langchain/releases/tag/v0.0.127\"\u003ev0.0.127\u003c/a\u003e\u003c/strong\u003e, it's now possible to trace LangChain agents and chains with Aim using just a few lines of code! \u003cstrong\u003eAll you need to do is configure the Aim callback and run your executions as usual.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAim does the rest for you by tracking tools and LLMs’ inputs and outputs, agents' actions, and chains results. As well as, it tracks CLI command and arguments, system info and resource usage, env variables, git info, and terminal outputs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/langchain_aim.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's move forward and build an agent with LangChain, configure Aim to trace executions, and take a quick journey around the UI to see how Aim can help with debugging and monitoring.\u003c/p\u003e\n\u003ch1\u003eHands-On Example: Building a Multi-Task AI Agent\u003c/h1\u003e\n\u003ch2\u003eSetting up the agent and the Aim callback\u003c/h2\u003e\n\u003cp\u003eLet’s build an agent equipped with two tools:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe SerpApi tool to access Google search results,\u003c/li\u003e\n\u003cli\u003ethe LLM-math tool to perform required mathematical operations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this particular example, we'll prompt the agent to discover who Leonardo DiCaprio's girlfriend is and calculate her current age raised to the 0.43 power:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003etools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callback_manager=manager)\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\",\n    callback_manager=manager,\n    verbose=True,\n)\nagent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that the chain is set up, let's integrate the Aim callback. It takes just a few lines of code and Aim will capture all the moving pieces during the execution.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain.callbacks import AimCallbackHandler\n\naim_callback = AimCallbackHandler(\n    repo=\".\",\n    experiment_name=\"scenario 1: OpenAI LLM\",\n)\n\naim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eAim is entirely open-source and self-hosted, which means your data remains private and isn't shared with third parties.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFind the full script and more examples in the official LangChain docs: \u003ca href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"\u003e\u003c/a\u003e\u003cstrong\u003e\u003ca href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"\u003ehttps://python.langchain.com/en/latest/ecosystem/aim_tracking.html\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eExecuting the agent and running Aim\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eBefore executing the agent, ensure that Aim is installed by executing the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003epip install aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, let's run multiple executions and launch the Aim UI to visualize and explore the results:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eexecute the script by running \u003ccode\u003epython example.py\u003c/code\u003e,\u003c/li\u003e\n\u003cli\u003ethen, start the UI with \u003ccode\u003eaim up\u003c/code\u003e command.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWith the Aim up and running, you can effortlessly dive into the details of each execution, compare results, and gain insights that will help you to debug and iterate over your chains.\u003c/p\u003e\n\u003ch2\u003eExploring executions via Aim\u003c/h2\u003e\n\u003ch3\u003eHome page\u003c/h3\u003e\n\u003cp\u003eOn the home page, you'll find an organized view of all your tracked executions, making it easy to keep track of your progress and recent runs. To navigate to a specific execution, simply click on the link, and you'll be taken to a dedicated page with comprehensive information about that particular execution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/home.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003eDeep dive into a single execution\u003c/h3\u003e\n\u003cp\u003eWhen navigating to an individual execution page, you'll find an overview of system information and execution details. Here you can access:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLI command and arguments,\u003c/li\u003e\n\u003cli\u003eEnvironment variables,\u003c/li\u003e\n\u003cli\u003ePackages,\u003c/li\u003e\n\u003cli\u003eGit information,\u003c/li\u003e\n\u003cli\u003eSystem resource usage,\u003c/li\u003e\n\u003cli\u003eand other relevant information about an individual execution.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/individual_exec.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim automatically captures terminal outputs during execution. Access these logs in the “Logs” tab to easily keep track of the progress of your AI system and identify issues.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/logs.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the \"Text\" tab, you can explore the inner workings of a chain, including agent actions, tools and LLMs inputs and outputs. This in-depth view allows you to review the metadata collected at every step of execution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/text_tab.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWith Aim's Text Explorer, you can effortlessly compare multiple executions, examining their actions, inputs, and outputs side by side. It helps to identify patterns or spot discrepancies.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/explorer.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFor instance, in the given example, two executions produced the response, \"Camila Morrone is Leo DiCaprio's girlfriend, and her current age raised to the 0.43 power is 3.8507291225496925.\" However, another execution returned the answer \"3.991298452658078\". This discrepancy occurred because the first two executions incorrectly identified Camila Morrone's age as 23 instead of 25.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith Text Explorer, you can easily compare and analyze the outcomes of various executions and make decisions to adjust agents and prompts further.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1\u003eWrapping Up\u003c/h1\u003e\n\u003cp\u003eIn conclusion, as AI systems become more complex and powerful, the need for comprehensive tracing and debugging tools becomes increasingly essential. LangChain, when combined with Aim, provides a powerful solution for building and monitoring sophisticated AI applications. By following the practical examples in this blog post, you can effectively monitor and debug your LangChain-based systems!\u003c/p\u003e\n\u003ch1\u003eLearn more\u003c/h1\u003e\n\u003cp\u003eCheck out the Aim + LangChain integration docs \u003ca href=\"https://python.langchain.com/en/latest/ecosystem/aim_tracking.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eLangChain repo: \u003ca href=\"https://github.com/hwchase17/langchain\"\u003e\u003c/a\u003e\u003ca href=\"https://github.com/hwchase17/langchain\"\u003ehttps://github.com/hwchase17/langchain\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAim repo: \u003ca href=\"https://github.com/aimhubio/aim\"\u003e\u003c/a\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003ehttps://github.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have questions, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. You’re most welcome! 🙌\u003c/p\u003e\n\u003cp\u003eDrop a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","_raw":{"sourceFilePath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileName":"langchain-aim-building-and-debugging-ai-systems-made-easy.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/langchain-aim-building-and-debugging-ai-systems-made-easy"},"type":"Post"},{"title":"Launching Aim on Hugging Face Spaces","date":"2023-04-19T13:41:34.213Z","author":"Gor Arakelyan","description":"Deploy Aim on Hugging Face Spaces using the Docker template. Aim empowers you to explore logs with interactive visualizations, easily compare training runs at scale and be on top of ML development insights.","slug":"launching-aim-on-hugging-face-spaces","image":"/images/dynamic/hf_space_header.png","draft":false,"categories":["Integrations"],"body":{"raw":"We are excited to announce the launch of Aim on Hugging Face Spaces! 🚀\n\nWith just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.\n\n![](/images/dynamic/hf_space_me.png)\n\nAim is an open-source, self-hosted AI Metadata tracking tool.\\\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata — perfect for automations and Jupyter Notebook analysis.\n\nIn this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Let’s dive in and get started!\n\nLearn more about Aim on the GitHub repository: **[github.com/aimhubio/aim](https://github.com/aimhubio/aim)**\n\n## Deploy Aim on Hugging Face Spaces within seconds using the Docker template\n\nTo get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the “Create new Space” button, or open the page directly by the following link: [](https://huggingface.co/new-space?template=aimstack/aim)**\u003chttps://huggingface.co/new-space?template=aimstack/aim\u003e**\n\n![](/images/dynamic/hf_space_deploy.png)\n\nSet up your Aim Space in no time:\n\n1. Choose a name for your Space.\n2. Adjust Space hardware and the visibility mode.\n3. Submit your Space!\n\nAfter submitting the Space, you'll be able to monitor its progress through the building status:\n\n![](/images/dynamic/hf_space_building.png)\n\nOnce it transitions to “Running”, your space is ready to go!\n\n![](/images/dynamic/hf_space_running.png)\n\n**Ta-da! 🎉 You're all set to start using Aim on Hugging Face.**\n\nBy pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:\n\n1. Run the following bash command to compress `.aim` directory:\n\n   ```shell\n   tar -czvf aim_repo.tar.gz .aim\n   ```\n2. Commit and push files to your Space.\n\nThat's it! Now open the App section of your Space, and Aim will display your training logs.\n\nUpdating Spaces is incredibly convenient – you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. 🔥\n\n## See Aim in Action with Existing Demos on the Hub\n\nLet’s explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.\n\n* Neural machine translation task: [](https://huggingface.co/spaces/aimstack/nmt)\u003chttps://huggingface.co/spaces/aimstack/nmt\u003e\n* Simple handwritten digits recognition task: [](https://huggingface.co/spaces/aimstack/digit-recognition)\u003chttps://huggingface.co/spaces/aimstack/digit-recognition\u003e\n* Image generation task with lightweight GAN implementation: [](https://huggingface.co/spaces/aimstack/image-generation)\u003chttps://huggingface.co/spaces/aimstack/image-generation\u003e\n\n![](/images/dynamic/hf_space_demos.png)\n\nWhen navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.\n\n![Aim homepage](/images/dynamic/hf_space_overview.png \"Aim homepage\")\n\nOpen the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.\n\n![Individual run page](/images/dynamic/hf_space_individual_run.png \"Individual run page\")\n\nTake your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.\n\nMetrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.\n\n![Metrics Explorer](/images/dynamic/hf_space_me.png \"Metrics Explorer\")\n\nExplorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.\n\n![Pythonic search](/images/dynamic/hf_space_pythonic_search.png \"Pythonic search\")\n\nIn addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.\n\n![Images Explorer](/images/dynamic/hf_space_media.png \"Images Explorer\")\n\nUse Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!\n\n## One more thing… 👀\n\nHaving Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.\n\nTo embed your Space, construct the following link based on Space owner and Space name: **`https://owner-space-name.hf.space`**. This link can be used to embed your Space in any website or notebook using the following HTML code:\n\n```html\n%%html\n\u003ciframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n\u003e\n\u003c/iframe\u003e\n```\n\n## Next steps\n\nWe are going to continuously iterate over Aim Space onboarding and usability, including:\n\n* the ability to read logs directly from Hugging Face Hub model repos,\n* automatic conversion of TensorBoard logs to Aim format,\n* Aim HF Space-specific onboarding steps.\n\nMuch more coming soon... stay tuned for the updates!\n\n## Learn more\n\nCheck out Aim Space documentation [here](https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html)\n\nAim repo on GitHub: [github.com/aimhubio/aim](http://github.com/aimhubio/aim)\n\nIf you have questions, join the [Aim community](https://community.aimstack.io/), share your feedback, open issues for new features and bugs. You’re most welcome! 🙌\n\nDrop a ⭐️ on [GitHub](https://github.com/aimhubio/aim), if you find Aim useful.","html":"\u003cp\u003eWe are excited to announce the launch of Aim on Hugging Face Spaces! 🚀\u003c/p\u003e\n\u003cp\u003eWith just a few clicks, you can now deploy Aim on the Hugging Face Hub and seamlessly share your training results with anyone.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_me.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAim is an open-source, self-hosted AI Metadata tracking tool.\u003cbr\u003e\nIt provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata — perfect for automations and Jupyter Notebook analysis.\u003c/p\u003e\n\u003cp\u003eIn this article, you will learn how to deploy and share your own Aim Space. Also, we will have a quick tour over Aim and learn how it can help to explore and compare your training logs with ease. Let’s dive in and get started!\u003c/p\u003e\n\u003cp\u003eLearn more about Aim on the GitHub repository: \u003cstrong\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003egithub.com/aimhubio/aim\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003eDeploy Aim on Hugging Face Spaces within seconds using the Docker template\u003c/h2\u003e\n\u003cp\u003eTo get started, simply navigate to the Spaces page on the Hugging Face Hub and click on the “Create new Space” button, or open the page directly by the following link: \u003ca href=\"https://huggingface.co/new-space?template=aimstack/aim\"\u003e\u003c/a\u003e\u003cstrong\u003e\u003ca href=\"https://huggingface.co/new-space?template=aimstack/aim\"\u003ehttps://huggingface.co/new-space?template=aimstack/aim\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_deploy.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eSet up your Aim Space in no time:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChoose a name for your Space.\u003c/li\u003e\n\u003cli\u003eAdjust Space hardware and the visibility mode.\u003c/li\u003e\n\u003cli\u003eSubmit your Space!\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAfter submitting the Space, you'll be able to monitor its progress through the building status:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_building.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eOnce it transitions to “Running”, your space is ready to go!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_running.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTa-da! 🎉 You're all set to start using Aim on Hugging Face.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBy pushing your logs to your Space, you can easily explore, compare, and share them with anyone who has access. Here's how to do it in just two simple steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRun the following bash command to compress \u003ccode\u003e.aim\u003c/code\u003e directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003etar -czvf aim_repo.tar.gz .aim\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCommit and push files to your Space.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThat's it! Now open the App section of your Space, and Aim will display your training logs.\u003c/p\u003e\n\u003cp\u003eUpdating Spaces is incredibly convenient – you just need to commit the changes to the repository, and it will automatically re-deploy the application for you. 🔥\u003c/p\u003e\n\u003ch2\u003eSee Aim in Action with Existing Demos on the Hub\u003c/h2\u003e\n\u003cp\u003eLet’s explore live Aim demos already available on the Hub. Each demo highlights a distinct use case and demonstrates the power of Aim in action.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeural machine translation task: \u003ca href=\"https://huggingface.co/spaces/aimstack/nmt\"\u003e\u003c/a\u003e\u003ca href=\"https://huggingface.co/spaces/aimstack/nmt\"\u003ehttps://huggingface.co/spaces/aimstack/nmt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSimple handwritten digits recognition task: \u003ca href=\"https://huggingface.co/spaces/aimstack/digit-recognition\"\u003e\u003c/a\u003e\u003ca href=\"https://huggingface.co/spaces/aimstack/digit-recognition\"\u003ehttps://huggingface.co/spaces/aimstack/digit-recognition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eImage generation task with lightweight GAN implementation: \u003ca href=\"https://huggingface.co/spaces/aimstack/image-generation\"\u003e\u003c/a\u003e\u003ca href=\"https://huggingface.co/spaces/aimstack/image-generation\"\u003ehttps://huggingface.co/spaces/aimstack/image-generation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_demos.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_overview.png\" alt=\"Aim homepage\" title=\"Aim homepage\"\u003e\u003c/p\u003e\n\u003cp\u003eOpen the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_individual_run.png\" alt=\"Individual run page\" title=\"Individual run page\"\u003e\u003c/p\u003e\n\u003cp\u003eTake your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs.\u003c/p\u003e\n\u003cp\u003eMetrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_me.png\" alt=\"Metrics Explorer\" title=\"Metrics Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eExplorers provide fully Python-compatible expressions for search, allowing you to query metadata with ease.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_pythonic_search.png\" alt=\"Pythonic search\" title=\"Pythonic search\"\u003e\u003c/p\u003e\n\u003cp\u003eIn addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/hf_space_media.png\" alt=\"Images Explorer\" title=\"Images Explorer\"\u003e\u003c/p\u003e\n\u003cp\u003eUse Aim Space on Hugging Face to effortlessly upload and share your training results with the community in a few steps. Aim empowers you to explore your logs with interactive visualizations at your fingertips, easily compare training runs at scale and be on top of your ML development insights!\u003c/p\u003e\n\u003ch2\u003eOne more thing… 👀\u003c/h2\u003e\n\u003cp\u003eHaving Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.\u003c/p\u003e\n\u003cp\u003eTo embed your Space, construct the following link based on Space owner and Space name: \u003cstrong\u003e\u003ccode\u003ehttps://owner-space-name.hf.space\u003c/code\u003e\u003c/strong\u003e. This link can be used to embed your Space in any website or notebook using the following HTML code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-html\"\u003e%%html\n\u0026#x3C;iframe\n    src=\"https://owner-space-name.hf.space\"\n    frameborder=\"0\"\n    width=100%\n    height=\"800\"\n\u003e\n\u0026#x3C;/iframe\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eNext steps\u003c/h2\u003e\n\u003cp\u003eWe are going to continuously iterate over Aim Space onboarding and usability, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe ability to read logs directly from Hugging Face Hub model repos,\u003c/li\u003e\n\u003cli\u003eautomatic conversion of TensorBoard logs to Aim format,\u003c/li\u003e\n\u003cli\u003eAim HF Space-specific onboarding steps.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMuch more coming soon... stay tuned for the updates!\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003eCheck out Aim Space documentation \u003ca href=\"https://aimstack.readthedocs.io/en/latest/using/huggingface_spaces.html\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAim repo on GitHub: \u003ca href=\"http://github.com/aimhubio/aim\"\u003egithub.com/aimhubio/aim\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have questions, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features and bugs. You’re most welcome! 🙌\u003c/p\u003e\n\u003cp\u003eDrop a ⭐️ on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eGitHub\u003c/a\u003e, if you find Aim useful.\u003c/p\u003e"},"_id":"posts/launching-aim-on-hugging-face-spaces.md","_raw":{"sourceFilePath":"posts/launching-aim-on-hugging-face-spaces.md","sourceFileName":"launching-aim-on-hugging-face-spaces.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/launching-aim-on-hugging-face-spaces"},"type":"Post"},{"title":"Streamlining your Hugging Face Experiments with AimOS","date":"2023-12-29T13:33:30.989Z","author":"Hovhannes Tamoyan","description":"AimCallback for Hugging Face is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy.","slug":"streamlining-your-hugging-face-experiments-with-aimos","image":"/images/dynamic/hugging-face-1-.png","draft":false,"categories":["Integrations"],"body":{"raw":"AimCallback for [Hugging Face](https://huggingface.co/) is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy. Moreover, it offers comprehensive system usage tracking, keeping an eye on CPU and GPU memory utilization.\n\n## Introduction\n\nDiscover how [AimOS](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems) effortlessly monitors your Hugging Face experiments, capturing every detail of hyperparameters and metrics. We'll dive into a sample script to demonstrate the powerful integration between AimOS and Hugging Face.\n\n## Running a Sample Training with Hugging Face: A Step-by-Step Guide\n\nExplore a hands-on example using the `yelp_review_full` dataset consisting of Yelp reviews, with the review scores ranging from 1 to 5 to full-tune a BERT base model for sequence classification.\n\n\n\n## Setting the Stage\n\nBefore diving into the script, ensure that AimOS is installed. If not, simply run “pip install aimos”, and head over to the [GitHub page](https://github.com/aimhubio/aimos) for more.\n\n## Sample Script\n\n\n\n### 1. Importing Required Modules\n\nBegin by importing the necessary modules.\n\n```\nimport evaluate\nimport numpy as np\nfrom aimstack.experiment_tracker.hugging_face import Callback as AimCallback\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n)\n```\n\n### 2. Preparing the Dataset and the Tokenizer\n\nDefine a function for tokenization.\n\n```\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n```\n\nTokenize the dataset using the BERT base-cased tokenizer.\n\n```\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\nSelect a small subset of the dataset for training and evaluation.\n\n```\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\nDefine the metrics for evaluation.\n\n```\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nmetric = evaluate.load(\"accuracy\")\n```\n\n### 3. Instantiate the Model\n\nInstantiate the BERT base-cased model for sequence classification.\n\n```\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-cased\", num_labels=5\n)\n```\n\n### 4. Initialize AimOS Callback\n\nInitialize the AimOS callback by providing the repository address and experiment name. Here the `aim://0.0.0.0:53800` is the default address of your aimos server if it’s running on your local machine:\n\n```\naim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:53800\", experiment_name=\"yelp_classification\"\n)\n```\n\nEnsure that your AimOS server is active on the default `53800` port. Start the AimOS server with the following command:\n\n```\naimos server\n```\n\n### 5. Training\n\nSet the training arguments, specifying the output directory, number of epochs, and learning rate. Train the model by providing the AimOS callback in the Trainer arguments.\n\n```\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=10, learning_rate=5e-7)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[aim_callback],\n)\n\ntrainer.train()\n```\n\nAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the .aim repository is initialized and execute:\n\n```\naimos ui\n```\n\nThis is the view you'll see after opening the provided URL: \n\n![AimOS Apps](/images/dynamic/seamless-experimentation-screenshot-dec-13.png \"AimOS Apps\")\n\n## Interpreting Experiment Results\n\nExplore the wealth of information provided by AimOS to gain insights into your Hugging Face experiment. We'll guide you through the key aspects to understand experiment's effectiveness.\n\n\n\n## AimOS Overview Page\n\nHead over to the AimOS Overview page to witness the containers and sequences overview tables of your Hugging Face experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\n\n![Overview](/images/dynamic/seam.png \"Overview\")\n\n## AimOS Runs Page\n\nVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\n\n![Runs page](/images/dynamic/seamless-experimentation-screenshot-dec-13-1-.png \"Runs page\")\n\nBy navigating to each run’s individual page, you'll have a detailed view of the activities within that run.\n\n## Run Overview\n\n\n\nNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\n\n![Run overview](/images/dynamic/seamless-experimentation-screenshot-dec-13-2-.png \"Run overview\")\n\n## Params Tab\n\nFor a more detailed view of all parameters, navigate to the Params tab.\n\n![Params tab](/images/dynamic/seamless-experimentation-screenshot-dec-13-3-.png \"Params tab\")\n\n## Metrics Tab\n\nIn the Metrics tab, explore all the tracked metrics graphs, including the change of learning_rate, loss, samples_per_second, steps_per_second, total_flos, and steps.\n\n![Metrics tab](/images/dynamic/seamless-experimentation-screenshot-dec-13-4-.png \"Metrics tab \")\n\nIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\n\n## Wrapping up\n\nAimOS provides a comprehensive suite of tools to analyze and understand your Hugging Face experiment thoroughly. By leveraging these insights, you can elevate your experimentation process and make data-driven decisions. \n\nFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on [AimStack blog.](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\n## Learn more\n\n[](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\n[AimOS is on a mission to democratize AI Systems logging tools. 🙌](https://aimos.readthedocs.io/en/latest/apps/overview.html)[](https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems)\n\nTry out [AimOS](https://github.com/aimhubio/aimos), join the [Aim community](https://community.aimstack.io/) and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\n\nDon’t forget to leave us a star on [GitHub](https://github.com/aimhubio/aimos) if you think AimOS is useful, and here is the repository of [Aim](https://github.com/aimhubio/aim), an easy-to-use \u0026 supercharged open-source experiment tracker.⭐️","html":"\u003cp\u003eAimCallback for \u003ca href=\"https://huggingface.co/\"\u003eHugging Face\u003c/a\u003e is designed to enhance your experiment logging and monitoring. It thoroughly records essential information, including hyperparameters, training, validation, and test time metrics like loss and accuracy. Moreover, it offers comprehensive system usage tracking, keeping an eye on CPU and GPU memory utilization.\u003c/p\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eDiscover how \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimOS\u003c/a\u003e effortlessly monitors your Hugging Face experiments, capturing every detail of hyperparameters and metrics. We'll dive into a sample script to demonstrate the powerful integration between AimOS and Hugging Face.\u003c/p\u003e\n\u003ch2\u003eRunning a Sample Training with Hugging Face: A Step-by-Step Guide\u003c/h2\u003e\n\u003cp\u003eExplore a hands-on example using the \u003ccode\u003eyelp_review_full\u003c/code\u003e dataset consisting of Yelp reviews, with the review scores ranging from 1 to 5 to full-tune a BERT base model for sequence classification.\u003c/p\u003e\n\u003ch2\u003eSetting the Stage\u003c/h2\u003e\n\u003cp\u003eBefore diving into the script, ensure that AimOS is installed. If not, simply run “pip install aimos”, and head over to the \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eGitHub page\u003c/a\u003e for more.\u003c/p\u003e\n\u003ch2\u003eSample Script\u003c/h2\u003e\n\u003ch3\u003e1. Importing Required Modules\u003c/h3\u003e\n\u003cp\u003eBegin by importing the necessary modules.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport evaluate\nimport numpy as np\nfrom aimstack.experiment_tracker.hugging_face import Callback as AimCallback\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Preparing the Dataset and the Tokenizer\u003c/h3\u003e\n\u003cp\u003eDefine a function for tokenization.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTokenize the dataset using the BERT base-cased tokenizer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSelect a small subset of the dataset for training and evaluation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDefine the metrics for evaluation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nmetric = evaluate.load(\"accuracy\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e3. Instantiate the Model\u003c/h3\u003e\n\u003cp\u003eInstantiate the BERT base-cased model for sequence classification.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-cased\", num_labels=5\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e4. Initialize AimOS Callback\u003c/h3\u003e\n\u003cp\u003eInitialize the AimOS callback by providing the repository address and experiment name. Here the \u003ccode\u003eaim://0.0.0.0:53800\u003c/code\u003e is the default address of your aimos server if it’s running on your local machine:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaim_callback = AimCallback(\n    repo=\"aim://0.0.0.0:53800\", experiment_name=\"yelp_classification\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnsure that your AimOS server is active on the default \u003ccode\u003e53800\u003c/code\u003e port. Start the AimOS server with the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos server\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e5. Training\u003c/h3\u003e\n\u003cp\u003eSet the training arguments, specifying the output directory, number of epochs, and learning rate. Train the model by providing the AimOS callback in the Trainer arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etraining_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=10, learning_rate=5e-7)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[aim_callback],\n)\n\ntrainer.train()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter completing these steps, you will successfully log all actions on AimOS. Now, run the AimOS UI to observe the tracked metadata. Navigate to the folder where the .aim repository is initialized and execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eaimos ui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is the view you'll see after opening the provided URL:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13.png\" alt=\"AimOS Apps\" title=\"AimOS Apps\"\u003e\u003c/p\u003e\n\u003ch2\u003eInterpreting Experiment Results\u003c/h2\u003e\n\u003cp\u003eExplore the wealth of information provided by AimOS to gain insights into your Hugging Face experiment. We'll guide you through the key aspects to understand experiment's effectiveness.\u003c/p\u003e\n\u003ch2\u003eAimOS Overview Page\u003c/h2\u003e\n\u003cp\u003eHead over to the AimOS Overview page to witness the containers and sequences overview tables of your Hugging Face experiment. This page offers an abstract overview of the number of experiments, sequences, metrics, and system metrics tracked within your experiment.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seam.png\" alt=\"Overview\" title=\"Overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eAimOS Runs Page\u003c/h2\u003e\n\u003cp\u003eVisit the AimOS Runs page to view the table of all runs, containing basic information about the experiment. This includes hash system parameters, hyperparameters, and model configurations/information.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-1-.png\" alt=\"Runs page\" title=\"Runs page\"\u003e\u003c/p\u003e\n\u003cp\u003eBy navigating to each run’s individual page, you'll have a detailed view of the activities within that run.\u003c/p\u003e\n\u003ch2\u003eRun Overview\u003c/h2\u003e\n\u003cp\u003eNavigate through the Overview tab to grasp essential information such as Run Parameters, Metrics (last metrics scores), CLI Arguments used to execute the experiment, Environment Variables, Packages, and their versions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-2-.png\" alt=\"Run overview\" title=\"Run overview\"\u003e\u003c/p\u003e\n\u003ch2\u003eParams Tab\u003c/h2\u003e\n\u003cp\u003eFor a more detailed view of all parameters, navigate to the Params tab.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-3-.png\" alt=\"Params tab\" title=\"Params tab\"\u003e\u003c/p\u003e\n\u003ch2\u003eMetrics Tab\u003c/h2\u003e\n\u003cp\u003eIn the Metrics tab, explore all the tracked metrics graphs, including the change of learning_rate, loss, samples_per_second, steps_per_second, total_flos, and steps.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/seamless-experimentation-screenshot-dec-13-4-.png\" alt=\"Metrics tab\" title=\"Metrics tab \"\u003e\u003c/p\u003e\n\u003cp\u003eIn the remaining tabs, discover corresponding types of tracked objects, such as Figures, Text, and Audios.\u003c/p\u003e\n\u003ch2\u003eWrapping up\u003c/h2\u003e\n\u003cp\u003eAimOS provides a comprehensive suite of tools to analyze and understand your Hugging Face experiment thoroughly. By leveraging these insights, you can elevate your experimentation process and make data-driven decisions.\u003c/p\u003e\n\u003cp\u003eFor further insights into AI system monitoring and observability of software lifecycle, check out our latest article on \u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003eAimStack blog.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aimos.readthedocs.io/en/latest/apps/overview.html\"\u003eAimOS is on a mission to democratize AI Systems logging tools. 🙌\u003c/a\u003e\u003ca href=\"https://aimstack.io/blog/new-releases/aim-4-0-open-source-modular-observability-for-ai-systems\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eAimOS\u003c/a\u003e, join the \u003ca href=\"https://community.aimstack.io/\"\u003eAim community\u003c/a\u003e and contribute by sharing your thoughts, suggesting new features, or reporting bugs.\u003c/p\u003e\n\u003cp\u003eDon’t forget to leave us a star on \u003ca href=\"https://github.com/aimhubio/aimos\"\u003eGitHub\u003c/a\u003e if you think AimOS is useful, and here is the repository of \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, an easy-to-use \u0026#x26; supercharged open-source experiment tracker.⭐️\u003c/p\u003e"},"_id":"posts/streamlining-your-hugging-face-experiments-with-aimos.md","_raw":{"sourceFilePath":"posts/streamlining-your-hugging-face-experiments-with-aimos.md","sourceFileName":"streamlining-your-hugging-face-experiments-with-aimos.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/streamlining-your-hugging-face-experiments-with-aimos"},"type":"Post"}]},"__N_SSG":true},"page":"/blog/[category]","query":{"category":"integrations"},"buildId":"XVbXvPM_xexbgQ-lZ1_Ds","isFallback":false,"gsp":true,"scriptLoader":[{"src":"https://www.googletagmanager.com/gtag/js?id=G-G8YKCN2HLS","strategy":"afterInteractive"},{"id":"google-analytics","strategy":"afterInteractive","children":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){window.dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config','G-G8YKCN2HLS');\n            "}]}</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-G8YKCN2HLS"
            height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript></body></html>