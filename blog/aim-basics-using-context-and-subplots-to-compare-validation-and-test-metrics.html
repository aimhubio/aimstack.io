<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Aim basics: using context and subplots to compare validation and test metrics</title><meta name="robots" content="index,follow"/><meta name="description" content="Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model"/><meta name="twitter:card" content="summary_large_image"/><meta property="og:title" content="Aim basics: using context and subplots to compare validation and test metrics"/><meta property="og:description" content="Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model"/><meta property="og:url" content="https://v4.aimstack.io/blog/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"/><meta property="og:type" content="website"/><meta property="og:image" content="/images/dynamic/1.gif"/><meta property="og:image:alt" content="Aim basics: using context and subplots to compare validation and test metrics"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1224"/><meta property="og:image:height" content="724"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="Aimstack"/><meta name="next-head-count" content="17"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><style id="stitches">--sxs{--sxs:0 t-fsRyUS}@media{:root,.t-fsRyUS{--fonts-OpenSans:'Open Sans', sans-serif;--fonts-Lora:'Lora', serif;--fonts-Inconsolata:'Inconsolata', monospace;--fonts-base:var(--fonts-OpenSans);--colors-blue:#1093F2;--colors-blueHover:#0F7AC8;--colors-lightBlue:rgba(16, 147, 242, 0.1);--colors-lightBlueHover:rgba(16, 147, 242, 0.2);--colors-darkBlue:#0C1031;--colors-darkBlue500:rgba(12,16,49,0.5);--colors-bigStone:#191D3C;--colors-bigStoneHover:#313551;--colors-green:#14C89D;--colors-black:#000000;--colors-black003:rgba(0,0,0,0.03);--colors-black700:rgba(0,0,0,0.7);--colors-black800:rgba(0,0,0,0.8);--colors-white:#ffffff;--colors-white100:rgba(255,255,255,0.1);--colors-white500:rgba(255,255,255,0.5);--colors-white700:rgba(255,255,255,0.7);--colors-red:#CC231A;--colors-lightGrey:#F4F7F9;--colors-lightGreyHover:#ECEEF0;--colors-grey:#CFD3D6;--colors-darkGrey:#737379;--colors-darkGreyHover:#393940;--colors-primary:var(--colors-blue);--colors-primaryHover:var(--colors-blueHover);--colors-primaryLight:var(--colors-lightBlue);--colors-primaryLightHover:var(--colors-lightBlueHover);--colors-secondary:var(--colors-green);--colors-textColor:var(--colors-black);--colors-bgColor:var(--colors-darkBlue);--colors-danger:var(--colors-red);--fontSizes-1:14px;--fontSizes-2:16px;--fontSizes-3:18px;--fontSizes-4:20px;--fontSizes-5:22px;--fontSizes-6:24px;--fontSizes-7:32px;--fontSizes-8:44px;--fontSizes-9:64px;--fontSizes-10:68px;--fontSizes-baseSize:var(--fontSizes-2);--space-1:4px;--space-2:8px;--space-3:12px;--space-4:16px;--space-5:20px;--space-6:24px;--space-7:28px;--space-8:32px;--space-9:36px;--space-10:40px;--space-11:44px;--space-12:48px;--fontWeights-1:400;--fontWeights-2:500;--fontWeights-3:600;--fontWeights-4:700;--fontWeights-5:800;--shadows-1:0px 60px 66px -65px rgba(11, 47, 97, 0.2);--shadows-2:0px 96px 66px -65px rgba(11, 47, 97, 0.2);--shadows-3:1px 1px 10px 3px  rgb(0, 0, 0, 10%);--shadows-4:0px 10px 24px -10px rgba(11, 47, 97, 0.4);--radii-1:6px;--radii-2:8px;--transitions-main:0.2s ease-out}}--sxs{--sxs:1 cmZuWL k-iKtTFk}@media{html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,main,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section{display:block}*[hidden]{display:none}*{box-sizing:border-box}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:none}table{border-spacing:0}strong{font-weight:var(--fontWeights-5)}body{background:var(--colors-white);color:var(--colors-textColor);font-family:var(--fonts-base);font-size:var(--fontSizes-baseSize);line-height:1.35}a{text-decoration:none;color:inherit}a.link{color:var(--colors-primary);font-weight:var(--fontWeights-4)}.text-center{text-align:center}@keyframes k-iKtTFk{0%{opacity:0}10%{opacity:1}90%{opacity:1}100%{opacity:0}}}--sxs{--sxs:2 c-iSkjJi c-guYHoi c-cdUZgw c-hCkBfr c-ePqJJt c-kFnRKY c-lmSDjj c-kPczbf c-lizetl c-fOPBY c-jCnBs c-iKzMen c-PJLV c-doWOai c-dhzjXW c-cZmHrB c-kSnMQa c-kXnqgD c-bMqEGV c-fgXcmv c-gPAtaN c-dMnFVR c-kHrbHn c-hJzboU c-dRMHzO c-FsNLu c-eJqRfT c-jJiyUf c-MNZuo c-cChYXC c-jcfAOr c-jYGtqD c-fKjLcl c-kKhLoP c-gRRuTe c-hnRRWM c-hQOWqi c-jiSXep c-fRHVpw c-bzrsgE c-cBxMqI c-cgVSTP c-cPiarr c-jsGLMb c-fVgYPG c-fxrEBZ c-hTKfPd c-kBUzHM c-hePpyc c-iiCAPf c-kqsoHo c-eCJxrf c-hMVjTK c-cQwwIn c-fagtgx c-gsaXpY c-eOYVvF c-fWgRbD c-gswrlc c-jVwFOG c-eYYUfS c-depOeu c-dOghFk c-ckoDwU c-covwPd c-cSRvhF c-hGDYPf c-cnJPJj c-bjrmKQ c-hdKdh c-eVsMjz}@media{.c-iSkjJi{position:relative}.c-iSkjJi .bg-top{z-index:2}.c-iSkjJi .bg-bottom{z-index:1;position:absolute;bottom:300px;left:0;right:0;object-fit:contain;width:100%;height:60%}.c-guYHoi{position:relative;z-index:3;display:flex;flex-direction:column;min-height:100vh}.c-cdUZgw{height:72px;position:fixed;top:0px;left:0px;right:0px;z-index:99;transition:var(--transitions-main)}.c-cdUZgw.fixed{box-shadow:var(--shadows-3);background-color:var(--colors-white)}.c-hCkBfr{margin-left:auto;margin-right:auto;padding-left:var(--space-6);padding-right:var(--space-6);width:100%;max-width:1300px}.c-ePqJJt{display:flex;align-items:center;height:100%}.c-kFnRKY{margin-right:50px}.c-kFnRKY .logo{max-width:158px;width:100%;display:block}@media (max-width: 1024px){.c-kFnRKY{position:relative;z-index:11}}.c-lmSDjj{display:flex;justify-content:center}.c-lmSDjj .nav-list{display:flex}.c-lmSDjj .nav-list li:not(:last-child){margin-right:var(--space-6)}@media (max-width: 1440px){.c-lmSDjj .nav-list li:not(:last-child){margin-right:var(--space-4)}}.c-lmSDjj .nav-list li a{-webkit-backface-visibility:hidden;backface-visibility:hidden;display:inline-flex}.c-lmSDjj .nav-list li a .text{transition:var(--transitions-main)}.c-lmSDjj .nav-list li a .badge{display:inline-block;height:18px;line-height:18px;padding:0 4px;border-radius:2px;background-color:var(--colors-primary);color:var(--colors-white);font-weight:700;font-size:10px;margin-left:6px}.c-lmSDjj .nav-list li a:hover .text{opacity:.6}@media (max-width: 1024px){.c-lmSDjj{position:fixed;top:72px;right:0;bottom:0;left:0;overflow-y:auto;z-index:10;height:0;transition:height 0.5s;background-color:var(--colors-white);flex-direction:column;justify-content:space-between}}@media (max-width: 1024px){.open .c-lmSDjj{height:calc(100% - 72px)}}@media (max-width: 1024px){.c-lmSDjj .nav-inner{width:100%;flex-direction:column;align-items:inherit;justify-content:space-between;padding-top:16px}}@media (max-width: 1024px){.c-lmSDjj .nav-list{flex-direction:column;align-items:flex-start}}@media (max-width: 1024px){.c-lmSDjj .nav-list > li{font-size:var(--fontSizes-2);font-weight:var(--fontWeights-3);width:100%}}@media (max-width: 1024px){.c-lmSDjj .nav-list > li a{display:block;padding:var(--space-3) var(--space-5)}}@media (max-width: 1024px){.c-lmSDjj .nav-list > li a:active{background-color:var(--colors-primaryLight)}}@media (max-width: 1024px){.c-lmSDjj .nav-list > li:not(:last-child){margin-right:0}}.c-kPczbf{margin-left:auto}.c-kPczbf span span{display:flex;align-items:center}.c-kPczbf.desktop-btn span span{justify-content:flex-end}.c-lizetl{display:none}@media (max-width: 1024px){.c-lizetl{display:flex;justify-content:center;padding-top:var(--space-6);padding-bottom:var(--space-6)}}@media (max-width: 1024px){.c-lizetl > li{margin-right:var(--space-6)}}.c-fOPBY{display:none}@media (max-width: 1024px){.c-fOPBY{position:relative;z-index:11;display:inline-block;-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer}}.c-jCnBs{flex:1;padding-top:72px}.c-iKzMen{text-align:center;padding-top:80px;padding-bottom:80px}.c-doWOai{padding-top:80px;padding-bottom:80px;color:var(--colors-white);background-color:var(--colors-darkBlue);position:relative}@media (max-width: 1024px){.c-doWOai{padding-top:44px;padding-bottom:44px}}.c-dhzjXW{display:flex}.c-cZmHrB{flex:1}.c-kSnMQa{display:flex;flex-wrap:wrap;margin-left:-60px;margin-top:60px}@media (max-width: 743px){.c-kSnMQa{margin-left:0;margin-top:var(--space-8)}}.c-kXnqgD{width:calc((100% / 2) - 60px);margin-left:60px;margin-bottom:60px}@media (max-width: 743px){.c-kXnqgD{width:100%;margin-left:0;margin-bottom:var(--space-10)}}.c-bMqEGV{-webkit-appearance:none;appearance:none;border:none;background-color:transparent;line-height:1;cursor:pointer;border-radius:var(--radii-1);display:inline-block;transition:var(--transitions-main)}.c-fgXcmv{max-width:650px}.c-fgXcmv img{width:100%;height:auto}@media (max-width: 1024px){.c-fgXcmv{margin:0 auto}}.c-gPAtaN{text-align:center;padding-top:100px}.c-dMnFVR{display:flex;flex-wrap:wrap;margin-left:-40px;margin-top:100px}@media (max-width: 1024px){.c-dMnFVR{justify-content:center}}@media (max-width: 743px){.c-dMnFVR{margin-left:0}}.c-kHrbHn{width:calc((100% / 4) - 40px);margin-left:40px;margin-bottom:60px}@media (max-width: 1024px){.c-kHrbHn{width:calc((100% / 2) - 40px);margin-bottom:40px}}@media (max-width: 743px){.c-kHrbHn{width:100%;margin-left:0}}.c-kHrbHn .card-content{margin-top:var(--space-6)}@media (max-width: 743px){.c-kHrbHn .card-content{text-align:left;margin-top:0}}.c-hJzboU{-webkit-mask-size:100%;mask-size:100%;-webkit-mask-repeat:no-repeat}.c-hJzboU img{object-fit:contain;height:100%;width:100%}@media (max-width: 743px){.c-hJzboU{width:100px;min-width:100px;margin-right:var(--space-6)}}.c-dRMHzO{background-color:var(--colors-darkBlue)}.c-FsNLu{display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;color:var(--colors-white);padding-top:27px;padding-bottom:27px;border-bottom:1px solid var(--colors-white100)}.c-eJqRfT{margin-right:var(--space-9)}@media (max-width: 1440px){.c-eJqRfT{margin-right:var(--space-6)}}@media (max-width: 743px){.c-eJqRfT{width:100%;margin-right:0;margin-bottom:var(--space-1);text-align:center}}.c-jJiyUf{display:flex}.c-jJiyUf li:not(:last-child){margin-right:var(--space-9)}@media (max-width: 1440px){.c-jJiyUf li:not(:last-child){margin-right:var(--space-4)}}.c-jJiyUf li{transition:var(--transitions-main)}.c-jJiyUf li:hover{opacity:.6}@media (max-width: 1024px){.c-jJiyUf{order:3;width:100%;margin-top:var(--space-7)}}@media (max-width: 1024px){.c-jJiyUf li:last-child{margin-right:0}}@media (max-width: 743px){.c-jJiyUf{order:2;width:100%;margin-bottom:var(--space-2);flex-direction:column;text-align:center}}@media (max-width: 743px){.c-jJiyUf li{margin-bottom:var(--space-6)}}@media (max-width: 743px){.c-jJiyUf li:not(:last-child){margin-right:0}}.c-MNZuo{display:flex}.c-MNZuo li:not(:last-child){margin-right:var(--space-6)}.c-MNZuo li a{transition:var(--transitions-main)}.c-MNZuo li a:hover{opacity:.6}@media (max-width: 743px){.c-MNZuo{margin:0 auto;order:3}}.c-cChYXC{padding-top:var(--space-4);padding-bottom:var(--space-4);color:var(--colors-white500);text-align:center}.c-jcfAOr{display:flex;flex-wrap:wrap;margin-left:-40px}@media (max-width: 1024px){.c-jcfAOr{margin-left:-54px}}@media (max-width: 575px){.c-jcfAOr{margin-left:0}}.c-jYGtqD{width:calc((100% / 3) - 40px);margin-left:40px;margin-bottom:40px}@media (max-width: 1024px){.c-jYGtqD{width:calc((100% / 2) - 54px);margin-left:54px;margin-bottom:54px}}@media (max-width: 575px){.c-jYGtqD{width:100%;margin-left:0;margin-bottom:48px}}.c-fKjLcl{position:relative;height:0;padding-bottom:65%}.c-kKhLoP{padding-top:var(--space-6)}.c-gRRuTe{margin-bottom:var(--space-2);display:flex;align-items:center;color:var(--colors-grey);transition:var(--transitions-main)}.c-gRRuTe .icon{margin-right:var(--space-1);fill:var(--colors-grey);transition:var(--transitions-main)}.c-gRRuTe:hover{color:var(--colors-darkGray)}.c-gRRuTe:hover .icon{fill:var(--colors-darkGray)}.c-hnRRWM{display:flex;align-items:center;justify-content:flex-end}.c-hnRRWM .icon{margin-right:var(--space-1);fill:var(--colors-grey)}.c-hQOWqi{margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-jiSXep{display:flex;align-items:center;justify-content:center}.c-jiSXep li{height:44px;width:44px;font-size:var(--fontSizes-1);font-weight:var(--fontWeights-4);text-align:center;margin-right:var(--space-1)}.c-jiSXep li a{display:block;line-height:44px;transition:var(--transitions-main)}.c-jiSXep li a.active{background-color:var(--colors-primary);color:var(--colors-white)}.c-jiSXep li a:hover:not(.active){color:var(--colors-primary)}.c-jiSXep li a:hover .icon{fill:var(--colors-primary)}.c-fRHVpw{padding:80px 0 64px;text-align:center}@media (max-width: 1024px){.c-fRHVpw{padding:44px 0}}@media (max-width: 743px){.c-fRHVpw{padding:44px 0 24px}}.c-bzrsgE{max-width:948px;width:100%;margin-left:auto;margin-right:auto;position:relative}.c-bzrsgE .github-btn{position:absolute;bottom:0;left:50%;translate:-50% 0}.c-cBxMqI{height:150px;display:flex;background-color:var(--colors-white500);box-shadow:var(--shadows-1)}.c-cgVSTP{display:flex;align-items:center;overflow:visible !important}.c-cgVSTP img{width:95px !important}.c-cPiarr{padding:64px 0 150px}@media (max-width: 1024px){.c-cPiarr{padding:44px 0 80px}}@media (max-width: 743px){.c-cPiarr{padding:24px 0 60px}}.c-jsGLMb{position:relative}.c-jsGLMb .yt-lite{border-radius:var(--radii-2)}.c-jsGLMb .yt-lite > .lty-playbtn{width:68px;height:48px;border:none;border-radius:25%}.c-jsGLMb iframe{position:absolute;width:100%;height:100%;top:0;left:0;border-radius:var(--radii-2);box-shadow:var(--shadows-2)}.c-fVgYPG{position:relative}.c-fVgYPG .float-text{float:left;width:45%}@media (max-width: 1024px){.c-fVgYPG .float-text{width:100%}}.c-fxrEBZ{overflow:hidden}.c-hTKfPd{width:47%;padding-top:80px;padding-bottom:var(--space-9);float:left}@media (max-width: 1024px){.c-hTKfPd{width:100%;float:none;text-align:center}}.c-kBUzHM{float:right;margin-left:80px}@media (max-width: 1024px){.c-kBUzHM{float:none;margin-left:0}}.c-hePpyc{padding-top:var(--space-5)}.c-iiCAPf{position:relative}.c-iiCAPf button{position:absolute;right:0;top:6px;background:none;border:none;cursor:pointer;height:42px;width:42px;transition:var(--transitions-main)}.c-iiCAPf button:hover{opacity:.6}.c-iiCAPf .copied{display:block;opacity:0;position:absolute;right:0;top:-20px;animation:k-iKtTFk 2.5s ease 0s alternate;color:var(--colors-secondary);font-size:var(--fontSizes-1);font-weight:var(--fontWeights-2);transition:var(--transitions-main);z-index:10}.c-iiCAPf .hljs{display:block;overflow-x:auto;color:#abb2bf;background:#282c34;margin-bottom:20px;border-radius:var(--radii-1);padding:var(--space-4)}.c-iiCAPf .hljs-comment,.c-iiCAPf .hljs-quote{color:#5c6370;font-style:italic}.c-iiCAPf .hljs-doctag,.c-iiCAPf .hljs-keyword,.c-iiCAPf .hljs-formula{color:#c678dd}.c-iiCAPf .hljs-section,.c-iiCAPf .hljs-name,.c-iiCAPf .hljs-selector-tag,.c-iiCAPf .hljs-deletion,.c-iiCAPf .hljs-subst{color:#e06c75}.c-iiCAPf .hljs-literal{color:#56b6c2}.c-iiCAPf .hljs-string,.c-iiCAPf .hljs-regexp,.c-iiCAPf .hljs-addition,.c-iiCAPf .hljs-attribute,.c-iiCAPf .hljs-meta-string{color:#98c379}.c-iiCAPf .hljs-built_in,.c-iiCAPf .hljs-class .hljs-title{color:#e6c07b}.c-iiCAPf .hljs-attr,.c-iiCAPf .hljs-variable,.c-iiCAPf .hljs-template-variable,.c-iiCAPf .hljs-type,.c-iiCAPf .hljs-selector-class,.c-iiCAPf .hljs-selector-attr,.c-iiCAPf .hljs-selector-pseudo,.c-iiCAPf .hljs-number{color:#d19a66}.c-iiCAPf .hljs-symbol,.c-iiCAPf .hljs-bullet,.c-iiCAPf .hljs-link,.c-iiCAPf .hljs-meta,.c-iiCAPf .hljs-selector-id,.c-iiCAPf .hljs-title{color:#61aeee}.c-iiCAPf .hljs-emphasis{font-style:italic}.c-iiCAPf .hljs-strong{font-weight:bold}.c-iiCAPf .hljs-link{text-decoration:underline}.c-kqsoHo{padding-top:150px;padding-bottom:150px}.c-kqsoHo .title{text-align:center;margin-bottom:100px}@media (max-width: 1024px){.c-kqsoHo{padding-top:80px;padding-bottom:80px}}@media (max-width: 1024px){.c-kqsoHo .title{margin-bottom:64px}}@media (max-width: 743px){.c-kqsoHo{padding-top:60px;padding-bottom:60px}}@media (max-width: 743px){.c-kqsoHo .title{margin-bottom:44px}}.c-eCJxrf{display:flex;align-items:center;justify-content:space-between}.c-eCJxrf .title-mobile{display:none}@media (max-width: 1024px){.c-eCJxrf{flex-wrap:wrap;align-items:flex-start}}@media (max-width: 1024px){.c-eCJxrf .title-mobile{display:block;width:100%}}@media (max-width: 743px){.c-eCJxrf{flex-direction:column;margin-bottom:60px}}.c-hMVjTK ul li{margin-bottom:var(--space-4);position:relative;padding-left:16px}.c-hMVjTK ul li:before{content:"•";position:absolute;left:0;top:-5px;font-size:24px;color:var(--colors-secondary)}@media (max-width: 1024px){.c-hMVjTK ul{padding-top:var(--space-4)}}@media (max-width: 1024px){.c-hMVjTK{max-width:280px;width:100%}}@media (max-width: 1024px){.c-hMVjTK .title-desktop{display:none}}@media (max-width: 743px){.c-hMVjTK{max-width:100%;order:2}}.c-cQwwIn{display:flex;align-items:center;color:var(--colors-primary);font-family:var(--fonts-OpenSans600);text-decoration:none}.c-cQwwIn .icon{fill:var(--colors-primary);margin-left:var(--space-4)}.c-fagtgx{max-width:610px;width:100%;position:relative}.c-fagtgx img{width:100%;height:auto}@media (max-width: 1024px){.c-fagtgx{max-width:396px;margin-left:0}}@media (max-width: 743px){.c-fagtgx{max-width:100%;order:1}}.c-gsaXpY{padding:60px 0 100px;background-color:var(--colors-darkBlue);color:var(--colors-white)}@media (max-width: 1024px){.c-gsaXpY{padding:60px 0 26px}}.c-eOYVvF{display:flex;flex-wrap:wrap;margin-left:-24px}@media (max-width: 1024px){.c-eOYVvF{margin-left:-54px}}@media (max-width: 575px){.c-eOYVvF{margin-left:0}}.c-fWgRbD{width:calc((100% / 4) - 24px);margin-left:24px;margin-bottom:24px}.c-fWgRbD a{background-color:var(--colors-bigStone);display:block;height:100%;border-radius:var(--radii-1);transition:var(--transitions-main)}.c-fWgRbD a .inner{padding:var(--space-6)}.c-fWgRbD a:hover{background-color:var(--colors-bigStoneHover)}.c-fWgRbD img{display:block;border-radius:var(--radii-1) var(--radii-1) 0 0}@media (max-width: 1024px){.c-fWgRbD{width:calc((100% / 2) - 54px);margin-left:54px;margin-bottom:54px}}@media (max-width: 575px){.c-fWgRbD{width:100%;margin-left:0;margin-bottom:24px}}.c-gswrlc{padding:100px 0}.c-gswrlc .input{max-width:400px}@media (max-width: 1024px){.c-gswrlc{padding:80px 0;text-align:center}}@media (max-width: 1024px){.c-gswrlc .input{margin:0 auto}}@media (max-width: 743px){.c-gswrlc{padding:60px 0}}.c-jVwFOG{display:flex;align-items:center;justify-content:space-between}.c-eYYUfS{flex:1;margin-right:80px}@media (max-width: 1024px){.c-eYYUfS{margin-right:0}}.c-depOeu{position:relative;display:block}.c-depOeu .error-message{position:absolute;top:100%;transform:translateY(2px);color:var(--colors-danger);font-size:20px;padding-left: 12px}.c-depOeu input{border:none;border-radius:var(--radii-1);width:100%;transition:var(--transitions-main)}.c-dOghFk{flex:1}@media (max-width: 1024px){.c-dOghFk{display:none}}.c-ckoDwU{display:flex;flex:1;box-shadow:var(--shadows-4);border-radius:var(--radii-2);padding:var(--space-6);background-color:var(--colors-white)}.c-covwPd{margin-bottom:var(--space-6);padding-left:var(--space-5);list-style-type:disc;font-size:var(--fontSizes-2)}.c-covwPd li{color:var(--colors-secondary)}.c-covwPd li:not(:last-child){margin-bottom:var(--space-3)}.c-covwPd li span{color:var(--colors-black700)}.c-cSRvhF{position:relative}.c-cSRvhF img{position:static !important}.c-hGDYPf{margin:var(--space-10) auto;line-height:1.7}.c-hGDYPf h1,.c-hGDYPf h2,.c-hGDYPf h3,.c-hGDYPf h4,.c-hGDYPf h5,.c-hGDYPf h6{margin-top:var(--space-8);margin-bottom:var(--space-5)}.c-hGDYPf h2{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-hGDYPf h2{font-size:var(--fontSizes-6)}}.c-hGDYPf h3{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-hGDYPf h3{font-size:var(--fontSizes-4)}}.c-hGDYPf p{margin-bottom:var(--space-5);font-size:var(--fontSizes-4);font-family:var(--fonts-Lora);color:var(--colors-black800)}.c-hGDYPf ul{margin-bottom:var(--space-5);padding-left:var(--space-10);list-style-type:disc;font-size:var(--fontSizes-3);font-family:var(--fonts-Lora)}.c-hGDYPf ul li:not(:last-child){margin-bottom:var(--space-2)}.c-hGDYPf a{text-decoration:underline;word-break:break-all;font-family:var(--fonts-Lora)}.c-hGDYPf a:hover{color:var(--colors-primary)}.c-hGDYPf img{width:100%;height:auto}.c-hGDYPf strong{font-weight:var(--fontWeights-4)}.c-hGDYPf blockquote{padding-left:var(--space-9);border-left:2px solid var(--colors-grey);font-size:var(--fontSizes-2);font-style:italic}.c-hGDYPf em,.c-hGDYPf q{font-style:italic}.c-hGDYPf pre{font-size:var(--fontSizes-2);font-family:var(--fonts-Inconsolata);line-height:1.7;overflow:auto;white-space:pre;word-break:normal;margin:0 0 var(--space-5);padding:var(--space-10);color:var(--colors-darkGreyHover);background-color:var(--colors-lightGrey);border:none;border-radius:2px}.c-hGDYPf code{white-space:pre-wrap}.c-cnJPJj{display:flex;justify-content:flex-end;margin-bottom:var(--space-10)}.c-cnJPJj button{height:40px;width:40px;background-color:var(--colors-lightGrey) !important;border-radius:var(--radii-1);margin-left:var(--space-3);transition:var(--transitions-main)}.c-cnJPJj button:hover[aria-label=twitter]{background-color:#43b1ff !important}.c-cnJPJj button:hover[aria-label=linkedin]{background-color:#0A66C2 !important}.c-cnJPJj button:hover[aria-label=facebook]{background-color:#6e8dd0 !important}.c-cnJPJj button:hover[aria-label=reddit]{background-color:#fb411c !important}.c-cnJPJj button:hover .icon{fill:var(--colors-white)}.c-bjrmKQ{padding:50px 0;border-top:2px solid var(--colors-lightGrey);border-bottom:2px solid var(--colors-lightGrey);margin-bottom:50px}.c-bjrmKQ a{transition:var(--transitions-main)}.c-bjrmKQ a .text,.c-bjrmKQ a .chevron-text{transition:var(--transitions-main)}.c-bjrmKQ a:hover .text{color:var(--colors-primary)}.c-hdKdh{flex:1;padding-right:var(--space-3);border-right:2px solid var(--colors-lightGrey)}.c-hdKdh a:hover .chevron-text{transform:translateX(15px)}@media (max-width: 743px){.c-hdKdh a:hover .chevron-text{transform:translateX(0)}}.c-eVsMjz{flex:1;text-align:right;padding-left:var(--space-3)}.c-eVsMjz a:hover .chevron-text{transform:translateX(-15px)}@media (max-width: 743px){.c-eVsMjz a:hover .chevron-text{transform:translateX(0)}}}--sxs{--sxs:3 c-PJLV-htjRrP-size-10 c-PJLV-dnvIlH-size-4 c-PJLV-ohsET-size-8 c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-PJLV-ypqAk-size-3 c-bMqEGV-kwxnul-size-1 c-bMqEGV-dxfqfJ-variant-primary c-dhzjXW-iTKOFX-direction-column c-PJLV-cBLpOj-size-2 c-PJLV-EDrvg-size-1 c-dhzjXW-bICGYT-justify-center c-PJLV-hlFDyt-size-6 c-PJLV-iMgaFX-truncate-true c-PJLV-cllNQK-lineClamp-true c-PJLV-pmxQl-size-9 c-depOeu-RBfUm-size-1 c-dhzjXW-irEjuD-align-stretch c-dhzjXW-dOBaZS-gap-12 c-dhzjXW-knmidH-justify-between c-bMqEGV-jDfpYu-variant-secondary c-dhzjXW-eKWVTQ-gap-5 c-dhzjXW-kdofoX-gap-2 c-PJLV-jszWhv-size-7 c-dhzjXW-bZmKkd-justify-end}@media{.c-PJLV-htjRrP-size-10{font-size:var(--fontSizes-10);font-weight:var(--fontWeights-5);line-height:1.25}@media (max-width: 1024px){.c-PJLV-htjRrP-size-10{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-htjRrP-size-10{font-size:var(--fontSizes-7)}}.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-4)}@media (max-width: 1024px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-3)}}@media (max-width: 743px){.c-PJLV-dnvIlH-size-4{font-size:var(--fontSizes-2)}}.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-8);font-weight:var(--fontWeights-5);line-height:1.5}@media (max-width: 743px){.c-PJLV-ohsET-size-8{font-size:var(--fontSizes-7)}}.c-dhzjXW-ejCoEP-direction-row{flex-direction:row}.c-dhzjXW-jroWjL-align-center{align-items:center}.c-dhzjXW-awKDG-justify-start{justify-content:flex-start}.c-dhzjXW-kVNAnR-wrap-noWrap{flex-wrap:nowrap}.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-3)}@media (max-width: 743px){.c-PJLV-ypqAk-size-3{font-size:var(--fontSizes-2)}}.c-bMqEGV-kwxnul-size-1{height:53px;line-height:53px;font-size:var(--fontSizes-3);padding-left:var(--space-6);padding-right:var(--space-6)}@media (max-width: 1024px){.c-bMqEGV-kwxnul-size-1{height:50px;line-height:50px}}.c-bMqEGV-dxfqfJ-variant-primary{background-color:var(--colors-primary);color:var(--colors-white)}.c-bMqEGV-dxfqfJ-variant-primary:hover{background-color:var(--colors-primaryHover)}.c-dhzjXW-iTKOFX-direction-column{flex-direction:column}.c-PJLV-cBLpOj-size-2{font-size:var(--fontSizes-2)}.c-PJLV-EDrvg-size-1{font-size:var(--fontSizes-1)}.c-dhzjXW-bICGYT-justify-center{justify-content:center}.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-6);font-weight:var(--fontWeights-3)}@media (max-width: 743px){.c-PJLV-hlFDyt-size-6{font-size:var(--fontSizes-4)}}.c-PJLV-iMgaFX-truncate-true{max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.c-PJLV-cllNQK-lineClamp-true{display:-webkit-box;-webkit-line-clamp:var(---lineClamp);-webkit-box-orient:vertical;overflow:hidden}.c-PJLV-pmxQl-size-9{font-size:var(--fontSizes-9);font-weight:var(--fontWeights-5);line-height:1.1}@media (max-width: 1024px){.c-PJLV-pmxQl-size-9{font-size:var(--fontSizes-8)}}@media (max-width: 743px){.c-PJLV-pmxQl-size-9{font-size:var(--fontSizes-7)}}.c-depOeu-RBfUm-size-1 input{padding:17px 24px;font-size:var(--fontSizes-2);background-color:var(--colors-lightGrey)}.c-depOeu-RBfUm-size-1 input:hover{background-color:var(--colors-lightGreyHover)}.c-dhzjXW-irEjuD-align-stretch{align-items:stretch}.c-dhzjXW-dOBaZS-gap-12{gap:var(--space-12)}.c-dhzjXW-knmidH-justify-between{justify-content:space-between}.c-bMqEGV-jDfpYu-variant-secondary{background-color:var(--colors-primaryLight);color:var(--colors-primary)}.c-bMqEGV-jDfpYu-variant-secondary:hover{background-color:var(--colors-primaryLightHover)}.c-dhzjXW-eKWVTQ-gap-5{gap:var(--space-5)}.c-dhzjXW-kdofoX-gap-2{gap:var(--space-2)}.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-7);font-weight:var(--fontWeights-2)}@media (max-width: 743px){.c-PJLV-jszWhv-size-7{font-size:var(--fontSizes-6)}}.c-dhzjXW-bZmKkd-justify-end{justify-content:flex-end}}--sxs{--sxs:4 c-dhzjXW-lelvPF-direction-columnReverse c-dhzjXW-cLKPGv-direction-row c-dhzjXW-ewYpoe-align-start c-dhzjXW-dAvekr-direction-column}@media{@media (max-width: 1024px){.c-dhzjXW-lelvPF-direction-columnReverse{flex-direction:column-reverse}}@media (max-width: 743px){.c-dhzjXW-cLKPGv-direction-row{flex-direction:row}}@media (max-width: 743px){.c-dhzjXW-ewYpoe-align-start{align-items:flex-start}}@media (max-width: 743px){.c-dhzjXW-dAvekr-direction-column{flex-direction:column}}}--sxs{--sxs:6 c-hCkBfr-ilkBNdM-css c-kPczbf-igSvvfr-css c-kPczbf-idOghFk-css c-fOPBY-ieBuAdh-css c-PJLV-ietOTGQ-css c-hCkBfr-ibrRWTZ-css c-hCkBfr-ikYMMba-css c-PJLV-igHOLBX-css c-PJLV-ijskIFF-css c-PJLV-ijpcBqa-css c-bMqEGV-icBLpOj-css c-hJzboU-ieOwGK-css c-PJLV-idgasAY-css c-PJLV-ieFlRcC-css c-PJLV-iciGHnC-css c-hJzboU-icdsEyN-css c-hJzboU-iYSetQ-css c-hJzboU-iebhThe-css c-hJzboU-ifZMaZh-css c-hJzboU-ieVlAUk-css c-dhzjXW-iguWOGj-css c-PJLV-ieRZfPH-css c-PJLV-iUazGY-css c-PJLV-ikMWyde-css c-PJLV-iedNKou-css c-bMqEGV-idKitzl-css c-PJLV-ijfuMJQ-css c-PJLV-ieBSoMY-css c-PJLV-iiMrHxl-css c-cQwwIn-ihZnFyC-css c-PJLV-igjdJOs-css c-PJLV-ieNyba-css c-PJLV-ibZBPXN-css c-PJLV-iejLhMq-css c-bMqEGV-igyJvzA-css c-dhzjXW-ielgvHS-css c-dhzjXW-ieHGUxU-css c-dhzjXW-iejLhMq-css c-PJLV-idHAijf-css c-PJLV-ieXfYvc-css c-PJLV-ijoSYcG-css c-dhzjXW-icNlJTv-css c-PJLV-ihjojsL-css c-dhzjXW-igyJvzA-css c-PJLV-ikaqJbg-css c-PJLV-idnFJmc-css}@media{.c-hCkBfr-ilkBNdM-css{height:100%}.c-kPczbf-igSvvfr-css{display:none}@media (max-width: 1024px){.c-kPczbf-igSvvfr-css{display:block;padding:var(--space-5)}}.c-kPczbf-idOghFk-css{flex:1}@media (max-width: 1024px){.c-kPczbf-idOghFk-css{display:none}}.c-fOPBY-ieBuAdh-css{margin-left:auto;padding:var(--space-3)}.c-PJLV-ietOTGQ-css{margin-bottom:var(--space-6)}.c-hCkBfr-ibrRWTZ-css{max-width:848px}.c-hCkBfr-ikYMMba-css{position:relative;z-index:2}@media (max-width: 1024px){.c-PJLV-igHOLBX-css{text-align:center}}.c-PJLV-ijskIFF-css{font-weight:var(--fontWeights-3);padding:var(--space-3) 0 var(--space-2)}.c-PJLV-ijpcBqa-css{color:var(--colors-white700)}.c-bMqEGV-icBLpOj-css{font-size:var(--fontSizes-2)}.c-hJzboU-ieOwGK-css{-webkit-mask-image:url(/images/static/about-us/shapes/1.svg);mask-image:url(/images/static/about-us/shapes/1.svg)}.c-PJLV-idgasAY-css{font-weight:var(--fontWeights-4)}.c-PJLV-ieFlRcC-css{padding-top:var(--space-2);padding-bottom:var(--space-2);font-weight:var(--fontWeights-3)}.c-PJLV-iciGHnC-css{color:var(--colors-black700)}.c-hJzboU-icdsEyN-css{-webkit-mask-image:url(/images/static/about-us/shapes/2.svg);mask-image:url(/images/static/about-us/shapes/2.svg)}.c-hJzboU-iYSetQ-css{-webkit-mask-image:url(/images/static/about-us/shapes/3.svg);mask-image:url(/images/static/about-us/shapes/3.svg)}.c-hJzboU-iebhThe-css{-webkit-mask-image:url(/images/static/about-us/shapes/5.svg);mask-image:url(/images/static/about-us/shapes/5.svg)}.c-hJzboU-ifZMaZh-css{-webkit-mask-image:url(/images/static/about-us/shapes/6.svg);mask-image:url(/images/static/about-us/shapes/6.svg)}.c-hJzboU-ieVlAUk-css{-webkit-mask-image:url(/images/static/about-us/shapes/7.svg);mask-image:url(/images/static/about-us/shapes/7.svg)}.c-dhzjXW-iguWOGj-css{height:100vh}.c-PJLV-ieRZfPH-css{text-align:center;margin-top:var(--space-10);margin-bottom:var(--space-10)}.c-PJLV-iUazGY-css{display:flex;align-items:center}.c-PJLV-ikMWyde-css{margin-top:var(--space-6);margin-bottom:var(--space-6);---lineClamp:3;font-family:var(--fonts-Lora)}.c-PJLV-iedNKou-css{margin-bottom:48px}.c-bMqEGV-idKitzl-css{margin-bottom:var(--space-12)}.c-PJLV-ijfuMJQ-css{opacity:0;transition:var(--transitions-main)}.c-PJLV-ieBSoMY-css span{background-color:var(--colors-secondary);color:var(--colors-white)}.c-PJLV-iiMrHxl-css{margin-bottom:var(--space-6)}.c-PJLV-iiMrHxl-css strong{font-weight:var(--fontWeights-5)}.c-cQwwIn-ihZnFyC-css{margin-top:var(--space-6);font-weight:var(--fontWeights-3)}.c-PJLV-igjdJOs-css{text-align:center}.c-PJLV-ieNyba-css{margin:0 auto 60px;text-align:center;max-width:560px}@media (max-width: 1024px){.c-PJLV-ieNyba-css{margin-bottom:44px}}.c-PJLV-ibZBPXN-css{margin-bottom:var(--space-2);font-weight:var(--fontWeights-4)}.c-PJLV-iejLhMq-css{margin-bottom:var(--space-4)}.c-bMqEGV-igyJvzA-css{margin-top:var(--space-6)}.c-dhzjXW-ielgvHS-css{padding-top:80px;padding-bottom:80px}.c-dhzjXW-ieHGUxU-css{padding-bottom:104px}.c-dhzjXW-iejLhMq-css{margin-bottom:var(--space-4)}.c-PJLV-idHAijf-css{font-weight:var(--fontWeights-4);margin-left:var(--space-4)}.c-PJLV-ieXfYvc-css{margin-bottom:var(--space-5);color:var(--colors-black700)}.c-PJLV-ijoSYcG-css{margin-bottom:var(--space-6);font-weight:var(--fontWeights-3)}.c-dhzjXW-icNlJTv-css{margin-top:var(--space-10)}.c-PJLV-ihjojsL-css{font-weight:var(--fontWeights-3)}.c-dhzjXW-igyJvzA-css{margin-top:var(--space-6)}.c-PJLV-ikaqJbg-css{margin-top:var(--space-6);margin-bottom:var(--space-6);font-weight:var(--fontWeights-4)}.c-PJLV-idnFJmc-css{margin-top:var(--space-3);---lineClamp:2}}</style><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-d52c472f627fec66.js" defer=""></script><script src="/_next/static/chunks/framework-ffee79c6390da51e.js" defer=""></script><script src="/_next/static/chunks/main-12fb79ef298a4077.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6e8a0d43ef511586.js" defer=""></script><script src="/_next/static/chunks/962-0a950ce516e89aa5.js" defer=""></script><script src="/_next/static/chunks/818-22d94b5f4e0f022d.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-56cb444a3acd6d07.js" defer=""></script><script src="/_next/static/ASeMQklLZoYg-bry69KPG/_buildManifest.js" defer=""></script><script src="/_next/static/ASeMQklLZoYg-bry69KPG/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Inconsolata&family=Lora&family=Open+Sans:wght@400;500;600;700;800&display=swap">@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8aRk.woff) format('woff')}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuyJF.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjr0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsgH1y4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsg-1y4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgshZ1y4k.woff) format('woff')}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WRL2kXWdycuJDETf.woff) format('woff');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WRP2kXWdycuJDETf.woff) format('woff');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WR32kXWdycuJDA.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJMkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJFkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJOkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJPkqt8ndeYxZ2JTg.woff) format('woff');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/lora/v26/0QI6MX1D_JOuGQbT0gvTJPa787weuxJBkqt8ndeYxZ0.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:800;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v34/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><main class="c-iSkjJi"><div class="c-guYHoi"><header class="c-cdUZgw"><div class="c-hCkBfr c-hCkBfr-ilkBNdM-css"><div class="c-ePqJJt"><div class="c-kFnRKY"><a class="logo" href="/"><img alt="AimStack" srcSet="/_next/static/chunks/images/images/static/main/logo_256_75.svg 1x, /_next/static/chunks/images/images/static/main/logo_384_75.svg 2x" src="/_next/static/chunks/images/images/static/main/logo_384_75.svg" width="156" height="37" decoding="async" data-nimg="1" loading="lazy" style="color:transparent"/></a></div><nav class="c-lmSDjj"><div class="nav-inner"><ul class="nav-list"><li><a target="_self" class="" href="/#quick-start"><span class="text">Quick start</span></a></li><li><a target="_self" class="" href="/#features"><span class="text">Features</span></a></li><li><a target="_self" class="" href="/#demos"><span class="text">Demos</span></a></li><li><a target="_blank" class="" href="https://aimstack.readthedocs.io/en/latest/"><span class="text">Docs</span></a></li><li><a target="_self" class="" href="/pricing"><span class="text">Pricing</span></a></li><li><a target="_self" class="active" href="/blog"><span class="text">Blog</span></a></li><li><a target="_self" class="" href="/about-us"><span class="text">About Us</span></a></li><li><a target="_blank" class="" href="/blog/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics#"><span class="text">Career</span><span class="badge">Hiring</span></a></li></ul><div class="c-kPczbf c-kPczbf-igSvvfr-css"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div></div><ul class="c-lizetl"><li><a href="https://aimstack.slack.com/ssb/redirect#/shared-invite/email" rel="noopener noreferrer" target="_blank" aria-label="slack"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M380.907 539.056c-57.742 0-104.574 46.837-104.574 104.585v261.609c0 57.748 46.832 104.585 104.574 104.585s104.574-46.837 104.574-104.585v-261.609c-0.042-57.748-46.874-104.585-104.574-104.585z"></path><path d="M15.111 643.63c0 57.799 46.874 104.687 104.659 104.687s104.659-46.888 104.659-104.687v-104.685h-104.577c-0.042 0-0.042 0-0.082 0-57.785 0-104.659 46.886-104.659 104.685z"></path><path d="M381.003 14.167c-0.042 0-0.083 0-0.125 0-57.783 0-104.656 46.886-104.656 104.684s46.874 104.685 104.656 104.685h104.574v-104.685c0-0.041 0-0.124 0-0.207-0.042-57.715-46.791-104.477-104.449-104.477z"></path><path d="M118.88 485.95h262.080c57.784 0 104.657-46.892 104.657-104.697s-46.874-104.697-104.657-104.697h-262.080c-57.784 0-104.658 46.892-104.658 104.697s46.874 104.697 104.658 104.697z"></path><path d="M904.118 276.5c-57.702 0-104.454 46.767-104.454 104.49v104.905h104.573c57.788 0 104.658-46.892 104.658-104.698s-46.871-104.697-104.658-104.697c-0.040 0-0.080 0-0.119 0z"></path><path d="M538.444 118.795v262.365c0 57.741 46.834 104.573 104.577 104.573 57.737 0 104.573-46.832 104.573-104.573v-262.365c0-57.741-46.837-104.573-104.573-104.573-57.742 0-104.577 46.832-104.577 104.573z"></path><path d="M747.594 905.028c0-57.748-46.837-104.585-104.573-104.585h-104.577v104.67c0.042 57.702 46.834 104.499 104.577 104.499 57.737 0 104.573-46.837 104.573-104.585z"></path><path d="M905.068 538.944h-262.076c-57.788 0-104.659 46.886-104.659 104.685s46.871 104.687 104.659 104.687h262.076c57.788 0 104.658-46.888 104.658-104.687s-46.871-104.685-104.658-104.685z"></path></svg></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="twitter"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M322.14 927.976c386.322 0 597.681-320.14 597.681-597.678 0-9-0.199-18.2-0.604-27.2 41.116-29.734 76.601-66.565 104.782-108.76-38.292 17.037-78.95 28.164-120.579 33 43.833-26.275 76.654-67.553 92.381-116.18-41.24 24.439-86.339 41.679-133.363 50.98-31.685-33.666-73.577-55.957-119.199-63.427s-92.44 0.299-133.206 22.103c-40.766 21.805-73.211 56.432-92.324 98.527s-23.826 89.314-13.411 134.357c-83.5-4.19-165.188-25.881-239.767-63.667s-140.386-90.823-193.153-155.673c-26.819 46.239-35.025 100.955-22.952 153.027s43.521 97.594 87.952 127.313c-33.356-1.059-65.981-10.040-95.18-26.2v2.6c-0.030 48.525 16.745 95.562 47.475 133.116 30.729 37.552 73.515 63.309 121.085 72.886-30.899 8.451-63.328 9.685-94.78 3.6 13.424 41.731 39.54 78.228 74.706 104.405 35.166 26.171 77.626 40.712 121.454 41.591-74.408 58.449-166.322 90.155-260.94 90.004-16.78-0.027-33.543-1.056-50.2-3.083 96.122 61.666 207.937 94.424 322.14 94.359z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="linkedIn"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M948.201 0h-872.601c-41.8 0-75.6 33-75.6 73.8v876.199c0 40.801 33.8 74.001 75.6 74.001h872.601c41.796 0 75.799-33.2 75.799-73.802v-876.398c0-40.8-34.002-73.8-75.799-73.8zM303.8 872.602h-152v-488.802h152v488.802zM227.8 317.2c-48.8 0-88.2-39.4-88.2-88s39.4-88 88.2-88c48.6 0 88 39.4 88 88 0 48.4-39.4 88-88 88zM872.602 872.602h-151.802v-237.602c0-56.599-1.001-129.6-79.002-129.6-78.998 0-90.998 61.8-90.998 125.6v241.601h-151.6v-488.802h145.6v66.8h2c20.2-38.4 69.802-79 143.598-79 153.805 0 182.204 101.2 182.204 232.799v268.203z"></path></svg></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="fb"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M1024 512c0-282.77-229.228-512-512-512-282.77 0-512 229.23-512 512 0 255.551 187.23 467.371 432 505.782v-357.78h-130v-148.002h130v-112.8c0-128.32 76.44-199.2 193.391-199.2 56.001 0 114.608 10 114.608 10v126h-64.558c-63.602 0-83.445 39.47-83.445 80v96h142l-22.699 148.002h-119.302v357.78c244.77-38.411 432.003-250.231 432.003-505.782z"></path></svg></a></li></ul></nav><div class="c-kPczbf c-kPczbf-idOghFk-css desktop-btn"><span><a href="https://github.com/aimhubio/aim" data-size="large" data-show-count="true" aria-label="Star aimhubio/aim on GitHub" data-text="Star"></a></span></div><button type="button" aria-label="menu" class="c-fOPBY c-fOPBY-ieBuAdh-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M0 146.286c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.289 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM0 512.008c0-33.663 27.289-60.952 60.952-60.952h902.097c33.661 0 60.951 27.29 60.951 60.952s-27.29 60.952-60.951 60.952h-902.097c-33.663 0-60.952-27.29-60.952-60.952zM60.952 816.748c-33.663 0-60.952 27.29-60.952 60.956 0 33.661 27.289 60.951 60.952 60.951h902.097c33.661 0 60.951-27.29 60.951-60.951 0-33.667-27.29-60.956-60.951-60.956h-902.097z"></path></svg></button></div></div></header><div class="c-jCnBs"><div class="c-PJLV"><div class="c-hCkBfr"><a href="/blog"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-icNlJTv-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M640 213.333c23.56 0.005 42.658 19.106 42.658 42.667 0 11.78-4.774 22.445-12.492 30.165l-225.832 225.835 225.835 225.835c7.607 7.701 12.306 18.289 12.306 29.975 0 23.562-19.1 42.662-42.662 42.662-11.687 0-22.276-4.699-29.981-12.311l0.004 0.004-256-256c-7.723-7.719-12.5-18.385-12.5-30.167s4.777-22.448 12.5-30.166l256-256c7.7-7.722 18.35-12.499 30.115-12.499 0.018 0 0.036 0 0.054 0l-0.003-0z"></path></svg><p class="c-PJLV c-PJLV-ypqAk-size-3 c-PJLV-ihjojsL-css">Go Back</p></div></a></div><div class="c-hCkBfr c-hCkBfr-ibrRWTZ-css"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-irEjuD-align-stretch c-dhzjXW-knmidH-justify-between c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-eKWVTQ-gap-5 c-dhzjXW-igyJvzA-css"><div class="c-gRRuTe"><a href="/category/tutorials"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-iUazGY-css"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M877.67 862.925h-771.994c8.090-21.811 16.077-43.52 24.166-65.229 39.834-106.086 80.077-212.070 119.194-318.362 13.722-37.171 57.549-63.795 91.546-63.693 209.306 0.922 418.611 0.717 627.917 0.205 23.654-0.102 41.984 5.939 52.838 28.16 2.662 7.68 4.403 13.517 0 25.907-47.206 129.638-96.563 263.373-143.667 393.011zM321.126 160.768c1.024 3.072 1.638 6.246 3.072 9.114 18.33 36.864 36.864 73.523 54.989 110.49 3.072 6.246 6.656 8.294 13.517 8.294 146.227-0.205 292.557-0.205 438.784-0.205 30.003 0 56.422 22.323 61.645 52.122 0.614 3.482 0 7.168 0 11.674h-12.186c-178.176 0-356.25-0.102-534.426 0-62.362 0.102-111.104 26.726-143.667 79.667-11.162 18.125-17.613 39.219-25.19 59.392-37.478 99.43-74.752 199.066-111.821 298.803-2.97 7.987-6.554 9.933-14.541 7.987-24.576-6.144-48.845-26.010-51.302-48.947v-538.522c0-46.592 34.406-49.869 49.869-49.869h271.258z"></path></svg>Tutorials</p></a></div><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap c-dhzjXW-kdofoX-gap-2"><svg class="icon " style="display:inline-block;vertical-align:middle" width="14" height="14" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M658.744 749.256l-210.744-210.746v-282.51h128v229.49l173.256 173.254zM512 0c-282.77 0-512 229.23-512 512s229.23 512 512 512 512-229.23 512-512-229.23-512-512-512zM512 896c-212.078 0-384-171.922-384-384s171.922-384 384-384c212.078 0 384 171.922 384 384s-171.922 384-384 384z"></path></svg><p class="c-PJLV c-PJLV-EDrvg-size-1">Feb 16, 2021</p></div></div><h1 class="c-PJLV c-PJLV-jszWhv-size-7 c-PJLV-ikaqJbg-css title">Aim basics: using context and subplots to compare validation and test metrics</h1></div><div class="c-hCkBfr"><div class="c-cSRvhF"><img alt="Aim basics: using context and subplots to compare validation and test metrics" title="Aim basics: using context and subplots to compare validation and test metrics" sizes="100vw" srcSet="/_next/static/chunks/images/images/dynamic/1_640_75.gif 640w, /_next/static/chunks/images/images/dynamic/1_750_75.gif 750w, /_next/static/chunks/images/images/dynamic/1_828_75.gif 828w, /_next/static/chunks/images/images/dynamic/1_1080_75.gif 1080w, /_next/static/chunks/images/images/dynamic/1_1200_75.gif 1200w, /_next/static/chunks/images/images/dynamic/1_1920_75.gif 1920w, /_next/static/chunks/images/images/dynamic/1_2048_75.gif 2048w, /_next/static/chunks/images/images/dynamic/1_3840_75.gif 3840w" src="/_next/static/chunks/images/images/dynamic/1_3840_75.gif" decoding="async" data-nimg="fill" class="card-img-top" loading="lazy" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;color:transparent"/></div></div><div class="c-hCkBfr c-hCkBfr-ibrRWTZ-css"><div class="c-hGDYPf"><p>Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.</p>
<p>The model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.</p>
<p>The validation and test sets are used to test the model on additional unseen data to verify how well it generalise.</p>
<p>Models are usually ran on validation subset after each epoch.<br>
Once the training is done, models are tested on the test subset to verify the final performance and generalisation.</p>
<p>There is a need to collect and effectively compare all these metrics.</p>
<p>Here is how to do that on <a href="https://github.com/aimhubio/aim">Aim</a></p>
<h1>Using context to track for different subsets?</h1>
<p>Use the <a href="https://github.com/aimhubio/aim#track">aim.track</a> context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.</p>
<p>![]( "Here is how it looks like on the code")</p>
<p>Once the training is ran, execute <code>aim up</code> in your terminal and start the Aim UI.</p>
<h1>Using subplots to compare test, val loss and bleu metrics</h1>
<blockquote>
<p><strong>*Note:</strong> The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*</p>
</blockquote>
<p>Let’s go step-by-step on how to break down lots of experiments using subplots.</p>
<p><strong>Step 1.</strong> Explore the runs, the context table, play with the query language.</p>
<p><img src="/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif" alt="" title="Explore the training runs"></p>
<p><strong>Step 2.</strong> Add the <code>bleu</code> metric to the Select input — query both metrics at the same time. Divide into subplots by metric.</p>
<p><img src="https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif" alt="" title="Divide into subplots by metric"></p>
<p><strong>Step 3.</strong> Search by <code>context.subset</code> to show both <code>test</code> and <code>val</code> <code>loss</code> and <code>bleu</code> metrics. Divide into subplots further by <code>context.subset</code> too so Aim UI shows <code>test</code> and <code>val</code> metrics on different subplots for better comparison.</p>
<p><img src="https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif" alt="" title="Divide into subplots by context / subset"></p>
<p>Not it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.</p>
<h1>Summary</h1>
<p>Here is a full summary video on how to do it on the UI.</p>
<p><img src="https://www.youtube.com/watch?v=DGI8S7SUfEk" alt=""></p>
<h1>Learn More</h1>
<p>If you find Aim useful, support us and <a href="https://github.com/aimhubio/aim">star the project</a> on GitHub. Join the <a href="https://slack.aimstack.io/">Aim community</a> and share more about your use-cases and how we can improve Aim to suit them.</p></div><div class="c-cnJPJj"><button aria-label="twitter" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M322.14 927.976c386.322 0 597.681-320.14 597.681-597.678 0-9-0.199-18.2-0.604-27.2 41.116-29.734 76.601-66.565 104.782-108.76-38.292 17.037-78.95 28.164-120.579 33 43.833-26.275 76.654-67.553 92.381-116.18-41.24 24.439-86.339 41.679-133.363 50.98-31.685-33.666-73.577-55.957-119.199-63.427s-92.44 0.299-133.206 22.103c-40.766 21.805-73.211 56.432-92.324 98.527s-23.826 89.314-13.411 134.357c-83.5-4.19-165.188-25.881-239.767-63.667s-140.386-90.823-193.153-155.673c-26.819 46.239-35.025 100.955-22.952 153.027s43.521 97.594 87.952 127.313c-33.356-1.059-65.981-10.040-95.18-26.2v2.6c-0.030 48.525 16.745 95.562 47.475 133.116 30.729 37.552 73.515 63.309 121.085 72.886-30.899 8.451-63.328 9.685-94.78 3.6 13.424 41.731 39.54 78.228 74.706 104.405 35.166 26.171 77.626 40.712 121.454 41.591-74.408 58.449-166.322 90.155-260.94 90.004-16.78-0.027-33.543-1.056-50.2-3.083 96.122 61.666 207.937 94.424 322.14 94.359z"></path></svg></button><button aria-label="linkedin" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M948.201 0h-872.601c-41.8 0-75.6 33-75.6 73.8v876.199c0 40.801 33.8 74.001 75.6 74.001h872.601c41.796 0 75.799-33.2 75.799-73.802v-876.398c0-40.8-34.002-73.8-75.799-73.8zM303.8 872.602h-152v-488.802h152v488.802zM227.8 317.2c-48.8 0-88.2-39.4-88.2-88s39.4-88 88.2-88c48.6 0 88 39.4 88 88 0 48.4-39.4 88-88 88zM872.602 872.602h-151.802v-237.602c0-56.599-1.001-129.6-79.002-129.6-78.998 0-90.998 61.8-90.998 125.6v241.601h-151.6v-488.802h145.6v66.8h2c20.2-38.4 69.802-79 143.598-79 153.805 0 182.204 101.2 182.204 232.799v268.203z"></path></svg></button><button aria-label="facebook" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M1024 512c0-282.77-229.228-512-512-512-282.77 0-512 229.23-512 512 0 255.551 187.23 467.371 432 505.782v-357.78h-130v-148.002h130v-112.8c0-128.32 76.44-199.2 193.391-199.2 56.001 0 114.608 10 114.608 10v126h-64.558c-63.602 0-83.445 39.47-83.445 80v96h142l-22.699 148.002h-119.302v357.78c244.77-38.411 432.003-250.231 432.003-505.782z"></path></svg></button><button aria-label="reddit" style="background-color:transparent;border:none;padding:0;font:inherit;color:inherit;cursor:pointer;outline:none"><svg class="icon " style="display:inline-block;vertical-align:middle" width="16" height="16" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M1024 502.571c0-62.251-50.859-112.853-113.365-112.853-30.507 0-58.155 12.203-78.507 31.829-77.227-50.816-181.717-83.157-297.429-87.296l63.275-199.211 171.349 40.149-0.256 2.475c0 50.901 41.6 92.288 92.757 92.288 51.115 0 92.672-41.387 92.672-92.288s-41.6-92.331-92.672-92.331c-39.253 0-72.704 24.491-86.229 58.837l-184.704-43.307c-8.064-1.963-16.256 2.688-18.773 10.624l-70.571 222.165c-121.088 1.451-230.784 34.048-311.467 86.4-20.224-18.688-47.061-30.379-76.757-30.379-62.507 0-113.323 50.645-113.323 112.896 0 41.387 22.741 77.269 56.192 96.896-2.219 12.032-3.669 24.192-3.669 36.565 0 166.869 205.141 302.635 457.344 302.635s457.387-135.765 457.387-302.635c0-11.691-1.237-23.211-3.2-34.56 35.499-19.072 59.947-55.979 59.947-98.901zM289.109 580.053c0-37.035 30.293-67.2 67.499-67.2s67.456 30.165 67.456 67.2-30.251 67.157-67.456 67.157-67.499-30.123-67.499-67.157zM675.712 779.264c-34.005 33.835-87.381 50.304-163.157 50.304l-0.555-0.128-0.555 0.128c-75.819 0-129.195-16.469-163.157-50.304-6.187-6.144-6.187-16.171 0-22.315 6.187-6.187 16.256-6.187 22.443 0 27.733 27.605 73.771 41.003 140.715 41.003l0.555 0.128 0.555-0.128c66.944 0 112.981-13.44 140.715-41.045 6.187-6.187 16.256-6.144 22.443 0 6.187 6.187 6.187 16.171 0 22.357zM667.648 647.211c-37.205 0-67.456-30.123-67.456-67.157s30.251-67.2 67.456-67.2 67.456 30.165 67.456 67.2-30.251 67.157-67.456 67.157z"></path></svg></button></div></div><div class="c-bjrmKQ"><div class="c-hCkBfr c-hCkBfr-ibrRWTZ-css"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-irEjuD-align-stretch c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap"><div class="c-hdKdh"><a href="/blog/aim-3-1-%E2%80%94-images-tracker-and-images-explorer"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-awKDG-justify-start c-dhzjXW-kVNAnR-wrap-noWrap"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M360.96 475.802l-36.198 36.198 289.638 289.638 72.397-72.397-217.19-217.242 217.19-217.242-72.397-72.397z"></path></svg><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-ihjojsL-css chevron-text">PREVIOUS POST</p></div><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-cllNQK-lineClamp-true c-PJLV-idnFJmc-css text">Aim 3.1 — Images Tracker and Images Explorer</p></a></div><div class="c-eVsMjz"><a href="/blog/aim-v2-2-0-%E2%80%94-hugging-face-integration"><div class="c-dhzjXW c-dhzjXW-ejCoEP-direction-row c-dhzjXW-jroWjL-align-center c-dhzjXW-bZmKkd-justify-end c-dhzjXW-kVNAnR-wrap-noWrap"><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-ihjojsL-css chevron-text">NEXT POST</p><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M663.040 548.198l36.198-36.198-289.638-289.638-72.397 72.397 217.19 217.242-217.19 217.242 72.397 72.397 253.44-253.44z"></path></svg></div><p class="c-PJLV c-PJLV-EDrvg-size-1 c-PJLV-cllNQK-lineClamp-true c-PJLV-idnFJmc-css text">Aim v2.2.0 — Hugging Face integration</p></a></div></div></div></div></div></div><footer class="c-dRMHzO"><div class="c-hCkBfr"><div class="c-FsNLu"><div class="c-eJqRfT"><a class="logo" href="/"><picture><source height="26" width="109" media="(max-width: 1199px)" srcSet="/images/static/main/aim-logo-resp.svg"/><img height="26" width="26" src="/images/static/main/aim-logo.svg" alt="Aimstack"/></picture></a></div><ul class="c-jJiyUf"><li><a target="_self" href="/#quick-start">Quick start</a></li><li><a target="_self" href="/#features">Features</a></li><li><a target="_self" href="/#demos">Demos</a></li><li><a target="_blank" href="https://aimstack.readthedocs.io/en/latest/">Docs</a></li><li><a target="_self" href="/pricing">Pricing</a></li><li><a target="_self" href="/blog">Blog</a></li><li><a target="_self" href="/about-us">About Us</a></li><li><a target="_blank" href="/blog/aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics#">Career</a></li></ul><ul class="c-MNZuo"><li><a href="https://aimstack.slack.com/ssb/redirect#/shared-invite/email" rel="noopener noreferrer" target="_blank" aria-label="slack"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M380.907 539.056c-57.742 0-104.574 46.837-104.574 104.585v261.609c0 57.748 46.832 104.585 104.574 104.585s104.574-46.837 104.574-104.585v-261.609c-0.042-57.748-46.874-104.585-104.574-104.585z"></path><path style="fill:white" d="M15.111 643.63c0 57.799 46.874 104.687 104.659 104.687s104.659-46.888 104.659-104.687v-104.685h-104.577c-0.042 0-0.042 0-0.082 0-57.785 0-104.659 46.886-104.659 104.685z"></path><path style="fill:white" d="M381.003 14.167c-0.042 0-0.083 0-0.125 0-57.783 0-104.656 46.886-104.656 104.684s46.874 104.685 104.656 104.685h104.574v-104.685c0-0.041 0-0.124 0-0.207-0.042-57.715-46.791-104.477-104.449-104.477z"></path><path style="fill:white" d="M118.88 485.95h262.080c57.784 0 104.657-46.892 104.657-104.697s-46.874-104.697-104.657-104.697h-262.080c-57.784 0-104.658 46.892-104.658 104.697s46.874 104.697 104.658 104.697z"></path><path style="fill:white" d="M904.118 276.5c-57.702 0-104.454 46.767-104.454 104.49v104.905h104.573c57.788 0 104.658-46.892 104.658-104.698s-46.871-104.697-104.658-104.697c-0.040 0-0.080 0-0.119 0z"></path><path style="fill:white" d="M538.444 118.795v262.365c0 57.741 46.834 104.573 104.577 104.573 57.737 0 104.573-46.832 104.573-104.573v-262.365c0-57.741-46.837-104.573-104.573-104.573-57.742 0-104.577 46.832-104.577 104.573z"></path><path style="fill:white" d="M747.594 905.028c0-57.748-46.837-104.585-104.573-104.585h-104.577v104.67c0.042 57.702 46.834 104.499 104.577 104.499 57.737 0 104.573-46.837 104.573-104.585z"></path><path style="fill:white" d="M905.068 538.944h-262.076c-57.788 0-104.659 46.886-104.659 104.685s46.871 104.687 104.659 104.687h262.076c57.788 0 104.658-46.888 104.658-104.687s-46.871-104.685-104.658-104.685z"></path></svg></a></li><li><a href="https://twitter.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="twitter"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M322.14 927.976c386.322 0 597.681-320.14 597.681-597.678 0-9-0.199-18.2-0.604-27.2 41.116-29.734 76.601-66.565 104.782-108.76-38.292 17.037-78.95 28.164-120.579 33 43.833-26.275 76.654-67.553 92.381-116.18-41.24 24.439-86.339 41.679-133.363 50.98-31.685-33.666-73.577-55.957-119.199-63.427s-92.44 0.299-133.206 22.103c-40.766 21.805-73.211 56.432-92.324 98.527s-23.826 89.314-13.411 134.357c-83.5-4.19-165.188-25.881-239.767-63.667s-140.386-90.823-193.153-155.673c-26.819 46.239-35.025 100.955-22.952 153.027s43.521 97.594 87.952 127.313c-33.356-1.059-65.981-10.040-95.18-26.2v2.6c-0.030 48.525 16.745 95.562 47.475 133.116 30.729 37.552 73.515 63.309 121.085 72.886-30.899 8.451-63.328 9.685-94.78 3.6 13.424 41.731 39.54 78.228 74.706 104.405 35.166 26.171 77.626 40.712 121.454 41.591-74.408 58.449-166.322 90.155-260.94 90.004-16.78-0.027-33.543-1.056-50.2-3.083 96.122 61.666 207.937 94.424 322.14 94.359z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/aimstackio/" rel="noopener noreferrer" target="_blank" aria-label="linkedIn"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M948.201 0h-872.601c-41.8 0-75.6 33-75.6 73.8v876.199c0 40.801 33.8 74.001 75.6 74.001h872.601c41.796 0 75.799-33.2 75.799-73.802v-876.398c0-40.8-34.002-73.8-75.799-73.8zM303.8 872.602h-152v-488.802h152v488.802zM227.8 317.2c-48.8 0-88.2-39.4-88.2-88s39.4-88 88.2-88c48.6 0 88 39.4 88 88 0 48.4-39.4 88-88 88zM872.602 872.602h-151.802v-237.602c0-56.599-1.001-129.6-79.002-129.6-78.998 0-90.998 61.8-90.998 125.6v241.601h-151.6v-488.802h145.6v66.8h2c20.2-38.4 69.802-79 143.598-79 153.805 0 182.204 101.2 182.204 232.799v268.203z"></path></svg></a></li><li><a href="https://www.facebook.com/aimstackio" rel="noopener noreferrer" target="_blank" aria-label="fb"><svg class="icon " style="display:inline-block;vertical-align:middle" width="20" height="20" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path style="fill:white" d="M1024 512c0-282.77-229.228-512-512-512-282.77 0-512 229.23-512 512 0 255.551 187.23 467.371 432 505.782v-357.78h-130v-148.002h130v-112.8c0-128.32 76.44-199.2 193.391-199.2 56.001 0 114.608 10 114.608 10v126h-64.558c-63.602 0-83.445 39.47-83.445 80v96h142l-22.699 148.002h-119.302v357.78c244.77-38.411 432.003-250.231 432.003-505.782z"></path></svg></a></li></ul></div><div class="c-cChYXC"><p class="c-PJLV c-PJLV-EDrvg-size-1">Copyright © <!-- -->2023<!-- --> Aimstack</p></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that on [Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\nUse the [aim.track](https://github.com/aimhubio/aim#track) context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n![](\u003cscript src=\"https://gist.github.com/SGevorg/e08524b3538d3f71d14bf1857a7bc6e9.js\"\u003e\u003c/script\u003e \"Here is how it looks like on the code\")\n\nOnce the training is ran, execute `aim up` in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n\u003e **\\*Note:** The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLet’s go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.** Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.** Add the `bleu` metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.** Search by `context.subset` to show both `test` and `val` `loss` and `bleu` metrics. Divide into subplots further by `context.subset` too so Aim UI shows `test` and `val` metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\nHere is a full summary video on how to do it on the UI.\n\n![](https://www.youtube.com/watch?v=DGI8S7SUfEk)\n\n# Learn More\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://slack.aimstack.io/) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003cp\u003eResearchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\u003c/p\u003e\n\u003cp\u003eThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\u003c/p\u003e\n\u003cp\u003eThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\u003c/p\u003e\n\u003cp\u003eModels are usually ran on validation subset after each epoch.\u003cbr\u003e\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\u003c/p\u003e\n\u003cp\u003eThere is a need to collect and effectively compare all these metrics.\u003c/p\u003e\n\u003cp\u003eHere is how to do that on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003eUsing context to track for different subsets?\u003c/h1\u003e\n\u003cp\u003eUse the \u003ca href=\"https://github.com/aimhubio/aim#track\"\u003eaim.track\u003c/a\u003e context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\u003c/p\u003e\n\u003cp\u003e![]( \"Here is how it looks like on the code\")\u003c/p\u003e\n\u003cp\u003eOnce the training is ran, execute \u003ccode\u003eaim up\u003c/code\u003e in your terminal and start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eUsing subplots to compare test, val loss and bleu metrics\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e*Note:\u003c/strong\u003e The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet’s go step-by-step on how to break down lots of experiments using subplots.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1.\u003c/strong\u003e Explore the runs, the context table, play with the query language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2.\u003c/strong\u003e Add the \u003ccode\u003ebleu\u003c/code\u003e metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3.\u003c/strong\u003e Search by \u003ccode\u003econtext.subset\u003c/code\u003e to show both \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003ebleu\u003c/code\u003e metrics. Divide into subplots further by \u003ccode\u003econtext.subset\u003c/code\u003e too so Aim UI shows \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e metrics on different subplots for better comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"\u003e\u003c/p\u003e\n\u003cp\u003eNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\u003c/p\u003e\n\u003ch1\u003eSummary\u003c/h1\u003e\n\u003cp\u003eHere is a full summary video on how to do it on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://www.youtube.com/watch?v=DGI8S7SUfEk\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},"posts":[{"title":"Aim 1.3.5 — Activity View and X-axis alignment","date":"2021-02-05T11:16:41.748Z","author":"Gev Soghomonian","description":"Aim v1.3.5 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open source MLOps tools. Check out the new features […]","slug":"aim-1-3-5-—-activity-view-and-x-axis-alignment","image":"/images/dynamic/1st.gif","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n## Aim Release — Activity View and X-axis alignment\n\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is now available. Thanks to the incredible [Aim community](https://discord.com/invite/zXq2NfVdtF) for the feedback and support on building democratized open source AI dev tools.\n\nCheck out the new features at play.aimstack.io\n\nHere are the highlights of the features:\n\n## Activity View on the Dashboard\n\nThe **Activity View** shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away\n\n**Statistics** displays the overall count of [Experiments and Runs. ](https://github.com/aimhubio/aim#concepts)\n\n![](/images/dynamic/1st.gif \"The new dashboard in action!\")\n\n## **X-axis alignment by epoch**\n\nX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.\n\n![](/images/dynamic/2nd.gif \"No more misaligned metrics!\")\n\n## **Ordering runs both on Explore and on Dashboard**\n\n\n\nNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\n\nOn Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\n\n![](/images/dynamic/3.gif \"Sort the runs both on Dashboard and Explore!\")\n\n## Learn More\n\n\n\nWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\n\nWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month 😊). Try out Aim, join the [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support 🙌.","html":"\u003ch2\u003eAim Release — Activity View and X-axis alignment\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e v1.3.5 is now available. Thanks to the incredible \u003ca href=\"https://discord.com/invite/zXq2NfVdtF\"\u003eAim community\u003c/a\u003e for the feedback and support on building democratized open source AI dev tools.\u003c/p\u003e\n\u003cp\u003eCheck out the new features at play.aimstack.io\u003c/p\u003e\n\u003cp\u003eHere are the highlights of the features:\u003c/p\u003e\n\u003ch2\u003eActivity View on the Dashboard\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eActivity View\u003c/strong\u003e shows the daily experiment runs. With one click you can search each day’s runs and explore them straight away\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStatistics\u003c/strong\u003e displays the overall count of \u003ca href=\"https://github.com/aimhubio/aim#concepts\"\u003eExperiments and Runs. \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1st.gif\" alt=\"\" title=\"The new dashboard in action!\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eX-axis alignment by epoch\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eX-axis alignment adds another layer of superpower for metric comparison. If you have tracked metrics in different time-frequencies (e.g. train loss — 1000 steps, validation loss — 15 steps) then you can align them by epoch, relative time or absolute time.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/2nd.gif\" alt=\"\" title=\"No more misaligned metrics!\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eOrdering runs both on Explore and on Dashboard\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNow you can order runs both on the Dashboard and on the Explore. Sort the runs by a hyperparam value of metric value on dashboard and explore.\u003c/p\u003e\n\u003cp\u003eOn Explore, when sorted by a hyperparam that’s also used for dividing into subplots, the subplots will be positioned by the hyperparam value too.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/3.gif\" alt=\"\" title=\"Sort the runs both on Dashboard and Explore!\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eWe have been working on Aim for the past 6 months and excited to see this much interest from the community.\u003c/p\u003e\n\u003cp\u003eWe are working tirelessly for adding new community-driven features to Aim (and those features have been doubling every month 😊). Try out Aim, join the \u003ca href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e"},"_id":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","_raw":{"sourceFilePath":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileName":"aim-1-3-5-—-activity-view-and-x-axis-alignment.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-1-3-5-—-activity-view-and-x-axis-alignment"},"type":"Post"},{"title":"Aim 1.3.8 — Enhanced Context Table and Advanced Group Coloring","date":"2021-03-01T11:09:06.858Z","author":"Gev Soghomonian","description":"Aim 1.3.8 featuring Advanced Group Coloring for AI metrics  is now available. ","slug":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring","image":"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp","draft":false,"categories":["New Releases"],"body":{"raw":"\n\n[Aim](https://aimstack.io/) 1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\n\nCheck out the new features at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=).\n\nHere are the more notable changes:\n\n### Enhanced Context Table\n\n\n\nThe context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\n\nSo we have made 3 changes to tackle this problem\n\n### New table groups view\n\nBelow is the new modified look of the table. Here is what’s changed:\n\n* The empty per group is removed\n* In addition to the group details popover, there is an in-place list group config is available for instant lookup\n\n![](https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp \"New improved Aim Context Table on Explore\")\n\n\n\n* **\\[In Progress for next release]** Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\n\n### Column Management\n\n\n\nThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\n\nYou can also re-order the columns by dragging the mid-section up/down.\n\nThis is super-handy when dealing with 100s of columns on the table for parameters.\n\n![](https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp \"Drag columns left and right to pin, search and disable less important ones\")\n\n\n\n### Advanced Group Coloring\n\n\n\nThis is the latest iteration over the group coloring.\n\n* Now Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\n* You can shuffle the colors in case they aren’t pick to your liking\n* You can pick two different color palette for better distribution of the colors.\n\n  ![](https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp \"Effective coloring of the groups is important!\")\n\n  ### Thanks to\n* [mahnerak](https://github.com/mahnerak), Lars, [Joe](https://github.com/jafioti) for detailed feedback — helped us immensely in this iteration.\n\n  ### Learn More\n\n  We have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\n\n  Try out [Aim](https://aimstack.io/), join the [Aim community](https://slack.aimstack.io/), share your feedback.\n\n  We are building the next generation of democratized AI dev tools.\n\n  And don’t forget to leave [Aim](https://aimstack.io/) a star on GitHub for support 🙌.","html":"\u003cp\u003e\u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e 1.3.8 is now available. Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools\u003c/p\u003e\n\u003cp\u003eCheck out the new features at \u003ca href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjp0cnVlLCJzdGVwIjoxNCwicnVuSGFzaCI6IjRiY2Y4ZDUyLWZjYTgtMTFlYS05MDJiLTBhMTk1NDdlYmIyZSIsIm1ldHJpY05hbWUiOiJsb3NzIiwidHJhY2VDb250ZXh0IjoiZXlKemRXSnpaWFFpT2lKMFpYTjBJbjAifX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCA9PSB0ZXN0IiwidiI6MX0sImNvbnRleHRGaWx0ZXIiOnsiZ3JvdXBCeUNvbG9yIjpbInBhcmFtcy5kYXRhc2V0LnByZXByb2MiXSwiZ3JvdXBCeVN0eWxlIjpbXSwiZ3JvdXBCeUNoYXJ0IjpbIm1ldHJpYyJdLCJhZ2dyZWdhdGVkIjpmYWxzZSwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere are the more notable changes:\u003c/p\u003e\n\u003ch3\u003eEnhanced Context Table\u003c/h3\u003e\n\u003cp\u003eThe context table used to not use the screen real-estate effectively — an empty line per grouping, non-efficient column management (imagine you have 200 columns to deal with).\u003c/p\u003e\n\u003cp\u003eSo we have made 3 changes to tackle this problem\u003c/p\u003e\n\u003ch3\u003eNew table groups view\u003c/h3\u003e\n\u003cp\u003eBelow is the new modified look of the table. Here is what’s changed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe empty per group is removed\u003c/li\u003e\n\u003cli\u003eIn addition to the group details popover, there is an in-place list group config is available for instant lookup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*l27Xjb6pYIMdEkABLkRBRA.webp\" alt=\"\" title=\"New improved Aim Context Table on Explore\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e[In Progress for next release]\u003c/strong\u003e Column resize — this will allow to manage the wide columns and free much-needed screen real-estate for the data that matters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eColumn Management\u003c/h3\u003e\n\u003cp\u003eThis feature allows to seamlessly pin the important columns to both sides of the table and hide the useless columns in between.\u003c/p\u003e\n\u003cp\u003eYou can also re-order the columns by dragging the mid-section up/down.\u003c/p\u003e\n\u003cp\u003eThis is super-handy when dealing with 100s of columns on the table for parameters.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*71RL39uLxxr3vnxMhn1uKg.webp\" alt=\"\" title=\"Drag columns left and right to pin, search and disable less important ones\"\u003e\u003c/p\u003e\n\u003ch3\u003eAdvanced Group Coloring\u003c/h3\u003e\n\u003cp\u003eThis is the latest iteration over the group coloring.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNow Aim enables persistent coloring mode which makes sure the group with same configuration always receives the same color.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can shuffle the colors in case they aren’t pick to your liking\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can pick two different color palette for better distribution of the colors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*2pIoR-ZkaPsQ90UQh2W7Fg.webp\" alt=\"\" title=\"Effective coloring of the groups is important!\"\u003e\u003c/p\u003e\n\u003ch3\u003eThanks to\u003c/h3\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mahnerak\"\u003emahnerak\u003c/a\u003e, Lars, \u003ca href=\"https://github.com/jafioti\"\u003eJoe\u003c/a\u003e for detailed feedback — helped us immensely in this iteration.\u003c/p\u003e\n\u003ch3\u003eLearn More\u003c/h3\u003e\n\u003cp\u003eWe have been working on Aim for the past 6 months and excited to see the overwhelming interest from the community.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eWe are building the next generation of democratized AI dev tools.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://aimstack.io/\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e"},"_id":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","_raw":{"sourceFilePath":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileName":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-1-3-8-—-enhanced-context-table-and-advanced-group-coloring"},"type":"Post"},{"title":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","date":"2021-05-18T13:58:29.035Z","author":"Gev Soghomonian","description":"Aim 2.4.0 is out! XGBoost integration, Confidence interval aggregation and lots of UI performance improvements!","slug":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) 2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\n\nCheck out the updated Aim at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\n\nFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes [here](https://github.com/aimhubio/aim/milestone/6?closed=1).\n\n## XGBoost Inegration\n\n\n\nTo begin with,  `aim.callback` is now available that exports `AimCallback` to be passed to the `xgb.train` as a callback to log the experiments.\n\nCheck out this [blogpost](https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/) for additional details on how to integrate Aim to your XGBoost code.\n\n## Confidence Interval as the aggregation method\n\n![](https://aimstack.io/wp-content/uploads/2022/02/confidence_interval.png)\n\nNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups. \n\n## Learn More\n\n\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nCheck out the latest [Aim integration](https://aimstack.io/aim-v2-2-0-hugging-face-integration/)!\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e 2.4.0 featuring XGBoost Integration is out ! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools.\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim at \u003ca href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor this release, there have been lots of performance updates and small tweaks to the UI. Above all, this post highlights the two features of Aim 2.4.0. Please see the full list of changes \u003ca href=\"https://github.com/aimhubio/aim/milestone/6?closed=1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eXGBoost Inegration\u003c/h2\u003e\n\u003cp\u003eTo begin with,  \u003ccode\u003eaim.callback\u003c/code\u003e is now available that exports \u003ccode\u003eAimCallback\u003c/code\u003e to be passed to the \u003ccode\u003exgb.train\u003c/code\u003e as a callback to log the experiments.\u003c/p\u003e\n\u003cp\u003eCheck out this \u003ca href=\"https://aimstack.io/blog/tutorials/an-end-to-end-example-of-aim-logger-used-with-xgboost-library/\"\u003eblogpost\u003c/a\u003e for additional details on how to integrate Aim to your XGBoost code.\u003c/p\u003e\n\u003ch2\u003eConfidence Interval as the aggregation method\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/confidence_interval.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can use confidence intervals for aggregation. The community has been requesting this feature, since it completes the list of necessary aggregation methods. As a result, if you have outlier metrics in your group, confidence interval will show logical aggregation and get better comparison of the groups. \u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eCheck out the latest \u003ca href=\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\"\u003eAim integration\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","_raw":{"sourceFilePath":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileName":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-2-4-0-is-out-xgboost-integration-confidence-interval-aggregation-and-lots-of-ui-performance-improvements"},"type":"Post"},{"title":"Aim 3.1 — Images Tracker and Images Explorer","date":"2021-11-25T14:57:05.232Z","author":"Gev Soghomonian","description":"Excited to share with you the Aim 3.1! 😊 A huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community","slug":"aim-3-1-—-images-tracker-and-images-explorer","image":"https://aimstack.io/wp-content/uploads/2021/08/aim-scaled.jpeg","draft":false,"categories":["New Releases"],"body":{"raw":"Excited to share with you the Aim 3.1! \n\nA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\n\nWe have added a new demo to showcase the new Explore Images feature. We have also forked the [lightweight-gan from lucidrains](https://github.com/lucidrains/lightweight-gan) and added the Aim integration.\n\nDemo code: \u003chttps://github.com/aimhubio/lightweight-gan\u003e\\\nDemo site: [play.aimstack.io:10002](http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq\u0026select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS\u0026images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK)\n\n**The notable changes in the Aim 3.1.0:**\n\n* Images tracker and explorer\n* Runs navigation from the single run page\n* Performance improvements and tests\n\nThanks to srikanth, Mohamad Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\n\n## Images Tracker and Explorer\n\nFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\n\nAll the regular grouping and search goodies are available on Images Explorer too \n\nAdd an image tracker in your training code like this:\n\n```\nrun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n```\n\nOnce you run your training, the images will be saved and become available on the Images Explorer tab.\n\nOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\n\nHere is a quick promo video of what it can do. The rest of the details can be found in the docs[ here](https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html).\n\n![](https://aimstack.io/wp-content/uploads/2021/08/1_3IkmtAOLGy-e3eJneuv2bA.png)\n\nA quick demo video.\n\n\u003chttps://www.youtube.com/watch?v=4mtUFV8yG_o\u0026feature=youtu.be\u003e\n\nThe Images view on the single run page is coming soon …\n\n## Runs Navigation\n\nOne of the regressions from the Aim 2.x to [3.0.0](https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/) was the ability to navigate between the runs of the same experiment from the single run page.\n\n![](https://aimstack.io/wp-content/uploads/2021/08/111.png)\n\nWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\n\nSorry for the regression \n\n## Performance improvements and assurances\n\nTwo groups of performance improvements have been made— on the UI and on the SDK side.\n\n### **Column virtualization on Aim UI Table**\n\nThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\n\nWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem — and this is a key step towards it. \n\n### **Storage performance tests**\n\nOne of the unique challenges in building Aim is to make sure each newly added data type won’t impact the overall performance. We have added continuously add more performance tests to ensure the new releases don’t impact the performance.\n\nIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\n\nPerformance reliability is key for Aim and this is a major iteration towards that!\n\n## Learn more\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003eExcited to share with you the Aim 3.1! \u003c/p\u003e\n\u003cp\u003eA huge milestone on our journey towards democratizing AI dev tools. Thanks to the awesome Aim community for testing this early and sharing issues, feedback.\u003c/p\u003e\n\u003cp\u003eWe have added a new demo to showcase the new Explore Images feature. We have also forked the \u003ca href=\"https://github.com/lucidrains/lightweight-gan\"\u003elightweight-gan from lucidrains\u003c/a\u003e and added the Aim integration.\u003c/p\u003e\n\u003cp\u003eDemo code: \u003ca href=\"https://github.com/aimhubio/lightweight-gan\"\u003ehttps://github.com/aimhubio/lightweight-gan\u003c/a\u003e\u003cbr\u003e\nDemo site: \u003ca href=\"http://play.aimstack.io:10002/images?grouping=fkjeugYpZXU3Qy6AUQLGWaYfh7MDvRvP3eTRyMMzRUsZhgqFHwn98jPLEyjZEG4Vxj4XGRnBpUZJoD1XJ6vS7mw7twViU5iQ7AD1GqnqrTZTToPUkfrq\u0026#x26;select=2qYYRs1i7GzkG9QbRNTbd8ZwMcueRtLqTWPLqhUQ9L4eeRJgEVkmbjfhnh9n2iZACZFGDuLzqGWHq53o4QQcp9wxb9oJnqhPZPp6bmro8UFDikUqmemyHqTP5q7dyaNPSxhuX7TrLQcsKKbDVKX4gXeJa8q95cTNa6Z9PrNSupd5teqBqStbnAuDzEzSfCjTqzG39A5VzoHGYxWjhc242HSowezeq7wHTSqMz8FbKfrBpWgtsbcdvmJKefZQm4eiMLw6zhwcSCJ1LnqKtJ4VpVtG1mu8NBukKAXVUoCDfprjdTF8my4Z8avf4VG4LCVZTCfL6aKT3FH6rThUDCdtCufS97jkWnAwgP36BjYq4vTMCmJbUcKfUnwGmfDEnGKe7ESSNyC6at6YG2eHJZTqvYGzErTVoEFG7MrSUFzA5QqBvuGzw6bibP4WtjJrDqqQojHhMJ7379dsm3azQHCAjyPsJaNR1XSNAiCSPzCzQ6nuRqdqhqkMPsfzUxB7DTZM6fGFx78P4PCaJWw3537GhX7q4t6tUz5tc7jtc4zfMqeX8TapZ39amp1hxaNjDpjPPCgi7st5BPPtHZHS\u0026#x26;images=C9LyLE4XToLCLCAZ2wfH7CYgUFiTvbGGA4eYrDP2ByUR1VMjkinEgCM6CChQcwzwrNpeYxTwam78SGQqMNgqQTVho6CWTVfzxoy8cPxH74shAmToyAGjPpfgMiGqx1jD5BbHP9ZY99kKwYMXwTLEpnyiGa63gaoEfwJ2j2SA9EPC17tTaUEeK5PViLNb4CLzJ7GHxA4Bua1XSVHoZxcyU4RK\"\u003eplay.aimstack.io:10002\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe notable changes in the Aim 3.1.0:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImages tracker and explorer\u003c/li\u003e\n\u003cli\u003eRuns navigation from the single run page\u003c/li\u003e\n\u003cli\u003ePerformance improvements and tests\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThanks to srikanth, Mohamad Elgaar, Mahnerak, Andrew Xu and Bo yu for testing, raising issues and overall support.\u003c/p\u003e\n\u003ch2\u003eImages Tracker and Explorer\u003c/h2\u003e\n\u003cp\u003eFinally we are shipping the highly requested feature for images tracking. Now you can track images during training and evaluation to explore them on the Aim UI.\u003c/p\u003e\n\u003cp\u003eAll the regular grouping and search goodies are available on Images Explorer too \u003c/p\u003e\n\u003cp\u003eAdd an image tracker in your training code like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun.track(Image(tensor_or_pil, caption), name='gen', step=step, context={ \"subset\": \"train\" })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you run your training, the images will be saved and become available on the Images Explorer tab.\u003c/p\u003e\n\u003cp\u003eOn Images Explorer you can filter and group by the images all the params available (just like you would do for metrics). As a result, you get lots of flexibility to compare the runs via tracked images.\u003c/p\u003e\n\u003cp\u003eHere is a quick promo video of what it can do. The rest of the details can be found in the docs\u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/pages/explorers.html\"\u003e here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/08/1_3IkmtAOLGy-e3eJneuv2bA.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eA quick demo video.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=4mtUFV8yG_o\u0026#x26;feature=youtu.be\"\u003ehttps://www.youtube.com/watch?v=4mtUFV8yG_o\u0026#x26;feature=youtu.be\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe Images view on the single run page is coming soon …\u003c/p\u003e\n\u003ch2\u003eRuns Navigation\u003c/h2\u003e\n\u003cp\u003eOne of the regressions from the Aim 2.x to \u003ca href=\"https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/\"\u003e3.0.0\u003c/a\u003e was the ability to navigate between the runs of the same experiment from the single run page.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/08/111.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eWe have added that back as the top-most dropdown with the list of all experiments and their corresponding runs for easy navigation.\u003c/p\u003e\n\u003cp\u003eSorry for the regression \u003c/p\u003e\n\u003ch2\u003ePerformance improvements and assurances\u003c/h2\u003e\n\u003cp\u003eTwo groups of performance improvements have been made— on the UI and on the SDK side.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eColumn virtualization on Aim UI Table\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe ability to smoothly navigate through metrics and be able to interact with a table is surely the ultimate experience we aim to provide.\u003c/p\u003e\n\u003cp\u003eWe are really proud of our craft and aim to make the best possible usage experience for the experiment comparison problem — and this is a key step towards it. \u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eStorage performance tests\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eOne of the unique challenges in building Aim is to make sure each newly added data type won’t impact the overall performance. We have added continuously add more performance tests to ensure the new releases don’t impact the performance.\u003c/p\u003e\n\u003cp\u003eIn the past few weeks we learned that users like to record LOTS (up to millions) of steps for their metrics which opened up room for performance improvements of Aim, haha.\u003c/p\u003e\n\u003cp\u003ePerformance reliability is key for Aim and this is a major iteration towards that!\u003c/p\u003e\n\u003ch2\u003eLearn more\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"aim-3-1-—-images-tracker-and-images-explorer.md","_raw":{"sourceFilePath":"aim-3-1-—-images-tracker-and-images-explorer.md","sourceFileName":"aim-3-1-—-images-tracker-and-images-explorer.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-3-1-—-images-tracker-and-images-explorer"},"type":"Post"},{"title":"Aim basics: using context and subplots to compare validation and test metrics","date":"2021-02-16T12:18:06.528Z","author":"Gev Soghomonian","description":"Validation and test metrics comparison is a crucial step in ML experiments. ML researchers divide datasets into three subsets — train, validation and test so they can test their model","slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics","image":"/images/dynamic/1.gif","draft":false,"categories":["Tutorials"],"body":{"raw":"Researchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\n\nThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\n\nThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\n\nModels are usually ran on validation subset after each epoch.\\\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\n\nThere is a need to collect and effectively compare all these metrics.\n\nHere is how to do that on [Aim](https://github.com/aimhubio/aim)\n\n# Using context to track for different subsets?\n\nUse the [aim.track](https://github.com/aimhubio/aim#track) context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\n\n![](\u003cscript src=\"https://gist.github.com/SGevorg/e08524b3538d3f71d14bf1857a7bc6e9.js\"\u003e\u003c/script\u003e \"Here is how it looks like on the code\")\n\nOnce the training is ran, execute `aim up` in your terminal and start the Aim UI.\n\n# Using subplots to compare test, val loss and bleu metrics\n\n\u003e **\\*Note:** The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\n\nLet’s go step-by-step on how to break down lots of experiments using subplots.\n\n**Step 1.** Explore the runs, the context table, play with the query language.\n\n![](/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif \"Explore the training runs\")\n\n**Step 2.** Add the `bleu` metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\n\n![](https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif \"Divide into subplots by metric\")\n\n**Step 3.** Search by `context.subset` to show both `test` and `val` `loss` and `bleu` metrics. Divide into subplots further by `context.subset` too so Aim UI shows `test` and `val` metrics on different subplots for better comparison.\n\n![](https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif \"Divide into subplots by context / subset\")\n\nNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\n\n# Summary\n\nHere is a full summary video on how to do it on the UI.\n\n![](https://www.youtube.com/watch?v=DGI8S7SUfEk)\n\n# Learn More\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://slack.aimstack.io/) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003cp\u003eResearchers divide datasets into three subsets — train, validation and test so they can test their model performance at different levels.\u003c/p\u003e\n\u003cp\u003eThe model is trained on the train subset and subsequent metrics are collected to evaluate how well the training is going. Loss, accuracy and other metrics are computed.\u003c/p\u003e\n\u003cp\u003eThe validation and test sets are used to test the model on additional unseen data to verify how well it generalise.\u003c/p\u003e\n\u003cp\u003eModels are usually ran on validation subset after each epoch.\u003cbr\u003e\nOnce the training is done, models are tested on the test subset to verify the final performance and generalisation.\u003c/p\u003e\n\u003cp\u003eThere is a need to collect and effectively compare all these metrics.\u003c/p\u003e\n\u003cp\u003eHere is how to do that on \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003eUsing context to track for different subsets?\u003c/h1\u003e\n\u003cp\u003eUse the \u003ca href=\"https://github.com/aimhubio/aim#track\"\u003eaim.track\u003c/a\u003e context arguments to pass additional information about the metrics. All context parameters can be used to query, group and do other operations on top of the metrics.\u003c/p\u003e\n\u003cp\u003e![]( \"Here is how it looks like on the code\")\u003c/p\u003e\n\u003cp\u003eOnce the training is ran, execute \u003ccode\u003eaim up\u003c/code\u003e in your terminal and start the Aim UI.\u003c/p\u003e\n\u003ch1\u003eUsing subplots to compare test, val loss and bleu metrics\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e*Note:\u003c/strong\u003e The bleu metric is used here instead of accuracy as we are looking at Neural Machine Translation experiments. But this works with every other metric too.*\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet’s go step-by-step on how to break down lots of experiments using subplots.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1.\u003c/strong\u003e Explore the runs, the context table, play with the query language.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/dynamic/1_tfc_fuc-axk07z3-a7jsmg.gif\" alt=\"\" title=\"Explore the training runs\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2.\u003c/strong\u003e Add the \u003ccode\u003ebleu\u003c/code\u003e metric to the Select input — query both metrics at the same time. Divide into subplots by metric.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*BQK8qGoG3v4KMpssvzC0hw.gif\" alt=\"\" title=\"Divide into subplots by metric\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3.\u003c/strong\u003e Search by \u003ccode\u003econtext.subset\u003c/code\u003e to show both \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e \u003ccode\u003eloss\u003c/code\u003e and \u003ccode\u003ebleu\u003c/code\u003e metrics. Divide into subplots further by \u003ccode\u003econtext.subset\u003c/code\u003e too so Aim UI shows \u003ccode\u003etest\u003c/code\u003e and \u003ccode\u003eval\u003c/code\u003e metrics on different subplots for better comparison.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/1400/1*fSm2PyNwbcBaAoZceq6Qsw.gif\" alt=\"\" title=\"Divide into subplots by context / subset\"\u003e\u003c/p\u003e\n\u003cp\u003eNot it’s easy and straightforward to simultaneously compare both 4 metrics and find the best version of the model.\u003c/p\u003e\n\u003ch1\u003eSummary\u003c/h1\u003e\n\u003cp\u003eHere is a full summary video on how to do it on the UI.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://www.youtube.com/watch?v=DGI8S7SUfEk\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch1\u003eLearn More\u003c/h1\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","_raw":{"sourceFilePath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileName":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"type":"Post"},{"title":"Aim v2.2.0 — Hugging Face integration","date":"2021-03-24T13:20:24.937Z","author":"Gev Soghomonian","description":"Aim 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.","slug":"aim-v2-2-0-—-hugging-face-integration","image":"https://aimstack.io/wp-content/uploads/2022/02/HF.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\n\nThanks to [siddk](https://github.com/siddk), [TommasoBendinelli](https://github.com/TommasoBendinelli) and [Khazhak](https://github.com/Khazhak) for their contribution to this release.\n\n\u003e **Note on the Aim versioning:** The previous two release posts: [Aim 1.3.5](https://aimstack.io/blog/new-releases/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/) and [Aim 1.3.8](https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring/) had used the version number of [AimUI](https://aimstack.readthedocs.io/en/latest/ui/overview.html) as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion. [Check out the Aim CHANGELOG](https://github.com/aimhubio/aim/blob/main/CHANGELOG.md).\n\nWe have also added [milestones](https://github.com/aimhubio/aim/milestones) for each version. As well as the [Roadmap](https://github.com/aimhubio/aim#roadmap).\n\nCheck out the new features at [play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\n## Hugging Face integration\n\nHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\n\n```\nfrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n```\n\nHere is how it works: `aim_callback` will automatically open an Aim `Session` and close it when the trainer is done. When `trainer.train()`, `trainer.evaluate()` or `trainer.predict()` is called, `aim_callback` will automatically log the hyper-params and respective metrics.\n\nFind a full example [here](https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py).\n\n## Metric Visibility Control\n\nWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\n\nA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/demo.gif \"Hide individual metrics as well as collectively\")\n\n## Column resize\n\n\n\nWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table. Resize is available both on Explore and Dashboard tables.Here is a quick demo:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/demo2.gif \"Drag column edges back-and-forth to resize\")\n\n## Hide columns with similar values\n\n\n\nMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\n\nThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/demo3.gif \"Leave only different columns with a button click, if needed customize afterwards\")\n\n## Logscale\n\n\n\nOne of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!\n\n![](https://aimstack.io/wp-content/uploads/2022/02/demo4.gif \"log-scale on Aim\")\n\n## New methods for aggregation\n\n\n\nNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\n\n![](https://aimstack.io/wp-content/uploads/2022/02/demo5.gif)\n\n## Learn More\n\n[Aim is on the mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://slack.aimstack.io/), share your feedback.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support 🙌.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e 2.2.0 featuring Hugging Face integration is out! Thanks to the incredible Aim community for the feedback and support on building democratized open-source AI dev tools.\u003c/p\u003e\n\u003cp\u003eThanks to \u003ca href=\"https://github.com/siddk\"\u003esiddk\u003c/a\u003e, \u003ca href=\"https://github.com/TommasoBendinelli\"\u003eTommasoBendinelli\u003c/a\u003e and \u003ca href=\"https://github.com/Khazhak\"\u003eKhazhak\u003c/a\u003e for their contribution to this release.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote on the Aim versioning:\u003c/strong\u003e The previous two release posts: \u003ca href=\"https://aimstack.io/blog/new-releases/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\"\u003eAim 1.3.5\u003c/a\u003e and \u003ca href=\"https://aimstack.io/blog/new-releases/aim-1-3-8-enhanced-context-table-and-advanced-group-coloring/\"\u003eAim 1.3.8\u003c/a\u003e had used the version number of \u003ca href=\"https://aimstack.readthedocs.io/en/latest/ui/overview.html\"\u003eAimUI\u003c/a\u003e as those contained only UI changes. From now on we are going to stick to the Aim versions only regardless of the type of changes to avoid any confusion. \u003ca href=\"https://github.com/aimhubio/aim/blob/main/CHANGELOG.md\"\u003eCheck out the Aim CHANGELOG\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe have also added \u003ca href=\"https://github.com/aimhubio/aim/milestones\"\u003emilestones\u003c/a\u003e for each version. As well as the \u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eRoadmap\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCheck out the new features at \u003ca href=\"http://play.aimstack.io:43900/dashboard\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eHugging Face integration\u003c/h2\u003e\n\u003cp\u003eHugging Face integration has been one of the most requested features so far and we are excited to finally ship it. Here is a code snippet of easy it is to integrate Aim and Hugging Face.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom aim.hugging_face import AimCallback\nfrom transformers import Trainer\n\naim_callback = AimCallback(repo='/log/dir/path', experiment='your_experiment_name')\n\ntrainer = Trainer(\n       model=model,\n       args=training_args,\n       callbacks=[aim_callback]\n    )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is how it works: \u003ccode\u003eaim_callback\u003c/code\u003e will automatically open an Aim \u003ccode\u003eSession\u003c/code\u003e and close it when the trainer is done. When \u003ccode\u003etrainer.train()\u003c/code\u003e, \u003ccode\u003etrainer.evaluate()\u003c/code\u003e or \u003ccode\u003etrainer.predict()\u003c/code\u003e is called, \u003ccode\u003eaim_callback\u003c/code\u003e will automatically log the hyper-params and respective metrics.\u003c/p\u003e\n\u003cp\u003eFind a full example \u003ca href=\"https://github.com/aimhubio/aim/blob/main/examples/hugging_face_track.py\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eMetric Visibility Control\u003c/h2\u003e\n\u003cp\u003eWhen dealing with lots of experiments some metrics may need hide (while still being in the Search) to allow better visibility of the overall picture.\u003c/p\u003e\n\u003cp\u003eA toggle is now available both for each metric/row as well as for global on/off. Check out the demo below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/demo.gif\" alt=\"\" title=\"Hide individual metrics as well as collectively\"\u003e\u003c/p\u003e\n\u003ch2\u003eColumn resize\u003c/h2\u003e\n\u003cp\u003eWith lots of params tracked, some of them are too wide for table and take over the whole screen real estate. Column resize will allow to fully control data width on the table. Resize is available both on Explore and Dashboard tables.Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/demo2.gif\" alt=\"\" title=\"Drag column edges back-and-forth to resize\"\u003e\u003c/p\u003e\n\u003ch2\u003eHide columns with similar values\u003c/h2\u003e\n\u003cp\u003eMore often than not params have lots of repetition. Especially when tracking 100s of them. In that case the explore table becomes super-noisy.\u003c/p\u003e\n\u003cp\u003eThis feature allows showing only the relevant info. This has been certainly the missing piece of the column management that many had requested. Here is a quick demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/demo3.gif\" alt=\"\" title=\"Leave only different columns with a button click, if needed customize afterwards\"\u003e\u003c/p\u003e\n\u003ch2\u003eLogscale\u003c/h2\u003e\n\u003cp\u003eOne of the must-have features that we hadn’t had a chance to work on. Finally it’s available!!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/demo4.gif\" alt=\"\" title=\"log-scale on Aim\"\u003e\u003c/p\u003e\n\u003ch2\u003eNew methods for aggregation\u003c/h2\u003e\n\u003cp\u003eNow you can select different types of aggregations as well as whether to see the whole area or just the aggregated lines.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/demo5.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on the mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, share your feedback.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support 🙌.\u003c/p\u003e"},"_id":"aim-v2-2-0-—-hugging-face-integration.md","_raw":{"sourceFilePath":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileName":"aim-v2-2-0-—-hugging-face-integration.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-2-0-—-hugging-face-integration"},"type":"Post"},{"title":"Aim v2.3.0 — System Resource Usage and Reverse Grouping","date":"2021-04-20T13:39:39.516Z","author":"Gev Soghomonian","description":"Aim 2.3.0 allowing System Resource Usage optimization is out! Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out...","slug":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping","image":"https://aimstack.io/wp-content/uploads/2021/04/2.3.0-1.png","draft":false,"categories":["New Releases"],"body":{"raw":"[Aim](https://github.com/aimhubio/aim) 2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\n\nCheck out the updated Aim at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==).\n\nBelow are the highlight features of the update. Find the full list of changes [here](https://github.com/aimhubio/aim/milestone/3?closed=1).\n\n## System Resource Usage\n\n\n\nNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with `system_tracking_interval==0`.\n\nOnce you run the training, you will see the following. Here is a demo:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/demo-1.gif \"Aim automatic system resource tracking demo\")\n\nOf course you can also [query all those new metrics](https://aimstack.io/wp-admin/post.php?post=706\u0026action=edit) in the Explore page and compare, group, aggregate with the rest of the metrics!\n\n## Reverse grouping (“against” the param)\n\n\n\nAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a Reverse Grouping. Here is how it works:\n\n![](https://aimstack.io/wp-content/uploads/2021/04/reverse-grouping-1024x566.gif \"Aim Reverse grouping demo\")\n\nUse the toggle next to switch between grouping modes.\n\n## Line Chart Smoothing\n\n\n\nNow you can apply smoothing on metrics. Here is how it works:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/metric-smoothing.gif \"Aim metric smoothing\")\n\nThere are two type of smoothing options available: `Exponential Moving Average` and `Central Moving Average`. Further info of [how](https://en.wikipedia.org/wiki/Moving_average) it’s calculated.\n\n## New aggregation modes\n\n\n\nWe have additionally added two more aggregation modes: `standard error` and `standard deviation` . Here is how it works:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/aggr-modes.gif)\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e 2.3.0 is out! Thanks to the community for feedback and support on our journey towards democratizing AI dev tools.\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim at \u003ca href=\"http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjp0cnVlLCJpbmRpY2F0b3IiOmZhbHNlLCJ4QWxpZ25tZW50Ijoic3RlcCIsInhTY2FsZSI6MCwieVNjYWxlIjowLCJwb2ludHNDb3VudCI6NTAsInNtb290aGluZ0FsZ29yaXRobSI6ImVtYSIsInNtb290aEZhY3RvciI6MC40NSwiYWdncmVnYXRlZCI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUsIGxvc3MgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImdyb3VwQWdhaW5zdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZSwiY2hhcnQiOmZhbHNlfSwiYWdncmVnYXRlZEFyZWEiOiJzdGRfZGV2IiwiYWdncmVnYXRlZExpbmUiOiJtZWRpYW4iLCJzZWVkIjp7ImNvbG9yIjoxMCwic3R5bGUiOjEwfSwicGVyc2lzdCI6eyJjb2xvciI6ZmFsc2UsInN0eWxlIjpmYWxzZX0sImFnZ3JlZ2F0ZWQiOnRydWV9fQ==\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow are the highlight features of the update. Find the full list of changes \u003ca href=\"https://github.com/aimhubio/aim/milestone/3?closed=1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSystem Resource Usage\u003c/h2\u003e\n\u003cp\u003eNow you can use automatic tracking of your system resources (GPU, CPU, Memory, etc). In order to disable system tracking, just initialize the session with \u003ccode\u003esystem_tracking_interval==0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce you run the training, you will see the following. Here is a demo:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/demo-1.gif\" alt=\"\" title=\"Aim automatic system resource tracking demo\"\u003e\u003c/p\u003e\n\u003cp\u003eOf course you can also \u003ca href=\"https://aimstack.io/wp-admin/post.php?post=706\u0026#x26;action=edit\"\u003equery all those new metrics\u003c/a\u003e in the Explore page and compare, group, aggregate with the rest of the metrics!\u003c/p\u003e\n\u003ch2\u003eReverse grouping (“against” the param)\u003c/h2\u003e\n\u003cp\u003eAs much as grouping is needed to divide pile of metrics by param values into manageable clusters for comparison, there are cases when the metrics need to be divided by everything BUT one param (yes, I am looking at you seed!). We have called it a Reverse Grouping. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/04/reverse-grouping-1024x566.gif\" alt=\"\" title=\"Aim Reverse grouping demo\"\u003e\u003c/p\u003e\n\u003cp\u003eUse the toggle next to switch between grouping modes.\u003c/p\u003e\n\u003ch2\u003eLine Chart Smoothing\u003c/h2\u003e\n\u003cp\u003eNow you can apply smoothing on metrics. Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/metric-smoothing.gif\" alt=\"\" title=\"Aim metric smoothing\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are two type of smoothing options available: \u003ccode\u003eExponential Moving Average\u003c/code\u003e and \u003ccode\u003eCentral Moving Average\u003c/code\u003e. Further info of \u003ca href=\"https://en.wikipedia.org/wiki/Moving_average\"\u003ehow\u003c/a\u003e it’s calculated.\u003c/p\u003e\n\u003ch2\u003eNew aggregation modes\u003c/h2\u003e\n\u003cp\u003eWe have additionally added two more aggregation modes: \u003ccode\u003estandard error\u003c/code\u003e and \u003ccode\u003estandard deviation\u003c/code\u003e . Here is how it works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/aggr-modes.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. In fact, It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","_raw":{"sourceFilePath":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileName":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-3-0-—-system-resource-usage-and-reverse-grouping"},"type":"Post"},{"title":"Aim v2.6.0 — Docker requirement removed and Metric as x-axis","date":"2021-06-24T14:25:34.071Z","author":"Gev Soghomonian","description":"Before and after Aim v2.5.0","slug":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis","image":"https://aimstack.io/wp-content/uploads/2021/06/2.6.0.png","draft":false,"categories":["New Releases"],"body":{"raw":"## Metric as an alternative x-axis\n\nSometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.\n\nFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared — `loss` and`bleu`. One of the main indicators of a performant model in NMT, is when higher values of `bleu` correlates with the lower values of `loss`.\n\nJust plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use `loss` as the x-axis to directly see the correlation between the `loss`and the`bleu` metrics for two groups of runs ( `max_k==3`and `max_k == 1`).\n\n![](https://aimstack.io/wp-content/uploads/2022/02/1_Y0wAL9nZ7IhgLOUr-cCRRQ.gif \"Demo for metric as alternative x-axis\")\n\nUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\n\n\u003e **Note**: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support.","html":"\u003ch2\u003eMetric as an alternative x-axis\u003c/h2\u003e\n\u003cp\u003eSometimes when comparing the metrics, it’s not enough to just put them into perspective via subplots.\u003c/p\u003e\n\u003cp\u003eFor instance in the quick demo below, we are looking at a Neural Machine Translation (NMT) problem where two metrics are being compared — \u003ccode\u003eloss\u003c/code\u003e and\u003ccode\u003ebleu\u003c/code\u003e. One of the main indicators of a performant model in NMT, is when higher values of \u003ccode\u003ebleu\u003c/code\u003e correlates with the lower values of \u003ccode\u003eloss\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eJust plotting them besides each other sometimes isn’t enough to see how they correlate. In this case we use \u003ccode\u003eloss\u003c/code\u003e as the x-axis to directly see the correlation between the \u003ccode\u003eloss\u003c/code\u003eand the\u003ccode\u003ebleu\u003c/code\u003e metrics for two groups of runs ( \u003ccode\u003emax_k==3\u003c/code\u003eand \u003ccode\u003emax_k == 1\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/1_Y0wAL9nZ7IhgLOUr-cCRRQ.gif\" alt=\"\" title=\"Demo for metric as alternative x-axis\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing a different metric as the x-axis helps to see the correlation between two metrics of training runs better.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: if the metric is not increasing monotonously, its values re-order. Then they set as the x-axis (see in case of the loss).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support.\u003c/p\u003e"},"_id":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","_raw":{"sourceFilePath":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileName":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-6-0 — docker-requirement-removed-and-metric-as-x-axis"},"type":"Post"},{"title":"Aim v2.7.1 — Table export and Explore view bookmarks","date":"2021-07-15T14:31:32.317Z","author":"Gev Soghomonian","description":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback! Check out the updated","slug":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks","image":"https://aimstack.io/wp-content/uploads/2021/07/v2.7.1.gif","draft":false,"categories":["New Releases"],"body":{"raw":"Aim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\n\nCheck out the updated Aim demo at [play.aimstack.io](http://play.aimstack.io:43900/dashboard).\n\nBelow are the two main highlights of Aim v2.7.1.\n\n## Aim Table CSV export\n\nNow you can export both tables from Explore and Dashboard to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with [Pandas](https://pandas.pydata.org/)) for your MLOps needs.\n\n![](https://aimstack.io/wp-content/uploads/2021/07/csv-export.gif \"Exporting CSV from the Explore table\")\n\n## Bookmark/Save the Explore State \\[experimental]\n\nThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you’d like to revisit.\n\nIn the future [releases](https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/), we will build on top of this feature to potentially add notes, fully manage and share them. *For now, this feature is in experimental mode*. Please feel free to share more feedback and how we can improve its experience.\n\n![](https://aimstack.io/wp-content/uploads/2021/07/bookmark.gif \"Use bookmarks to save important explore states\")\n\n## Learn More\n\n[Aim is on a mission to democratize MLOps tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\n\nTry out [Aim](https://github.com/aimhubio/aim), join the [Aim community](https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ), share your feedback, open issues for new features, bugs.\n\nAnd don’t forget to leave [Aim](https://github.com/aimhubio/aim) a star on GitHub for support .","html":"\u003cp\u003eAim v2.7.1 is out! We are on a journey to democratize MLOps tools. Thanks to the community for continuous support and feedback!\u003c/p\u003e\n\u003cp\u003eCheck out the updated Aim demo at \u003ca href=\"http://play.aimstack.io:43900/dashboard\"\u003eplay.aimstack.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBelow are the two main highlights of Aim v2.7.1.\u003c/p\u003e\n\u003ch2\u003eAim Table CSV export\u003c/h2\u003e\n\u003cp\u003eNow you can export both tables from Explore and Dashboard to a CSV file. Afterwards feel free to use the exported CSV file to import in your spreadsheets for reports or just import in your notebook and explore further in other ways (e.g. with \u003ca href=\"https://pandas.pydata.org/\"\u003ePandas\u003c/a\u003e) for your MLOps needs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/07/csv-export.gif\" alt=\"\" title=\"Exporting CSV from the Explore table\"\u003e\u003c/p\u003e\n\u003ch2\u003eBookmark/Save the Explore State [experimental]\u003c/h2\u003e\n\u003cp\u003eThis new feature helps to save/bookmark the Explore state for future access. The bookmarking ability is particularly helpful to save Explore views that contain certain insights you’d like to revisit.\u003c/p\u003e\n\u003cp\u003eIn the future \u003ca href=\"https://aimstack.io/aim-foundations-why-we-re-building-a-tensorboard-alternative/\"\u003ereleases\u003c/a\u003e, we will build on top of this feature to potentially add notes, fully manage and share them. \u003cem\u003eFor now, this feature is in experimental mode\u003c/em\u003e. Please feel free to share more feedback and how we can improve its experience.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/07/bookmark.gif\" alt=\"\" title=\"Use bookmarks to save important explore states\"\u003e\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize MLOps tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe have been incredibly lucky to get help and contributions from the amazing Aim community. It’s humbling and inspiring.\u003c/p\u003e\n\u003cp\u003eTry out \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e, join the \u003ca href=\"https://join.slack.com/t/aimstack/shared_invite/zt-193hk43nr-vmi7zQkLwoxQXn8LW9CQWQ\"\u003eAim community\u003c/a\u003e, share your feedback, open issues for new features, bugs.\u003c/p\u003e\n\u003cp\u003eAnd don’t forget to leave \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e a star on GitHub for support .\u003c/p\u003e"},"_id":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","_raw":{"sourceFilePath":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","sourceFileName":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim-v2-7-1-—-table-export-and-explore-view-bookmarks"},"type":"Post"},{"title":"Aim’s foundations \u0026 why we’re building a Tensorboard alternative","date":"2022-10-22T14:48:48.452Z","author":"Gev Soghomonian","description":"The origins of Aim In the fateful summer of 2020, our friend mahnerak – a researcher at a non-profit lab was hitting the limits of Tensorboard. He wasn’t","slug":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative","image":"https://aimstack.io/wp-content/uploads/2021/08/3.0.png","draft":false,"categories":["New Releases"],"body":{"raw":"## The origins of Aim\n\nIn the fateful summer of 2020, our friend [mahnerak](https://twitter.com/mahnerak) – a researcher at [a non-profit lab](https://yerevann.com/) was hitting the limits of Tensorboard. He wasn’t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That’s how we decided to build a Tensorboard alternative.\n\n[Gor](https://github.com/gorarakelyan) and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\n\nBy fall 2020, [Aim 2.0](https://aimstack.io/aim-v2-2-0-hugging-face-integration/) launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even [r/MachineLearning loved it](https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/).\n\nBy spring 2021, mahnerak co-authoerd a paper [WARP](https://aclanthology.org/2021.acl-long.381/) ([code](https://github.com/yerevann/warp)): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\n\n## A scale problem\n\nBut Aim’s power users — who often do 5K+ runs —were hitting issues.\n\nAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests. *Live updates, image tracking, distribution tracking*… and Aim 2.0 was hitting the limits of Aim 1.0’s design.\n\nIn order to support the future, we had to make changes to the foundation now.\n\n## Launching Aim 3.0.0\n\nAn additional [317 pull requests](https://github.com/aimhubio/aim/milestone/13?closed=1) later, we are excited to launch Aim v3.0.0 !!!\n\nAs a result, the most important changes include:\n\n**A completely revamped UI**\n\n* Home page and run detail page\n* Runs, metrics and params explorers\n* Bookmarks and Tags\n\n**A completely revamped Aim Python SDK**\n\n* New and much more intuitive (but still quite vanilla) API to track your training runs\n* New and 10x faster embedded storage based on [Rocksdb](http://rocksdb.org/). This will allow us to store virtually any type of AI metadata. On the contrary, [AimRecords](https://github.com/aimhubio/aimrecords) was designed for metrics and hyperparams only.\n\nEnjoy the changes!\n\n![](https://aimstack.io/wp-content/uploads/2021/08/changes.gif)\n\n## Performance improvements\n\n* Average run query execution time on ~2000 runs: 0.784s.\n* Average metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\n* New UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (*for comparison, v2 was performant with limitation for only 100 metrics*).\n\n## Comparisons to familiar tools\n\n### Tensorboard\n\n**Training run comparison**\n\n* The tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\n* With Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params. After all, TensorBoard doesn’t have features to group, aggregate the metrics\n\n**Scalability**\n\n* Aim can handle 1000s of training runs both on the backend and on the UI.\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\n\nAim will have the beloved TB visualizations\n\n* Embedding projector.\n* Neural network visualization.\n\n### [](https://github.com/aimhubio/aim#mlflow)MLFlow\n\nMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\n\n**Run comparison**\n\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\n* MLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\n\n**UI Scalability**\n\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\n\n### [](https://github.com/aimhubio/aim#weights-and-biases)Weights and Biases\n\n\n\n**Hosted vs self-hosted**\n\n* Weights and Biases is a hosted closed-source MLOps platform.\n* Aim is self-hosted, free and open-source experiment tracking tool.\n\n## Aim Roadmap[](https://github.com/aimhubio/aim#tensorboard)\n\nWith this version we are also publishing the Aim [roadmap](https://github.com/aimhubio/aim#roadmap) for the next 3 months.\n\nThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\n\nWe are also [inviting community contributors](https://github.com/aimhubio/aim#community) to help us get there faster!\n\n## Why are we building Aim?\n\n\n\nWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\n\nExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\n\nHowever we see lots of improvements to be made. Especially around issues like:\n\n* ability to handle 1000s of large-scale experiments\n* actionable, beautiful and performant visualizations\n* extensibility — how easy are the apis for extension/democratization?\n\nWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\n\nOur mission…\n\nAim’s mission is to democratize AI dev tools. We believe that the best AI tools need to be:\n\n* open-source, open-data-format, community-driven\n* have great UI/UX, CLI and other interfaces for automation\n* performant both on UI and data\n* extensible — enable ways to build around for so many use-cases\n\n\n\n## Thanks to\n\n\n\n[Ruben Karapetyan](https://twitter.com/roubkar) for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\n\n[Mahnerak](https://twitter.com/mahnerak) for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\n\nAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\n\nThe contributors who have been relentlessly iterating over the course of the summer.\n\nOn to the next generation of ML tools!!\n\n## Join Us!\n\nJoin the [Aim community](https://slack.aimstack.io/), test Aim out, ask questions, help us build the future of AI tooling!\n\nIf you find Aim useful, drop by and star the repo ⭐","html":"\u003ch2\u003eThe origins of Aim\u003c/h2\u003e\n\u003cp\u003eIn the fateful summer of 2020, our friend \u003ca href=\"https://twitter.com/mahnerak\"\u003emahnerak\u003c/a\u003e – a researcher at \u003ca href=\"https://yerevann.com/\"\u003ea non-profit lab\u003c/a\u003e was hitting the limits of Tensorboard. He wasn’t going to send the training logs to a third-party cloud. Meanwhile, spending hours on Tensorboard bothered to focus on his actual research. That’s how we decided to build a Tensorboard alternative.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/gorarakelyan\"\u003eGor\u003c/a\u003e and I started hacking on an open-source library to store metrics and hyperparameters. In a month, mahnerak was using Aim 1.0 instead of Tensorboard to track, store, search and group his metrics.\u003c/p\u003e\n\u003cp\u003eBy fall 2020, \u003ca href=\"https://aimstack.io/aim-v2-2-0-hugging-face-integration/\"\u003eAim 2.0\u003c/a\u003e launched as a free, open-source and self-hosted alternative to Weights and Biases, Tensorboard and MLflow. To our surprise even \u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/jfgbij/project_aim_a_supereasy_way_to_record_search_and/\"\u003er/MachineLearning loved it\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBy spring 2021, mahnerak co-authoerd a paper \u003ca href=\"https://aclanthology.org/2021.acl-long.381/\"\u003eWARP\u003c/a\u003e (\u003ca href=\"https://github.com/yerevann/warp\"\u003ecode\u003c/a\u003e): Word-level Adverserial ReProgramming on his ACL-published work. At that point Aim users had contributed over 100 feature requests already.\u003c/p\u003e\n\u003ch2\u003eA scale problem\u003c/h2\u003e\n\u003cp\u003eBut Aim’s power users — who often do 5K+ runs —were hitting issues.\u003c/p\u003e\n\u003cp\u003eAfter over 250 pull requests, 1.2K GitHub stars and 200 feature requests. \u003cem\u003eLive updates, image tracking, distribution tracking\u003c/em\u003e… and Aim 2.0 was hitting the limits of Aim 1.0’s design.\u003c/p\u003e\n\u003cp\u003eIn order to support the future, we had to make changes to the foundation now.\u003c/p\u003e\n\u003ch2\u003eLaunching Aim 3.0.0\u003c/h2\u003e\n\u003cp\u003eAn additional \u003ca href=\"https://github.com/aimhubio/aim/milestone/13?closed=1\"\u003e317 pull requests\u003c/a\u003e later, we are excited to launch Aim v3.0.0 !!!\u003c/p\u003e\n\u003cp\u003eAs a result, the most important changes include:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA completely revamped UI\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHome page and run detail page\u003c/li\u003e\n\u003cli\u003eRuns, metrics and params explorers\u003c/li\u003e\n\u003cli\u003eBookmarks and Tags\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eA completely revamped Aim Python SDK\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNew and much more intuitive (but still quite vanilla) API to track your training runs\u003c/li\u003e\n\u003cli\u003eNew and 10x faster embedded storage based on \u003ca href=\"http://rocksdb.org/\"\u003eRocksdb\u003c/a\u003e. This will allow us to store virtually any type of AI metadata. On the contrary, \u003ca href=\"https://github.com/aimhubio/aimrecords\"\u003eAimRecords\u003c/a\u003e was designed for metrics and hyperparams only.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEnjoy the changes!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/08/changes.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003ePerformance improvements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAverage run query execution time on ~2000 runs: 0.784s.\u003c/li\u003e\n\u003cli\u003eAverage metrics query execution time on ~2000 runs with 6000 metrics: 1.552s.\u003c/li\u003e\n\u003cli\u003eNew UI works smooth with ~500 metrics displayed at the same time with full Aim table interactions (\u003cem\u003efor comparison, v2 was performant with limitation for only 100 metrics\u003c/em\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eComparisons to familiar tools\u003c/h2\u003e\n\u003ch3\u003eTensorboard\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTraining run comparison\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe tracked params are first class citizens at Aim. So, you can search, group, and aggregate via params. Aim allows to deeply explore all the tracked data (metrics, params, images) on the UI.\u003c/li\u003e\n\u003cli\u003eWith Tensorboard alternative solution the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super tedious comparison experience and usability issues on the UI when there are many experiments and params. After all, TensorBoard doesn’t have features to group, aggregate the metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eScalability\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim can handle 1000s of training runs both on the backend and on the UI.\u003c/li\u003e\n\u003cli\u003eTensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAim will have the beloved TB visualizations\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEmbedding projector.\u003c/li\u003e\n\u003cli\u003eNeural network visualization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003ca href=\"https://github.com/aimhubio/aim#mlflow\"\u003e\u003c/a\u003eMLFlow\u003c/h3\u003e\n\u003cp\u003eMLflow is an end-to-end ML Lifecycle tool. Aim is focused on training tracking. In general, the differences of Aim and MLflow are around the UI scalability and run comparison features.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRun comparison\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim treats tracked parameters as first-class citizens. Users can query runs, metrics and images. Also, they can filter using the params.\u003c/li\u003e\n\u003cli\u003eMLflow does have a search by tracked config. However, there is no feature availability such as grouping, aggregation, subplotting by hyparparams .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eUI Scalability\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\u003c/li\u003e\n\u003cli\u003eMLflow UI becomes slow to use when there are a few hundreds of runs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003ca href=\"https://github.com/aimhubio/aim#weights-and-biases\"\u003e\u003c/a\u003eWeights and Biases\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHosted vs self-hosted\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWeights and Biases is a hosted closed-source MLOps platform.\u003c/li\u003e\n\u003cli\u003eAim is self-hosted, free and open-source experiment tracking tool.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAim Roadmap\u003ca href=\"https://github.com/aimhubio/aim#tensorboard\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWith this version we are also publishing the Aim \u003ca href=\"https://github.com/aimhubio/aim#roadmap\"\u003eroadmap\u003c/a\u003e for the next 3 months.\u003c/p\u003e\n\u003cp\u003eThis is a living document and we hope that the community will help us shape it towards supporting the most important use-cases.\u003c/p\u003e\n\u003cp\u003eWe are also \u003ca href=\"https://github.com/aimhubio/aim#community\"\u003einviting community contributors\u003c/a\u003e to help us get there faster!\u003c/p\u003e\n\u003ch2\u003eWhy are we building Aim?\u003c/h2\u003e\n\u003cp\u003eWe have started to work on Aim with strong belief that the open-source is in the DNA of AI software (2.0) development.\u003c/p\u003e\n\u003cp\u003eExisting open-source tools (TensorBoard, MLFlow) are super-inspiring for us.\u003c/p\u003e\n\u003cp\u003eHowever we see lots of improvements to be made. Especially around issues like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eability to handle 1000s of large-scale experiments\u003c/li\u003e\n\u003cli\u003eactionable, beautiful and performant visualizations\u003c/li\u003e\n\u003cli\u003eextensibility — how easy are the apis for extension/democratization?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWith this in mind, we are inspired to build beautiful and performant AI dev tools with great APIs.\u003c/p\u003e\n\u003cp\u003eOur mission…\u003c/p\u003e\n\u003cp\u003eAim’s mission is to democratize AI dev tools. We believe that the best AI tools need to be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen-source, open-data-format, community-driven\u003c/li\u003e\n\u003cli\u003ehave great UI/UX, CLI and other interfaces for automation\u003c/li\u003e\n\u003cli\u003eperformant both on UI and data\u003c/li\u003e\n\u003cli\u003eextensible — enable ways to build around for so many use-cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThanks to\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/roubkar\"\u003eRuben Karapetyan\u003c/a\u003e for being the first to believe in this project and spending lots of his time and setting the foundations for the beautiful UI.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/mahnerak\"\u003eMahnerak\u003c/a\u003e for sharing his problems and continuously testing and coming up with better solutions on UX, features. Also for helping us build the next-gen storage for Aim.\u003c/p\u003e\n\u003cp\u003eAim users Mohammad Elgaar, Vopani for continuous feedback on our work.\u003c/p\u003e\n\u003cp\u003eThe contributors who have been relentlessly iterating over the course of the summer.\u003c/p\u003e\n\u003cp\u003eOn to the next generation of ML tools!!\u003c/p\u003e\n\u003ch2\u003eJoin Us!\u003c/h2\u003e\n\u003cp\u003eJoin the \u003ca href=\"https://slack.aimstack.io/\"\u003eAim community\u003c/a\u003e, test Aim out, ask questions, help us build the future of AI tooling!\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, drop by and star the repo ⭐\u003c/p\u003e"},"_id":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","_raw":{"sourceFilePath":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","sourceFileName":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"aim’s-foundations-why-we’re-building-a-tensorboard-alternative"},"type":"Post"},{"title":"An end-to-end example of Aim logger used with XGBoost library","date":"2021-05-17T14:12:33.016Z","author":"Khazhak Galstyan","description":" Thanks to the community for feedback and support on our journey towards democratizing MLOps tools. Check out […]  Read More 122  0 XGBoost  Tutorials An end-to-end example of Aim logger used… What is Aim? Aim is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. ","slug":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library","image":"https://aimstack.io/wp-content/uploads/2022/02/xgboost.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is Aim?\n\n[Aim](https://github.com/aimhubio/aim) is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\n\n## What is XGBoost?\n\n[XGBoost](https://github.com/dmlc/xgboost) is an optimized gradient boosting library with highly  *efficient*,  *flexible,*  and  *portable* design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\n## How to use Aim with XGBoost?\n\nCheck out end-to-end [Aim integration](https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/) examples with multiple frameworks [here](https://github.com/aimhubio/aim/tree/main/examples). In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\n\n```\n# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n```\n\nAs you can see on line 49, AimCallback is imported from `aim.xgboost` and passed to `xgb.train` as one of the callbacks. Aim session can open and close by the AimCallback and the metrics and hparamsstore by XGBoost. In addition to that, thesystem measures pass to Aim as well.\n\n## What it looks like?\n\nAfter you run the experiment and the `aim up` command in the `aim_logs`directory, Aim UI will be running. When first opened, the dashboard page will come up.\n\n![](https://aimstack.io/wp-content/uploads/2021/05/dashboard.png \"Aim UI dashboard page\")\n\nTo explore the run, we should:\n\n* Choose the `xgboost_test`experiment.\n* Select the metrics to explore.\n* Divide into charts by metrics.\n\nFor example, the gif below illustrates the steps above.\n\n![](https://aimstack.io/wp-content/uploads/2021/05/1.gif)\n\n\\\n So this is what the final result looks like.\n\n![](https://aimstack.io/wp-content/uploads/2021/05/2.png)\n\nIn short, we can easily analyze the runs and the system usage.\n\n## Learn More\n\n[Aim is on a mission to democratize AI dev tools.](https://github.com/aimhubio/aim#democratizing-ai-dev-tools)\n\nIf you find Aim useful, support us and star [the project](https://github.com/aimhubio/aim) on GitHub. Also, join the [Aim community](https://aimstack.slack.com/ssb/redirect) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is Aim?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e is an open-source tool for AI experiment comparison. With more resources and complex models, more experiments are ran than ever. You can indeed use Aim to deeply inspect thousands of hyperparameter-sensitive training runs.\u003c/p\u003e\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dmlc/xgboost\"\u003eXGBoost\u003c/a\u003e is an optimized gradient boosting library with highly  \u003cem\u003eefficient\u003c/em\u003e,  \u003cem\u003eflexible,\u003c/em\u003e  and  \u003cem\u003eportable\u003c/em\u003e design. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. Subsequently, the same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\u003c/p\u003e\n\u003ch2\u003eHow to use Aim with XGBoost?\u003c/h2\u003e\n\u003cp\u003eCheck out end-to-end \u003ca href=\"https://aimstack.io/aim-2-4-0-xgboost-integration-and-confidence-interval-aggregation/\"\u003eAim integration\u003c/a\u003e examples with multiple frameworks \u003ca href=\"https://github.com/aimhubio/aim/tree/main/examples\"\u003ehere\u003c/a\u003e. In this tutorial, we are going to show how to integrate Aim and use AimCallback in your XGBoost code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# You should download and extract the data beforehand. Simply by doing this: \n# wget https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\n\nfrom __future__ import division\n\nimport numpy as np\nimport xgboost as xgb\nfrom aim.xgboost import AimCallback\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('./dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\nprint(len(train_X))\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['nthread'] = 4\nparam['num_class'] = 6\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 50\nbst = xgb.train(param, xg_train, num_round, watchlist)\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\n# do the same thing again, but output probabilities\nparam['objective'] = 'multi:softprob'\nbst = xgb.train(param, xg_train, num_round, watchlist, \n                callbacks=[AimCallback(repo='.', experiment='xgboost_test')])\n# Note: this convention has been changed since xgboost-unity\n# get prediction, this is in 1D array, need reshape to (ndata, nclass)\npred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 6)\npred_label = np.argmax(pred_prob, axis=1)\nerror_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\nprint('Test error using softprob = {}'.format(error_rate))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs you can see on line 49, AimCallback is imported from \u003ccode\u003eaim.xgboost\u003c/code\u003e and passed to \u003ccode\u003exgb.train\u003c/code\u003e as one of the callbacks. Aim session can open and close by the AimCallback and the metrics and hparamsstore by XGBoost. In addition to that, thesystem measures pass to Aim as well.\u003c/p\u003e\n\u003ch2\u003eWhat it looks like?\u003c/h2\u003e\n\u003cp\u003eAfter you run the experiment and the \u003ccode\u003eaim up\u003c/code\u003e command in the \u003ccode\u003eaim_logs\u003c/code\u003edirectory, Aim UI will be running. When first opened, the dashboard page will come up.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/05/dashboard.png\" alt=\"\" title=\"Aim UI dashboard page\"\u003e\u003c/p\u003e\n\u003cp\u003eTo explore the run, we should:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoose the \u003ccode\u003exgboost_test\u003c/code\u003eexperiment.\u003c/li\u003e\n\u003cli\u003eSelect the metrics to explore.\u003c/li\u003e\n\u003cli\u003eDivide into charts by metrics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, the gif below illustrates the steps above.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/05/1.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\nSo this is what the final result looks like.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2021/05/2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn short, we can easily analyze the runs and the system usage.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/aimhubio/aim#democratizing-ai-dev-tools\"\u003eAim is on a mission to democratize AI dev tools.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you find Aim useful, support us and star \u003ca href=\"https://github.com/aimhubio/aim\"\u003ethe project\u003c/a\u003e on GitHub. Also, join the \u003ca href=\"https://aimstack.slack.com/ssb/redirect\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","_raw":{"sourceFilePath":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileName":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"an-end-to-end-example-of-aim-logger-used-with-xgboost-library"},"type":"Post"},{"title":"How to tune hyperparams with fixed seeds using PyTorch Lightning and Aim","date":"2021-03-11T12:46:00.000Z","author":"Gev Soghomonian","description":"What is a random seed and how is it important? The random seed is a number for initializing the pseudorandom number generator. It can have...","slug":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim","image":"https://aimstack.io/wp-content/uploads/2021/03/pytorch.png","draft":false,"categories":["Tutorials"],"body":{"raw":"## What is a random seed and how is it important?\n\nThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\n\n* Initial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\n* Dropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\n* Augmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\n\nAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance. One thing you do not need when tuning hyper-parameters is variance.\n\nThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\n\nLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using [Aim](https://github.com/aimhubio/aim).\n\n## **How to fix the seed in PyTorch Lightning**\n\n\n\nFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python`random.seed(seed)`, but it is not enough for PL.\n\nPytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use `pl.seed_everything(seed)` . See the docs [here](https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html).\n\n\u003e Note: in other libraries you would use something like: `np.random.seed()` or `torch.manual_seed()` \n\n## **Implementation**\n\n\n\nFind the full code for this and other tutorials [here](https://github.com/aimhubio/tutorials/tree/main/fixed-seed).\n\n```\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n```\n\n\n\n## Analyzing the Training Runs\n\nAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after [Aim 1.3.5 release](https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/)) and have multiple charts of different metrics on the same screen.\n\nDo the following steps to see the different effects of the optimizers\n\n* Go to dashboard, explore by experiment\n* Add loss to `SELECT` and divide into subplots by metric\n* Group by experiment to make all metrics of similar color\n* Group by style by optimizer to see different optimizers on loss and accuracy and its effects\n\nHere is how it looks on Aim:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/10.gif)\n\n![](https://aimstack.io/wp-content/uploads/2022/02/11.png)\n\n\n\nFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\n\nIf you apply the same settings to the learning rate, this is the result:\n\n## For the next step to analyze how learning rate affects the experiments, do the following steps:\n\n\n\n* Remove both previous groupings\n* Group by color by learning rate\n\n![](https://aimstack.io/wp-content/uploads/2022/02/12.gif)\n\n![](https://aimstack.io/wp-content/uploads/2022/02/13.png)\n\nAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\n\nWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\n\nOn top of this, if we also add grouping by style by optimizer:\n\n![](https://aimstack.io/wp-content/uploads/2022/02/24.gif)\n\n![](https://aimstack.io/wp-content/uploads/2022/02/25.png)\n\nNow, it is obvious that the the runs with SGD optimizer and `lr=0.01` (green, broken lines) are the best choices for all the seeds we have tried.\n\nFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\n\n## Learn More\n\n\n\nIf you find Aim useful, support us and [star the project](https://github.com/aimhubio/aim) on GitHub. Join the [Aim community](https://aimstack.slack.com/?redir=%2Fssb%2Fredirect) and share more about your use-cases and how we can improve Aim to suit them.","html":"\u003ch2\u003eWhat is a random seed and how is it important?\u003c/h2\u003e\n\u003cp\u003eThe random seed is a number for initializing the pseudorandom number generator. It can have a huge impact on the training results. There are different ways of using the pseudorandom number generator in ML. Here are a few examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitial weights of the model. When using not fully pre-trained models, one of the most common approaches is to generate the uninitialized weights randomly.\u003c/li\u003e\n\u003cli\u003eDropout: a common technique in ML that freezes randomly chosen parts of the model during training and recovers them during evaluation.\u003c/li\u003e\n\u003cli\u003eAugmentation: a well-known technique, especially for semi-supervised problems. When the training data is limited, transformations on the available data are used to synthesize new data. Mostly you can randomly choose the transformations and how there application (e.g. change the brightness and its level).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you can see, the random seed can have an influence on the result of training in several ways and add a huge variance. One thing you do not need when tuning hyper-parameters is variance.\u003c/p\u003e\n\u003cp\u003eThe purpose of experimenting with hyper-parameters is to find the combination that produces the best results, but when the random seed is not fixed, it is not clear whether the difference was made by the hyperparameter change or the seed change. Therefore, you need to think about a way to train with fixed seed and different hyper-parameters. the need to train with a fixed seed, but different hyper-parameters (comes up)?.\u003c/p\u003e\n\u003cp\u003eLater in this tutorial, I will show you how to effectively fix a seed for tuning hyper-parameters and how to monitor the results using \u003ca href=\"https://github.com/aimhubio/aim\"\u003eAim\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eHow to fix the seed in PyTorch Lightning\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFixing the seed for all imported modules is not as easy as it may seem. The way to fix the random seed for vanilla, non-framework code is to use standard Python\u003ccode\u003erandom.seed(seed)\u003c/code\u003e, but it is not enough for PL.\u003c/p\u003e\n\u003cp\u003ePytorch Lightning, like other frameworks, uses its own generated seeds. There are several ways to fix the seed manually. For PL, we use \u003ccode\u003epl.seed_everything(seed)\u003c/code\u003e . See the docs \u003ca href=\"https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.trainer.seed.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: in other libraries you would use something like: \u003ccode\u003enp.random.seed()\u003c/code\u003e or \u003ccode\u003etorch.manual_seed()\u003c/code\u003e \u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eFind the full code for this and other tutorials \u003ca href=\"https://github.com/aimhubio/tutorials/tree/main/fixed-seed\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom aim.pytorch_lightning import AimLogger\n\nclass ImageClassifierModel(pl.LightningModule):\n\n    def __init__(self, seed, lr, optimizer):\n        super(ImageClassifierModel, self).__init__()\n        pl.seed_everything(seed)\n        # This fixes a seed for all the modules used by pytorch-lightning\n        # Note: using random.seed(seed) is not enough, there are multiple\n        # other seeds like hash seed, seed for numpy etc.\n        self.lr = lr\n        self.optimizer = optimizer\n        self.total_classified = 0\n        self.correctly_classified = 0\n        self.model = resnet18(pretrained = True, progress = True)\n        self.model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n        # changing the last layer from 1000 out_features to 10 because the model\n        # is pretrained on ImageNet which has 1000 classes but CIFAR10 has 10\n        self.model.to('cuda') # moving the model to cuda\n\n    def train_dataloader(self):\n        # makes the training dataloader\n        train_ds = CIFAR10('.', train = True, transform = transforms.ToTensor(), download = True)\n        train_loader = DataLoader(train_ds, batch_size=32)\n        return train_loader\n        \n    def val_dataloader(self):\n        # makes the validation dataloader\n        val_ds = CIFAR10('.', train = False, transform = transforms.ToTensor(), download = True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        return val_loader\n\n    def forward(self, x):\n        return self.model.forward(x)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.log('train_loss', loss)\n        # logging the loss with \"train_\" prefix\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        # calculating the cross entropy loss on the result\n        self.total_classified += y.shape[0]\n        self.correctly_classified += (y_hat.argmax(1) == y).sum().item()\n        # Calculating total and correctly classified images to determine the accuracy later\n        self.log('val_loss', loss) \n        # logging the loss with \"val_\" prefix\n        return loss\n\n    def validation_epoch_end(self, results):\n        accuracy = self.correctly_classified / self.total_classified\n        self.log('val_accuracy', accuracy)\n        # logging accuracy\n        self.total_classified = 0\n        self.correctly_classified = 0\n        return accuracy\n\n    def configure_optimizers(self):\n        # Choose an optimizer and set up a learning rate according to hyperparameters\n        if self.optimizer == 'Adam':\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        elif self.optimizer == 'SGD':\n            return torch.optim.SGD(self.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError\n\nif __name__ == \"__main__\":\n\n    seeds = [47, 881, 123456789]\n    lrs = [0.1, 0.01]\n    optimizers = ['SGD', 'Adam']\n\n    for seed in seeds:\n        for optimizer in optimizers:\n            for lr in lrs:\n                # choosing one set of hyperparaameters from the ones above\n            \n                model = ImageClassifierModel(seed, lr, optimizer)\n                # initializing the model we will train with the chosen hyperparameters\n\n                aim_logger = AimLogger(\n                    experiment='resnet18_classification',\n                    train_metric_prefix='train_',\n                    val_metric_prefix='val_',\n                )\n                aim_logger.log_hyperparams({\n                    'lr': lr,\n                    'optimizer': optimizer,\n                    'seed': seed\n                })\n                # initializing the aim logger and logging the hyperparameters\n\n                trainer = pl.Trainer(\n                    logger=aim_logger,\n                    gpus=1,\n                    max_epochs=5,\n                    progress_bar_refresh_rate=1,\n                    log_every_n_steps=10,\n                    check_val_every_n_epoch=1)\n                # making the pytorch-lightning trainer\n\n                trainer.fit(model)\n                # training the model\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAnalyzing the Training Runs\u003c/h2\u003e\n\u003cp\u003eAfter each set of training runs you need to analyze the results/logs. Use Aim to group the runs by metrics/hyper-parameters (this can be done both on Explore and Dashboard after \u003ca href=\"https://aimstack.io/mlops-tools-aim-1-3-5-activity-view-and-x-axis-alignment/\"\u003eAim 1.3.5 release\u003c/a\u003e) and have multiple charts of different metrics on the same screen.\u003c/p\u003e\n\u003cp\u003eDo the following steps to see the different effects of the optimizers\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGo to dashboard, explore by experiment\u003c/li\u003e\n\u003cli\u003eAdd loss to \u003ccode\u003eSELECT\u003c/code\u003e and divide into subplots by metric\u003c/li\u003e\n\u003cli\u003eGroup by experiment to make all metrics of similar color\u003c/li\u003e\n\u003cli\u003eGroup by style by optimizer to see different optimizers on loss and accuracy and its effects\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere is how it looks on Aim:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/10.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/11.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the final result it is clear that the SGD (broken lines) optimizer has achieved higher accuracy and lower loss during the training.\u003c/p\u003e\n\u003cp\u003eIf you apply the same settings to the learning rate, this is the result:\u003c/p\u003e\n\u003ch2\u003eFor the next step to analyze how learning rate affects the experiments, do the following steps:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemove both previous groupings\u003c/li\u003e\n\u003cli\u003eGroup by color by learning rate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/12.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/13.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, the purple lines (lr = 0.01) represent significantly lower loss and higher accuracy.\u003c/p\u003e\n\u003cp\u003eWe showed that in this case, the choice of the optimizer and learning rate is not dependent on the random seed and it is safe to say that the SGD optimizer with a 0.01 learning rate is the best choice we can make.\u003c/p\u003e\n\u003cp\u003eOn top of this, if we also add grouping by style by optimizer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/24.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://aimstack.io/wp-content/uploads/2022/02/25.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, it is obvious that the the runs with SGD optimizer and \u003ccode\u003elr=0.01\u003c/code\u003e (green, broken lines) are the best choices for all the seeds we have tried.\u003c/p\u003e\n\u003cp\u003eFixing random seeds is a useful technique that can help step-up your hyper-parameter tuning. This is how to use Aim and PyTorch Lightning to tune hyper-parameters with a fixed seed.\u003c/p\u003e\n\u003ch2\u003eLearn More\u003c/h2\u003e\n\u003cp\u003eIf you find Aim useful, support us and \u003ca href=\"https://github.com/aimhubio/aim\"\u003estar the project\u003c/a\u003e on GitHub. Join the \u003ca href=\"https://aimstack.slack.com/?redir=%2Fssb%2Fredirect\"\u003eAim community\u003c/a\u003e and share more about your use-cases and how we can improve Aim to suit them.\u003c/p\u003e"},"_id":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","_raw":{"sourceFilePath":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileName":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim.md","sourceFileDir":".","contentType":"markdown","flattenedPath":"how-to-tune-hyperparams-with-fixed-seeds-using-pytorch-lightning-and-aim"},"type":"Post"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"aim-basics-using-context-and-subplots-to-compare-validation-and-test-metrics"},"buildId":"ASeMQklLZoYg-bry69KPG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>